{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e0f313",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from mindspore import context\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e6b487",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of mnist_ds: <class 'mindspore.dataset.engine.datasets_vision.MnistDataset'>\n",
      "Number of pictures contained in the mnist_ds： 60000\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\") # Windows version, set to use CPU for graph calculation\n",
    "train_data_path = \"./MNIST_Data/train\"\n",
    "test_data_path = \"./MNIST_Data/test\"\n",
    "mnist_ds = ds.MnistDataset(train_data_path) # Load training dataset\n",
    "print('The type of mnist_ds:', type(mnist_ds))\n",
    "print(\"Number of pictures contained in the mnist_ds：\",mnist_ds.get_dataset_size()) # 60000 pictures in total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c526721d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The item of mnist_ds: dict_keys(['image', 'label'])\n",
      "Tensor of image in item: (28, 28, 1)\n",
      "The label of item: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic_ds = mnist_ds.create_dict_iterator() # Convert dataset to dictionary type\n",
    "item = dic_ds.__next__()\n",
    "img = item[\"image\"].asnumpy()\n",
    "label = item[\"label\"].asnumpy()\n",
    "\n",
    "print(\"The item of mnist_ds:\", item.keys()) # Take a single data to view the data structure, including two keys, image and label\n",
    "print(\"Tensor of image in item:\", img.shape) # View the tensor of image (28,28,1)\n",
    "print(\"The label of item:\", label)\n",
    "\n",
    "plt.imshow(np.squeeze(img))\n",
    "plt.title(\"number:%s\"% item[\"label\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead789b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing module\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.common import dtype as mstype\n",
    "\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" create dataset for train or test\n",
    "    Args:\n",
    "        data_path: Data path\n",
    "        batch_size: The number of data records in each group\n",
    "        repeat_size: The number of replicated data records\n",
    "        num_parallel_workers: The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # Define some parameters needed for data enhancement and rough justification\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # According to the parameters, generate the corresponding data enhancement method\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)  # Resize images to (32, 32) by bilinear interpolation\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml) # normalize images\n",
    "    rescale_op = CV.Rescale(rescale, shift) # rescale images\n",
    "    hwc2chw_op = CV.HWC2CHW() # change shape from (height, width, channel) to (channel, height, width) to fit network.\n",
    "    type_cast_op = C.TypeCast(mstype.int32) # change data type of label to int32 to fit network\n",
    "\n",
    "    # Using map () to apply operations to a dataset\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"label\", operations=type_cast_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=resize_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_nml_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=hwc2chw_op, num_parallel_workers=num_parallel_workers)\n",
    "    # Process the generated dataset\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3538b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:35.643.146 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:35.644.143 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:35.645.139 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:35.646.135 [mindspore\\dataset\\core\\validator_helpers.py:806] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:35.646.135 [mindspore\\dataset\\core\\validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups in the dataset: 1875\n"
     ]
    }
   ],
   "source": [
    "datas = create_dataset(train_data_path) # Process the train dataset\n",
    "print('Number of groups in the dataset:', datas.get_dataset_size()) # Number of query dataset groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datas.create_dict_iterator().__next__() # Take a set of datasets\n",
    "print(data.keys())\n",
    "images = data[\"image\"].asnumpy() # Take out the image data in this dataset\n",
    "labels = data[\"label\"].asnumpy() # Take out the label (subscript) of this data set\n",
    "print('Tensor of image:', images.shape) # Query the tensor of images in each dataset (32,1,32,32)\n",
    "print('labels:', labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for i in images:\n",
    "    plt.subplot(4, 8, count) \n",
    "    plt.imshow(np.squeeze(i))\n",
    "    plt.title('num:%s'%labels[count-1])\n",
    "    plt.xticks([])\n",
    "    count += 1\n",
    "    plt.axis(\"off\")\n",
    "plt.show() # Print a total of 32 pictures in the group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e49fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "# Initialize 2D convolution function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"Conv layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     weight_init=weight, has_bias=False, pad_mode=\"valid\")\n",
    "\n",
    "# Initialize full connection layer\n",
    "def fc_with_initialize(input_channels, out_channels):\n",
    "    \"\"\"Fc layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    bias = weight_variable()\n",
    "    return nn.Dense(input_channels, out_channels, weight, bias)\n",
    "\n",
    "# Set truncated normal distribution\n",
    "def weight_variable():\n",
    "    \"\"\"Weight initial.\"\"\"\n",
    "    return TruncatedNormal(0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "    # define the operator required\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.batch_size = 32 # 32 pictures in each group\n",
    "        self.conv1 = conv(1, 6, 5) # Convolution layer 1, 1 channel input (1 Figure), 6 channel output (6 figures), convolution core 5 * 5\n",
    "        self.conv2 = conv(6, 16, 5) # Convolution layer 2,6-channel input, 16 channel output, convolution kernel 5 * 5\n",
    "        self.fc1 = fc_with_initialize(16 * 5 * 5, 120)\n",
    "        self.fc2 = fc_with_initialize(120, 84)\n",
    "        self.fc3 = fc_with_initialize(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x) # 1*32*32-->6*28*28\n",
    "        x = self.relu(x) # 6*28*28-->6*14*14\n",
    "        x = self.max_pool2d(x) # Pool layer\n",
    "        x = self.conv2(x) # Convolution layer\n",
    "        x = self.relu(x) # Function excitation layer\n",
    "        x = self.max_pool2d(x) # Pool layer\n",
    "        x = self.flatten(x) # Dimensionality reduction\n",
    "        x = self.fc1(x) # Full connection\n",
    "        x = self.relu(x) # Function excitation layer\n",
    "        x = self.fc2(x) # Full connection\n",
    "        x = self.relu(x) # Function excitation layer\n",
    "        x = self.fc3(x) # Full connection\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d91120",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = LeNet5()\n",
    "print(network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34958812",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = network.trainable_params()\n",
    "param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing related modules\n",
    "import argparse\n",
    "from mindspore import Tensor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor,Callback\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "\n",
    "def train_net(model, epoch_size, mnist_path, repeat_size, ckpoint_cb, step_loss_info):\n",
    "    \"\"\"Define the training method.\"\"\"\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    # load training dataset\n",
    "    ds_train = create_dataset(os.path.join(mnist_path, \"train\"), 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(), step_loss_info], dataset_sink_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ae64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback function\n",
    "class Step_loss_info(Callback):\n",
    "    def step_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        # step_ Loss dictionary for saving loss value and step number information\n",
    "        step_loss[\"loss_value\"].append(str(cb_params.net_outputs))\n",
    "        step_loss[\"step\"].append(str(cb_params.cur_step_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce76d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    os.system('del/f/s/q *.ckpt *.meta')# Clean up old run files before in Windows\n",
    "else:\n",
    "    os.system('rm -f *.ckpt *.meta *.pb')# Clean up old run files before in Linux\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "momentum = 0.9 #\n",
    "\n",
    "# create the network\n",
    "network = LeNet5()\n",
    "\n",
    "# define the optimizer\n",
    "net_opt = nn.Momentum(network.trainable_params(), lr, momentum)\n",
    "\n",
    "\n",
    "# define the loss function\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6412395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:40.213.555 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:40.214.556 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:40.215.552 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:40.216.187 [mindspore\\dataset\\core\\validator_helpers.py:806] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:40.216.187 [mindspore\\dataset\\core\\validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:15:40.218.053 [mindspore\\train\\model.py:1079] For Step_loss_info callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.3003487586975098\n",
      "epoch: 1 step: 2, loss is 2.290452480316162\n",
      "epoch: 1 step: 3, loss is 2.302185297012329\n",
      "epoch: 1 step: 4, loss is 2.3046181201934814\n",
      "epoch: 1 step: 5, loss is 2.3020522594451904\n",
      "epoch: 1 step: 6, loss is 2.2919387817382812\n",
      "epoch: 1 step: 7, loss is 2.3050315380096436\n",
      "epoch: 1 step: 8, loss is 2.3036491870880127\n",
      "epoch: 1 step: 9, loss is 2.307202100753784\n",
      "epoch: 1 step: 10, loss is 2.3037939071655273\n",
      "epoch: 1 step: 11, loss is 2.30085825920105\n",
      "epoch: 1 step: 12, loss is 2.3018219470977783\n",
      "epoch: 1 step: 13, loss is 2.3042964935302734\n",
      "epoch: 1 step: 14, loss is 2.3053927421569824\n",
      "epoch: 1 step: 15, loss is 2.3023974895477295\n",
      "epoch: 1 step: 16, loss is 2.313068389892578\n",
      "epoch: 1 step: 17, loss is 2.309704065322876\n",
      "epoch: 1 step: 18, loss is 2.3036530017852783\n",
      "epoch: 1 step: 19, loss is 2.3093321323394775\n",
      "epoch: 1 step: 20, loss is 2.3059241771698\n",
      "epoch: 1 step: 21, loss is 2.3092241287231445\n",
      "epoch: 1 step: 22, loss is 2.3039469718933105\n",
      "epoch: 1 step: 23, loss is 2.2951478958129883\n",
      "epoch: 1 step: 24, loss is 2.2970809936523438\n",
      "epoch: 1 step: 25, loss is 2.3072290420532227\n",
      "epoch: 1 step: 26, loss is 2.3032116889953613\n",
      "epoch: 1 step: 27, loss is 2.3012890815734863\n",
      "epoch: 1 step: 28, loss is 2.2919955253601074\n",
      "epoch: 1 step: 29, loss is 2.3071367740631104\n",
      "epoch: 1 step: 30, loss is 2.3043065071105957\n",
      "epoch: 1 step: 31, loss is 2.2911019325256348\n",
      "epoch: 1 step: 32, loss is 2.305990695953369\n",
      "epoch: 1 step: 33, loss is 2.296030044555664\n",
      "epoch: 1 step: 34, loss is 2.2965846061706543\n",
      "epoch: 1 step: 35, loss is 2.3055500984191895\n",
      "epoch: 1 step: 36, loss is 2.302849531173706\n",
      "epoch: 1 step: 37, loss is 2.31149959564209\n",
      "epoch: 1 step: 38, loss is 2.3063085079193115\n",
      "epoch: 1 step: 39, loss is 2.2905988693237305\n",
      "epoch: 1 step: 40, loss is 2.302983045578003\n",
      "epoch: 1 step: 41, loss is 2.305128574371338\n",
      "epoch: 1 step: 42, loss is 2.312018394470215\n",
      "epoch: 1 step: 43, loss is 2.2909507751464844\n",
      "epoch: 1 step: 44, loss is 2.307297468185425\n",
      "epoch: 1 step: 45, loss is 2.3008317947387695\n",
      "epoch: 1 step: 46, loss is 2.2836596965789795\n",
      "epoch: 1 step: 47, loss is 2.3051164150238037\n",
      "epoch: 1 step: 48, loss is 2.290635585784912\n",
      "epoch: 1 step: 49, loss is 2.3076329231262207\n",
      "epoch: 1 step: 50, loss is 2.2986032962799072\n",
      "epoch: 1 step: 51, loss is 2.3051600456237793\n",
      "epoch: 1 step: 52, loss is 2.3045825958251953\n",
      "epoch: 1 step: 53, loss is 2.292243719100952\n",
      "epoch: 1 step: 54, loss is 2.302891492843628\n",
      "epoch: 1 step: 55, loss is 2.2906088829040527\n",
      "epoch: 1 step: 56, loss is 2.316387414932251\n",
      "epoch: 1 step: 57, loss is 2.3003249168395996\n",
      "epoch: 1 step: 58, loss is 2.3197052478790283\n",
      "epoch: 1 step: 59, loss is 2.2916393280029297\n",
      "epoch: 1 step: 60, loss is 2.295755386352539\n",
      "epoch: 1 step: 61, loss is 2.299368143081665\n",
      "epoch: 1 step: 62, loss is 2.3131117820739746\n",
      "epoch: 1 step: 63, loss is 2.3040473461151123\n",
      "epoch: 1 step: 64, loss is 2.3055920600891113\n",
      "epoch: 1 step: 65, loss is 2.290540933609009\n",
      "epoch: 1 step: 66, loss is 2.303572654724121\n",
      "epoch: 1 step: 67, loss is 2.295708179473877\n",
      "epoch: 1 step: 68, loss is 2.2950987815856934\n",
      "epoch: 1 step: 69, loss is 2.3026435375213623\n",
      "epoch: 1 step: 70, loss is 2.292285919189453\n",
      "epoch: 1 step: 71, loss is 2.2997663021087646\n",
      "epoch: 1 step: 72, loss is 2.32033109664917\n",
      "epoch: 1 step: 73, loss is 2.3100404739379883\n",
      "epoch: 1 step: 74, loss is 2.3037261962890625\n",
      "epoch: 1 step: 75, loss is 2.2967441082000732\n",
      "epoch: 1 step: 76, loss is 2.3129055500030518\n",
      "epoch: 1 step: 77, loss is 2.293022871017456\n",
      "epoch: 1 step: 78, loss is 2.3075101375579834\n",
      "epoch: 1 step: 79, loss is 2.29658579826355\n",
      "epoch: 1 step: 80, loss is 2.313060760498047\n",
      "epoch: 1 step: 81, loss is 2.3048079013824463\n",
      "epoch: 1 step: 82, loss is 2.307875871658325\n",
      "epoch: 1 step: 83, loss is 2.307875394821167\n",
      "epoch: 1 step: 84, loss is 2.2970736026763916\n",
      "epoch: 1 step: 85, loss is 2.3049063682556152\n",
      "epoch: 1 step: 86, loss is 2.3106908798217773\n",
      "epoch: 1 step: 87, loss is 2.310429096221924\n",
      "epoch: 1 step: 88, loss is 2.282914400100708\n",
      "epoch: 1 step: 89, loss is 2.300307512283325\n",
      "epoch: 1 step: 90, loss is 2.301633834838867\n",
      "epoch: 1 step: 91, loss is 2.2927398681640625\n",
      "epoch: 1 step: 92, loss is 2.3123090267181396\n",
      "epoch: 1 step: 93, loss is 2.29634690284729\n",
      "epoch: 1 step: 94, loss is 2.2982168197631836\n",
      "epoch: 1 step: 95, loss is 2.308558464050293\n",
      "epoch: 1 step: 96, loss is 2.3089802265167236\n",
      "epoch: 1 step: 97, loss is 2.3086142539978027\n",
      "epoch: 1 step: 98, loss is 2.3101038932800293\n",
      "epoch: 1 step: 99, loss is 2.288239002227783\n",
      "epoch: 1 step: 100, loss is 2.3008689880371094\n",
      "epoch: 1 step: 101, loss is 2.295353412628174\n",
      "epoch: 1 step: 102, loss is 2.3010435104370117\n",
      "epoch: 1 step: 103, loss is 2.28411865234375\n",
      "epoch: 1 step: 104, loss is 2.3039865493774414\n",
      "epoch: 1 step: 105, loss is 2.318122148513794\n",
      "epoch: 1 step: 106, loss is 2.297539472579956\n",
      "epoch: 1 step: 107, loss is 2.2942893505096436\n",
      "epoch: 1 step: 108, loss is 2.3057749271392822\n",
      "epoch: 1 step: 109, loss is 2.2792229652404785\n",
      "epoch: 1 step: 110, loss is 2.293452739715576\n",
      "epoch: 1 step: 111, loss is 2.292726993560791\n",
      "epoch: 1 step: 112, loss is 2.2964324951171875\n",
      "epoch: 1 step: 113, loss is 2.284447431564331\n",
      "epoch: 1 step: 114, loss is 2.2852072715759277\n",
      "epoch: 1 step: 115, loss is 2.3036012649536133\n",
      "epoch: 1 step: 116, loss is 2.3057022094726562\n",
      "epoch: 1 step: 117, loss is 2.3182106018066406\n",
      "epoch: 1 step: 118, loss is 2.304365396499634\n",
      "epoch: 1 step: 119, loss is 2.3039026260375977\n",
      "epoch: 1 step: 120, loss is 2.299933671951294\n",
      "epoch: 1 step: 121, loss is 2.3013088703155518\n",
      "epoch: 1 step: 122, loss is 2.278352975845337\n",
      "epoch: 1 step: 123, loss is 2.3370742797851562\n",
      "epoch: 1 step: 124, loss is 2.277818441390991\n",
      "epoch: 1 step: 125, loss is 2.3062081336975098\n",
      "epoch: 1 step: 126, loss is 2.318693161010742\n",
      "epoch: 1 step: 127, loss is 2.2911031246185303\n",
      "epoch: 1 step: 128, loss is 2.2872796058654785\n",
      "epoch: 1 step: 129, loss is 2.284775972366333\n",
      "epoch: 1 step: 130, loss is 2.3239715099334717\n",
      "epoch: 1 step: 131, loss is 2.309497117996216\n",
      "epoch: 1 step: 132, loss is 2.314582109451294\n",
      "epoch: 1 step: 133, loss is 2.3095624446868896\n",
      "epoch: 1 step: 134, loss is 2.2913103103637695\n",
      "epoch: 1 step: 135, loss is 2.3168768882751465\n",
      "epoch: 1 step: 136, loss is 2.2885565757751465\n",
      "epoch: 1 step: 137, loss is 2.300931453704834\n",
      "epoch: 1 step: 138, loss is 2.316693067550659\n",
      "epoch: 1 step: 139, loss is 2.2948975563049316\n",
      "epoch: 1 step: 140, loss is 2.302572011947632\n",
      "epoch: 1 step: 141, loss is 2.287898540496826\n",
      "epoch: 1 step: 142, loss is 2.3109569549560547\n",
      "epoch: 1 step: 143, loss is 2.292325735092163\n",
      "epoch: 1 step: 144, loss is 2.3233323097229004\n",
      "epoch: 1 step: 145, loss is 2.2857377529144287\n",
      "epoch: 1 step: 146, loss is 2.3019917011260986\n",
      "epoch: 1 step: 147, loss is 2.2950289249420166\n",
      "epoch: 1 step: 148, loss is 2.3053112030029297\n",
      "epoch: 1 step: 149, loss is 2.2913310527801514\n",
      "epoch: 1 step: 150, loss is 2.3108105659484863\n",
      "epoch: 1 step: 151, loss is 2.303745746612549\n",
      "epoch: 1 step: 152, loss is 2.3122923374176025\n",
      "epoch: 1 step: 153, loss is 2.3050966262817383\n",
      "epoch: 1 step: 154, loss is 2.2985024452209473\n",
      "epoch: 1 step: 155, loss is 2.2888214588165283\n",
      "epoch: 1 step: 156, loss is 2.303089141845703\n",
      "epoch: 1 step: 157, loss is 2.298619508743286\n",
      "epoch: 1 step: 158, loss is 2.2862253189086914\n",
      "epoch: 1 step: 159, loss is 2.307588577270508\n",
      "epoch: 1 step: 160, loss is 2.2981576919555664\n",
      "epoch: 1 step: 161, loss is 2.2960920333862305\n",
      "epoch: 1 step: 162, loss is 2.3297104835510254\n",
      "epoch: 1 step: 163, loss is 2.295933723449707\n",
      "epoch: 1 step: 164, loss is 2.3121883869171143\n",
      "epoch: 1 step: 165, loss is 2.319735527038574\n",
      "epoch: 1 step: 166, loss is 2.302610158920288\n",
      "epoch: 1 step: 167, loss is 2.289564847946167\n",
      "epoch: 1 step: 168, loss is 2.3107194900512695\n",
      "epoch: 1 step: 169, loss is 2.295224905014038\n",
      "epoch: 1 step: 170, loss is 2.288381576538086\n",
      "epoch: 1 step: 171, loss is 2.2934577465057373\n",
      "epoch: 1 step: 172, loss is 2.2921547889709473\n",
      "epoch: 1 step: 173, loss is 2.311793088912964\n",
      "epoch: 1 step: 174, loss is 2.306469440460205\n",
      "epoch: 1 step: 175, loss is 2.30165433883667\n",
      "epoch: 1 step: 176, loss is 2.300687789916992\n",
      "epoch: 1 step: 177, loss is 2.2900545597076416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 178, loss is 2.293227434158325\n",
      "epoch: 1 step: 179, loss is 2.2831759452819824\n",
      "epoch: 1 step: 180, loss is 2.309358596801758\n",
      "epoch: 1 step: 181, loss is 2.3009684085845947\n",
      "epoch: 1 step: 182, loss is 2.2895491123199463\n",
      "epoch: 1 step: 183, loss is 2.2946550846099854\n",
      "epoch: 1 step: 184, loss is 2.302689790725708\n",
      "epoch: 1 step: 185, loss is 2.280441999435425\n",
      "epoch: 1 step: 186, loss is 2.311687707901001\n",
      "epoch: 1 step: 187, loss is 2.31898832321167\n",
      "epoch: 1 step: 188, loss is 2.3073785305023193\n",
      "epoch: 1 step: 189, loss is 2.318356513977051\n",
      "epoch: 1 step: 190, loss is 2.3153786659240723\n",
      "epoch: 1 step: 191, loss is 2.3047194480895996\n",
      "epoch: 1 step: 192, loss is 2.3186240196228027\n",
      "epoch: 1 step: 193, loss is 2.310842990875244\n",
      "epoch: 1 step: 194, loss is 2.3054590225219727\n",
      "epoch: 1 step: 195, loss is 2.299099922180176\n",
      "epoch: 1 step: 196, loss is 2.3205535411834717\n",
      "epoch: 1 step: 197, loss is 2.2927966117858887\n",
      "epoch: 1 step: 198, loss is 2.320255756378174\n",
      "epoch: 1 step: 199, loss is 2.285374641418457\n",
      "epoch: 1 step: 200, loss is 2.3110949993133545\n",
      "epoch: 1 step: 201, loss is 2.281163454055786\n",
      "epoch: 1 step: 202, loss is 2.292651414871216\n",
      "epoch: 1 step: 203, loss is 2.326235055923462\n",
      "epoch: 1 step: 204, loss is 2.3068604469299316\n",
      "epoch: 1 step: 205, loss is 2.2854084968566895\n",
      "epoch: 1 step: 206, loss is 2.2864041328430176\n",
      "epoch: 1 step: 207, loss is 2.282595634460449\n",
      "epoch: 1 step: 208, loss is 2.2950997352600098\n",
      "epoch: 1 step: 209, loss is 2.308352470397949\n",
      "epoch: 1 step: 210, loss is 2.3096415996551514\n",
      "epoch: 1 step: 211, loss is 2.302006721496582\n",
      "epoch: 1 step: 212, loss is 2.2923378944396973\n",
      "epoch: 1 step: 213, loss is 2.29982328414917\n",
      "epoch: 1 step: 214, loss is 2.3050143718719482\n",
      "epoch: 1 step: 215, loss is 2.281965732574463\n",
      "epoch: 1 step: 216, loss is 2.283963918685913\n",
      "epoch: 1 step: 217, loss is 2.2698867321014404\n",
      "epoch: 1 step: 218, loss is 2.3245325088500977\n",
      "epoch: 1 step: 219, loss is 2.3115382194519043\n",
      "epoch: 1 step: 220, loss is 2.3007373809814453\n",
      "epoch: 1 step: 221, loss is 2.286029815673828\n",
      "epoch: 1 step: 222, loss is 2.3163726329803467\n",
      "epoch: 1 step: 223, loss is 2.306718349456787\n",
      "epoch: 1 step: 224, loss is 2.309873580932617\n",
      "epoch: 1 step: 225, loss is 2.291879177093506\n",
      "epoch: 1 step: 226, loss is 2.2859435081481934\n",
      "epoch: 1 step: 227, loss is 2.3168091773986816\n",
      "epoch: 1 step: 228, loss is 2.332779884338379\n",
      "epoch: 1 step: 229, loss is 2.33866548538208\n",
      "epoch: 1 step: 230, loss is 2.29034161567688\n",
      "epoch: 1 step: 231, loss is 2.310933828353882\n",
      "epoch: 1 step: 232, loss is 2.3064916133880615\n",
      "epoch: 1 step: 233, loss is 2.3210103511810303\n",
      "epoch: 1 step: 234, loss is 2.282058000564575\n",
      "epoch: 1 step: 235, loss is 2.2974777221679688\n",
      "epoch: 1 step: 236, loss is 2.291137933731079\n",
      "epoch: 1 step: 237, loss is 2.2994189262390137\n",
      "epoch: 1 step: 238, loss is 2.283940553665161\n",
      "epoch: 1 step: 239, loss is 2.3191003799438477\n",
      "epoch: 1 step: 240, loss is 2.278510570526123\n",
      "epoch: 1 step: 241, loss is 2.3088974952697754\n",
      "epoch: 1 step: 242, loss is 2.297926902770996\n",
      "epoch: 1 step: 243, loss is 2.2945499420166016\n",
      "epoch: 1 step: 244, loss is 2.302626132965088\n",
      "epoch: 1 step: 245, loss is 2.3023362159729004\n",
      "epoch: 1 step: 246, loss is 2.2977683544158936\n",
      "epoch: 1 step: 247, loss is 2.3200461864471436\n",
      "epoch: 1 step: 248, loss is 2.3052141666412354\n",
      "epoch: 1 step: 249, loss is 2.310141086578369\n",
      "epoch: 1 step: 250, loss is 2.2937827110290527\n",
      "epoch: 1 step: 251, loss is 2.3224384784698486\n",
      "epoch: 1 step: 252, loss is 2.307332754135132\n",
      "epoch: 1 step: 253, loss is 2.3229868412017822\n",
      "epoch: 1 step: 254, loss is 2.2964224815368652\n",
      "epoch: 1 step: 255, loss is 2.294254779815674\n",
      "epoch: 1 step: 256, loss is 2.307922124862671\n",
      "epoch: 1 step: 257, loss is 2.3069369792938232\n",
      "epoch: 1 step: 258, loss is 2.285529851913452\n",
      "epoch: 1 step: 259, loss is 2.286756992340088\n",
      "epoch: 1 step: 260, loss is 2.298351287841797\n",
      "epoch: 1 step: 261, loss is 2.3053667545318604\n",
      "epoch: 1 step: 262, loss is 2.3161232471466064\n",
      "epoch: 1 step: 263, loss is 2.283534049987793\n",
      "epoch: 1 step: 264, loss is 2.293639659881592\n",
      "epoch: 1 step: 265, loss is 2.3044121265411377\n",
      "epoch: 1 step: 266, loss is 2.2774271965026855\n",
      "epoch: 1 step: 267, loss is 2.3267831802368164\n",
      "epoch: 1 step: 268, loss is 2.3179056644439697\n",
      "epoch: 1 step: 269, loss is 2.3231329917907715\n",
      "epoch: 1 step: 270, loss is 2.282529354095459\n",
      "epoch: 1 step: 271, loss is 2.328932523727417\n",
      "epoch: 1 step: 272, loss is 2.2885987758636475\n",
      "epoch: 1 step: 273, loss is 2.3016107082366943\n",
      "epoch: 1 step: 274, loss is 2.301913261413574\n",
      "epoch: 1 step: 275, loss is 2.2949180603027344\n",
      "epoch: 1 step: 276, loss is 2.2979183197021484\n",
      "epoch: 1 step: 277, loss is 2.3153653144836426\n",
      "epoch: 1 step: 278, loss is 2.301509141921997\n",
      "epoch: 1 step: 279, loss is 2.3084750175476074\n",
      "epoch: 1 step: 280, loss is 2.298020124435425\n",
      "epoch: 1 step: 281, loss is 2.3072469234466553\n",
      "epoch: 1 step: 282, loss is 2.306687355041504\n",
      "epoch: 1 step: 283, loss is 2.313859701156616\n",
      "epoch: 1 step: 284, loss is 2.2943382263183594\n",
      "epoch: 1 step: 285, loss is 2.301518678665161\n",
      "epoch: 1 step: 286, loss is 2.3205997943878174\n",
      "epoch: 1 step: 287, loss is 2.2967772483825684\n",
      "epoch: 1 step: 288, loss is 2.27877140045166\n",
      "epoch: 1 step: 289, loss is 2.309241771697998\n",
      "epoch: 1 step: 290, loss is 2.3088021278381348\n",
      "epoch: 1 step: 291, loss is 2.3068175315856934\n",
      "epoch: 1 step: 292, loss is 2.3024661540985107\n",
      "epoch: 1 step: 293, loss is 2.302726984024048\n",
      "epoch: 1 step: 294, loss is 2.3010458946228027\n",
      "epoch: 1 step: 295, loss is 2.3171844482421875\n",
      "epoch: 1 step: 296, loss is 2.313061475753784\n",
      "epoch: 1 step: 297, loss is 2.2985386848449707\n",
      "epoch: 1 step: 298, loss is 2.2955000400543213\n",
      "epoch: 1 step: 299, loss is 2.3086931705474854\n",
      "epoch: 1 step: 300, loss is 2.305388927459717\n",
      "epoch: 1 step: 301, loss is 2.2970328330993652\n",
      "epoch: 1 step: 302, loss is 2.3000802993774414\n",
      "epoch: 1 step: 303, loss is 2.309004545211792\n",
      "epoch: 1 step: 304, loss is 2.306145668029785\n",
      "epoch: 1 step: 305, loss is 2.3063104152679443\n",
      "epoch: 1 step: 306, loss is 2.3093857765197754\n",
      "epoch: 1 step: 307, loss is 2.304276704788208\n",
      "epoch: 1 step: 308, loss is 2.2935798168182373\n",
      "epoch: 1 step: 309, loss is 2.3057682514190674\n",
      "epoch: 1 step: 310, loss is 2.294782876968384\n",
      "epoch: 1 step: 311, loss is 2.305826187133789\n",
      "epoch: 1 step: 312, loss is 2.3153300285339355\n",
      "epoch: 1 step: 313, loss is 2.3115339279174805\n",
      "epoch: 1 step: 314, loss is 2.298906087875366\n",
      "epoch: 1 step: 315, loss is 2.299984931945801\n",
      "epoch: 1 step: 316, loss is 2.317929983139038\n",
      "epoch: 1 step: 317, loss is 2.2873005867004395\n",
      "epoch: 1 step: 318, loss is 2.290459632873535\n",
      "epoch: 1 step: 319, loss is 2.306175947189331\n",
      "epoch: 1 step: 320, loss is 2.30488657951355\n",
      "epoch: 1 step: 321, loss is 2.311429738998413\n",
      "epoch: 1 step: 322, loss is 2.303157091140747\n",
      "epoch: 1 step: 323, loss is 2.296686887741089\n",
      "epoch: 1 step: 324, loss is 2.307950496673584\n",
      "epoch: 1 step: 325, loss is 2.289102554321289\n",
      "epoch: 1 step: 326, loss is 2.3113019466400146\n",
      "epoch: 1 step: 327, loss is 2.3044650554656982\n",
      "epoch: 1 step: 328, loss is 2.295093059539795\n",
      "epoch: 1 step: 329, loss is 2.3039605617523193\n",
      "epoch: 1 step: 330, loss is 2.3065171241760254\n",
      "epoch: 1 step: 331, loss is 2.305032730102539\n",
      "epoch: 1 step: 332, loss is 2.2918031215667725\n",
      "epoch: 1 step: 333, loss is 2.2965240478515625\n",
      "epoch: 1 step: 334, loss is 2.3034114837646484\n",
      "epoch: 1 step: 335, loss is 2.306964874267578\n",
      "epoch: 1 step: 336, loss is 2.3166656494140625\n",
      "epoch: 1 step: 337, loss is 2.3203115463256836\n",
      "epoch: 1 step: 338, loss is 2.300658702850342\n",
      "epoch: 1 step: 339, loss is 2.3091256618499756\n",
      "epoch: 1 step: 340, loss is 2.3045334815979004\n",
      "epoch: 1 step: 341, loss is 2.2970380783081055\n",
      "epoch: 1 step: 342, loss is 2.2974693775177\n",
      "epoch: 1 step: 343, loss is 2.2981624603271484\n",
      "epoch: 1 step: 344, loss is 2.2946958541870117\n",
      "epoch: 1 step: 345, loss is 2.3017055988311768\n",
      "epoch: 1 step: 346, loss is 2.2969043254852295\n",
      "epoch: 1 step: 347, loss is 2.313783884048462\n",
      "epoch: 1 step: 348, loss is 2.3019931316375732\n",
      "epoch: 1 step: 349, loss is 2.3008553981781006\n",
      "epoch: 1 step: 350, loss is 2.2973556518554688\n",
      "epoch: 1 step: 351, loss is 2.3080077171325684\n",
      "epoch: 1 step: 352, loss is 2.3148608207702637\n",
      "epoch: 1 step: 353, loss is 2.3095602989196777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 354, loss is 2.3016042709350586\n",
      "epoch: 1 step: 355, loss is 2.3022098541259766\n",
      "epoch: 1 step: 356, loss is 2.298671245574951\n",
      "epoch: 1 step: 357, loss is 2.2975761890411377\n",
      "epoch: 1 step: 358, loss is 2.2934134006500244\n",
      "epoch: 1 step: 359, loss is 2.3183341026306152\n",
      "epoch: 1 step: 360, loss is 2.2967898845672607\n",
      "epoch: 1 step: 361, loss is 2.287968397140503\n",
      "epoch: 1 step: 362, loss is 2.2985105514526367\n",
      "epoch: 1 step: 363, loss is 2.300441265106201\n",
      "epoch: 1 step: 364, loss is 2.2997593879699707\n",
      "epoch: 1 step: 365, loss is 2.309478998184204\n",
      "epoch: 1 step: 366, loss is 2.3163039684295654\n",
      "epoch: 1 step: 367, loss is 2.2997424602508545\n",
      "epoch: 1 step: 368, loss is 2.2921109199523926\n",
      "epoch: 1 step: 369, loss is 2.29447078704834\n",
      "epoch: 1 step: 370, loss is 2.291069746017456\n",
      "epoch: 1 step: 371, loss is 2.2904562950134277\n",
      "epoch: 1 step: 372, loss is 2.2894818782806396\n",
      "epoch: 1 step: 373, loss is 2.3039493560791016\n",
      "epoch: 1 step: 374, loss is 2.3058745861053467\n",
      "epoch: 1 step: 375, loss is 2.299542188644409\n",
      "epoch: 1 step: 376, loss is 2.3060572147369385\n",
      "epoch: 1 step: 377, loss is 2.309251308441162\n",
      "epoch: 1 step: 378, loss is 2.308323383331299\n",
      "epoch: 1 step: 379, loss is 2.3018648624420166\n",
      "epoch: 1 step: 380, loss is 2.300485134124756\n",
      "epoch: 1 step: 381, loss is 2.2989003658294678\n",
      "epoch: 1 step: 382, loss is 2.302201509475708\n",
      "epoch: 1 step: 383, loss is 2.315772294998169\n",
      "epoch: 1 step: 384, loss is 2.306884288787842\n",
      "epoch: 1 step: 385, loss is 2.3017385005950928\n",
      "epoch: 1 step: 386, loss is 2.3027806282043457\n",
      "epoch: 1 step: 387, loss is 2.301548480987549\n",
      "epoch: 1 step: 388, loss is 2.2981715202331543\n",
      "epoch: 1 step: 389, loss is 2.295107364654541\n",
      "epoch: 1 step: 390, loss is 2.3072688579559326\n",
      "epoch: 1 step: 391, loss is 2.2873878479003906\n",
      "epoch: 1 step: 392, loss is 2.302495002746582\n",
      "epoch: 1 step: 393, loss is 2.2975053787231445\n",
      "epoch: 1 step: 394, loss is 2.3102738857269287\n",
      "epoch: 1 step: 395, loss is 2.2921273708343506\n",
      "epoch: 1 step: 396, loss is 2.3038394451141357\n",
      "epoch: 1 step: 397, loss is 2.300206184387207\n",
      "epoch: 1 step: 398, loss is 2.293016195297241\n",
      "epoch: 1 step: 399, loss is 2.3104755878448486\n",
      "epoch: 1 step: 400, loss is 2.3010990619659424\n",
      "epoch: 1 step: 401, loss is 2.303328275680542\n",
      "epoch: 1 step: 402, loss is 2.3154759407043457\n",
      "epoch: 1 step: 403, loss is 2.3043479919433594\n",
      "epoch: 1 step: 404, loss is 2.2997710704803467\n",
      "epoch: 1 step: 405, loss is 2.2911291122436523\n",
      "epoch: 1 step: 406, loss is 2.3095309734344482\n",
      "epoch: 1 step: 407, loss is 2.301076889038086\n",
      "epoch: 1 step: 408, loss is 2.2767081260681152\n",
      "epoch: 1 step: 409, loss is 2.301405906677246\n",
      "epoch: 1 step: 410, loss is 2.293400526046753\n",
      "epoch: 1 step: 411, loss is 2.2798874378204346\n",
      "epoch: 1 step: 412, loss is 2.3195157051086426\n",
      "epoch: 1 step: 413, loss is 2.313654661178589\n",
      "epoch: 1 step: 414, loss is 2.3010425567626953\n",
      "epoch: 1 step: 415, loss is 2.2954580783843994\n",
      "epoch: 1 step: 416, loss is 2.3066956996917725\n",
      "epoch: 1 step: 417, loss is 2.317852258682251\n",
      "epoch: 1 step: 418, loss is 2.3086154460906982\n",
      "epoch: 1 step: 419, loss is 2.296444892883301\n",
      "epoch: 1 step: 420, loss is 2.300488233566284\n",
      "epoch: 1 step: 421, loss is 2.312727928161621\n",
      "epoch: 1 step: 422, loss is 2.2967920303344727\n",
      "epoch: 1 step: 423, loss is 2.295149326324463\n",
      "epoch: 1 step: 424, loss is 2.3058712482452393\n",
      "epoch: 1 step: 425, loss is 2.3164010047912598\n",
      "epoch: 1 step: 426, loss is 2.2947516441345215\n",
      "epoch: 1 step: 427, loss is 2.289808988571167\n",
      "epoch: 1 step: 428, loss is 2.29970121383667\n",
      "epoch: 1 step: 429, loss is 2.316492795944214\n",
      "epoch: 1 step: 430, loss is 2.2967581748962402\n",
      "epoch: 1 step: 431, loss is 2.2998123168945312\n",
      "epoch: 1 step: 432, loss is 2.3231396675109863\n",
      "epoch: 1 step: 433, loss is 2.315552234649658\n",
      "epoch: 1 step: 434, loss is 2.2915828227996826\n",
      "epoch: 1 step: 435, loss is 2.295976161956787\n",
      "epoch: 1 step: 436, loss is 2.3137047290802\n",
      "epoch: 1 step: 437, loss is 2.288006067276001\n",
      "epoch: 1 step: 438, loss is 2.3143856525421143\n",
      "epoch: 1 step: 439, loss is 2.2942821979522705\n",
      "epoch: 1 step: 440, loss is 2.311216354370117\n",
      "epoch: 1 step: 441, loss is 2.301858901977539\n",
      "epoch: 1 step: 442, loss is 2.2988691329956055\n",
      "epoch: 1 step: 443, loss is 2.302107095718384\n",
      "epoch: 1 step: 444, loss is 2.3115034103393555\n",
      "epoch: 1 step: 445, loss is 2.271665096282959\n",
      "epoch: 1 step: 446, loss is 2.314997673034668\n",
      "epoch: 1 step: 447, loss is 2.304732084274292\n",
      "epoch: 1 step: 448, loss is 2.3119633197784424\n",
      "epoch: 1 step: 449, loss is 2.2984180450439453\n",
      "epoch: 1 step: 450, loss is 2.3033130168914795\n",
      "epoch: 1 step: 451, loss is 2.296804666519165\n",
      "epoch: 1 step: 452, loss is 2.3012146949768066\n",
      "epoch: 1 step: 453, loss is 2.3194806575775146\n",
      "epoch: 1 step: 454, loss is 2.3153133392333984\n",
      "epoch: 1 step: 455, loss is 2.3133316040039062\n",
      "epoch: 1 step: 456, loss is 2.2886087894439697\n",
      "epoch: 1 step: 457, loss is 2.304161548614502\n",
      "epoch: 1 step: 458, loss is 2.3019015789031982\n",
      "epoch: 1 step: 459, loss is 2.29226016998291\n",
      "epoch: 1 step: 460, loss is 2.299410343170166\n",
      "epoch: 1 step: 461, loss is 2.317537546157837\n",
      "epoch: 1 step: 462, loss is 2.2909905910491943\n",
      "epoch: 1 step: 463, loss is 2.3038411140441895\n",
      "epoch: 1 step: 464, loss is 2.311009645462036\n",
      "epoch: 1 step: 465, loss is 2.3043160438537598\n",
      "epoch: 1 step: 466, loss is 2.2948882579803467\n",
      "epoch: 1 step: 467, loss is 2.3027772903442383\n",
      "epoch: 1 step: 468, loss is 2.287700653076172\n",
      "epoch: 1 step: 469, loss is 2.304908275604248\n",
      "epoch: 1 step: 470, loss is 2.327554225921631\n",
      "epoch: 1 step: 471, loss is 2.2910311222076416\n",
      "epoch: 1 step: 472, loss is 2.3058290481567383\n",
      "epoch: 1 step: 473, loss is 2.2997915744781494\n",
      "epoch: 1 step: 474, loss is 2.311091661453247\n",
      "epoch: 1 step: 475, loss is 2.3082447052001953\n",
      "epoch: 1 step: 476, loss is 2.3106775283813477\n",
      "epoch: 1 step: 477, loss is 2.3109889030456543\n",
      "epoch: 1 step: 478, loss is 2.3171825408935547\n",
      "epoch: 1 step: 479, loss is 2.3125288486480713\n",
      "epoch: 1 step: 480, loss is 2.3001654148101807\n",
      "epoch: 1 step: 481, loss is 2.2994437217712402\n",
      "epoch: 1 step: 482, loss is 2.288360595703125\n",
      "epoch: 1 step: 483, loss is 2.303741216659546\n",
      "epoch: 1 step: 484, loss is 2.2932181358337402\n",
      "epoch: 1 step: 485, loss is 2.291032552719116\n",
      "epoch: 1 step: 486, loss is 2.309077501296997\n",
      "epoch: 1 step: 487, loss is 2.3046913146972656\n",
      "epoch: 1 step: 488, loss is 2.3029632568359375\n",
      "epoch: 1 step: 489, loss is 2.3070034980773926\n",
      "epoch: 1 step: 490, loss is 2.2969846725463867\n",
      "epoch: 1 step: 491, loss is 2.2941036224365234\n",
      "epoch: 1 step: 492, loss is 2.309318780899048\n",
      "epoch: 1 step: 493, loss is 2.3179636001586914\n",
      "epoch: 1 step: 494, loss is 2.294219732284546\n",
      "epoch: 1 step: 495, loss is 2.3020591735839844\n",
      "epoch: 1 step: 496, loss is 2.305126905441284\n",
      "epoch: 1 step: 497, loss is 2.32024884223938\n",
      "epoch: 1 step: 498, loss is 2.298079013824463\n",
      "epoch: 1 step: 499, loss is 2.2948763370513916\n",
      "epoch: 1 step: 500, loss is 2.3120310306549072\n",
      "epoch: 1 step: 501, loss is 2.29641056060791\n",
      "epoch: 1 step: 502, loss is 2.2840561866760254\n",
      "epoch: 1 step: 503, loss is 2.3046000003814697\n",
      "epoch: 1 step: 504, loss is 2.303567409515381\n",
      "epoch: 1 step: 505, loss is 2.3152318000793457\n",
      "epoch: 1 step: 506, loss is 2.296121835708618\n",
      "epoch: 1 step: 507, loss is 2.2840652465820312\n",
      "epoch: 1 step: 508, loss is 2.2967236042022705\n",
      "epoch: 1 step: 509, loss is 2.303576707839966\n",
      "epoch: 1 step: 510, loss is 2.2991559505462646\n",
      "epoch: 1 step: 511, loss is 2.300259828567505\n",
      "epoch: 1 step: 512, loss is 2.302823781967163\n",
      "epoch: 1 step: 513, loss is 2.2974958419799805\n",
      "epoch: 1 step: 514, loss is 2.3092212677001953\n",
      "epoch: 1 step: 515, loss is 2.313084125518799\n",
      "epoch: 1 step: 516, loss is 2.2834365367889404\n",
      "epoch: 1 step: 517, loss is 2.2921764850616455\n",
      "epoch: 1 step: 518, loss is 2.3024325370788574\n",
      "epoch: 1 step: 519, loss is 2.2943434715270996\n",
      "epoch: 1 step: 520, loss is 2.315413236618042\n",
      "epoch: 1 step: 521, loss is 2.311931848526001\n",
      "epoch: 1 step: 522, loss is 2.3002827167510986\n",
      "epoch: 1 step: 523, loss is 2.325965404510498\n",
      "epoch: 1 step: 524, loss is 2.304291009902954\n",
      "epoch: 1 step: 525, loss is 2.3031890392303467\n",
      "epoch: 1 step: 526, loss is 2.314723491668701\n",
      "epoch: 1 step: 527, loss is 2.30877685546875\n",
      "epoch: 1 step: 528, loss is 2.299673557281494\n",
      "epoch: 1 step: 529, loss is 2.3087871074676514\n",
      "epoch: 1 step: 530, loss is 2.2894914150238037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 531, loss is 2.302962303161621\n",
      "epoch: 1 step: 532, loss is 2.3032584190368652\n",
      "epoch: 1 step: 533, loss is 2.2971620559692383\n",
      "epoch: 1 step: 534, loss is 2.291928768157959\n",
      "epoch: 1 step: 535, loss is 2.3004233837127686\n",
      "epoch: 1 step: 536, loss is 2.2898075580596924\n",
      "epoch: 1 step: 537, loss is 2.305870532989502\n",
      "epoch: 1 step: 538, loss is 2.3005318641662598\n",
      "epoch: 1 step: 539, loss is 2.308342456817627\n",
      "epoch: 1 step: 540, loss is 2.2931206226348877\n",
      "epoch: 1 step: 541, loss is 2.307864189147949\n",
      "epoch: 1 step: 542, loss is 2.2912685871124268\n",
      "epoch: 1 step: 543, loss is 2.3220086097717285\n",
      "epoch: 1 step: 544, loss is 2.2858426570892334\n",
      "epoch: 1 step: 545, loss is 2.3006317615509033\n",
      "epoch: 1 step: 546, loss is 2.2889277935028076\n",
      "epoch: 1 step: 547, loss is 2.2942075729370117\n",
      "epoch: 1 step: 548, loss is 2.3047914505004883\n",
      "epoch: 1 step: 549, loss is 2.2920186519622803\n",
      "epoch: 1 step: 550, loss is 2.297638177871704\n",
      "epoch: 1 step: 551, loss is 2.3059346675872803\n",
      "epoch: 1 step: 552, loss is 2.300006628036499\n",
      "epoch: 1 step: 553, loss is 2.3085427284240723\n",
      "epoch: 1 step: 554, loss is 2.303400993347168\n",
      "epoch: 1 step: 555, loss is 2.308357000350952\n",
      "epoch: 1 step: 556, loss is 2.312389612197876\n",
      "epoch: 1 step: 557, loss is 2.293004274368286\n",
      "epoch: 1 step: 558, loss is 2.3048312664031982\n",
      "epoch: 1 step: 559, loss is 2.31594181060791\n",
      "epoch: 1 step: 560, loss is 2.296539783477783\n",
      "epoch: 1 step: 561, loss is 2.2955195903778076\n",
      "epoch: 1 step: 562, loss is 2.2989537715911865\n",
      "epoch: 1 step: 563, loss is 2.3060073852539062\n",
      "epoch: 1 step: 564, loss is 2.306515693664551\n",
      "epoch: 1 step: 565, loss is 2.30904221534729\n",
      "epoch: 1 step: 566, loss is 2.2968010902404785\n",
      "epoch: 1 step: 567, loss is 2.3001556396484375\n",
      "epoch: 1 step: 568, loss is 2.3031325340270996\n",
      "epoch: 1 step: 569, loss is 2.3187921047210693\n",
      "epoch: 1 step: 570, loss is 2.302274703979492\n",
      "epoch: 1 step: 571, loss is 2.3071706295013428\n",
      "epoch: 1 step: 572, loss is 2.287386178970337\n",
      "epoch: 1 step: 573, loss is 2.2803421020507812\n",
      "epoch: 1 step: 574, loss is 2.309631824493408\n",
      "epoch: 1 step: 575, loss is 2.3177387714385986\n",
      "epoch: 1 step: 576, loss is 2.3078160285949707\n",
      "epoch: 1 step: 577, loss is 2.302187204360962\n",
      "epoch: 1 step: 578, loss is 2.3157131671905518\n",
      "epoch: 1 step: 579, loss is 2.298509359359741\n",
      "epoch: 1 step: 580, loss is 2.3047869205474854\n",
      "epoch: 1 step: 581, loss is 2.308229923248291\n",
      "epoch: 1 step: 582, loss is 2.303375005722046\n",
      "epoch: 1 step: 583, loss is 2.290851593017578\n",
      "epoch: 1 step: 584, loss is 2.3012351989746094\n",
      "epoch: 1 step: 585, loss is 2.2865757942199707\n",
      "epoch: 1 step: 586, loss is 2.3099679946899414\n",
      "epoch: 1 step: 587, loss is 2.296949863433838\n",
      "epoch: 1 step: 588, loss is 2.2999606132507324\n",
      "epoch: 1 step: 589, loss is 2.2912683486938477\n",
      "epoch: 1 step: 590, loss is 2.303365707397461\n",
      "epoch: 1 step: 591, loss is 2.312044620513916\n",
      "epoch: 1 step: 592, loss is 2.298610210418701\n",
      "epoch: 1 step: 593, loss is 2.2999603748321533\n",
      "epoch: 1 step: 594, loss is 2.3054075241088867\n",
      "epoch: 1 step: 595, loss is 2.3028852939605713\n",
      "epoch: 1 step: 596, loss is 2.3138844966888428\n",
      "epoch: 1 step: 597, loss is 2.2859344482421875\n",
      "epoch: 1 step: 598, loss is 2.3093464374542236\n",
      "epoch: 1 step: 599, loss is 2.304129123687744\n",
      "epoch: 1 step: 600, loss is 2.298161506652832\n",
      "epoch: 1 step: 601, loss is 2.3160481452941895\n",
      "epoch: 1 step: 602, loss is 2.3140368461608887\n",
      "epoch: 1 step: 603, loss is 2.31066632270813\n",
      "epoch: 1 step: 604, loss is 2.294649362564087\n",
      "epoch: 1 step: 605, loss is 2.3024940490722656\n",
      "epoch: 1 step: 606, loss is 2.313936710357666\n",
      "epoch: 1 step: 607, loss is 2.295659303665161\n",
      "epoch: 1 step: 608, loss is 2.2941782474517822\n",
      "epoch: 1 step: 609, loss is 2.319486141204834\n",
      "epoch: 1 step: 610, loss is 2.3161885738372803\n",
      "epoch: 1 step: 611, loss is 2.291562795639038\n",
      "epoch: 1 step: 612, loss is 2.3138034343719482\n",
      "epoch: 1 step: 613, loss is 2.2869019508361816\n",
      "epoch: 1 step: 614, loss is 2.3030712604522705\n",
      "epoch: 1 step: 615, loss is 2.3061912059783936\n",
      "epoch: 1 step: 616, loss is 2.302431344985962\n",
      "epoch: 1 step: 617, loss is 2.2816073894500732\n",
      "epoch: 1 step: 618, loss is 2.3094747066497803\n",
      "epoch: 1 step: 619, loss is 2.2951273918151855\n",
      "epoch: 1 step: 620, loss is 2.3132853507995605\n",
      "epoch: 1 step: 621, loss is 2.2922048568725586\n",
      "epoch: 1 step: 622, loss is 2.3067641258239746\n",
      "epoch: 1 step: 623, loss is 2.307025194168091\n",
      "epoch: 1 step: 624, loss is 2.2954037189483643\n",
      "epoch: 1 step: 625, loss is 2.3004026412963867\n",
      "epoch: 1 step: 626, loss is 2.2900795936584473\n",
      "epoch: 1 step: 627, loss is 2.2970662117004395\n",
      "epoch: 1 step: 628, loss is 2.3110039234161377\n",
      "epoch: 1 step: 629, loss is 2.316699266433716\n",
      "epoch: 1 step: 630, loss is 2.2965903282165527\n",
      "epoch: 1 step: 631, loss is 2.2930023670196533\n",
      "epoch: 1 step: 632, loss is 2.306800127029419\n",
      "epoch: 1 step: 633, loss is 2.3069634437561035\n",
      "epoch: 1 step: 634, loss is 2.3021814823150635\n",
      "epoch: 1 step: 635, loss is 2.306933641433716\n",
      "epoch: 1 step: 636, loss is 2.300283908843994\n",
      "epoch: 1 step: 637, loss is 2.3103954792022705\n",
      "epoch: 1 step: 638, loss is 2.3012025356292725\n",
      "epoch: 1 step: 639, loss is 2.300501585006714\n",
      "epoch: 1 step: 640, loss is 2.302689790725708\n",
      "epoch: 1 step: 641, loss is 2.3096659183502197\n",
      "epoch: 1 step: 642, loss is 2.297635793685913\n",
      "epoch: 1 step: 643, loss is 2.295745849609375\n",
      "epoch: 1 step: 644, loss is 2.2926480770111084\n",
      "epoch: 1 step: 645, loss is 2.304614305496216\n",
      "epoch: 1 step: 646, loss is 2.3044309616088867\n",
      "epoch: 1 step: 647, loss is 2.3117592334747314\n",
      "epoch: 1 step: 648, loss is 2.299929618835449\n",
      "epoch: 1 step: 649, loss is 2.3053691387176514\n",
      "epoch: 1 step: 650, loss is 2.2923340797424316\n",
      "epoch: 1 step: 651, loss is 2.2969858646392822\n",
      "epoch: 1 step: 652, loss is 2.296410083770752\n",
      "epoch: 1 step: 653, loss is 2.3199880123138428\n",
      "epoch: 1 step: 654, loss is 2.2953200340270996\n",
      "epoch: 1 step: 655, loss is 2.298727512359619\n",
      "epoch: 1 step: 656, loss is 2.316495418548584\n",
      "epoch: 1 step: 657, loss is 2.298367977142334\n",
      "epoch: 1 step: 658, loss is 2.2985496520996094\n",
      "epoch: 1 step: 659, loss is 2.302215099334717\n",
      "epoch: 1 step: 660, loss is 2.295109272003174\n",
      "epoch: 1 step: 661, loss is 2.3122546672821045\n",
      "epoch: 1 step: 662, loss is 2.2822105884552\n",
      "epoch: 1 step: 663, loss is 2.301680326461792\n",
      "epoch: 1 step: 664, loss is 2.2966675758361816\n",
      "epoch: 1 step: 665, loss is 2.312936782836914\n",
      "epoch: 1 step: 666, loss is 2.311894655227661\n",
      "epoch: 1 step: 667, loss is 2.2991037368774414\n",
      "epoch: 1 step: 668, loss is 2.2867743968963623\n",
      "epoch: 1 step: 669, loss is 2.296529769897461\n",
      "epoch: 1 step: 670, loss is 2.284003734588623\n",
      "epoch: 1 step: 671, loss is 2.2957606315612793\n",
      "epoch: 1 step: 672, loss is 2.2978310585021973\n",
      "epoch: 1 step: 673, loss is 2.28833270072937\n",
      "epoch: 1 step: 674, loss is 2.309494972229004\n",
      "epoch: 1 step: 675, loss is 2.307675361633301\n",
      "epoch: 1 step: 676, loss is 2.3050572872161865\n",
      "epoch: 1 step: 677, loss is 2.2857823371887207\n",
      "epoch: 1 step: 678, loss is 2.288296937942505\n",
      "epoch: 1 step: 679, loss is 2.308567523956299\n",
      "epoch: 1 step: 680, loss is 2.304173231124878\n",
      "epoch: 1 step: 681, loss is 2.3017730712890625\n",
      "epoch: 1 step: 682, loss is 2.290633201599121\n",
      "epoch: 1 step: 683, loss is 2.299210548400879\n",
      "epoch: 1 step: 684, loss is 2.2957000732421875\n",
      "epoch: 1 step: 685, loss is 2.3264670372009277\n",
      "epoch: 1 step: 686, loss is 2.314591884613037\n",
      "epoch: 1 step: 687, loss is 2.3107075691223145\n",
      "epoch: 1 step: 688, loss is 2.2844064235687256\n",
      "epoch: 1 step: 689, loss is 2.3044791221618652\n",
      "epoch: 1 step: 690, loss is 2.314023971557617\n",
      "epoch: 1 step: 691, loss is 2.31189227104187\n",
      "epoch: 1 step: 692, loss is 2.2908151149749756\n",
      "epoch: 1 step: 693, loss is 2.2998945713043213\n",
      "epoch: 1 step: 694, loss is 2.308136463165283\n",
      "epoch: 1 step: 695, loss is 2.2921745777130127\n",
      "epoch: 1 step: 696, loss is 2.306507110595703\n",
      "epoch: 1 step: 697, loss is 2.304586887359619\n",
      "epoch: 1 step: 698, loss is 2.2928221225738525\n",
      "epoch: 1 step: 699, loss is 2.31083083152771\n",
      "epoch: 1 step: 700, loss is 2.299023389816284\n",
      "epoch: 1 step: 701, loss is 2.3064489364624023\n",
      "epoch: 1 step: 702, loss is 2.289694309234619\n",
      "epoch: 1 step: 703, loss is 2.296311616897583\n",
      "epoch: 1 step: 704, loss is 2.301313877105713\n",
      "epoch: 1 step: 705, loss is 2.3061423301696777\n",
      "epoch: 1 step: 706, loss is 2.2993736267089844\n",
      "epoch: 1 step: 707, loss is 2.305352210998535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 708, loss is 2.291566848754883\n",
      "epoch: 1 step: 709, loss is 2.304319381713867\n",
      "epoch: 1 step: 710, loss is 2.296205520629883\n",
      "epoch: 1 step: 711, loss is 2.304431200027466\n",
      "epoch: 1 step: 712, loss is 2.3052165508270264\n",
      "epoch: 1 step: 713, loss is 2.2953901290893555\n",
      "epoch: 1 step: 714, loss is 2.3068766593933105\n",
      "epoch: 1 step: 715, loss is 2.304093837738037\n",
      "epoch: 1 step: 716, loss is 2.300747871398926\n",
      "epoch: 1 step: 717, loss is 2.314241886138916\n",
      "epoch: 1 step: 718, loss is 2.2961273193359375\n",
      "epoch: 1 step: 719, loss is 2.2999303340911865\n",
      "epoch: 1 step: 720, loss is 2.3066020011901855\n",
      "epoch: 1 step: 721, loss is 2.308279514312744\n",
      "epoch: 1 step: 722, loss is 2.2885804176330566\n",
      "epoch: 1 step: 723, loss is 2.305025100708008\n",
      "epoch: 1 step: 724, loss is 2.2911009788513184\n",
      "epoch: 1 step: 725, loss is 2.293337345123291\n",
      "epoch: 1 step: 726, loss is 2.3032853603363037\n",
      "epoch: 1 step: 727, loss is 2.2957937717437744\n",
      "epoch: 1 step: 728, loss is 2.3129215240478516\n",
      "epoch: 1 step: 729, loss is 2.2996718883514404\n",
      "epoch: 1 step: 730, loss is 2.2943789958953857\n",
      "epoch: 1 step: 731, loss is 2.2970945835113525\n",
      "epoch: 1 step: 732, loss is 2.3067593574523926\n",
      "epoch: 1 step: 733, loss is 2.29891037940979\n",
      "epoch: 1 step: 734, loss is 2.29129695892334\n",
      "epoch: 1 step: 735, loss is 2.2986679077148438\n",
      "epoch: 1 step: 736, loss is 2.3040733337402344\n",
      "epoch: 1 step: 737, loss is 2.3036341667175293\n",
      "epoch: 1 step: 738, loss is 2.281494617462158\n",
      "epoch: 1 step: 739, loss is 2.304933547973633\n",
      "epoch: 1 step: 740, loss is 2.314934492111206\n",
      "epoch: 1 step: 741, loss is 2.301884412765503\n",
      "epoch: 1 step: 742, loss is 2.314899206161499\n",
      "epoch: 1 step: 743, loss is 2.3029582500457764\n",
      "epoch: 1 step: 744, loss is 2.3177602291107178\n",
      "epoch: 1 step: 745, loss is 2.2978427410125732\n",
      "epoch: 1 step: 746, loss is 2.305061101913452\n",
      "epoch: 1 step: 747, loss is 2.3047068119049072\n",
      "epoch: 1 step: 748, loss is 2.3016912937164307\n",
      "epoch: 1 step: 749, loss is 2.292872190475464\n",
      "epoch: 1 step: 750, loss is 2.306906223297119\n",
      "epoch: 1 step: 751, loss is 2.2909889221191406\n",
      "epoch: 1 step: 752, loss is 2.302605628967285\n",
      "epoch: 1 step: 753, loss is 2.3039321899414062\n",
      "epoch: 1 step: 754, loss is 2.3038477897644043\n",
      "epoch: 1 step: 755, loss is 2.289738178253174\n",
      "epoch: 1 step: 756, loss is 2.300671339035034\n",
      "epoch: 1 step: 757, loss is 2.298379898071289\n",
      "epoch: 1 step: 758, loss is 2.3099045753479004\n",
      "epoch: 1 step: 759, loss is 2.2928078174591064\n",
      "epoch: 1 step: 760, loss is 2.3058977127075195\n",
      "epoch: 1 step: 761, loss is 2.2935054302215576\n",
      "epoch: 1 step: 762, loss is 2.2982161045074463\n",
      "epoch: 1 step: 763, loss is 2.2916083335876465\n",
      "epoch: 1 step: 764, loss is 2.310293674468994\n",
      "epoch: 1 step: 765, loss is 2.3124704360961914\n",
      "epoch: 1 step: 766, loss is 2.309549331665039\n",
      "epoch: 1 step: 767, loss is 2.31260347366333\n",
      "epoch: 1 step: 768, loss is 2.295027256011963\n",
      "epoch: 1 step: 769, loss is 2.305122137069702\n",
      "epoch: 1 step: 770, loss is 2.307730197906494\n",
      "epoch: 1 step: 771, loss is 2.2988240718841553\n",
      "epoch: 1 step: 772, loss is 2.3036084175109863\n",
      "epoch: 1 step: 773, loss is 2.302004337310791\n",
      "epoch: 1 step: 774, loss is 2.288485527038574\n",
      "epoch: 1 step: 775, loss is 2.313563585281372\n",
      "epoch: 1 step: 776, loss is 2.305665969848633\n",
      "epoch: 1 step: 777, loss is 2.305811643600464\n",
      "epoch: 1 step: 778, loss is 2.3007614612579346\n",
      "epoch: 1 step: 779, loss is 2.291841745376587\n",
      "epoch: 1 step: 780, loss is 2.2983522415161133\n",
      "epoch: 1 step: 781, loss is 2.3056671619415283\n",
      "epoch: 1 step: 782, loss is 2.292604446411133\n",
      "epoch: 1 step: 783, loss is 2.3037424087524414\n",
      "epoch: 1 step: 784, loss is 2.3010094165802\n",
      "epoch: 1 step: 785, loss is 2.3058149814605713\n",
      "epoch: 1 step: 786, loss is 2.2981293201446533\n",
      "epoch: 1 step: 787, loss is 2.298597574234009\n",
      "epoch: 1 step: 788, loss is 2.2971677780151367\n",
      "epoch: 1 step: 789, loss is 2.290565013885498\n",
      "epoch: 1 step: 790, loss is 2.3095409870147705\n",
      "epoch: 1 step: 791, loss is 2.32401180267334\n",
      "epoch: 1 step: 792, loss is 2.2927982807159424\n",
      "epoch: 1 step: 793, loss is 2.3135273456573486\n",
      "epoch: 1 step: 794, loss is 2.3012382984161377\n",
      "epoch: 1 step: 795, loss is 2.3121092319488525\n",
      "epoch: 1 step: 796, loss is 2.3092551231384277\n",
      "epoch: 1 step: 797, loss is 2.322618007659912\n",
      "epoch: 1 step: 798, loss is 2.2853848934173584\n",
      "epoch: 1 step: 799, loss is 2.295485019683838\n",
      "epoch: 1 step: 800, loss is 2.2899563312530518\n",
      "epoch: 1 step: 801, loss is 2.3140172958374023\n",
      "epoch: 1 step: 802, loss is 2.29010272026062\n",
      "epoch: 1 step: 803, loss is 2.290569305419922\n",
      "epoch: 1 step: 804, loss is 2.2853946685791016\n",
      "epoch: 1 step: 805, loss is 2.2899563312530518\n",
      "epoch: 1 step: 806, loss is 2.302896022796631\n",
      "epoch: 1 step: 807, loss is 2.289163827896118\n",
      "epoch: 1 step: 808, loss is 2.295781373977661\n",
      "epoch: 1 step: 809, loss is 2.296599864959717\n",
      "epoch: 1 step: 810, loss is 2.3076322078704834\n",
      "epoch: 1 step: 811, loss is 2.3216192722320557\n",
      "epoch: 1 step: 812, loss is 2.31166410446167\n",
      "epoch: 1 step: 813, loss is 2.319678783416748\n",
      "epoch: 1 step: 814, loss is 2.3083407878875732\n",
      "epoch: 1 step: 815, loss is 2.282425880432129\n",
      "epoch: 1 step: 816, loss is 2.298708438873291\n",
      "epoch: 1 step: 817, loss is 2.3012893199920654\n",
      "epoch: 1 step: 818, loss is 2.304112195968628\n",
      "epoch: 1 step: 819, loss is 2.3150007724761963\n",
      "epoch: 1 step: 820, loss is 2.29593563079834\n",
      "epoch: 1 step: 821, loss is 2.3054556846618652\n",
      "epoch: 1 step: 822, loss is 2.287306070327759\n",
      "epoch: 1 step: 823, loss is 2.300499200820923\n",
      "epoch: 1 step: 824, loss is 2.323967456817627\n",
      "epoch: 1 step: 825, loss is 2.2960119247436523\n",
      "epoch: 1 step: 826, loss is 2.3015103340148926\n",
      "epoch: 1 step: 827, loss is 2.2956509590148926\n",
      "epoch: 1 step: 828, loss is 2.3009705543518066\n",
      "epoch: 1 step: 829, loss is 2.3069567680358887\n",
      "epoch: 1 step: 830, loss is 2.323648691177368\n",
      "epoch: 1 step: 831, loss is 2.307309865951538\n",
      "epoch: 1 step: 832, loss is 2.303981065750122\n",
      "epoch: 1 step: 833, loss is 2.312002182006836\n",
      "epoch: 1 step: 834, loss is 2.3042404651641846\n",
      "epoch: 1 step: 835, loss is 2.307318925857544\n",
      "epoch: 1 step: 836, loss is 2.2783541679382324\n",
      "epoch: 1 step: 837, loss is 2.288029909133911\n",
      "epoch: 1 step: 838, loss is 2.2938895225524902\n",
      "epoch: 1 step: 839, loss is 2.297686815261841\n",
      "epoch: 1 step: 840, loss is 2.3089210987091064\n",
      "epoch: 1 step: 841, loss is 2.2939672470092773\n",
      "epoch: 1 step: 842, loss is 2.2785162925720215\n",
      "epoch: 1 step: 843, loss is 2.295226573944092\n",
      "epoch: 1 step: 844, loss is 2.2878506183624268\n",
      "epoch: 1 step: 845, loss is 2.28996205329895\n",
      "epoch: 1 step: 846, loss is 2.3123316764831543\n",
      "epoch: 1 step: 847, loss is 2.2938406467437744\n",
      "epoch: 1 step: 848, loss is 2.3191919326782227\n",
      "epoch: 1 step: 849, loss is 2.302457094192505\n",
      "epoch: 1 step: 850, loss is 2.269880771636963\n",
      "epoch: 1 step: 851, loss is 2.3156850337982178\n",
      "epoch: 1 step: 852, loss is 2.2998857498168945\n",
      "epoch: 1 step: 853, loss is 2.3167312145233154\n",
      "epoch: 1 step: 854, loss is 2.3024754524230957\n",
      "epoch: 1 step: 855, loss is 2.307802677154541\n",
      "epoch: 1 step: 856, loss is 2.2989253997802734\n",
      "epoch: 1 step: 857, loss is 2.2768001556396484\n",
      "epoch: 1 step: 858, loss is 2.31992244720459\n",
      "epoch: 1 step: 859, loss is 2.3020589351654053\n",
      "epoch: 1 step: 860, loss is 2.3020355701446533\n",
      "epoch: 1 step: 861, loss is 2.295766830444336\n",
      "epoch: 1 step: 862, loss is 2.2963626384735107\n",
      "epoch: 1 step: 863, loss is 2.3227930068969727\n",
      "epoch: 1 step: 864, loss is 2.3011765480041504\n",
      "epoch: 1 step: 865, loss is 2.294711112976074\n",
      "epoch: 1 step: 866, loss is 2.2992660999298096\n",
      "epoch: 1 step: 867, loss is 2.301223039627075\n",
      "epoch: 1 step: 868, loss is 2.302809000015259\n",
      "epoch: 1 step: 869, loss is 2.3022313117980957\n",
      "epoch: 1 step: 870, loss is 2.2789158821105957\n",
      "epoch: 1 step: 871, loss is 2.298156499862671\n",
      "epoch: 1 step: 872, loss is 2.31479811668396\n",
      "epoch: 1 step: 873, loss is 2.2901835441589355\n",
      "epoch: 1 step: 874, loss is 2.296189069747925\n",
      "epoch: 1 step: 875, loss is 2.299715995788574\n",
      "epoch: 1 step: 876, loss is 2.3148446083068848\n",
      "epoch: 1 step: 877, loss is 2.3156378269195557\n",
      "epoch: 1 step: 878, loss is 2.301791191101074\n",
      "epoch: 1 step: 879, loss is 2.303619623184204\n",
      "epoch: 1 step: 880, loss is 2.2945642471313477\n",
      "epoch: 1 step: 881, loss is 2.320905923843384\n",
      "epoch: 1 step: 882, loss is 2.2859413623809814\n",
      "epoch: 1 step: 883, loss is 2.2941012382507324\n",
      "epoch: 1 step: 884, loss is 2.3083503246307373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 885, loss is 2.2973873615264893\n",
      "epoch: 1 step: 886, loss is 2.287482976913452\n",
      "epoch: 1 step: 887, loss is 2.312176465988159\n",
      "epoch: 1 step: 888, loss is 2.2914323806762695\n",
      "epoch: 1 step: 889, loss is 2.295954465866089\n",
      "epoch: 1 step: 890, loss is 2.3087596893310547\n",
      "epoch: 1 step: 891, loss is 2.2998640537261963\n",
      "epoch: 1 step: 892, loss is 2.300652027130127\n",
      "epoch: 1 step: 893, loss is 2.2922441959381104\n",
      "epoch: 1 step: 894, loss is 2.310140609741211\n",
      "epoch: 1 step: 895, loss is 2.3014299869537354\n",
      "epoch: 1 step: 896, loss is 2.3048906326293945\n",
      "epoch: 1 step: 897, loss is 2.308513641357422\n",
      "epoch: 1 step: 898, loss is 2.2685461044311523\n",
      "epoch: 1 step: 899, loss is 2.294870615005493\n",
      "epoch: 1 step: 900, loss is 2.2717981338500977\n",
      "epoch: 1 step: 901, loss is 2.2866530418395996\n",
      "epoch: 1 step: 902, loss is 2.294490337371826\n",
      "epoch: 1 step: 903, loss is 2.3010525703430176\n",
      "epoch: 1 step: 904, loss is 2.2857165336608887\n",
      "epoch: 1 step: 905, loss is 2.2839245796203613\n",
      "epoch: 1 step: 906, loss is 2.28475284576416\n",
      "epoch: 1 step: 907, loss is 2.2864818572998047\n",
      "epoch: 1 step: 908, loss is 2.3048694133758545\n",
      "epoch: 1 step: 909, loss is 2.275020122528076\n",
      "epoch: 1 step: 910, loss is 2.3065080642700195\n",
      "epoch: 1 step: 911, loss is 2.308608055114746\n",
      "epoch: 1 step: 912, loss is 2.3097565174102783\n",
      "epoch: 1 step: 913, loss is 2.294928550720215\n",
      "epoch: 1 step: 914, loss is 2.300412893295288\n",
      "epoch: 1 step: 915, loss is 2.31341814994812\n",
      "epoch: 1 step: 916, loss is 2.3318819999694824\n",
      "epoch: 1 step: 917, loss is 2.3085594177246094\n",
      "epoch: 1 step: 918, loss is 2.2968602180480957\n",
      "epoch: 1 step: 919, loss is 2.295165538787842\n",
      "epoch: 1 step: 920, loss is 2.2896745204925537\n",
      "epoch: 1 step: 921, loss is 2.2833142280578613\n",
      "epoch: 1 step: 922, loss is 2.300746202468872\n",
      "epoch: 1 step: 923, loss is 2.3210322856903076\n",
      "epoch: 1 step: 924, loss is 2.2861297130584717\n",
      "epoch: 1 step: 925, loss is 2.3164870738983154\n",
      "epoch: 1 step: 926, loss is 2.294229745864868\n",
      "epoch: 1 step: 927, loss is 2.270110607147217\n",
      "epoch: 1 step: 928, loss is 2.303572177886963\n",
      "epoch: 1 step: 929, loss is 2.304377317428589\n",
      "epoch: 1 step: 930, loss is 2.298384428024292\n",
      "epoch: 1 step: 931, loss is 2.3228635787963867\n",
      "epoch: 1 step: 932, loss is 2.3235864639282227\n",
      "epoch: 1 step: 933, loss is 2.31308650970459\n",
      "epoch: 1 step: 934, loss is 2.3244709968566895\n",
      "epoch: 1 step: 935, loss is 2.29691481590271\n",
      "epoch: 1 step: 936, loss is 2.2821760177612305\n",
      "epoch: 1 step: 937, loss is 2.30334210395813\n",
      "epoch: 1 step: 938, loss is 2.2975540161132812\n",
      "epoch: 1 step: 939, loss is 2.302110195159912\n",
      "epoch: 1 step: 940, loss is 2.303375482559204\n",
      "epoch: 1 step: 941, loss is 2.284940242767334\n",
      "epoch: 1 step: 942, loss is 2.306706428527832\n",
      "epoch: 1 step: 943, loss is 2.30741024017334\n",
      "epoch: 1 step: 944, loss is 2.3039515018463135\n",
      "epoch: 1 step: 945, loss is 2.3096654415130615\n",
      "epoch: 1 step: 946, loss is 2.2962658405303955\n",
      "epoch: 1 step: 947, loss is 2.3059589862823486\n",
      "epoch: 1 step: 948, loss is 2.293313980102539\n",
      "epoch: 1 step: 949, loss is 2.3091723918914795\n",
      "epoch: 1 step: 950, loss is 2.31406831741333\n",
      "epoch: 1 step: 951, loss is 2.2984700202941895\n",
      "epoch: 1 step: 952, loss is 2.2789316177368164\n",
      "epoch: 1 step: 953, loss is 2.307051420211792\n",
      "epoch: 1 step: 954, loss is 2.297299385070801\n",
      "epoch: 1 step: 955, loss is 2.3204832077026367\n",
      "epoch: 1 step: 956, loss is 2.2954986095428467\n",
      "epoch: 1 step: 957, loss is 2.3160400390625\n",
      "epoch: 1 step: 958, loss is 2.3163161277770996\n",
      "epoch: 1 step: 959, loss is 2.303157091140747\n",
      "epoch: 1 step: 960, loss is 2.301881790161133\n",
      "epoch: 1 step: 961, loss is 2.3095149993896484\n",
      "epoch: 1 step: 962, loss is 2.303849697113037\n",
      "epoch: 1 step: 963, loss is 2.3094189167022705\n",
      "epoch: 1 step: 964, loss is 2.302846670150757\n",
      "epoch: 1 step: 965, loss is 2.2974672317504883\n",
      "epoch: 1 step: 966, loss is 2.287794828414917\n",
      "epoch: 1 step: 967, loss is 2.3033411502838135\n",
      "epoch: 1 step: 968, loss is 2.3083066940307617\n",
      "epoch: 1 step: 969, loss is 2.2758641242980957\n",
      "epoch: 1 step: 970, loss is 2.295677900314331\n",
      "epoch: 1 step: 971, loss is 2.310865640640259\n",
      "epoch: 1 step: 972, loss is 2.2979695796966553\n",
      "epoch: 1 step: 973, loss is 2.3099961280822754\n",
      "epoch: 1 step: 974, loss is 2.294774055480957\n",
      "epoch: 1 step: 975, loss is 2.291243314743042\n",
      "epoch: 1 step: 976, loss is 2.3062710762023926\n",
      "epoch: 1 step: 977, loss is 2.280819892883301\n",
      "epoch: 1 step: 978, loss is 2.3161048889160156\n",
      "epoch: 1 step: 979, loss is 2.306276321411133\n",
      "epoch: 1 step: 980, loss is 2.2950236797332764\n",
      "epoch: 1 step: 981, loss is 2.283621072769165\n",
      "epoch: 1 step: 982, loss is 2.3082616329193115\n",
      "epoch: 1 step: 983, loss is 2.274658679962158\n",
      "epoch: 1 step: 984, loss is 2.28945255279541\n",
      "epoch: 1 step: 985, loss is 2.332648515701294\n",
      "epoch: 1 step: 986, loss is 2.29504656791687\n",
      "epoch: 1 step: 987, loss is 2.3077056407928467\n",
      "epoch: 1 step: 988, loss is 2.309242010116577\n",
      "epoch: 1 step: 989, loss is 2.2931625843048096\n",
      "epoch: 1 step: 990, loss is 2.299156427383423\n",
      "epoch: 1 step: 991, loss is 2.3025214672088623\n",
      "epoch: 1 step: 992, loss is 2.310232639312744\n",
      "epoch: 1 step: 993, loss is 2.297973394393921\n",
      "epoch: 1 step: 994, loss is 2.290584087371826\n",
      "epoch: 1 step: 995, loss is 2.3146893978118896\n",
      "epoch: 1 step: 996, loss is 2.2821452617645264\n",
      "epoch: 1 step: 997, loss is 2.314748525619507\n",
      "epoch: 1 step: 998, loss is 2.2880475521087646\n",
      "epoch: 1 step: 999, loss is 2.293745279312134\n",
      "epoch: 1 step: 1000, loss is 2.3063740730285645\n",
      "epoch: 1 step: 1001, loss is 2.3006415367126465\n",
      "epoch: 1 step: 1002, loss is 2.297211170196533\n",
      "epoch: 1 step: 1003, loss is 2.289679527282715\n",
      "epoch: 1 step: 1004, loss is 2.285140037536621\n",
      "epoch: 1 step: 1005, loss is 2.297253131866455\n",
      "epoch: 1 step: 1006, loss is 2.2979001998901367\n",
      "epoch: 1 step: 1007, loss is 2.304927110671997\n",
      "epoch: 1 step: 1008, loss is 2.289506196975708\n",
      "epoch: 1 step: 1009, loss is 2.2945196628570557\n",
      "epoch: 1 step: 1010, loss is 2.310035228729248\n",
      "epoch: 1 step: 1011, loss is 2.2893359661102295\n",
      "epoch: 1 step: 1012, loss is 2.2995667457580566\n",
      "epoch: 1 step: 1013, loss is 2.3064723014831543\n",
      "epoch: 1 step: 1014, loss is 2.2961013317108154\n",
      "epoch: 1 step: 1015, loss is 2.3088581562042236\n",
      "epoch: 1 step: 1016, loss is 2.3077645301818848\n",
      "epoch: 1 step: 1017, loss is 2.3011879920959473\n",
      "epoch: 1 step: 1018, loss is 2.296478748321533\n",
      "epoch: 1 step: 1019, loss is 2.2809269428253174\n",
      "epoch: 1 step: 1020, loss is 2.306396722793579\n",
      "epoch: 1 step: 1021, loss is 2.290682554244995\n",
      "epoch: 1 step: 1022, loss is 2.2947418689727783\n",
      "epoch: 1 step: 1023, loss is 2.312676191329956\n",
      "epoch: 1 step: 1024, loss is 2.2742855548858643\n",
      "epoch: 1 step: 1025, loss is 2.315048933029175\n",
      "epoch: 1 step: 1026, loss is 2.3061180114746094\n",
      "epoch: 1 step: 1027, loss is 2.300278663635254\n",
      "epoch: 1 step: 1028, loss is 2.298145055770874\n",
      "epoch: 1 step: 1029, loss is 2.2867496013641357\n",
      "epoch: 1 step: 1030, loss is 2.294788360595703\n",
      "epoch: 1 step: 1031, loss is 2.282318353652954\n",
      "epoch: 1 step: 1032, loss is 2.2864089012145996\n",
      "epoch: 1 step: 1033, loss is 2.313965320587158\n",
      "epoch: 1 step: 1034, loss is 2.2933547496795654\n",
      "epoch: 1 step: 1035, loss is 2.298815965652466\n",
      "epoch: 1 step: 1036, loss is 2.2951977252960205\n",
      "epoch: 1 step: 1037, loss is 2.2998874187469482\n",
      "epoch: 1 step: 1038, loss is 2.281385898590088\n",
      "epoch: 1 step: 1039, loss is 2.2932779788970947\n",
      "epoch: 1 step: 1040, loss is 2.302401065826416\n",
      "epoch: 1 step: 1041, loss is 2.3135826587677\n",
      "epoch: 1 step: 1042, loss is 2.292842149734497\n",
      "epoch: 1 step: 1043, loss is 2.2860288619995117\n",
      "epoch: 1 step: 1044, loss is 2.28735089302063\n",
      "epoch: 1 step: 1045, loss is 2.2897284030914307\n",
      "epoch: 1 step: 1046, loss is 2.2843642234802246\n",
      "epoch: 1 step: 1047, loss is 2.298473834991455\n",
      "epoch: 1 step: 1048, loss is 2.2948622703552246\n",
      "epoch: 1 step: 1049, loss is 2.30161452293396\n",
      "epoch: 1 step: 1050, loss is 2.298327684402466\n",
      "epoch: 1 step: 1051, loss is 2.295473337173462\n",
      "epoch: 1 step: 1052, loss is 2.2901718616485596\n",
      "epoch: 1 step: 1053, loss is 2.283879280090332\n",
      "epoch: 1 step: 1054, loss is 2.2750484943389893\n",
      "epoch: 1 step: 1055, loss is 2.2873711585998535\n",
      "epoch: 1 step: 1056, loss is 2.2854247093200684\n",
      "epoch: 1 step: 1057, loss is 2.2868223190307617\n",
      "epoch: 1 step: 1058, loss is 2.283184766769409\n",
      "epoch: 1 step: 1059, loss is 2.290252685546875\n",
      "epoch: 1 step: 1060, loss is 2.283036231994629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1061, loss is 2.285492181777954\n",
      "epoch: 1 step: 1062, loss is 2.2772583961486816\n",
      "epoch: 1 step: 1063, loss is 2.2928802967071533\n",
      "epoch: 1 step: 1064, loss is 2.2690162658691406\n",
      "epoch: 1 step: 1065, loss is 2.275785446166992\n",
      "epoch: 1 step: 1066, loss is 2.2747344970703125\n",
      "epoch: 1 step: 1067, loss is 2.2600321769714355\n",
      "epoch: 1 step: 1068, loss is 2.285566806793213\n",
      "epoch: 1 step: 1069, loss is 2.255173444747925\n",
      "epoch: 1 step: 1070, loss is 2.2788524627685547\n",
      "epoch: 1 step: 1071, loss is 2.260751962661743\n",
      "epoch: 1 step: 1072, loss is 2.2694194316864014\n",
      "epoch: 1 step: 1073, loss is 2.261918306350708\n",
      "epoch: 1 step: 1074, loss is 2.26137375831604\n",
      "epoch: 1 step: 1075, loss is 2.267711639404297\n",
      "epoch: 1 step: 1076, loss is 2.226271867752075\n",
      "epoch: 1 step: 1077, loss is 2.2558915615081787\n",
      "epoch: 1 step: 1078, loss is 2.2561843395233154\n",
      "epoch: 1 step: 1079, loss is 2.224090814590454\n",
      "epoch: 1 step: 1080, loss is 2.2335572242736816\n",
      "epoch: 1 step: 1081, loss is 2.232692241668701\n",
      "epoch: 1 step: 1082, loss is 2.215843915939331\n",
      "epoch: 1 step: 1083, loss is 2.2137744426727295\n",
      "epoch: 1 step: 1084, loss is 2.2294716835021973\n",
      "epoch: 1 step: 1085, loss is 2.1748266220092773\n",
      "epoch: 1 step: 1086, loss is 2.238173723220825\n",
      "epoch: 1 step: 1087, loss is 2.181474208831787\n",
      "epoch: 1 step: 1088, loss is 2.2071385383605957\n",
      "epoch: 1 step: 1089, loss is 2.199389696121216\n",
      "epoch: 1 step: 1090, loss is 2.199705123901367\n",
      "epoch: 1 step: 1091, loss is 2.159764528274536\n",
      "epoch: 1 step: 1092, loss is 2.112344741821289\n",
      "epoch: 1 step: 1093, loss is 2.124537944793701\n",
      "epoch: 1 step: 1094, loss is 2.1368985176086426\n",
      "epoch: 1 step: 1095, loss is 2.0980122089385986\n",
      "epoch: 1 step: 1096, loss is 1.909751296043396\n",
      "epoch: 1 step: 1097, loss is 1.9560145139694214\n",
      "epoch: 1 step: 1098, loss is 1.9474706649780273\n",
      "epoch: 1 step: 1099, loss is 2.006364107131958\n",
      "epoch: 1 step: 1100, loss is 1.9745988845825195\n",
      "epoch: 1 step: 1101, loss is 1.9022406339645386\n",
      "epoch: 1 step: 1102, loss is 1.926944613456726\n",
      "epoch: 1 step: 1103, loss is 1.8270039558410645\n",
      "epoch: 1 step: 1104, loss is 1.9184650182724\n",
      "epoch: 1 step: 1105, loss is 1.655260443687439\n",
      "epoch: 1 step: 1106, loss is 1.940850019454956\n",
      "epoch: 1 step: 1107, loss is 1.7529109716415405\n",
      "epoch: 1 step: 1108, loss is 1.583454966545105\n",
      "epoch: 1 step: 1109, loss is 1.4872918128967285\n",
      "epoch: 1 step: 1110, loss is 1.9691170454025269\n",
      "epoch: 1 step: 1111, loss is 1.767999529838562\n",
      "epoch: 1 step: 1112, loss is 1.5230308771133423\n",
      "epoch: 1 step: 1113, loss is 1.3806337118148804\n",
      "epoch: 1 step: 1114, loss is 1.4970921277999878\n",
      "epoch: 1 step: 1115, loss is 1.8716732263565063\n",
      "epoch: 1 step: 1116, loss is 1.4411898851394653\n",
      "epoch: 1 step: 1117, loss is 1.2375720739364624\n",
      "epoch: 1 step: 1118, loss is 1.4495351314544678\n",
      "epoch: 1 step: 1119, loss is 1.2026515007019043\n",
      "epoch: 1 step: 1120, loss is 1.4445750713348389\n",
      "epoch: 1 step: 1121, loss is 1.3509938716888428\n",
      "epoch: 1 step: 1122, loss is 1.2456728219985962\n",
      "epoch: 1 step: 1123, loss is 1.1727097034454346\n",
      "epoch: 1 step: 1124, loss is 1.3198301792144775\n",
      "epoch: 1 step: 1125, loss is 1.0468231439590454\n",
      "epoch: 1 step: 1126, loss is 1.2613698244094849\n",
      "epoch: 1 step: 1127, loss is 0.8917912244796753\n",
      "epoch: 1 step: 1128, loss is 1.1468937397003174\n",
      "epoch: 1 step: 1129, loss is 1.314394474029541\n",
      "epoch: 1 step: 1130, loss is 1.138391375541687\n",
      "epoch: 1 step: 1131, loss is 1.9964944124221802\n",
      "epoch: 1 step: 1132, loss is 1.3275723457336426\n",
      "epoch: 1 step: 1133, loss is 1.5101650953292847\n",
      "epoch: 1 step: 1134, loss is 1.0131170749664307\n",
      "epoch: 1 step: 1135, loss is 1.317825198173523\n",
      "epoch: 1 step: 1136, loss is 0.8467440009117126\n",
      "epoch: 1 step: 1137, loss is 1.264206886291504\n",
      "epoch: 1 step: 1138, loss is 0.7520555853843689\n",
      "epoch: 1 step: 1139, loss is 0.9670925140380859\n",
      "epoch: 1 step: 1140, loss is 1.1095994710922241\n",
      "epoch: 1 step: 1141, loss is 0.6538688540458679\n",
      "epoch: 1 step: 1142, loss is 1.4021421670913696\n",
      "epoch: 1 step: 1143, loss is 0.8430449962615967\n",
      "epoch: 1 step: 1144, loss is 1.0971848964691162\n",
      "epoch: 1 step: 1145, loss is 0.7959765195846558\n",
      "epoch: 1 step: 1146, loss is 0.7321054935455322\n",
      "epoch: 1 step: 1147, loss is 1.1066385507583618\n",
      "epoch: 1 step: 1148, loss is 0.9159770607948303\n",
      "epoch: 1 step: 1149, loss is 0.986150324344635\n",
      "epoch: 1 step: 1150, loss is 0.5980088710784912\n",
      "epoch: 1 step: 1151, loss is 0.8456465601921082\n",
      "epoch: 1 step: 1152, loss is 0.8258177042007446\n",
      "epoch: 1 step: 1153, loss is 0.7968341112136841\n",
      "epoch: 1 step: 1154, loss is 0.9425435662269592\n",
      "epoch: 1 step: 1155, loss is 0.5298906564712524\n",
      "epoch: 1 step: 1156, loss is 0.9014537334442139\n",
      "epoch: 1 step: 1157, loss is 0.6368047595024109\n",
      "epoch: 1 step: 1158, loss is 0.8418726921081543\n",
      "epoch: 1 step: 1159, loss is 0.5659029483795166\n",
      "epoch: 1 step: 1160, loss is 0.8490630984306335\n",
      "epoch: 1 step: 1161, loss is 0.9428817629814148\n",
      "epoch: 1 step: 1162, loss is 1.5183769464492798\n",
      "epoch: 1 step: 1163, loss is 1.0533844232559204\n",
      "epoch: 1 step: 1164, loss is 0.8269651532173157\n",
      "epoch: 1 step: 1165, loss is 0.8550057411193848\n",
      "epoch: 1 step: 1166, loss is 0.7387174367904663\n",
      "epoch: 1 step: 1167, loss is 0.8461424708366394\n",
      "epoch: 1 step: 1168, loss is 0.8339353799819946\n",
      "epoch: 1 step: 1169, loss is 0.9971487522125244\n",
      "epoch: 1 step: 1170, loss is 0.8580813407897949\n",
      "epoch: 1 step: 1171, loss is 0.8083429932594299\n",
      "epoch: 1 step: 1172, loss is 0.6140406131744385\n",
      "epoch: 1 step: 1173, loss is 0.5287964344024658\n",
      "epoch: 1 step: 1174, loss is 0.8265654444694519\n",
      "epoch: 1 step: 1175, loss is 0.657328188419342\n",
      "epoch: 1 step: 1176, loss is 0.5985375642776489\n",
      "epoch: 1 step: 1177, loss is 0.7952749133110046\n",
      "epoch: 1 step: 1178, loss is 0.8602170348167419\n",
      "epoch: 1 step: 1179, loss is 0.5890927910804749\n",
      "epoch: 1 step: 1180, loss is 0.8354158997535706\n",
      "epoch: 1 step: 1181, loss is 0.41422125697135925\n",
      "epoch: 1 step: 1182, loss is 0.5788881778717041\n",
      "epoch: 1 step: 1183, loss is 0.6412881016731262\n",
      "epoch: 1 step: 1184, loss is 1.0485559701919556\n",
      "epoch: 1 step: 1185, loss is 0.592710018157959\n",
      "epoch: 1 step: 1186, loss is 1.076317310333252\n",
      "epoch: 1 step: 1187, loss is 1.6502633094787598\n",
      "epoch: 1 step: 1188, loss is 0.5460795164108276\n",
      "epoch: 1 step: 1189, loss is 0.7620092034339905\n",
      "epoch: 1 step: 1190, loss is 0.8304888606071472\n",
      "epoch: 1 step: 1191, loss is 0.9927152395248413\n",
      "epoch: 1 step: 1192, loss is 0.4802806079387665\n",
      "epoch: 1 step: 1193, loss is 1.1784234046936035\n",
      "epoch: 1 step: 1194, loss is 0.6450132727622986\n",
      "epoch: 1 step: 1195, loss is 0.6340383291244507\n",
      "epoch: 1 step: 1196, loss is 0.9145809412002563\n",
      "epoch: 1 step: 1197, loss is 0.66805499792099\n",
      "epoch: 1 step: 1198, loss is 0.8227717876434326\n",
      "epoch: 1 step: 1199, loss is 0.6241523623466492\n",
      "epoch: 1 step: 1200, loss is 0.8160483241081238\n",
      "epoch: 1 step: 1201, loss is 0.44638657569885254\n",
      "epoch: 1 step: 1202, loss is 0.6043736338615417\n",
      "epoch: 1 step: 1203, loss is 0.697798490524292\n",
      "epoch: 1 step: 1204, loss is 0.6600565910339355\n",
      "epoch: 1 step: 1205, loss is 0.506290853023529\n",
      "epoch: 1 step: 1206, loss is 0.5934325456619263\n",
      "epoch: 1 step: 1207, loss is 0.8500969409942627\n",
      "epoch: 1 step: 1208, loss is 0.5828840732574463\n",
      "epoch: 1 step: 1209, loss is 0.42606857419013977\n",
      "epoch: 1 step: 1210, loss is 0.32349610328674316\n",
      "epoch: 1 step: 1211, loss is 0.38826242089271545\n",
      "epoch: 1 step: 1212, loss is 0.6721563339233398\n",
      "epoch: 1 step: 1213, loss is 0.5425389409065247\n",
      "epoch: 1 step: 1214, loss is 0.6599020957946777\n",
      "epoch: 1 step: 1215, loss is 0.43296608328819275\n",
      "epoch: 1 step: 1216, loss is 0.3603428304195404\n",
      "epoch: 1 step: 1217, loss is 0.7974569201469421\n",
      "epoch: 1 step: 1218, loss is 0.28467699885368347\n",
      "epoch: 1 step: 1219, loss is 0.6438839435577393\n",
      "epoch: 1 step: 1220, loss is 0.5278400778770447\n",
      "epoch: 1 step: 1221, loss is 0.4555417001247406\n",
      "epoch: 1 step: 1222, loss is 0.32588550448417664\n",
      "epoch: 1 step: 1223, loss is 0.6223384737968445\n",
      "epoch: 1 step: 1224, loss is 0.2048976570367813\n",
      "epoch: 1 step: 1225, loss is 0.47620415687561035\n",
      "epoch: 1 step: 1226, loss is 0.28875666856765747\n",
      "epoch: 1 step: 1227, loss is 0.3076113760471344\n",
      "epoch: 1 step: 1228, loss is 0.44543740153312683\n",
      "epoch: 1 step: 1229, loss is 0.39895614981651306\n",
      "epoch: 1 step: 1230, loss is 0.3046815097332001\n",
      "epoch: 1 step: 1231, loss is 0.22197572886943817\n",
      "epoch: 1 step: 1232, loss is 0.3861357569694519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1233, loss is 0.6528486609458923\n",
      "epoch: 1 step: 1234, loss is 0.43434077501296997\n",
      "epoch: 1 step: 1235, loss is 0.671442985534668\n",
      "epoch: 1 step: 1236, loss is 0.4022964537143707\n",
      "epoch: 1 step: 1237, loss is 0.29868635535240173\n",
      "epoch: 1 step: 1238, loss is 0.2871129512786865\n",
      "epoch: 1 step: 1239, loss is 0.32641932368278503\n",
      "epoch: 1 step: 1240, loss is 0.2772595286369324\n",
      "epoch: 1 step: 1241, loss is 0.476924866437912\n",
      "epoch: 1 step: 1242, loss is 0.211623415350914\n",
      "epoch: 1 step: 1243, loss is 0.7279724478721619\n",
      "epoch: 1 step: 1244, loss is 0.27171021699905396\n",
      "epoch: 1 step: 1245, loss is 0.760542631149292\n",
      "epoch: 1 step: 1246, loss is 0.2652062475681305\n",
      "epoch: 1 step: 1247, loss is 0.8356221914291382\n",
      "epoch: 1 step: 1248, loss is 0.16872480511665344\n",
      "epoch: 1 step: 1249, loss is 0.4120922088623047\n",
      "epoch: 1 step: 1250, loss is 0.21625973284244537\n",
      "epoch: 1 step: 1251, loss is 0.21807588636875153\n",
      "epoch: 1 step: 1252, loss is 0.3030988574028015\n",
      "epoch: 1 step: 1253, loss is 0.33299607038497925\n",
      "epoch: 1 step: 1254, loss is 0.30149683356285095\n",
      "epoch: 1 step: 1255, loss is 0.2580288350582123\n",
      "epoch: 1 step: 1256, loss is 0.2101641297340393\n",
      "epoch: 1 step: 1257, loss is 0.43616485595703125\n",
      "epoch: 1 step: 1258, loss is 0.3026553690433502\n",
      "epoch: 1 step: 1259, loss is 0.4112374484539032\n",
      "epoch: 1 step: 1260, loss is 0.46378248929977417\n",
      "epoch: 1 step: 1261, loss is 0.4198088049888611\n",
      "epoch: 1 step: 1262, loss is 0.5365095138549805\n",
      "epoch: 1 step: 1263, loss is 0.3159683346748352\n",
      "epoch: 1 step: 1264, loss is 0.3604825437068939\n",
      "epoch: 1 step: 1265, loss is 0.4382183849811554\n",
      "epoch: 1 step: 1266, loss is 0.64070725440979\n",
      "epoch: 1 step: 1267, loss is 0.24392855167388916\n",
      "epoch: 1 step: 1268, loss is 0.45849910378456116\n",
      "epoch: 1 step: 1269, loss is 0.5222280025482178\n",
      "epoch: 1 step: 1270, loss is 0.18720246851444244\n",
      "epoch: 1 step: 1271, loss is 0.38696351647377014\n",
      "epoch: 1 step: 1272, loss is 0.4446135461330414\n",
      "epoch: 1 step: 1273, loss is 0.7067690491676331\n",
      "epoch: 1 step: 1274, loss is 0.12604619562625885\n",
      "epoch: 1 step: 1275, loss is 0.5458955764770508\n",
      "epoch: 1 step: 1276, loss is 0.24207761883735657\n",
      "epoch: 1 step: 1277, loss is 0.3766061067581177\n",
      "epoch: 1 step: 1278, loss is 0.44378897547721863\n",
      "epoch: 1 step: 1279, loss is 0.3121061623096466\n",
      "epoch: 1 step: 1280, loss is 0.16266852617263794\n",
      "epoch: 1 step: 1281, loss is 0.14713731408119202\n",
      "epoch: 1 step: 1282, loss is 0.23054008185863495\n",
      "epoch: 1 step: 1283, loss is 0.2435373216867447\n",
      "epoch: 1 step: 1284, loss is 0.24995222687721252\n",
      "epoch: 1 step: 1285, loss is 0.2477664351463318\n",
      "epoch: 1 step: 1286, loss is 0.15265391767024994\n",
      "epoch: 1 step: 1287, loss is 0.3209930658340454\n",
      "epoch: 1 step: 1288, loss is 0.393241286277771\n",
      "epoch: 1 step: 1289, loss is 0.2764289081096649\n",
      "epoch: 1 step: 1290, loss is 0.1263091117143631\n",
      "epoch: 1 step: 1291, loss is 0.11561813205480576\n",
      "epoch: 1 step: 1292, loss is 0.268202006816864\n",
      "epoch: 1 step: 1293, loss is 0.4409358501434326\n",
      "epoch: 1 step: 1294, loss is 0.30878186225891113\n",
      "epoch: 1 step: 1295, loss is 0.056088149547576904\n",
      "epoch: 1 step: 1296, loss is 0.07401260733604431\n",
      "epoch: 1 step: 1297, loss is 0.3556245267391205\n",
      "epoch: 1 step: 1298, loss is 0.2890051305294037\n",
      "epoch: 1 step: 1299, loss is 0.480047345161438\n",
      "epoch: 1 step: 1300, loss is 0.5880001187324524\n",
      "epoch: 1 step: 1301, loss is 0.420748233795166\n",
      "epoch: 1 step: 1302, loss is 0.34136271476745605\n",
      "epoch: 1 step: 1303, loss is 0.5054815411567688\n",
      "epoch: 1 step: 1304, loss is 0.4039195477962494\n",
      "epoch: 1 step: 1305, loss is 0.3678272068500519\n",
      "epoch: 1 step: 1306, loss is 0.7562412619590759\n",
      "epoch: 1 step: 1307, loss is 0.3879343867301941\n",
      "epoch: 1 step: 1308, loss is 0.262484073638916\n",
      "epoch: 1 step: 1309, loss is 0.13210999965667725\n",
      "epoch: 1 step: 1310, loss is 0.3575933277606964\n",
      "epoch: 1 step: 1311, loss is 0.3536391258239746\n",
      "epoch: 1 step: 1312, loss is 0.38233113288879395\n",
      "epoch: 1 step: 1313, loss is 0.39994651079177856\n",
      "epoch: 1 step: 1314, loss is 0.18711407482624054\n",
      "epoch: 1 step: 1315, loss is 0.3280373215675354\n",
      "epoch: 1 step: 1316, loss is 0.18400335311889648\n",
      "epoch: 1 step: 1317, loss is 0.13311436772346497\n",
      "epoch: 1 step: 1318, loss is 0.3201196789741516\n",
      "epoch: 1 step: 1319, loss is 0.23380689322948456\n",
      "epoch: 1 step: 1320, loss is 0.4244152009487152\n",
      "epoch: 1 step: 1321, loss is 0.20473065972328186\n",
      "epoch: 1 step: 1322, loss is 0.15362074971199036\n",
      "epoch: 1 step: 1323, loss is 0.1594908982515335\n",
      "epoch: 1 step: 1324, loss is 0.34529218077659607\n",
      "epoch: 1 step: 1325, loss is 0.05370552837848663\n",
      "epoch: 1 step: 1326, loss is 0.23420198261737823\n",
      "epoch: 1 step: 1327, loss is 0.17196452617645264\n",
      "epoch: 1 step: 1328, loss is 0.09598448127508163\n",
      "epoch: 1 step: 1329, loss is 0.07738568633794785\n",
      "epoch: 1 step: 1330, loss is 0.3388998210430145\n",
      "epoch: 1 step: 1331, loss is 0.25243324041366577\n",
      "epoch: 1 step: 1332, loss is 0.11100469529628754\n",
      "epoch: 1 step: 1333, loss is 0.08805039525032043\n",
      "epoch: 1 step: 1334, loss is 0.49548354744911194\n",
      "epoch: 1 step: 1335, loss is 0.35042309761047363\n",
      "epoch: 1 step: 1336, loss is 0.30522748827934265\n",
      "epoch: 1 step: 1337, loss is 0.10583552718162537\n",
      "epoch: 1 step: 1338, loss is 0.02964191883802414\n",
      "epoch: 1 step: 1339, loss is 0.35072824358940125\n",
      "epoch: 1 step: 1340, loss is 0.045404061675071716\n",
      "epoch: 1 step: 1341, loss is 0.04474613070487976\n",
      "epoch: 1 step: 1342, loss is 0.23189231753349304\n",
      "epoch: 1 step: 1343, loss is 0.20248769223690033\n",
      "epoch: 1 step: 1344, loss is 0.07452069967985153\n",
      "epoch: 1 step: 1345, loss is 0.1920556128025055\n",
      "epoch: 1 step: 1346, loss is 0.2805350422859192\n",
      "epoch: 1 step: 1347, loss is 0.15604060888290405\n",
      "epoch: 1 step: 1348, loss is 0.295899897813797\n",
      "epoch: 1 step: 1349, loss is 0.3813275396823883\n",
      "epoch: 1 step: 1350, loss is 0.19120024144649506\n",
      "epoch: 1 step: 1351, loss is 0.5495409965515137\n",
      "epoch: 1 step: 1352, loss is 0.36759674549102783\n",
      "epoch: 1 step: 1353, loss is 0.2024165689945221\n",
      "epoch: 1 step: 1354, loss is 0.3008252680301666\n",
      "epoch: 1 step: 1355, loss is 0.29346588253974915\n",
      "epoch: 1 step: 1356, loss is 0.15267464518547058\n",
      "epoch: 1 step: 1357, loss is 0.33567219972610474\n",
      "epoch: 1 step: 1358, loss is 0.2822352349758148\n",
      "epoch: 1 step: 1359, loss is 0.08021336793899536\n",
      "epoch: 1 step: 1360, loss is 0.2618682384490967\n",
      "epoch: 1 step: 1361, loss is 0.13852494955062866\n",
      "epoch: 1 step: 1362, loss is 0.28992751240730286\n",
      "epoch: 1 step: 1363, loss is 0.07434908300638199\n",
      "epoch: 1 step: 1364, loss is 0.15967635810375214\n",
      "epoch: 1 step: 1365, loss is 0.1694306582212448\n",
      "epoch: 1 step: 1366, loss is 0.490894615650177\n",
      "epoch: 1 step: 1367, loss is 0.053835429251194\n",
      "epoch: 1 step: 1368, loss is 0.2519836723804474\n",
      "epoch: 1 step: 1369, loss is 0.14722804725170135\n",
      "epoch: 1 step: 1370, loss is 0.24713510274887085\n",
      "epoch: 1 step: 1371, loss is 0.24709714949131012\n",
      "epoch: 1 step: 1372, loss is 0.17268596589565277\n",
      "epoch: 1 step: 1373, loss is 0.6431806683540344\n",
      "epoch: 1 step: 1374, loss is 0.40658649802207947\n",
      "epoch: 1 step: 1375, loss is 0.03897839039564133\n",
      "epoch: 1 step: 1376, loss is 0.16067717969417572\n",
      "epoch: 1 step: 1377, loss is 0.16267231106758118\n",
      "epoch: 1 step: 1378, loss is 0.28541338443756104\n",
      "epoch: 1 step: 1379, loss is 0.15078413486480713\n",
      "epoch: 1 step: 1380, loss is 0.11651258170604706\n",
      "epoch: 1 step: 1381, loss is 0.12417466938495636\n",
      "epoch: 1 step: 1382, loss is 0.009965273551642895\n",
      "epoch: 1 step: 1383, loss is 0.27027666568756104\n",
      "epoch: 1 step: 1384, loss is 0.27033793926239014\n",
      "epoch: 1 step: 1385, loss is 0.5016142129898071\n",
      "epoch: 1 step: 1386, loss is 0.1256590336561203\n",
      "epoch: 1 step: 1387, loss is 0.7132989764213562\n",
      "epoch: 1 step: 1388, loss is 0.23210962116718292\n",
      "epoch: 1 step: 1389, loss is 0.4591802954673767\n",
      "epoch: 1 step: 1390, loss is 0.15714071691036224\n",
      "epoch: 1 step: 1391, loss is 0.3835541307926178\n",
      "epoch: 1 step: 1392, loss is 0.07301013916730881\n",
      "epoch: 1 step: 1393, loss is 0.2800518274307251\n",
      "epoch: 1 step: 1394, loss is 0.4145517349243164\n",
      "epoch: 1 step: 1395, loss is 0.20079517364501953\n",
      "epoch: 1 step: 1396, loss is 0.21914705634117126\n",
      "epoch: 1 step: 1397, loss is 0.3517056405544281\n",
      "epoch: 1 step: 1398, loss is 0.19615575671195984\n",
      "epoch: 1 step: 1399, loss is 0.4375786781311035\n",
      "epoch: 1 step: 1400, loss is 0.2380308210849762\n",
      "epoch: 1 step: 1401, loss is 0.1449064165353775\n",
      "epoch: 1 step: 1402, loss is 0.332611620426178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1403, loss is 0.16834965348243713\n",
      "epoch: 1 step: 1404, loss is 0.3070136606693268\n",
      "epoch: 1 step: 1405, loss is 0.2969735562801361\n",
      "epoch: 1 step: 1406, loss is 0.3384643793106079\n",
      "epoch: 1 step: 1407, loss is 0.11425626277923584\n",
      "epoch: 1 step: 1408, loss is 0.196400448679924\n",
      "epoch: 1 step: 1409, loss is 0.08856234699487686\n",
      "epoch: 1 step: 1410, loss is 0.12501047551631927\n",
      "epoch: 1 step: 1411, loss is 0.164876788854599\n",
      "epoch: 1 step: 1412, loss is 0.17107968032360077\n",
      "epoch: 1 step: 1413, loss is 0.22010537981987\n",
      "epoch: 1 step: 1414, loss is 0.46276402473449707\n",
      "epoch: 1 step: 1415, loss is 0.43027836084365845\n",
      "epoch: 1 step: 1416, loss is 0.42547163367271423\n",
      "epoch: 1 step: 1417, loss is 0.27498388290405273\n",
      "epoch: 1 step: 1418, loss is 0.2856829762458801\n",
      "epoch: 1 step: 1419, loss is 0.060898225754499435\n",
      "epoch: 1 step: 1420, loss is 0.5560422539710999\n",
      "epoch: 1 step: 1421, loss is 0.21752509474754333\n",
      "epoch: 1 step: 1422, loss is 0.2478926181793213\n",
      "epoch: 1 step: 1423, loss is 0.12987592816352844\n",
      "epoch: 1 step: 1424, loss is 0.23574504256248474\n",
      "epoch: 1 step: 1425, loss is 0.273248553276062\n",
      "epoch: 1 step: 1426, loss is 0.3562504053115845\n",
      "epoch: 1 step: 1427, loss is 0.2127232700586319\n",
      "epoch: 1 step: 1428, loss is 0.36386725306510925\n",
      "epoch: 1 step: 1429, loss is 0.21820412576198578\n",
      "epoch: 1 step: 1430, loss is 0.42411115765571594\n",
      "epoch: 1 step: 1431, loss is 0.32885584235191345\n",
      "epoch: 1 step: 1432, loss is 0.06986404210329056\n",
      "epoch: 1 step: 1433, loss is 0.10184335708618164\n",
      "epoch: 1 step: 1434, loss is 0.15339981019496918\n",
      "epoch: 1 step: 1435, loss is 0.19841915369033813\n",
      "epoch: 1 step: 1436, loss is 0.33454760909080505\n",
      "epoch: 1 step: 1437, loss is 0.13383382558822632\n",
      "epoch: 1 step: 1438, loss is 0.12437400221824646\n",
      "epoch: 1 step: 1439, loss is 0.19888687133789062\n",
      "epoch: 1 step: 1440, loss is 0.1934794932603836\n",
      "epoch: 1 step: 1441, loss is 0.2227698415517807\n",
      "epoch: 1 step: 1442, loss is 0.10168378800153732\n",
      "epoch: 1 step: 1443, loss is 0.10716322809457779\n",
      "epoch: 1 step: 1444, loss is 0.2038346230983734\n",
      "epoch: 1 step: 1445, loss is 0.19834132492542267\n",
      "epoch: 1 step: 1446, loss is 0.18000037968158722\n",
      "epoch: 1 step: 1447, loss is 0.15613093972206116\n",
      "epoch: 1 step: 1448, loss is 0.05814063921570778\n",
      "epoch: 1 step: 1449, loss is 0.12128578126430511\n",
      "epoch: 1 step: 1450, loss is 0.19852039217948914\n",
      "epoch: 1 step: 1451, loss is 0.24436278641223907\n",
      "epoch: 1 step: 1452, loss is 0.3324602544307709\n",
      "epoch: 1 step: 1453, loss is 0.321792870759964\n",
      "epoch: 1 step: 1454, loss is 0.18787409365177155\n",
      "epoch: 1 step: 1455, loss is 0.07598798722028732\n",
      "epoch: 1 step: 1456, loss is 0.07760754972696304\n",
      "epoch: 1 step: 1457, loss is 0.07868485152721405\n",
      "epoch: 1 step: 1458, loss is 0.2653977870941162\n",
      "epoch: 1 step: 1459, loss is 0.08346555382013321\n",
      "epoch: 1 step: 1460, loss is 0.4571339190006256\n",
      "epoch: 1 step: 1461, loss is 0.10641135275363922\n",
      "epoch: 1 step: 1462, loss is 0.2033475935459137\n",
      "epoch: 1 step: 1463, loss is 0.08137237280607224\n",
      "epoch: 1 step: 1464, loss is 0.11955326795578003\n",
      "epoch: 1 step: 1465, loss is 0.1915525197982788\n",
      "epoch: 1 step: 1466, loss is 0.38111546635627747\n",
      "epoch: 1 step: 1467, loss is 0.18827813863754272\n",
      "epoch: 1 step: 1468, loss is 0.07391394674777985\n",
      "epoch: 1 step: 1469, loss is 0.11355338990688324\n",
      "epoch: 1 step: 1470, loss is 0.34108880162239075\n",
      "epoch: 1 step: 1471, loss is 0.04123872146010399\n",
      "epoch: 1 step: 1472, loss is 0.09837105870246887\n",
      "epoch: 1 step: 1473, loss is 0.19222095608711243\n",
      "epoch: 1 step: 1474, loss is 0.11304828524589539\n",
      "epoch: 1 step: 1475, loss is 0.11404857784509659\n",
      "epoch: 1 step: 1476, loss is 0.1771402359008789\n",
      "epoch: 1 step: 1477, loss is 0.2647016644477844\n",
      "epoch: 1 step: 1478, loss is 0.2602905035018921\n",
      "epoch: 1 step: 1479, loss is 0.14972642064094543\n",
      "epoch: 1 step: 1480, loss is 0.2604072391986847\n",
      "epoch: 1 step: 1481, loss is 0.535065770149231\n",
      "epoch: 1 step: 1482, loss is 0.04237325116991997\n",
      "epoch: 1 step: 1483, loss is 0.43269890546798706\n",
      "epoch: 1 step: 1484, loss is 0.26822569966316223\n",
      "epoch: 1 step: 1485, loss is 0.250964492559433\n",
      "epoch: 1 step: 1486, loss is 0.32984429597854614\n",
      "epoch: 1 step: 1487, loss is 0.06724438071250916\n",
      "epoch: 1 step: 1488, loss is 0.031192367896437645\n",
      "epoch: 1 step: 1489, loss is 0.2800477147102356\n",
      "epoch: 1 step: 1490, loss is 0.21632860600948334\n",
      "epoch: 1 step: 1491, loss is 0.11629745364189148\n",
      "epoch: 1 step: 1492, loss is 0.23180292546749115\n",
      "epoch: 1 step: 1493, loss is 0.06464944779872894\n",
      "epoch: 1 step: 1494, loss is 0.23316358029842377\n",
      "epoch: 1 step: 1495, loss is 0.3576115369796753\n",
      "epoch: 1 step: 1496, loss is 0.15261735022068024\n",
      "epoch: 1 step: 1497, loss is 0.0879550352692604\n",
      "epoch: 1 step: 1498, loss is 0.23714525997638702\n",
      "epoch: 1 step: 1499, loss is 0.14230455458164215\n",
      "epoch: 1 step: 1500, loss is 0.1376381665468216\n",
      "epoch: 1 step: 1501, loss is 0.14054587483406067\n",
      "epoch: 1 step: 1502, loss is 0.1942434459924698\n",
      "epoch: 1 step: 1503, loss is 0.2526463270187378\n",
      "epoch: 1 step: 1504, loss is 0.24491123855113983\n",
      "epoch: 1 step: 1505, loss is 0.14575573801994324\n",
      "epoch: 1 step: 1506, loss is 0.45865899324417114\n",
      "epoch: 1 step: 1507, loss is 0.13470976054668427\n",
      "epoch: 1 step: 1508, loss is 0.028472086414694786\n",
      "epoch: 1 step: 1509, loss is 0.06087586656212807\n",
      "epoch: 1 step: 1510, loss is 0.1336907595396042\n",
      "epoch: 1 step: 1511, loss is 0.06745365262031555\n",
      "epoch: 1 step: 1512, loss is 0.2686662971973419\n",
      "epoch: 1 step: 1513, loss is 0.13591432571411133\n",
      "epoch: 1 step: 1514, loss is 0.4029237627983093\n",
      "epoch: 1 step: 1515, loss is 0.20347397029399872\n",
      "epoch: 1 step: 1516, loss is 0.24251188337802887\n",
      "epoch: 1 step: 1517, loss is 0.2671665549278259\n",
      "epoch: 1 step: 1518, loss is 0.10860579460859299\n",
      "epoch: 1 step: 1519, loss is 0.10065226256847382\n",
      "epoch: 1 step: 1520, loss is 0.26600638031959534\n",
      "epoch: 1 step: 1521, loss is 0.11675577610731125\n",
      "epoch: 1 step: 1522, loss is 0.08833806961774826\n",
      "epoch: 1 step: 1523, loss is 0.1040673777461052\n",
      "epoch: 1 step: 1524, loss is 0.19265982508659363\n",
      "epoch: 1 step: 1525, loss is 0.2752682566642761\n",
      "epoch: 1 step: 1526, loss is 0.4522353410720825\n",
      "epoch: 1 step: 1527, loss is 0.2023705691099167\n",
      "epoch: 1 step: 1528, loss is 0.2641518712043762\n",
      "epoch: 1 step: 1529, loss is 0.23679296672344208\n",
      "epoch: 1 step: 1530, loss is 0.03724127262830734\n",
      "epoch: 1 step: 1531, loss is 0.09092579782009125\n",
      "epoch: 1 step: 1532, loss is 0.1529494971036911\n",
      "epoch: 1 step: 1533, loss is 0.3336610496044159\n",
      "epoch: 1 step: 1534, loss is 0.11768858879804611\n",
      "epoch: 1 step: 1535, loss is 0.06643008440732956\n",
      "epoch: 1 step: 1536, loss is 0.19858461618423462\n",
      "epoch: 1 step: 1537, loss is 0.13523982465267181\n",
      "epoch: 1 step: 1538, loss is 0.06091944873332977\n",
      "epoch: 1 step: 1539, loss is 0.2296915203332901\n",
      "epoch: 1 step: 1540, loss is 0.18542255461215973\n",
      "epoch: 1 step: 1541, loss is 0.2940046489238739\n",
      "epoch: 1 step: 1542, loss is 0.18325750529766083\n",
      "epoch: 1 step: 1543, loss is 0.054117314517498016\n",
      "epoch: 1 step: 1544, loss is 0.2776598036289215\n",
      "epoch: 1 step: 1545, loss is 0.1307850182056427\n",
      "epoch: 1 step: 1546, loss is 0.20402656495571136\n",
      "epoch: 1 step: 1547, loss is 0.10989098995923996\n",
      "epoch: 1 step: 1548, loss is 0.1199783980846405\n",
      "epoch: 1 step: 1549, loss is 0.2988651394844055\n",
      "epoch: 1 step: 1550, loss is 0.14532677829265594\n",
      "epoch: 1 step: 1551, loss is 0.10685572028160095\n",
      "epoch: 1 step: 1552, loss is 0.0679180771112442\n",
      "epoch: 1 step: 1553, loss is 0.12059854716062546\n",
      "epoch: 1 step: 1554, loss is 0.07702147960662842\n",
      "epoch: 1 step: 1555, loss is 0.009440653957426548\n",
      "epoch: 1 step: 1556, loss is 0.032245274633169174\n",
      "epoch: 1 step: 1557, loss is 0.3088306188583374\n",
      "epoch: 1 step: 1558, loss is 0.195736363530159\n",
      "epoch: 1 step: 1559, loss is 0.334878146648407\n",
      "epoch: 1 step: 1560, loss is 0.16661863029003143\n",
      "epoch: 1 step: 1561, loss is 0.12261977046728134\n",
      "epoch: 1 step: 1562, loss is 0.08554136753082275\n",
      "epoch: 1 step: 1563, loss is 0.22784212231636047\n",
      "epoch: 1 step: 1564, loss is 0.033649884164333344\n",
      "epoch: 1 step: 1565, loss is 0.19365625083446503\n",
      "epoch: 1 step: 1566, loss is 0.12971249222755432\n",
      "epoch: 1 step: 1567, loss is 0.04749739542603493\n",
      "epoch: 1 step: 1568, loss is 0.09670861065387726\n",
      "epoch: 1 step: 1569, loss is 0.11016558855772018\n",
      "epoch: 1 step: 1570, loss is 0.25809305906295776\n",
      "epoch: 1 step: 1571, loss is 0.3327515721321106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1572, loss is 0.028620189055800438\n",
      "epoch: 1 step: 1573, loss is 0.11335963010787964\n",
      "epoch: 1 step: 1574, loss is 0.04047133028507233\n",
      "epoch: 1 step: 1575, loss is 0.06826715916395187\n",
      "epoch: 1 step: 1576, loss is 0.06811244040727615\n",
      "epoch: 1 step: 1577, loss is 0.4699014127254486\n",
      "epoch: 1 step: 1578, loss is 0.054742250591516495\n",
      "epoch: 1 step: 1579, loss is 0.017278922721743584\n",
      "epoch: 1 step: 1580, loss is 0.25631579756736755\n",
      "epoch: 1 step: 1581, loss is 0.0387328676879406\n",
      "epoch: 1 step: 1582, loss is 0.18620191514492035\n",
      "epoch: 1 step: 1583, loss is 0.026882587000727654\n",
      "epoch: 1 step: 1584, loss is 0.021693237125873566\n",
      "epoch: 1 step: 1585, loss is 0.18536056578159332\n",
      "epoch: 1 step: 1586, loss is 0.02796868421137333\n",
      "epoch: 1 step: 1587, loss is 0.013030203990638256\n",
      "epoch: 1 step: 1588, loss is 0.01382145844399929\n",
      "epoch: 1 step: 1589, loss is 0.06778298318386078\n",
      "epoch: 1 step: 1590, loss is 0.009744333103299141\n",
      "epoch: 1 step: 1591, loss is 0.07657015323638916\n",
      "epoch: 1 step: 1592, loss is 0.226080521941185\n",
      "epoch: 1 step: 1593, loss is 0.050090279430150986\n",
      "epoch: 1 step: 1594, loss is 0.15977764129638672\n",
      "epoch: 1 step: 1595, loss is 0.06692057847976685\n",
      "epoch: 1 step: 1596, loss is 0.1253182590007782\n",
      "epoch: 1 step: 1597, loss is 0.07750105857849121\n",
      "epoch: 1 step: 1598, loss is 0.08755263686180115\n",
      "epoch: 1 step: 1599, loss is 0.23790018260478973\n",
      "epoch: 1 step: 1600, loss is 0.154706671833992\n",
      "epoch: 1 step: 1601, loss is 0.07163269072771072\n",
      "epoch: 1 step: 1602, loss is 0.14736559987068176\n",
      "epoch: 1 step: 1603, loss is 0.0421835333108902\n",
      "epoch: 1 step: 1604, loss is 0.04709189757704735\n",
      "epoch: 1 step: 1605, loss is 0.13166159391403198\n",
      "epoch: 1 step: 1606, loss is 0.09745139628648758\n",
      "epoch: 1 step: 1607, loss is 0.2251599282026291\n",
      "epoch: 1 step: 1608, loss is 0.2572300136089325\n",
      "epoch: 1 step: 1609, loss is 0.1046440377831459\n",
      "epoch: 1 step: 1610, loss is 0.08644573390483856\n",
      "epoch: 1 step: 1611, loss is 0.027229314669966698\n",
      "epoch: 1 step: 1612, loss is 0.015766579657793045\n",
      "epoch: 1 step: 1613, loss is 0.19237853586673737\n",
      "epoch: 1 step: 1614, loss is 0.38356491923332214\n",
      "epoch: 1 step: 1615, loss is 0.019975604489445686\n",
      "epoch: 1 step: 1616, loss is 0.03722377121448517\n",
      "epoch: 1 step: 1617, loss is 0.35226917266845703\n",
      "epoch: 1 step: 1618, loss is 0.04119851812720299\n",
      "epoch: 1 step: 1619, loss is 0.27015456557273865\n",
      "epoch: 1 step: 1620, loss is 0.30051764845848083\n",
      "epoch: 1 step: 1621, loss is 0.07384373247623444\n",
      "epoch: 1 step: 1622, loss is 0.11203885823488235\n",
      "epoch: 1 step: 1623, loss is 0.07946634292602539\n",
      "epoch: 1 step: 1624, loss is 0.12820525467395782\n",
      "epoch: 1 step: 1625, loss is 0.39087119698524475\n",
      "epoch: 1 step: 1626, loss is 0.07937874644994736\n",
      "epoch: 1 step: 1627, loss is 0.10790763050317764\n",
      "epoch: 1 step: 1628, loss is 0.1773117631673813\n",
      "epoch: 1 step: 1629, loss is 0.08883118629455566\n",
      "epoch: 1 step: 1630, loss is 0.11916700005531311\n",
      "epoch: 1 step: 1631, loss is 0.17510390281677246\n",
      "epoch: 1 step: 1632, loss is 0.05838709697127342\n",
      "epoch: 1 step: 1633, loss is 0.3026091158390045\n",
      "epoch: 1 step: 1634, loss is 0.16167262196540833\n",
      "epoch: 1 step: 1635, loss is 0.30498525500297546\n",
      "epoch: 1 step: 1636, loss is 0.19551557302474976\n",
      "epoch: 1 step: 1637, loss is 0.02761554904282093\n",
      "epoch: 1 step: 1638, loss is 0.19576139748096466\n",
      "epoch: 1 step: 1639, loss is 0.26473167538642883\n",
      "epoch: 1 step: 1640, loss is 0.29213038086891174\n",
      "epoch: 1 step: 1641, loss is 0.1658252626657486\n",
      "epoch: 1 step: 1642, loss is 0.15887512266635895\n",
      "epoch: 1 step: 1643, loss is 0.08547119051218033\n",
      "epoch: 1 step: 1644, loss is 0.03921093791723251\n",
      "epoch: 1 step: 1645, loss is 0.20014356076717377\n",
      "epoch: 1 step: 1646, loss is 0.14296941459178925\n",
      "epoch: 1 step: 1647, loss is 0.3505362272262573\n",
      "epoch: 1 step: 1648, loss is 0.24170298874378204\n",
      "epoch: 1 step: 1649, loss is 0.09480367600917816\n",
      "epoch: 1 step: 1650, loss is 0.11488018929958344\n",
      "epoch: 1 step: 1651, loss is 0.22951604425907135\n",
      "epoch: 1 step: 1652, loss is 0.15092450380325317\n",
      "epoch: 1 step: 1653, loss is 0.03173879161477089\n",
      "epoch: 1 step: 1654, loss is 0.2142471969127655\n",
      "epoch: 1 step: 1655, loss is 0.20542313158512115\n",
      "epoch: 1 step: 1656, loss is 0.158680722117424\n",
      "epoch: 1 step: 1657, loss is 0.013783307746052742\n",
      "epoch: 1 step: 1658, loss is 0.11775483936071396\n",
      "epoch: 1 step: 1659, loss is 0.1242590919137001\n",
      "epoch: 1 step: 1660, loss is 0.129424050450325\n",
      "epoch: 1 step: 1661, loss is 0.11296132206916809\n",
      "epoch: 1 step: 1662, loss is 0.13897857069969177\n",
      "epoch: 1 step: 1663, loss is 0.05605734884738922\n",
      "epoch: 1 step: 1664, loss is 0.3081246614456177\n",
      "epoch: 1 step: 1665, loss is 0.07764126360416412\n",
      "epoch: 1 step: 1666, loss is 0.29934990406036377\n",
      "epoch: 1 step: 1667, loss is 0.09941740334033966\n",
      "epoch: 1 step: 1668, loss is 0.025047382339835167\n",
      "epoch: 1 step: 1669, loss is 0.32100462913513184\n",
      "epoch: 1 step: 1670, loss is 0.06387383490800858\n",
      "epoch: 1 step: 1671, loss is 0.2327866107225418\n",
      "epoch: 1 step: 1672, loss is 0.06673633307218552\n",
      "epoch: 1 step: 1673, loss is 0.22105489671230316\n",
      "epoch: 1 step: 1674, loss is 0.23443983495235443\n",
      "epoch: 1 step: 1675, loss is 0.07094272971153259\n",
      "epoch: 1 step: 1676, loss is 0.18505868315696716\n",
      "epoch: 1 step: 1677, loss is 0.3825943171977997\n",
      "epoch: 1 step: 1678, loss is 0.06482478231191635\n",
      "epoch: 1 step: 1679, loss is 0.42338234186172485\n",
      "epoch: 1 step: 1680, loss is 0.37322667241096497\n",
      "epoch: 1 step: 1681, loss is 0.10510917752981186\n",
      "epoch: 1 step: 1682, loss is 0.05363934487104416\n",
      "epoch: 1 step: 1683, loss is 0.053960032761096954\n",
      "epoch: 1 step: 1684, loss is 0.21782134473323822\n",
      "epoch: 1 step: 1685, loss is 0.16164472699165344\n",
      "epoch: 1 step: 1686, loss is 0.1505589783191681\n",
      "epoch: 1 step: 1687, loss is 0.18540242314338684\n",
      "epoch: 1 step: 1688, loss is 0.4222949147224426\n",
      "epoch: 1 step: 1689, loss is 0.19806842505931854\n",
      "epoch: 1 step: 1690, loss is 0.15462344884872437\n",
      "epoch: 1 step: 1691, loss is 0.12123525887727737\n",
      "epoch: 1 step: 1692, loss is 0.09465556591749191\n",
      "epoch: 1 step: 1693, loss is 0.1839883178472519\n",
      "epoch: 1 step: 1694, loss is 0.20952148735523224\n",
      "epoch: 1 step: 1695, loss is 0.06509990990161896\n",
      "epoch: 1 step: 1696, loss is 0.06000594422221184\n",
      "epoch: 1 step: 1697, loss is 0.059479326009750366\n",
      "epoch: 1 step: 1698, loss is 0.07260922342538834\n",
      "epoch: 1 step: 1699, loss is 0.02524378150701523\n",
      "epoch: 1 step: 1700, loss is 0.191812202334404\n",
      "epoch: 1 step: 1701, loss is 0.029592828825116158\n",
      "epoch: 1 step: 1702, loss is 0.17720572650432587\n",
      "epoch: 1 step: 1703, loss is 0.11398086696863174\n",
      "epoch: 1 step: 1704, loss is 0.044418685138225555\n",
      "epoch: 1 step: 1705, loss is 0.13671061396598816\n",
      "epoch: 1 step: 1706, loss is 0.052076615393161774\n",
      "epoch: 1 step: 1707, loss is 0.1280296891927719\n",
      "epoch: 1 step: 1708, loss is 0.19078326225280762\n",
      "epoch: 1 step: 1709, loss is 0.10209840536117554\n",
      "epoch: 1 step: 1710, loss is 0.41282933950424194\n",
      "epoch: 1 step: 1711, loss is 0.19307032227516174\n",
      "epoch: 1 step: 1712, loss is 0.37595275044441223\n",
      "epoch: 1 step: 1713, loss is 0.06976421922445297\n",
      "epoch: 1 step: 1714, loss is 0.18822166323661804\n",
      "epoch: 1 step: 1715, loss is 0.5188669562339783\n",
      "epoch: 1 step: 1716, loss is 0.06790735572576523\n",
      "epoch: 1 step: 1717, loss is 0.03574137017130852\n",
      "epoch: 1 step: 1718, loss is 0.1709883213043213\n",
      "epoch: 1 step: 1719, loss is 0.1397828459739685\n",
      "epoch: 1 step: 1720, loss is 0.04396903142333031\n",
      "epoch: 1 step: 1721, loss is 0.24202606081962585\n",
      "epoch: 1 step: 1722, loss is 0.08939401060342789\n",
      "epoch: 1 step: 1723, loss is 0.36766940355300903\n",
      "epoch: 1 step: 1724, loss is 0.3098863661289215\n",
      "epoch: 1 step: 1725, loss is 0.25816667079925537\n",
      "epoch: 1 step: 1726, loss is 0.4492284953594208\n",
      "epoch: 1 step: 1727, loss is 0.14588695764541626\n",
      "epoch: 1 step: 1728, loss is 0.05313396826386452\n",
      "epoch: 1 step: 1729, loss is 0.12211233377456665\n",
      "epoch: 1 step: 1730, loss is 0.28116849064826965\n",
      "epoch: 1 step: 1731, loss is 0.07539112865924835\n",
      "epoch: 1 step: 1732, loss is 0.03317868709564209\n",
      "epoch: 1 step: 1733, loss is 0.055368151515722275\n",
      "epoch: 1 step: 1734, loss is 0.287134051322937\n",
      "epoch: 1 step: 1735, loss is 0.08709590137004852\n",
      "epoch: 1 step: 1736, loss is 0.16809682548046112\n",
      "epoch: 1 step: 1737, loss is 0.056692901998758316\n",
      "epoch: 1 step: 1738, loss is 0.10956323146820068\n",
      "epoch: 1 step: 1739, loss is 0.03314860910177231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1740, loss is 0.027233684435486794\n",
      "epoch: 1 step: 1741, loss is 0.05598892644047737\n",
      "epoch: 1 step: 1742, loss is 0.10664793103933334\n",
      "epoch: 1 step: 1743, loss is 0.10292337089776993\n",
      "epoch: 1 step: 1744, loss is 0.21761742234230042\n",
      "epoch: 1 step: 1745, loss is 0.17414402961730957\n",
      "epoch: 1 step: 1746, loss is 0.15943463146686554\n",
      "epoch: 1 step: 1747, loss is 0.14017193019390106\n",
      "epoch: 1 step: 1748, loss is 0.039394207298755646\n",
      "epoch: 1 step: 1749, loss is 0.13848736882209778\n",
      "epoch: 1 step: 1750, loss is 0.10443749278783798\n",
      "epoch: 1 step: 1751, loss is 0.16852238774299622\n",
      "epoch: 1 step: 1752, loss is 0.161595419049263\n",
      "epoch: 1 step: 1753, loss is 0.054970186203718185\n",
      "epoch: 1 step: 1754, loss is 0.12614603340625763\n",
      "epoch: 1 step: 1755, loss is 0.31639429926872253\n",
      "epoch: 1 step: 1756, loss is 0.06354096531867981\n",
      "epoch: 1 step: 1757, loss is 0.022911978885531425\n",
      "epoch: 1 step: 1758, loss is 0.3684399724006653\n",
      "epoch: 1 step: 1759, loss is 0.13949553668498993\n",
      "epoch: 1 step: 1760, loss is 0.022204946726560593\n",
      "epoch: 1 step: 1761, loss is 0.13714246451854706\n",
      "epoch: 1 step: 1762, loss is 0.23853066563606262\n",
      "epoch: 1 step: 1763, loss is 0.06755619496107101\n",
      "epoch: 1 step: 1764, loss is 0.026348216459155083\n",
      "epoch: 1 step: 1765, loss is 0.00900811143219471\n",
      "epoch: 1 step: 1766, loss is 0.21833908557891846\n",
      "epoch: 1 step: 1767, loss is 0.12998704612255096\n",
      "epoch: 1 step: 1768, loss is 0.16548633575439453\n",
      "epoch: 1 step: 1769, loss is 0.07333958894014359\n",
      "epoch: 1 step: 1770, loss is 0.17346975207328796\n",
      "epoch: 1 step: 1771, loss is 0.13568006455898285\n",
      "epoch: 1 step: 1772, loss is 0.23480811715126038\n",
      "epoch: 1 step: 1773, loss is 0.29031434655189514\n",
      "epoch: 1 step: 1774, loss is 0.03146391361951828\n",
      "epoch: 1 step: 1775, loss is 0.2523338496685028\n",
      "epoch: 1 step: 1776, loss is 0.1257888525724411\n",
      "epoch: 1 step: 1777, loss is 0.18415473401546478\n",
      "epoch: 1 step: 1778, loss is 0.025812244042754173\n",
      "epoch: 1 step: 1779, loss is 0.3573559820652008\n",
      "epoch: 1 step: 1780, loss is 0.1273442655801773\n",
      "epoch: 1 step: 1781, loss is 0.11534927785396576\n",
      "epoch: 1 step: 1782, loss is 0.20053744316101074\n",
      "epoch: 1 step: 1783, loss is 0.023242315277457237\n",
      "epoch: 1 step: 1784, loss is 0.01942557469010353\n",
      "epoch: 1 step: 1785, loss is 0.17769640684127808\n",
      "epoch: 1 step: 1786, loss is 0.014893999323248863\n",
      "epoch: 1 step: 1787, loss is 0.014749554917216301\n",
      "epoch: 1 step: 1788, loss is 0.09125541895627975\n",
      "epoch: 1 step: 1789, loss is 0.325150728225708\n",
      "epoch: 1 step: 1790, loss is 0.05749661475419998\n",
      "epoch: 1 step: 1791, loss is 0.010433400981128216\n",
      "epoch: 1 step: 1792, loss is 0.08343374729156494\n",
      "epoch: 1 step: 1793, loss is 0.10475127398967743\n",
      "epoch: 1 step: 1794, loss is 0.2330278605222702\n",
      "epoch: 1 step: 1795, loss is 0.03999103605747223\n",
      "epoch: 1 step: 1796, loss is 0.13163849711418152\n",
      "epoch: 1 step: 1797, loss is 0.027158891782164574\n",
      "epoch: 1 step: 1798, loss is 0.04435378685593605\n",
      "epoch: 1 step: 1799, loss is 0.19443748891353607\n",
      "epoch: 1 step: 1800, loss is 0.1375419795513153\n",
      "epoch: 1 step: 1801, loss is 0.16471245884895325\n",
      "epoch: 1 step: 1802, loss is 0.12122716009616852\n",
      "epoch: 1 step: 1803, loss is 0.019794808700680733\n",
      "epoch: 1 step: 1804, loss is 0.043439846485853195\n",
      "epoch: 1 step: 1805, loss is 0.16422851383686066\n",
      "epoch: 1 step: 1806, loss is 0.07236304134130478\n",
      "epoch: 1 step: 1807, loss is 0.26305368542671204\n",
      "epoch: 1 step: 1808, loss is 0.19105668365955353\n",
      "epoch: 1 step: 1809, loss is 0.3038948178291321\n",
      "epoch: 1 step: 1810, loss is 0.030124392360448837\n",
      "epoch: 1 step: 1811, loss is 0.15512949228286743\n",
      "epoch: 1 step: 1812, loss is 0.18238066136837006\n",
      "epoch: 1 step: 1813, loss is 0.22624662518501282\n",
      "epoch: 1 step: 1814, loss is 0.051951225847005844\n",
      "epoch: 1 step: 1815, loss is 0.020403502508997917\n",
      "epoch: 1 step: 1816, loss is 0.09530659765005112\n",
      "epoch: 1 step: 1817, loss is 0.3758280873298645\n",
      "epoch: 1 step: 1818, loss is 0.242120623588562\n",
      "epoch: 1 step: 1819, loss is 0.7121492028236389\n",
      "epoch: 1 step: 1820, loss is 0.016021057963371277\n",
      "epoch: 1 step: 1821, loss is 0.09040091186761856\n",
      "epoch: 1 step: 1822, loss is 0.053616125136613846\n",
      "epoch: 1 step: 1823, loss is 0.10008376091718674\n",
      "epoch: 1 step: 1824, loss is 0.12767480313777924\n",
      "epoch: 1 step: 1825, loss is 0.20296771824359894\n",
      "epoch: 1 step: 1826, loss is 0.0729939192533493\n",
      "epoch: 1 step: 1827, loss is 0.10334651917219162\n",
      "epoch: 1 step: 1828, loss is 0.21815532445907593\n",
      "epoch: 1 step: 1829, loss is 0.06634645164012909\n",
      "epoch: 1 step: 1830, loss is 0.026614682748913765\n",
      "epoch: 1 step: 1831, loss is 0.05499269813299179\n",
      "epoch: 1 step: 1832, loss is 0.12947814166545868\n",
      "epoch: 1 step: 1833, loss is 0.025488974526524544\n",
      "epoch: 1 step: 1834, loss is 0.281668096780777\n",
      "epoch: 1 step: 1835, loss is 0.2012430876493454\n",
      "epoch: 1 step: 1836, loss is 0.07249331474304199\n",
      "epoch: 1 step: 1837, loss is 0.11638153344392776\n",
      "epoch: 1 step: 1838, loss is 0.2573324739933014\n",
      "epoch: 1 step: 1839, loss is 0.05631159245967865\n",
      "epoch: 1 step: 1840, loss is 0.012112493626773357\n",
      "epoch: 1 step: 1841, loss is 0.08692818135023117\n",
      "epoch: 1 step: 1842, loss is 0.27938103675842285\n",
      "epoch: 1 step: 1843, loss is 0.15004730224609375\n",
      "epoch: 1 step: 1844, loss is 0.07444950193166733\n",
      "epoch: 1 step: 1845, loss is 0.134494811296463\n",
      "epoch: 1 step: 1846, loss is 0.0985855981707573\n",
      "epoch: 1 step: 1847, loss is 0.11698270589113235\n",
      "epoch: 1 step: 1848, loss is 0.05621068924665451\n",
      "epoch: 1 step: 1849, loss is 0.19074733555316925\n",
      "epoch: 1 step: 1850, loss is 0.4426621198654175\n",
      "epoch: 1 step: 1851, loss is 0.09636085480451584\n",
      "epoch: 1 step: 1852, loss is 0.13031305372714996\n",
      "epoch: 1 step: 1853, loss is 0.07961825281381607\n",
      "epoch: 1 step: 1854, loss is 0.0623789057135582\n",
      "epoch: 1 step: 1855, loss is 0.16961564123630524\n",
      "epoch: 1 step: 1856, loss is 0.2360038459300995\n",
      "epoch: 1 step: 1857, loss is 0.054383665323257446\n",
      "epoch: 1 step: 1858, loss is 0.0752679705619812\n",
      "epoch: 1 step: 1859, loss is 0.11171713471412659\n",
      "epoch: 1 step: 1860, loss is 0.3383968770503998\n",
      "epoch: 1 step: 1861, loss is 0.0676957443356514\n",
      "epoch: 1 step: 1862, loss is 0.10057082772254944\n",
      "epoch: 1 step: 1863, loss is 0.2250336855649948\n",
      "epoch: 1 step: 1864, loss is 0.05051696300506592\n",
      "epoch: 1 step: 1865, loss is 0.13744565844535828\n",
      "epoch: 1 step: 1866, loss is 0.013157756067812443\n",
      "epoch: 1 step: 1867, loss is 0.15882615745067596\n",
      "epoch: 1 step: 1868, loss is 0.1368650645017624\n",
      "epoch: 1 step: 1869, loss is 0.0435989610850811\n",
      "epoch: 1 step: 1870, loss is 0.16240830719470978\n",
      "epoch: 1 step: 1871, loss is 0.27743637561798096\n",
      "epoch: 1 step: 1872, loss is 0.013067126274108887\n",
      "epoch: 1 step: 1873, loss is 0.040639862418174744\n",
      "epoch: 1 step: 1874, loss is 0.019728591665625572\n",
      "epoch: 1 step: 1875, loss is 0.18945834040641785\n",
      "epoch: 2 step: 1, loss is 0.10685098171234131\n",
      "epoch: 2 step: 2, loss is 0.05396823585033417\n",
      "epoch: 2 step: 3, loss is 0.030273672193288803\n",
      "epoch: 2 step: 4, loss is 0.1068427786231041\n",
      "epoch: 2 step: 5, loss is 0.2946711480617523\n",
      "epoch: 2 step: 6, loss is 0.21976059675216675\n",
      "epoch: 2 step: 7, loss is 0.08512651175260544\n",
      "epoch: 2 step: 8, loss is 0.06989282369613647\n",
      "epoch: 2 step: 9, loss is 0.15794403851032257\n",
      "epoch: 2 step: 10, loss is 0.13072335720062256\n",
      "epoch: 2 step: 11, loss is 0.13642114400863647\n",
      "epoch: 2 step: 12, loss is 0.2415751814842224\n",
      "epoch: 2 step: 13, loss is 0.17132361233234406\n",
      "epoch: 2 step: 14, loss is 0.23360675573349\n",
      "epoch: 2 step: 15, loss is 0.020542584359645844\n",
      "epoch: 2 step: 16, loss is 0.027903707697987556\n",
      "epoch: 2 step: 17, loss is 0.13173185288906097\n",
      "epoch: 2 step: 18, loss is 0.04473642259836197\n",
      "epoch: 2 step: 19, loss is 0.11592184752225876\n",
      "epoch: 2 step: 20, loss is 0.05135096609592438\n",
      "epoch: 2 step: 21, loss is 0.1938261091709137\n",
      "epoch: 2 step: 22, loss is 0.173602893948555\n",
      "epoch: 2 step: 23, loss is 0.042816318571567535\n",
      "epoch: 2 step: 24, loss is 0.14065691828727722\n",
      "epoch: 2 step: 25, loss is 0.13111509382724762\n",
      "epoch: 2 step: 26, loss is 0.03152208775281906\n",
      "epoch: 2 step: 27, loss is 0.2851278781890869\n",
      "epoch: 2 step: 28, loss is 0.16301874816417694\n",
      "epoch: 2 step: 29, loss is 0.16504111886024475\n",
      "epoch: 2 step: 30, loss is 0.05117955058813095\n",
      "epoch: 2 step: 31, loss is 0.19996388256549835\n",
      "epoch: 2 step: 32, loss is 0.17814278602600098\n",
      "epoch: 2 step: 33, loss is 0.11743234097957611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 34, loss is 0.05976823344826698\n",
      "epoch: 2 step: 35, loss is 0.12994883954524994\n",
      "epoch: 2 step: 36, loss is 0.042095739394426346\n",
      "epoch: 2 step: 37, loss is 0.06396344304084778\n",
      "epoch: 2 step: 38, loss is 0.07395017892122269\n",
      "epoch: 2 step: 39, loss is 0.1667698174715042\n",
      "epoch: 2 step: 40, loss is 0.01727966219186783\n",
      "epoch: 2 step: 41, loss is 0.010975854471325874\n",
      "epoch: 2 step: 42, loss is 0.10047256201505661\n",
      "epoch: 2 step: 43, loss is 0.004411973524838686\n",
      "epoch: 2 step: 44, loss is 0.3514058291912079\n",
      "epoch: 2 step: 45, loss is 0.029206253588199615\n",
      "epoch: 2 step: 46, loss is 0.03923307731747627\n",
      "epoch: 2 step: 47, loss is 0.06134243309497833\n",
      "epoch: 2 step: 48, loss is 0.07811342179775238\n",
      "epoch: 2 step: 49, loss is 0.0034883262123912573\n",
      "epoch: 2 step: 50, loss is 0.03365014120936394\n",
      "epoch: 2 step: 51, loss is 0.02860383875668049\n",
      "epoch: 2 step: 52, loss is 0.10969986021518707\n",
      "epoch: 2 step: 53, loss is 0.012227118946611881\n",
      "epoch: 2 step: 54, loss is 0.03425151854753494\n",
      "epoch: 2 step: 55, loss is 0.042375024408102036\n",
      "epoch: 2 step: 56, loss is 0.0008034204947762191\n",
      "epoch: 2 step: 57, loss is 0.009202305227518082\n",
      "epoch: 2 step: 58, loss is 0.060772381722927094\n",
      "epoch: 2 step: 59, loss is 0.09019751846790314\n",
      "epoch: 2 step: 60, loss is 0.0018850910710170865\n",
      "epoch: 2 step: 61, loss is 0.045129962265491486\n",
      "epoch: 2 step: 62, loss is 0.17478282749652863\n",
      "epoch: 2 step: 63, loss is 0.492687463760376\n",
      "epoch: 2 step: 64, loss is 0.012025074101984501\n",
      "epoch: 2 step: 65, loss is 0.040445197373628616\n",
      "epoch: 2 step: 66, loss is 0.05350160971283913\n",
      "epoch: 2 step: 67, loss is 0.26018935441970825\n",
      "epoch: 2 step: 68, loss is 0.2380424290895462\n",
      "epoch: 2 step: 69, loss is 0.031053565442562103\n",
      "epoch: 2 step: 70, loss is 0.17510384321212769\n",
      "epoch: 2 step: 71, loss is 0.13816845417022705\n",
      "epoch: 2 step: 72, loss is 0.12404550611972809\n",
      "epoch: 2 step: 73, loss is 0.46423161029815674\n",
      "epoch: 2 step: 74, loss is 0.00937953032553196\n",
      "epoch: 2 step: 75, loss is 0.3304959535598755\n",
      "epoch: 2 step: 76, loss is 0.034574516117572784\n",
      "epoch: 2 step: 77, loss is 0.06408140063285828\n",
      "epoch: 2 step: 78, loss is 0.06841909885406494\n",
      "epoch: 2 step: 79, loss is 0.015497609041631222\n",
      "epoch: 2 step: 80, loss is 0.0899030938744545\n",
      "epoch: 2 step: 81, loss is 0.24970749020576477\n",
      "epoch: 2 step: 82, loss is 0.04681278020143509\n",
      "epoch: 2 step: 83, loss is 0.14939527213573456\n",
      "epoch: 2 step: 84, loss is 0.31960588693618774\n",
      "epoch: 2 step: 85, loss is 0.05404023081064224\n",
      "epoch: 2 step: 86, loss is 0.06734676659107208\n",
      "epoch: 2 step: 87, loss is 0.06075046584010124\n",
      "epoch: 2 step: 88, loss is 0.38114699721336365\n",
      "epoch: 2 step: 89, loss is 0.030417103320360184\n",
      "epoch: 2 step: 90, loss is 0.27519723773002625\n",
      "epoch: 2 step: 91, loss is 0.02562994882464409\n",
      "epoch: 2 step: 92, loss is 0.07762844860553741\n",
      "epoch: 2 step: 93, loss is 0.3251644968986511\n",
      "epoch: 2 step: 94, loss is 0.06045142188668251\n",
      "epoch: 2 step: 95, loss is 0.29217901825904846\n",
      "epoch: 2 step: 96, loss is 0.12220636010169983\n",
      "epoch: 2 step: 97, loss is 0.11731450259685516\n",
      "epoch: 2 step: 98, loss is 0.11616646498441696\n",
      "epoch: 2 step: 99, loss is 0.08145003765821457\n",
      "epoch: 2 step: 100, loss is 0.09146975725889206\n",
      "epoch: 2 step: 101, loss is 0.06555087864398956\n",
      "epoch: 2 step: 102, loss is 0.3748215436935425\n",
      "epoch: 2 step: 103, loss is 0.26214799284935\n",
      "epoch: 2 step: 104, loss is 0.05518870800733566\n",
      "epoch: 2 step: 105, loss is 0.11352749168872833\n",
      "epoch: 2 step: 106, loss is 0.10758860409259796\n",
      "epoch: 2 step: 107, loss is 0.2793753743171692\n",
      "epoch: 2 step: 108, loss is 0.038854293525218964\n",
      "epoch: 2 step: 109, loss is 0.11174418777227402\n",
      "epoch: 2 step: 110, loss is 0.3002161383628845\n",
      "epoch: 2 step: 111, loss is 0.19995176792144775\n",
      "epoch: 2 step: 112, loss is 0.2148781716823578\n",
      "epoch: 2 step: 113, loss is 0.09834291785955429\n",
      "epoch: 2 step: 114, loss is 0.2946358621120453\n",
      "epoch: 2 step: 115, loss is 0.27833399176597595\n",
      "epoch: 2 step: 116, loss is 0.024516776204109192\n",
      "epoch: 2 step: 117, loss is 0.20201914012432098\n",
      "epoch: 2 step: 118, loss is 0.2265615612268448\n",
      "epoch: 2 step: 119, loss is 0.03487882390618324\n",
      "epoch: 2 step: 120, loss is 0.18021869659423828\n",
      "epoch: 2 step: 121, loss is 0.16539695858955383\n",
      "epoch: 2 step: 122, loss is 0.06639254838228226\n",
      "epoch: 2 step: 123, loss is 0.09402614086866379\n",
      "epoch: 2 step: 124, loss is 0.016603533178567886\n",
      "epoch: 2 step: 125, loss is 0.05194087699055672\n",
      "epoch: 2 step: 126, loss is 0.0900641679763794\n",
      "epoch: 2 step: 127, loss is 0.10213148593902588\n",
      "epoch: 2 step: 128, loss is 0.04322907328605652\n",
      "epoch: 2 step: 129, loss is 0.033742815256118774\n",
      "epoch: 2 step: 130, loss is 0.010787969455122948\n",
      "epoch: 2 step: 131, loss is 0.27209243178367615\n",
      "epoch: 2 step: 132, loss is 0.10303153842687607\n",
      "epoch: 2 step: 133, loss is 0.07850873470306396\n",
      "epoch: 2 step: 134, loss is 0.1639823466539383\n",
      "epoch: 2 step: 135, loss is 0.30236726999282837\n",
      "epoch: 2 step: 136, loss is 0.07672487944364548\n",
      "epoch: 2 step: 137, loss is 0.03388276323676109\n",
      "epoch: 2 step: 138, loss is 0.07093648612499237\n",
      "epoch: 2 step: 139, loss is 0.27533695101737976\n",
      "epoch: 2 step: 140, loss is 0.07042015343904495\n",
      "epoch: 2 step: 141, loss is 0.0743747279047966\n",
      "epoch: 2 step: 142, loss is 0.20664559304714203\n",
      "epoch: 2 step: 143, loss is 0.311722069978714\n",
      "epoch: 2 step: 144, loss is 0.21546456217765808\n",
      "epoch: 2 step: 145, loss is 0.21787242591381073\n",
      "epoch: 2 step: 146, loss is 0.2139953076839447\n",
      "epoch: 2 step: 147, loss is 0.17263273894786835\n",
      "epoch: 2 step: 148, loss is 0.03188926354050636\n",
      "epoch: 2 step: 149, loss is 0.04643268883228302\n",
      "epoch: 2 step: 150, loss is 0.04473225027322769\n",
      "epoch: 2 step: 151, loss is 0.2027423083782196\n",
      "epoch: 2 step: 152, loss is 0.09834601730108261\n",
      "epoch: 2 step: 153, loss is 0.04590774327516556\n",
      "epoch: 2 step: 154, loss is 0.020151754841208458\n",
      "epoch: 2 step: 155, loss is 0.14125901460647583\n",
      "epoch: 2 step: 156, loss is 0.10396727174520493\n",
      "epoch: 2 step: 157, loss is 0.16898874938488007\n",
      "epoch: 2 step: 158, loss is 0.1166621595621109\n",
      "epoch: 2 step: 159, loss is 0.1344825029373169\n",
      "epoch: 2 step: 160, loss is 0.016585538163781166\n",
      "epoch: 2 step: 161, loss is 0.11740406602621078\n",
      "epoch: 2 step: 162, loss is 0.014050263911485672\n",
      "epoch: 2 step: 163, loss is 0.14823485910892487\n",
      "epoch: 2 step: 164, loss is 0.08235576748847961\n",
      "epoch: 2 step: 165, loss is 0.1658928245306015\n",
      "epoch: 2 step: 166, loss is 0.037504807114601135\n",
      "epoch: 2 step: 167, loss is 0.002293978352099657\n",
      "epoch: 2 step: 168, loss is 0.14005830883979797\n",
      "epoch: 2 step: 169, loss is 0.09712428599596024\n",
      "epoch: 2 step: 170, loss is 0.0896507203578949\n",
      "epoch: 2 step: 171, loss is 0.09586751461029053\n",
      "epoch: 2 step: 172, loss is 0.026973843574523926\n",
      "epoch: 2 step: 173, loss is 0.23926016688346863\n",
      "epoch: 2 step: 174, loss is 0.11667163670063019\n",
      "epoch: 2 step: 175, loss is 0.04514559730887413\n",
      "epoch: 2 step: 176, loss is 0.058575589209795\n",
      "epoch: 2 step: 177, loss is 0.10663076490163803\n",
      "epoch: 2 step: 178, loss is 0.1074230894446373\n",
      "epoch: 2 step: 179, loss is 0.02683597058057785\n",
      "epoch: 2 step: 180, loss is 0.1555103063583374\n",
      "epoch: 2 step: 181, loss is 0.145420104265213\n",
      "epoch: 2 step: 182, loss is 0.15376442670822144\n",
      "epoch: 2 step: 183, loss is 0.08579478412866592\n",
      "epoch: 2 step: 184, loss is 0.2722781002521515\n",
      "epoch: 2 step: 185, loss is 0.5925830006599426\n",
      "epoch: 2 step: 186, loss is 0.010822811163961887\n",
      "epoch: 2 step: 187, loss is 0.13300834596157074\n",
      "epoch: 2 step: 188, loss is 0.052685923874378204\n",
      "epoch: 2 step: 189, loss is 0.2205062061548233\n",
      "epoch: 2 step: 190, loss is 0.157762810587883\n",
      "epoch: 2 step: 191, loss is 0.22263512015342712\n",
      "epoch: 2 step: 192, loss is 0.23593463003635406\n",
      "epoch: 2 step: 193, loss is 0.05718040093779564\n",
      "epoch: 2 step: 194, loss is 0.11605311185121536\n",
      "epoch: 2 step: 195, loss is 0.07848870754241943\n",
      "epoch: 2 step: 196, loss is 0.11444904655218124\n",
      "epoch: 2 step: 197, loss is 0.18963445723056793\n",
      "epoch: 2 step: 198, loss is 0.052967578172683716\n",
      "epoch: 2 step: 199, loss is 0.1936485767364502\n",
      "epoch: 2 step: 200, loss is 0.050519268959760666\n",
      "epoch: 2 step: 201, loss is 0.04923614114522934\n",
      "epoch: 2 step: 202, loss is 0.10483743995428085\n",
      "epoch: 2 step: 203, loss is 0.012843050993978977\n",
      "epoch: 2 step: 204, loss is 0.12441519647836685\n",
      "epoch: 2 step: 205, loss is 0.17129269242286682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 206, loss is 0.21788503229618073\n",
      "epoch: 2 step: 207, loss is 0.16311004757881165\n",
      "epoch: 2 step: 208, loss is 0.05212005227804184\n",
      "epoch: 2 step: 209, loss is 0.304904967546463\n",
      "epoch: 2 step: 210, loss is 0.029729463160037994\n",
      "epoch: 2 step: 211, loss is 0.07567722350358963\n",
      "epoch: 2 step: 212, loss is 0.04605840891599655\n",
      "epoch: 2 step: 213, loss is 0.21708668768405914\n",
      "epoch: 2 step: 214, loss is 0.0901225209236145\n",
      "epoch: 2 step: 215, loss is 0.05898749455809593\n",
      "epoch: 2 step: 216, loss is 0.11935270577669144\n",
      "epoch: 2 step: 217, loss is 0.025069113820791245\n",
      "epoch: 2 step: 218, loss is 0.15348820388317108\n",
      "epoch: 2 step: 219, loss is 0.08632377535104752\n",
      "epoch: 2 step: 220, loss is 0.0394974984228611\n",
      "epoch: 2 step: 221, loss is 0.0976523905992508\n",
      "epoch: 2 step: 222, loss is 0.11417252570390701\n",
      "epoch: 2 step: 223, loss is 0.02688358910381794\n",
      "epoch: 2 step: 224, loss is 0.05042032152414322\n",
      "epoch: 2 step: 225, loss is 0.1804041862487793\n",
      "epoch: 2 step: 226, loss is 0.06084737181663513\n",
      "epoch: 2 step: 227, loss is 0.04440031200647354\n",
      "epoch: 2 step: 228, loss is 0.014357225969433784\n",
      "epoch: 2 step: 229, loss is 0.022130481898784637\n",
      "epoch: 2 step: 230, loss is 0.2042536735534668\n",
      "epoch: 2 step: 231, loss is 0.16188398003578186\n",
      "epoch: 2 step: 232, loss is 0.06983461230993271\n",
      "epoch: 2 step: 233, loss is 0.15918220579624176\n",
      "epoch: 2 step: 234, loss is 0.26064297556877136\n",
      "epoch: 2 step: 235, loss is 0.12346518039703369\n",
      "epoch: 2 step: 236, loss is 0.5082179307937622\n",
      "epoch: 2 step: 237, loss is 0.18151459097862244\n",
      "epoch: 2 step: 238, loss is 0.006087745539844036\n",
      "epoch: 2 step: 239, loss is 0.18983951210975647\n",
      "epoch: 2 step: 240, loss is 0.02254590019583702\n",
      "epoch: 2 step: 241, loss is 0.2125563770532608\n",
      "epoch: 2 step: 242, loss is 0.07383917272090912\n",
      "epoch: 2 step: 243, loss is 0.014381704851984978\n",
      "epoch: 2 step: 244, loss is 0.10847929120063782\n",
      "epoch: 2 step: 245, loss is 0.11233624070882797\n",
      "epoch: 2 step: 246, loss is 0.010589868761599064\n",
      "epoch: 2 step: 247, loss is 0.02026313915848732\n",
      "epoch: 2 step: 248, loss is 0.09071619063615799\n",
      "epoch: 2 step: 249, loss is 0.25250598788261414\n",
      "epoch: 2 step: 250, loss is 0.09618215262889862\n",
      "epoch: 2 step: 251, loss is 0.006306283175945282\n",
      "epoch: 2 step: 252, loss is 0.14538778364658356\n",
      "epoch: 2 step: 253, loss is 0.07270468771457672\n",
      "epoch: 2 step: 254, loss is 0.07681616395711899\n",
      "epoch: 2 step: 255, loss is 0.04618130624294281\n",
      "epoch: 2 step: 256, loss is 0.11584912985563278\n",
      "epoch: 2 step: 257, loss is 0.03797216713428497\n",
      "epoch: 2 step: 258, loss is 0.02350526675581932\n",
      "epoch: 2 step: 259, loss is 0.04321199283003807\n",
      "epoch: 2 step: 260, loss is 0.06768830865621567\n",
      "epoch: 2 step: 261, loss is 0.08579262346029282\n",
      "epoch: 2 step: 262, loss is 0.055927302688360214\n",
      "epoch: 2 step: 263, loss is 0.0664493516087532\n",
      "epoch: 2 step: 264, loss is 0.07141557335853577\n",
      "epoch: 2 step: 265, loss is 0.01507115364074707\n",
      "epoch: 2 step: 266, loss is 0.05700216069817543\n",
      "epoch: 2 step: 267, loss is 0.20091715455055237\n",
      "epoch: 2 step: 268, loss is 0.05854808911681175\n",
      "epoch: 2 step: 269, loss is 0.12810398638248444\n",
      "epoch: 2 step: 270, loss is 0.05349698290228844\n",
      "epoch: 2 step: 271, loss is 0.41889142990112305\n",
      "epoch: 2 step: 272, loss is 0.0135123860090971\n",
      "epoch: 2 step: 273, loss is 0.021343426778912544\n",
      "epoch: 2 step: 274, loss is 0.03633221611380577\n",
      "epoch: 2 step: 275, loss is 0.009574060328304768\n",
      "epoch: 2 step: 276, loss is 0.010257720947265625\n",
      "epoch: 2 step: 277, loss is 0.15012775361537933\n",
      "epoch: 2 step: 278, loss is 0.06917060166597366\n",
      "epoch: 2 step: 279, loss is 0.060943227261304855\n",
      "epoch: 2 step: 280, loss is 0.2635197639465332\n",
      "epoch: 2 step: 281, loss is 0.05445448309183121\n",
      "epoch: 2 step: 282, loss is 0.33880671858787537\n",
      "epoch: 2 step: 283, loss is 0.051295336335897446\n",
      "epoch: 2 step: 284, loss is 0.13244596123695374\n",
      "epoch: 2 step: 285, loss is 0.1551387906074524\n",
      "epoch: 2 step: 286, loss is 0.06559667736291885\n",
      "epoch: 2 step: 287, loss is 0.023398345336318016\n",
      "epoch: 2 step: 288, loss is 0.13550621271133423\n",
      "epoch: 2 step: 289, loss is 0.021391678601503372\n",
      "epoch: 2 step: 290, loss is 0.13631515204906464\n",
      "epoch: 2 step: 291, loss is 0.014504596590995789\n",
      "epoch: 2 step: 292, loss is 0.11169546097517014\n",
      "epoch: 2 step: 293, loss is 0.010003803297877312\n",
      "epoch: 2 step: 294, loss is 0.01679554022848606\n",
      "epoch: 2 step: 295, loss is 0.020183436572551727\n",
      "epoch: 2 step: 296, loss is 0.2098250836133957\n",
      "epoch: 2 step: 297, loss is 0.2935132384300232\n",
      "epoch: 2 step: 298, loss is 0.03498144447803497\n",
      "epoch: 2 step: 299, loss is 0.007807921152561903\n",
      "epoch: 2 step: 300, loss is 0.10964585840702057\n",
      "epoch: 2 step: 301, loss is 0.028248628601431847\n",
      "epoch: 2 step: 302, loss is 0.02478475496172905\n",
      "epoch: 2 step: 303, loss is 0.009687383659183979\n",
      "epoch: 2 step: 304, loss is 0.20291392505168915\n",
      "epoch: 2 step: 305, loss is 0.12824811041355133\n",
      "epoch: 2 step: 306, loss is 0.04256394878029823\n",
      "epoch: 2 step: 307, loss is 0.10410724580287933\n",
      "epoch: 2 step: 308, loss is 0.12289895117282867\n",
      "epoch: 2 step: 309, loss is 0.12611111998558044\n",
      "epoch: 2 step: 310, loss is 0.33633995056152344\n",
      "epoch: 2 step: 311, loss is 0.0345035158097744\n",
      "epoch: 2 step: 312, loss is 0.08106172829866409\n",
      "epoch: 2 step: 313, loss is 0.187266007065773\n",
      "epoch: 2 step: 314, loss is 0.03259832039475441\n",
      "epoch: 2 step: 315, loss is 0.08862566202878952\n",
      "epoch: 2 step: 316, loss is 0.1606566607952118\n",
      "epoch: 2 step: 317, loss is 0.01751677319407463\n",
      "epoch: 2 step: 318, loss is 0.13944213092327118\n",
      "epoch: 2 step: 319, loss is 0.06199086457490921\n",
      "epoch: 2 step: 320, loss is 0.015502545982599258\n",
      "epoch: 2 step: 321, loss is 0.06601713597774506\n",
      "epoch: 2 step: 322, loss is 0.2767491936683655\n",
      "epoch: 2 step: 323, loss is 0.1532808542251587\n",
      "epoch: 2 step: 324, loss is 0.01073750201612711\n",
      "epoch: 2 step: 325, loss is 0.1938982605934143\n",
      "epoch: 2 step: 326, loss is 0.025276729837059975\n",
      "epoch: 2 step: 327, loss is 0.14536285400390625\n",
      "epoch: 2 step: 328, loss is 0.023354295641183853\n",
      "epoch: 2 step: 329, loss is 0.02771209366619587\n",
      "epoch: 2 step: 330, loss is 0.01875976286828518\n",
      "epoch: 2 step: 331, loss is 0.009463389404118061\n",
      "epoch: 2 step: 332, loss is 0.13761906325817108\n",
      "epoch: 2 step: 333, loss is 0.06589213013648987\n",
      "epoch: 2 step: 334, loss is 0.17320726811885834\n",
      "epoch: 2 step: 335, loss is 0.06408988684415817\n",
      "epoch: 2 step: 336, loss is 0.14663605391979218\n",
      "epoch: 2 step: 337, loss is 0.07313776016235352\n",
      "epoch: 2 step: 338, loss is 0.05147714912891388\n",
      "epoch: 2 step: 339, loss is 0.05500229075551033\n",
      "epoch: 2 step: 340, loss is 0.004708040971308947\n",
      "epoch: 2 step: 341, loss is 0.13679346442222595\n",
      "epoch: 2 step: 342, loss is 0.056092649698257446\n",
      "epoch: 2 step: 343, loss is 0.011149602010846138\n",
      "epoch: 2 step: 344, loss is 0.14745332300662994\n",
      "epoch: 2 step: 345, loss is 0.019322004169225693\n",
      "epoch: 2 step: 346, loss is 0.03450166806578636\n",
      "epoch: 2 step: 347, loss is 0.026174966245889664\n",
      "epoch: 2 step: 348, loss is 0.0192576851695776\n",
      "epoch: 2 step: 349, loss is 0.05263155698776245\n",
      "epoch: 2 step: 350, loss is 0.12624099850654602\n",
      "epoch: 2 step: 351, loss is 0.0985596776008606\n",
      "epoch: 2 step: 352, loss is 0.029549935832619667\n",
      "epoch: 2 step: 353, loss is 0.023082170635461807\n",
      "epoch: 2 step: 354, loss is 0.04303912818431854\n",
      "epoch: 2 step: 355, loss is 0.1294921338558197\n",
      "epoch: 2 step: 356, loss is 0.05807997286319733\n",
      "epoch: 2 step: 357, loss is 0.09939701110124588\n",
      "epoch: 2 step: 358, loss is 0.16579993069171906\n",
      "epoch: 2 step: 359, loss is 0.007605196908116341\n",
      "epoch: 2 step: 360, loss is 0.02123875357210636\n",
      "epoch: 2 step: 361, loss is 0.0026324111968278885\n",
      "epoch: 2 step: 362, loss is 0.017065592110157013\n",
      "epoch: 2 step: 363, loss is 0.19495733082294464\n",
      "epoch: 2 step: 364, loss is 0.08256155997514725\n",
      "epoch: 2 step: 365, loss is 0.02424769103527069\n",
      "epoch: 2 step: 366, loss is 0.017182359471917152\n",
      "epoch: 2 step: 367, loss is 0.07916948199272156\n",
      "epoch: 2 step: 368, loss is 0.02204948477447033\n",
      "epoch: 2 step: 369, loss is 0.0949227437376976\n",
      "epoch: 2 step: 370, loss is 0.09427604079246521\n",
      "epoch: 2 step: 371, loss is 0.02308114618062973\n",
      "epoch: 2 step: 372, loss is 0.0918351486325264\n",
      "epoch: 2 step: 373, loss is 0.3064271807670593\n",
      "epoch: 2 step: 374, loss is 0.009358917362987995\n",
      "epoch: 2 step: 375, loss is 0.007901555858552456\n",
      "epoch: 2 step: 376, loss is 0.014970838092267513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 377, loss is 0.12900309264659882\n",
      "epoch: 2 step: 378, loss is 0.029634416103363037\n",
      "epoch: 2 step: 379, loss is 0.02136235311627388\n",
      "epoch: 2 step: 380, loss is 0.003389328718185425\n",
      "epoch: 2 step: 381, loss is 0.3583771884441376\n",
      "epoch: 2 step: 382, loss is 0.10509109497070312\n",
      "epoch: 2 step: 383, loss is 0.0429922379553318\n",
      "epoch: 2 step: 384, loss is 0.08663392066955566\n",
      "epoch: 2 step: 385, loss is 0.041700929403305054\n",
      "epoch: 2 step: 386, loss is 0.1214863583445549\n",
      "epoch: 2 step: 387, loss is 0.22445927560329437\n",
      "epoch: 2 step: 388, loss is 0.004747671075165272\n",
      "epoch: 2 step: 389, loss is 0.020346423611044884\n",
      "epoch: 2 step: 390, loss is 0.08340708166360855\n",
      "epoch: 2 step: 391, loss is 0.17042122781276703\n",
      "epoch: 2 step: 392, loss is 0.0222789216786623\n",
      "epoch: 2 step: 393, loss is 0.09055766463279724\n",
      "epoch: 2 step: 394, loss is 0.0019446740625426173\n",
      "epoch: 2 step: 395, loss is 0.016319291666150093\n",
      "epoch: 2 step: 396, loss is 0.06202937290072441\n",
      "epoch: 2 step: 397, loss is 0.2699989080429077\n",
      "epoch: 2 step: 398, loss is 0.013340315781533718\n",
      "epoch: 2 step: 399, loss is 0.2805957794189453\n",
      "epoch: 2 step: 400, loss is 0.07949578762054443\n",
      "epoch: 2 step: 401, loss is 0.006777279078960419\n",
      "epoch: 2 step: 402, loss is 0.09058912843465805\n",
      "epoch: 2 step: 403, loss is 0.02139650471508503\n",
      "epoch: 2 step: 404, loss is 0.06395680457353592\n",
      "epoch: 2 step: 405, loss is 0.06358083337545395\n",
      "epoch: 2 step: 406, loss is 0.45104554295539856\n",
      "epoch: 2 step: 407, loss is 0.09150995314121246\n",
      "epoch: 2 step: 408, loss is 0.03627932444214821\n",
      "epoch: 2 step: 409, loss is 0.04045936092734337\n",
      "epoch: 2 step: 410, loss is 0.003723894013091922\n",
      "epoch: 2 step: 411, loss is 0.05658402666449547\n",
      "epoch: 2 step: 412, loss is 0.025646265596151352\n",
      "epoch: 2 step: 413, loss is 0.24736160039901733\n",
      "epoch: 2 step: 414, loss is 0.008414947427809238\n",
      "epoch: 2 step: 415, loss is 0.05051251873373985\n",
      "epoch: 2 step: 416, loss is 0.005422323010861874\n",
      "epoch: 2 step: 417, loss is 0.28139856457710266\n",
      "epoch: 2 step: 418, loss is 0.05902116000652313\n",
      "epoch: 2 step: 419, loss is 0.2561432719230652\n",
      "epoch: 2 step: 420, loss is 0.02358063869178295\n",
      "epoch: 2 step: 421, loss is 0.043400444090366364\n",
      "epoch: 2 step: 422, loss is 0.04006364569067955\n",
      "epoch: 2 step: 423, loss is 0.04113641008734703\n",
      "epoch: 2 step: 424, loss is 0.25949934124946594\n",
      "epoch: 2 step: 425, loss is 0.17061100900173187\n",
      "epoch: 2 step: 426, loss is 0.2141420692205429\n",
      "epoch: 2 step: 427, loss is 0.23250514268875122\n",
      "epoch: 2 step: 428, loss is 0.005082705989480019\n",
      "epoch: 2 step: 429, loss is 0.024507930502295494\n",
      "epoch: 2 step: 430, loss is 0.34176814556121826\n",
      "epoch: 2 step: 431, loss is 0.009240586310625076\n",
      "epoch: 2 step: 432, loss is 0.0654807910323143\n",
      "epoch: 2 step: 433, loss is 0.20515961945056915\n",
      "epoch: 2 step: 434, loss is 0.19670766592025757\n",
      "epoch: 2 step: 435, loss is 0.03276155889034271\n",
      "epoch: 2 step: 436, loss is 0.11481095105409622\n",
      "epoch: 2 step: 437, loss is 0.08683983981609344\n",
      "epoch: 2 step: 438, loss is 0.028356537222862244\n",
      "epoch: 2 step: 439, loss is 0.06681376695632935\n",
      "epoch: 2 step: 440, loss is 0.02280585654079914\n",
      "epoch: 2 step: 441, loss is 0.07615738362073898\n",
      "epoch: 2 step: 442, loss is 0.34436899423599243\n",
      "epoch: 2 step: 443, loss is 0.02076396718621254\n",
      "epoch: 2 step: 444, loss is 0.04320003464818001\n",
      "epoch: 2 step: 445, loss is 0.11120454967021942\n",
      "epoch: 2 step: 446, loss is 0.011222518049180508\n",
      "epoch: 2 step: 447, loss is 0.007434791419655085\n",
      "epoch: 2 step: 448, loss is 0.01687265746295452\n",
      "epoch: 2 step: 449, loss is 0.008316777646541595\n",
      "epoch: 2 step: 450, loss is 0.06118070334196091\n",
      "epoch: 2 step: 451, loss is 0.19668753445148468\n",
      "epoch: 2 step: 452, loss is 0.20388591289520264\n",
      "epoch: 2 step: 453, loss is 0.15104568004608154\n",
      "epoch: 2 step: 454, loss is 0.00754567002877593\n",
      "epoch: 2 step: 455, loss is 0.05222901329398155\n",
      "epoch: 2 step: 456, loss is 0.12976466119289398\n",
      "epoch: 2 step: 457, loss is 0.012720167636871338\n",
      "epoch: 2 step: 458, loss is 0.1345353126525879\n",
      "epoch: 2 step: 459, loss is 0.006186317186802626\n",
      "epoch: 2 step: 460, loss is 0.0714036151766777\n",
      "epoch: 2 step: 461, loss is 0.02721267193555832\n",
      "epoch: 2 step: 462, loss is 0.0583038404583931\n",
      "epoch: 2 step: 463, loss is 0.19492290914058685\n",
      "epoch: 2 step: 464, loss is 0.020066648721694946\n",
      "epoch: 2 step: 465, loss is 0.12751060724258423\n",
      "epoch: 2 step: 466, loss is 0.1338377296924591\n",
      "epoch: 2 step: 467, loss is 0.19859743118286133\n",
      "epoch: 2 step: 468, loss is 0.12398570775985718\n",
      "epoch: 2 step: 469, loss is 0.006869528908282518\n",
      "epoch: 2 step: 470, loss is 0.011272485367953777\n",
      "epoch: 2 step: 471, loss is 0.02870560996234417\n",
      "epoch: 2 step: 472, loss is 0.030562762171030045\n",
      "epoch: 2 step: 473, loss is 0.15621374547481537\n",
      "epoch: 2 step: 474, loss is 0.25085023045539856\n",
      "epoch: 2 step: 475, loss is 0.11428064107894897\n",
      "epoch: 2 step: 476, loss is 0.24800273776054382\n",
      "epoch: 2 step: 477, loss is 0.12194299697875977\n",
      "epoch: 2 step: 478, loss is 0.07543400675058365\n",
      "epoch: 2 step: 479, loss is 0.025184398517012596\n",
      "epoch: 2 step: 480, loss is 0.2583104968070984\n",
      "epoch: 2 step: 481, loss is 0.17624446749687195\n",
      "epoch: 2 step: 482, loss is 0.11365871876478195\n",
      "epoch: 2 step: 483, loss is 0.04344404488801956\n",
      "epoch: 2 step: 484, loss is 0.04087793081998825\n",
      "epoch: 2 step: 485, loss is 0.006517705507576466\n",
      "epoch: 2 step: 486, loss is 0.020531339570879936\n",
      "epoch: 2 step: 487, loss is 0.004361146129667759\n",
      "epoch: 2 step: 488, loss is 0.12580938637256622\n",
      "epoch: 2 step: 489, loss is 0.006493004504591227\n",
      "epoch: 2 step: 490, loss is 0.06836335361003876\n",
      "epoch: 2 step: 491, loss is 0.0667974054813385\n",
      "epoch: 2 step: 492, loss is 0.05427568033337593\n",
      "epoch: 2 step: 493, loss is 0.074443519115448\n",
      "epoch: 2 step: 494, loss is 0.13904839754104614\n",
      "epoch: 2 step: 495, loss is 0.06673048436641693\n",
      "epoch: 2 step: 496, loss is 0.12315499782562256\n",
      "epoch: 2 step: 497, loss is 0.018684541806578636\n",
      "epoch: 2 step: 498, loss is 0.01616765931248665\n",
      "epoch: 2 step: 499, loss is 0.09005095809698105\n",
      "epoch: 2 step: 500, loss is 0.15113596618175507\n",
      "epoch: 2 step: 501, loss is 0.13268375396728516\n",
      "epoch: 2 step: 502, loss is 0.23909904062747955\n",
      "epoch: 2 step: 503, loss is 0.4398755431175232\n",
      "epoch: 2 step: 504, loss is 0.08086372166872025\n",
      "epoch: 2 step: 505, loss is 0.04797341302037239\n",
      "epoch: 2 step: 506, loss is 0.0035869635175913572\n",
      "epoch: 2 step: 507, loss is 0.08151093125343323\n",
      "epoch: 2 step: 508, loss is 0.03890537470579147\n",
      "epoch: 2 step: 509, loss is 0.08049406856298447\n",
      "epoch: 2 step: 510, loss is 0.12129814922809601\n",
      "epoch: 2 step: 511, loss is 0.045773353427648544\n",
      "epoch: 2 step: 512, loss is 0.03676038235425949\n",
      "epoch: 2 step: 513, loss is 0.05734928697347641\n",
      "epoch: 2 step: 514, loss is 0.040149953216314316\n",
      "epoch: 2 step: 515, loss is 0.19883719086647034\n",
      "epoch: 2 step: 516, loss is 0.15008170902729034\n",
      "epoch: 2 step: 517, loss is 0.04768148064613342\n",
      "epoch: 2 step: 518, loss is 0.04644410312175751\n",
      "epoch: 2 step: 519, loss is 0.022945832461118698\n",
      "epoch: 2 step: 520, loss is 0.014174442738294601\n",
      "epoch: 2 step: 521, loss is 0.02031543292105198\n",
      "epoch: 2 step: 522, loss is 0.3509882688522339\n",
      "epoch: 2 step: 523, loss is 0.011043268255889416\n",
      "epoch: 2 step: 524, loss is 0.17742951214313507\n",
      "epoch: 2 step: 525, loss is 0.009579729288816452\n",
      "epoch: 2 step: 526, loss is 0.2086118459701538\n",
      "epoch: 2 step: 527, loss is 0.10715246200561523\n",
      "epoch: 2 step: 528, loss is 0.24655896425247192\n",
      "epoch: 2 step: 529, loss is 0.047368504106998444\n",
      "epoch: 2 step: 530, loss is 0.019366135820746422\n",
      "epoch: 2 step: 531, loss is 0.18732985854148865\n",
      "epoch: 2 step: 532, loss is 0.15494680404663086\n",
      "epoch: 2 step: 533, loss is 0.26620590686798096\n",
      "epoch: 2 step: 534, loss is 0.06393276900053024\n",
      "epoch: 2 step: 535, loss is 0.17642110586166382\n",
      "epoch: 2 step: 536, loss is 0.15381287038326263\n",
      "epoch: 2 step: 537, loss is 0.14461295306682587\n",
      "epoch: 2 step: 538, loss is 0.2585035562515259\n",
      "epoch: 2 step: 539, loss is 0.08084244281053543\n",
      "epoch: 2 step: 540, loss is 0.13800372183322906\n",
      "epoch: 2 step: 541, loss is 0.12149866670370102\n",
      "epoch: 2 step: 542, loss is 0.02456103079020977\n",
      "epoch: 2 step: 543, loss is 0.30037418007850647\n",
      "epoch: 2 step: 544, loss is 0.03852509334683418\n",
      "epoch: 2 step: 545, loss is 0.15100516378879547\n",
      "epoch: 2 step: 546, loss is 0.042221471667289734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 547, loss is 0.08955302834510803\n",
      "epoch: 2 step: 548, loss is 0.010135838761925697\n",
      "epoch: 2 step: 549, loss is 0.10202246904373169\n",
      "epoch: 2 step: 550, loss is 0.0738346204161644\n",
      "epoch: 2 step: 551, loss is 0.35629937052726746\n",
      "epoch: 2 step: 552, loss is 0.08139225840568542\n",
      "epoch: 2 step: 553, loss is 0.0915411114692688\n",
      "epoch: 2 step: 554, loss is 0.1931271106004715\n",
      "epoch: 2 step: 555, loss is 0.25870537757873535\n",
      "epoch: 2 step: 556, loss is 0.04837726801633835\n",
      "epoch: 2 step: 557, loss is 0.06686127930879593\n",
      "epoch: 2 step: 558, loss is 0.018350880593061447\n",
      "epoch: 2 step: 559, loss is 0.013519372791051865\n",
      "epoch: 2 step: 560, loss is 0.04731248319149017\n",
      "epoch: 2 step: 561, loss is 0.09294187277555466\n",
      "epoch: 2 step: 562, loss is 0.42191633582115173\n",
      "epoch: 2 step: 563, loss is 0.18027417361736298\n",
      "epoch: 2 step: 564, loss is 0.021092383190989494\n",
      "epoch: 2 step: 565, loss is 0.16120290756225586\n",
      "epoch: 2 step: 566, loss is 0.08449762314558029\n",
      "epoch: 2 step: 567, loss is 0.01199879590421915\n",
      "epoch: 2 step: 568, loss is 0.1683684140443802\n",
      "epoch: 2 step: 569, loss is 0.11596772074699402\n",
      "epoch: 2 step: 570, loss is 0.060570117086172104\n",
      "epoch: 2 step: 571, loss is 0.15452918410301208\n",
      "epoch: 2 step: 572, loss is 0.009821486659348011\n",
      "epoch: 2 step: 573, loss is 0.032626427710056305\n",
      "epoch: 2 step: 574, loss is 0.12682442367076874\n",
      "epoch: 2 step: 575, loss is 0.10969962179660797\n",
      "epoch: 2 step: 576, loss is 0.07481761276721954\n",
      "epoch: 2 step: 577, loss is 0.14154936373233795\n",
      "epoch: 2 step: 578, loss is 0.061221882700920105\n",
      "epoch: 2 step: 579, loss is 0.12316986173391342\n",
      "epoch: 2 step: 580, loss is 0.1772630512714386\n",
      "epoch: 2 step: 581, loss is 0.04726441949605942\n",
      "epoch: 2 step: 582, loss is 0.084597647190094\n",
      "epoch: 2 step: 583, loss is 0.16508987545967102\n",
      "epoch: 2 step: 584, loss is 0.003908868413418531\n",
      "epoch: 2 step: 585, loss is 0.044697172939777374\n",
      "epoch: 2 step: 586, loss is 0.10685567557811737\n",
      "epoch: 2 step: 587, loss is 0.07326707243919373\n",
      "epoch: 2 step: 588, loss is 0.015798984095454216\n",
      "epoch: 2 step: 589, loss is 0.04265482723712921\n",
      "epoch: 2 step: 590, loss is 0.14744910597801208\n",
      "epoch: 2 step: 591, loss is 0.12611816823482513\n",
      "epoch: 2 step: 592, loss is 0.11627218127250671\n",
      "epoch: 2 step: 593, loss is 0.010458367876708508\n",
      "epoch: 2 step: 594, loss is 0.031343087553977966\n",
      "epoch: 2 step: 595, loss is 0.03661790117621422\n",
      "epoch: 2 step: 596, loss is 0.029831886291503906\n",
      "epoch: 2 step: 597, loss is 0.10193429887294769\n",
      "epoch: 2 step: 598, loss is 0.07624538987874985\n",
      "epoch: 2 step: 599, loss is 0.04891466721892357\n",
      "epoch: 2 step: 600, loss is 0.04122084006667137\n",
      "epoch: 2 step: 601, loss is 0.021195927634835243\n",
      "epoch: 2 step: 602, loss is 0.029588932171463966\n",
      "epoch: 2 step: 603, loss is 0.20710448920726776\n",
      "epoch: 2 step: 604, loss is 0.04402230679988861\n",
      "epoch: 2 step: 605, loss is 0.1086573377251625\n",
      "epoch: 2 step: 606, loss is 0.04226803779602051\n",
      "epoch: 2 step: 607, loss is 0.049846354871988297\n",
      "epoch: 2 step: 608, loss is 0.026094790548086166\n",
      "epoch: 2 step: 609, loss is 0.027705460786819458\n",
      "epoch: 2 step: 610, loss is 0.05764283612370491\n",
      "epoch: 2 step: 611, loss is 0.16115251183509827\n",
      "epoch: 2 step: 612, loss is 0.17165344953536987\n",
      "epoch: 2 step: 613, loss is 0.04016808792948723\n",
      "epoch: 2 step: 614, loss is 0.038046807050704956\n",
      "epoch: 2 step: 615, loss is 0.024577923119068146\n",
      "epoch: 2 step: 616, loss is 0.012238253839313984\n",
      "epoch: 2 step: 617, loss is 0.011062893085181713\n",
      "epoch: 2 step: 618, loss is 0.022662607952952385\n",
      "epoch: 2 step: 619, loss is 0.03485621511936188\n",
      "epoch: 2 step: 620, loss is 0.04765965789556503\n",
      "epoch: 2 step: 621, loss is 0.07352659851312637\n",
      "epoch: 2 step: 622, loss is 0.02885621227324009\n",
      "epoch: 2 step: 623, loss is 0.3395313620567322\n",
      "epoch: 2 step: 624, loss is 0.2774018347263336\n",
      "epoch: 2 step: 625, loss is 0.06460931152105331\n",
      "epoch: 2 step: 626, loss is 0.019185233861207962\n",
      "epoch: 2 step: 627, loss is 0.19481097161769867\n",
      "epoch: 2 step: 628, loss is 0.0291572418063879\n",
      "epoch: 2 step: 629, loss is 0.13730677962303162\n",
      "epoch: 2 step: 630, loss is 0.26463866233825684\n",
      "epoch: 2 step: 631, loss is 0.32269835472106934\n",
      "epoch: 2 step: 632, loss is 0.02229757234454155\n",
      "epoch: 2 step: 633, loss is 0.01236052718013525\n",
      "epoch: 2 step: 634, loss is 0.14180058240890503\n",
      "epoch: 2 step: 635, loss is 0.039788808673620224\n",
      "epoch: 2 step: 636, loss is 0.18775640428066254\n",
      "epoch: 2 step: 637, loss is 0.1310892254114151\n",
      "epoch: 2 step: 638, loss is 0.049729008227586746\n",
      "epoch: 2 step: 639, loss is 0.1546662151813507\n",
      "epoch: 2 step: 640, loss is 0.07341364026069641\n",
      "epoch: 2 step: 641, loss is 0.11706081032752991\n",
      "epoch: 2 step: 642, loss is 0.06134998798370361\n",
      "epoch: 2 step: 643, loss is 0.20454250276088715\n",
      "epoch: 2 step: 644, loss is 0.043796513229608536\n",
      "epoch: 2 step: 645, loss is 0.12254727631807327\n",
      "epoch: 2 step: 646, loss is 0.017935628071427345\n",
      "epoch: 2 step: 647, loss is 0.19435998797416687\n",
      "epoch: 2 step: 648, loss is 0.024786798283457756\n",
      "epoch: 2 step: 649, loss is 0.037640463560819626\n",
      "epoch: 2 step: 650, loss is 0.012615285813808441\n",
      "epoch: 2 step: 651, loss is 0.02612769417464733\n",
      "epoch: 2 step: 652, loss is 0.06705593317747116\n",
      "epoch: 2 step: 653, loss is 0.0663406103849411\n",
      "epoch: 2 step: 654, loss is 0.1527106612920761\n",
      "epoch: 2 step: 655, loss is 0.258634090423584\n",
      "epoch: 2 step: 656, loss is 0.13206900656223297\n",
      "epoch: 2 step: 657, loss is 0.055594153702259064\n",
      "epoch: 2 step: 658, loss is 0.1016109436750412\n",
      "epoch: 2 step: 659, loss is 0.06444857269525528\n",
      "epoch: 2 step: 660, loss is 0.11609285324811935\n",
      "epoch: 2 step: 661, loss is 0.08946959674358368\n",
      "epoch: 2 step: 662, loss is 0.004156888462603092\n",
      "epoch: 2 step: 663, loss is 0.11822965741157532\n",
      "epoch: 2 step: 664, loss is 0.08785425871610641\n",
      "epoch: 2 step: 665, loss is 0.027818376198410988\n",
      "epoch: 2 step: 666, loss is 0.0236311424523592\n",
      "epoch: 2 step: 667, loss is 0.01200472004711628\n",
      "epoch: 2 step: 668, loss is 0.0024913374800235033\n",
      "epoch: 2 step: 669, loss is 0.21051117777824402\n",
      "epoch: 2 step: 670, loss is 0.026252396404743195\n",
      "epoch: 2 step: 671, loss is 0.01593858189880848\n",
      "epoch: 2 step: 672, loss is 0.005983369890600443\n",
      "epoch: 2 step: 673, loss is 0.017804564908146858\n",
      "epoch: 2 step: 674, loss is 0.12362446635961533\n",
      "epoch: 2 step: 675, loss is 0.08838449418544769\n",
      "epoch: 2 step: 676, loss is 0.07206007093191147\n",
      "epoch: 2 step: 677, loss is 0.03070783242583275\n",
      "epoch: 2 step: 678, loss is 0.15045121312141418\n",
      "epoch: 2 step: 679, loss is 0.024400722235441208\n",
      "epoch: 2 step: 680, loss is 0.06945595890283585\n",
      "epoch: 2 step: 681, loss is 0.14635325968265533\n",
      "epoch: 2 step: 682, loss is 0.08630551397800446\n",
      "epoch: 2 step: 683, loss is 0.09491206705570221\n",
      "epoch: 2 step: 684, loss is 0.1569765955209732\n",
      "epoch: 2 step: 685, loss is 0.4580899477005005\n",
      "epoch: 2 step: 686, loss is 0.0037714154459536076\n",
      "epoch: 2 step: 687, loss is 0.07702741026878357\n",
      "epoch: 2 step: 688, loss is 0.02115386351943016\n",
      "epoch: 2 step: 689, loss is 0.0031228980515152216\n",
      "epoch: 2 step: 690, loss is 0.07325051724910736\n",
      "epoch: 2 step: 691, loss is 0.15595628321170807\n",
      "epoch: 2 step: 692, loss is 0.01683138497173786\n",
      "epoch: 2 step: 693, loss is 0.024686822667717934\n",
      "epoch: 2 step: 694, loss is 0.1337679773569107\n",
      "epoch: 2 step: 695, loss is 0.04491833224892616\n",
      "epoch: 2 step: 696, loss is 0.005852214526385069\n",
      "epoch: 2 step: 697, loss is 0.049699969589710236\n",
      "epoch: 2 step: 698, loss is 0.04098532348871231\n",
      "epoch: 2 step: 699, loss is 0.2630578577518463\n",
      "epoch: 2 step: 700, loss is 0.040745388716459274\n",
      "epoch: 2 step: 701, loss is 0.04225482419133186\n",
      "epoch: 2 step: 702, loss is 0.0144812548533082\n",
      "epoch: 2 step: 703, loss is 0.013500726781785488\n",
      "epoch: 2 step: 704, loss is 0.1848544329404831\n",
      "epoch: 2 step: 705, loss is 0.14432401955127716\n",
      "epoch: 2 step: 706, loss is 0.013133345171809196\n",
      "epoch: 2 step: 707, loss is 0.026605302467942238\n",
      "epoch: 2 step: 708, loss is 0.06561702489852905\n",
      "epoch: 2 step: 709, loss is 0.21068823337554932\n",
      "epoch: 2 step: 710, loss is 0.01737598143517971\n",
      "epoch: 2 step: 711, loss is 0.0017938428791239858\n",
      "epoch: 2 step: 712, loss is 0.056499287486076355\n",
      "epoch: 2 step: 713, loss is 0.0412060022354126\n",
      "epoch: 2 step: 714, loss is 0.035158731043338776\n",
      "epoch: 2 step: 715, loss is 0.18638917803764343\n",
      "epoch: 2 step: 716, loss is 0.07524225115776062\n",
      "epoch: 2 step: 717, loss is 0.02621557004749775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 718, loss is 0.004840596113353968\n",
      "epoch: 2 step: 719, loss is 0.2096341848373413\n",
      "epoch: 2 step: 720, loss is 0.012931177392601967\n",
      "epoch: 2 step: 721, loss is 0.03546064347028732\n",
      "epoch: 2 step: 722, loss is 0.0016481985803693533\n",
      "epoch: 2 step: 723, loss is 0.29191598296165466\n",
      "epoch: 2 step: 724, loss is 0.08229750394821167\n",
      "epoch: 2 step: 725, loss is 0.05881073698401451\n",
      "epoch: 2 step: 726, loss is 0.14628145098686218\n",
      "epoch: 2 step: 727, loss is 0.02201673947274685\n",
      "epoch: 2 step: 728, loss is 0.18065939843654633\n",
      "epoch: 2 step: 729, loss is 0.044192709028720856\n",
      "epoch: 2 step: 730, loss is 0.11907434463500977\n",
      "epoch: 2 step: 731, loss is 0.03736703842878342\n",
      "epoch: 2 step: 732, loss is 0.07366859912872314\n",
      "epoch: 2 step: 733, loss is 0.008488984778523445\n",
      "epoch: 2 step: 734, loss is 0.11248522251844406\n",
      "epoch: 2 step: 735, loss is 0.010820786468684673\n",
      "epoch: 2 step: 736, loss is 0.0419435016810894\n",
      "epoch: 2 step: 737, loss is 0.0046731834299862385\n",
      "epoch: 2 step: 738, loss is 0.14479027688503265\n",
      "epoch: 2 step: 739, loss is 0.25726014375686646\n",
      "epoch: 2 step: 740, loss is 0.1845243126153946\n",
      "epoch: 2 step: 741, loss is 0.09324891120195389\n",
      "epoch: 2 step: 742, loss is 0.020439764484763145\n",
      "epoch: 2 step: 743, loss is 0.062426384538412094\n",
      "epoch: 2 step: 744, loss is 0.028258146718144417\n",
      "epoch: 2 step: 745, loss is 0.21817201375961304\n",
      "epoch: 2 step: 746, loss is 0.13742126524448395\n",
      "epoch: 2 step: 747, loss is 0.09686844050884247\n",
      "epoch: 2 step: 748, loss is 0.008266494609415531\n",
      "epoch: 2 step: 749, loss is 0.13533899188041687\n",
      "epoch: 2 step: 750, loss is 0.0013743923045694828\n",
      "epoch: 2 step: 751, loss is 0.00831229705363512\n",
      "epoch: 2 step: 752, loss is 0.19880524277687073\n",
      "epoch: 2 step: 753, loss is 0.042401451617479324\n",
      "epoch: 2 step: 754, loss is 0.022058924660086632\n",
      "epoch: 2 step: 755, loss is 0.063120037317276\n",
      "epoch: 2 step: 756, loss is 0.03571142628788948\n",
      "epoch: 2 step: 757, loss is 0.005554110277444124\n",
      "epoch: 2 step: 758, loss is 0.010519645176827908\n",
      "epoch: 2 step: 759, loss is 0.22412744164466858\n",
      "epoch: 2 step: 760, loss is 0.0024658471811562777\n",
      "epoch: 2 step: 761, loss is 0.1620587557554245\n",
      "epoch: 2 step: 762, loss is 0.25252479314804077\n",
      "epoch: 2 step: 763, loss is 0.0020248896908015013\n",
      "epoch: 2 step: 764, loss is 0.1510133147239685\n",
      "epoch: 2 step: 765, loss is 0.0464007742702961\n",
      "epoch: 2 step: 766, loss is 0.1668115258216858\n",
      "epoch: 2 step: 767, loss is 0.001459202147088945\n",
      "epoch: 2 step: 768, loss is 0.04619549959897995\n",
      "epoch: 2 step: 769, loss is 0.01318773441016674\n",
      "epoch: 2 step: 770, loss is 0.008159796707332134\n",
      "epoch: 2 step: 771, loss is 0.06461069732904434\n",
      "epoch: 2 step: 772, loss is 0.07558752596378326\n",
      "epoch: 2 step: 773, loss is 0.09184843301773071\n",
      "epoch: 2 step: 774, loss is 0.2639411389827728\n",
      "epoch: 2 step: 775, loss is 0.06349987536668777\n",
      "epoch: 2 step: 776, loss is 0.022158699110150337\n",
      "epoch: 2 step: 777, loss is 0.18611377477645874\n",
      "epoch: 2 step: 778, loss is 0.0804668739438057\n",
      "epoch: 2 step: 779, loss is 0.1239396259188652\n",
      "epoch: 2 step: 780, loss is 0.23622393608093262\n",
      "epoch: 2 step: 781, loss is 0.2656646966934204\n",
      "epoch: 2 step: 782, loss is 0.07832731306552887\n",
      "epoch: 2 step: 783, loss is 0.24495568871498108\n",
      "epoch: 2 step: 784, loss is 0.08018960803747177\n",
      "epoch: 2 step: 785, loss is 0.06562494486570358\n",
      "epoch: 2 step: 786, loss is 0.017172304913401604\n",
      "epoch: 2 step: 787, loss is 0.165469229221344\n",
      "epoch: 2 step: 788, loss is 0.030581403523683548\n",
      "epoch: 2 step: 789, loss is 0.020972343161702156\n",
      "epoch: 2 step: 790, loss is 0.010416668839752674\n",
      "epoch: 2 step: 791, loss is 0.10445353388786316\n",
      "epoch: 2 step: 792, loss is 0.1130668967962265\n",
      "epoch: 2 step: 793, loss is 0.03122934326529503\n",
      "epoch: 2 step: 794, loss is 0.1706966757774353\n",
      "epoch: 2 step: 795, loss is 0.012107125483453274\n",
      "epoch: 2 step: 796, loss is 0.06578652560710907\n",
      "epoch: 2 step: 797, loss is 0.017120439559221268\n",
      "epoch: 2 step: 798, loss is 0.010613366030156612\n",
      "epoch: 2 step: 799, loss is 0.09044031798839569\n",
      "epoch: 2 step: 800, loss is 0.013245693407952785\n",
      "epoch: 2 step: 801, loss is 0.04741327092051506\n",
      "epoch: 2 step: 802, loss is 0.011435195803642273\n",
      "epoch: 2 step: 803, loss is 0.015807583928108215\n",
      "epoch: 2 step: 804, loss is 0.10072213411331177\n",
      "epoch: 2 step: 805, loss is 0.03842149302363396\n",
      "epoch: 2 step: 806, loss is 0.1105053722858429\n",
      "epoch: 2 step: 807, loss is 0.02594863995909691\n",
      "epoch: 2 step: 808, loss is 0.033826522529125214\n",
      "epoch: 2 step: 809, loss is 0.0589444562792778\n",
      "epoch: 2 step: 810, loss is 0.002224136609584093\n",
      "epoch: 2 step: 811, loss is 0.023564746603369713\n",
      "epoch: 2 step: 812, loss is 0.07956968247890472\n",
      "epoch: 2 step: 813, loss is 0.26578205823898315\n",
      "epoch: 2 step: 814, loss is 0.11083274334669113\n",
      "epoch: 2 step: 815, loss is 0.06874321401119232\n",
      "epoch: 2 step: 816, loss is 0.008709212765097618\n",
      "epoch: 2 step: 817, loss is 0.08985670655965805\n",
      "epoch: 2 step: 818, loss is 0.1659170538187027\n",
      "epoch: 2 step: 819, loss is 0.04689999297261238\n",
      "epoch: 2 step: 820, loss is 0.0883077085018158\n",
      "epoch: 2 step: 821, loss is 0.005135552491992712\n",
      "epoch: 2 step: 822, loss is 0.10998870432376862\n",
      "epoch: 2 step: 823, loss is 0.044149234890937805\n",
      "epoch: 2 step: 824, loss is 0.04848472774028778\n",
      "epoch: 2 step: 825, loss is 0.06692809611558914\n",
      "epoch: 2 step: 826, loss is 0.26345983147621155\n",
      "epoch: 2 step: 827, loss is 0.11485884338617325\n",
      "epoch: 2 step: 828, loss is 0.11608052253723145\n",
      "epoch: 2 step: 829, loss is 0.029872102662920952\n",
      "epoch: 2 step: 830, loss is 0.11682421714067459\n",
      "epoch: 2 step: 831, loss is 0.013896671123802662\n",
      "epoch: 2 step: 832, loss is 0.08783087879419327\n",
      "epoch: 2 step: 833, loss is 0.02650185488164425\n",
      "epoch: 2 step: 834, loss is 0.15178413689136505\n",
      "epoch: 2 step: 835, loss is 0.25579535961151123\n",
      "epoch: 2 step: 836, loss is 0.003302149008959532\n",
      "epoch: 2 step: 837, loss is 0.008386800065636635\n",
      "epoch: 2 step: 838, loss is 0.19178688526153564\n",
      "epoch: 2 step: 839, loss is 0.013290736824274063\n",
      "epoch: 2 step: 840, loss is 0.10588619858026505\n",
      "epoch: 2 step: 841, loss is 0.37407439947128296\n",
      "epoch: 2 step: 842, loss is 0.19693022966384888\n",
      "epoch: 2 step: 843, loss is 0.061146244406700134\n",
      "epoch: 2 step: 844, loss is 0.008029093965888023\n",
      "epoch: 2 step: 845, loss is 0.015885740518569946\n",
      "epoch: 2 step: 846, loss is 0.02700989320874214\n",
      "epoch: 2 step: 847, loss is 0.19424720108509064\n",
      "epoch: 2 step: 848, loss is 0.06897662580013275\n",
      "epoch: 2 step: 849, loss is 0.06342465430498123\n",
      "epoch: 2 step: 850, loss is 0.03255576267838478\n",
      "epoch: 2 step: 851, loss is 0.08489483594894409\n",
      "epoch: 2 step: 852, loss is 0.0641995295882225\n",
      "epoch: 2 step: 853, loss is 0.002969752298668027\n",
      "epoch: 2 step: 854, loss is 0.048654139041900635\n",
      "epoch: 2 step: 855, loss is 0.03653033450245857\n",
      "epoch: 2 step: 856, loss is 0.03157016262412071\n",
      "epoch: 2 step: 857, loss is 0.02907750941812992\n",
      "epoch: 2 step: 858, loss is 0.054398518055677414\n",
      "epoch: 2 step: 859, loss is 0.2821436822414398\n",
      "epoch: 2 step: 860, loss is 0.10334360599517822\n",
      "epoch: 2 step: 861, loss is 0.005977392662316561\n",
      "epoch: 2 step: 862, loss is 0.0061087035574018955\n",
      "epoch: 2 step: 863, loss is 0.12548765540122986\n",
      "epoch: 2 step: 864, loss is 0.005365271586924791\n",
      "epoch: 2 step: 865, loss is 0.02971842512488365\n",
      "epoch: 2 step: 866, loss is 0.013475713320076466\n",
      "epoch: 2 step: 867, loss is 0.0213175006210804\n",
      "epoch: 2 step: 868, loss is 0.0923888310790062\n",
      "epoch: 2 step: 869, loss is 0.011265959590673447\n",
      "epoch: 2 step: 870, loss is 0.09098439663648605\n",
      "epoch: 2 step: 871, loss is 0.09677236527204514\n",
      "epoch: 2 step: 872, loss is 0.022442689165472984\n",
      "epoch: 2 step: 873, loss is 0.17164398729801178\n",
      "epoch: 2 step: 874, loss is 0.03614729270339012\n",
      "epoch: 2 step: 875, loss is 0.008927174843847752\n",
      "epoch: 2 step: 876, loss is 0.007325694430619478\n",
      "epoch: 2 step: 877, loss is 0.06850983202457428\n",
      "epoch: 2 step: 878, loss is 0.0400807186961174\n",
      "epoch: 2 step: 879, loss is 0.09468149393796921\n",
      "epoch: 2 step: 880, loss is 0.16330917179584503\n",
      "epoch: 2 step: 881, loss is 0.07202068716287613\n",
      "epoch: 2 step: 882, loss is 0.05365274101495743\n",
      "epoch: 2 step: 883, loss is 0.011315659619867802\n",
      "epoch: 2 step: 884, loss is 0.2171616107225418\n",
      "epoch: 2 step: 885, loss is 0.21600598096847534\n",
      "epoch: 2 step: 886, loss is 0.10423387587070465\n",
      "epoch: 2 step: 887, loss is 0.011377714574337006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 888, loss is 0.001876220921985805\n",
      "epoch: 2 step: 889, loss is 0.2368759959936142\n",
      "epoch: 2 step: 890, loss is 0.029957303777337074\n",
      "epoch: 2 step: 891, loss is 0.06211409717798233\n",
      "epoch: 2 step: 892, loss is 0.04788995534181595\n",
      "epoch: 2 step: 893, loss is 0.024384280666708946\n",
      "epoch: 2 step: 894, loss is 0.01617944799363613\n",
      "epoch: 2 step: 895, loss is 0.0022446310613304377\n",
      "epoch: 2 step: 896, loss is 0.03722401708364487\n",
      "epoch: 2 step: 897, loss is 0.05889400839805603\n",
      "epoch: 2 step: 898, loss is 0.33900266885757446\n",
      "epoch: 2 step: 899, loss is 0.007505682297050953\n",
      "epoch: 2 step: 900, loss is 0.01585114561021328\n",
      "epoch: 2 step: 901, loss is 0.004554229788482189\n",
      "epoch: 2 step: 902, loss is 0.005087094381451607\n",
      "epoch: 2 step: 903, loss is 0.023943061009049416\n",
      "epoch: 2 step: 904, loss is 0.23371559381484985\n",
      "epoch: 2 step: 905, loss is 0.03636869788169861\n",
      "epoch: 2 step: 906, loss is 0.021334286779165268\n",
      "epoch: 2 step: 907, loss is 0.15292973816394806\n",
      "epoch: 2 step: 908, loss is 0.0027145380154252052\n",
      "epoch: 2 step: 909, loss is 0.033459778875112534\n",
      "epoch: 2 step: 910, loss is 0.16967883706092834\n",
      "epoch: 2 step: 911, loss is 0.020746180787682533\n",
      "epoch: 2 step: 912, loss is 0.0070615108124911785\n",
      "epoch: 2 step: 913, loss is 0.05168875306844711\n",
      "epoch: 2 step: 914, loss is 0.007163512520492077\n",
      "epoch: 2 step: 915, loss is 0.008040166459977627\n",
      "epoch: 2 step: 916, loss is 0.03153005242347717\n",
      "epoch: 2 step: 917, loss is 0.008922810666263103\n",
      "epoch: 2 step: 918, loss is 0.2830640971660614\n",
      "epoch: 2 step: 919, loss is 0.04611487314105034\n",
      "epoch: 2 step: 920, loss is 0.0175187811255455\n",
      "epoch: 2 step: 921, loss is 0.009663223288953304\n",
      "epoch: 2 step: 922, loss is 0.09857770800590515\n",
      "epoch: 2 step: 923, loss is 0.03961114212870598\n",
      "epoch: 2 step: 924, loss is 0.09963618218898773\n",
      "epoch: 2 step: 925, loss is 0.04058024659752846\n",
      "epoch: 2 step: 926, loss is 0.013147616758942604\n",
      "epoch: 2 step: 927, loss is 0.257588654756546\n",
      "epoch: 2 step: 928, loss is 0.08127798140048981\n",
      "epoch: 2 step: 929, loss is 0.02019335702061653\n",
      "epoch: 2 step: 930, loss is 0.08329638838768005\n",
      "epoch: 2 step: 931, loss is 0.04548180103302002\n",
      "epoch: 2 step: 932, loss is 0.005587495863437653\n",
      "epoch: 2 step: 933, loss is 0.061999451369047165\n",
      "epoch: 2 step: 934, loss is 0.07128999382257462\n",
      "epoch: 2 step: 935, loss is 0.4388275742530823\n",
      "epoch: 2 step: 936, loss is 0.014841992408037186\n",
      "epoch: 2 step: 937, loss is 0.04905262589454651\n",
      "epoch: 2 step: 938, loss is 0.07573604583740234\n",
      "epoch: 2 step: 939, loss is 0.35579100251197815\n",
      "epoch: 2 step: 940, loss is 0.09758081287145615\n",
      "epoch: 2 step: 941, loss is 0.013193010352551937\n",
      "epoch: 2 step: 942, loss is 0.03948191553354263\n",
      "epoch: 2 step: 943, loss is 0.017636915668845177\n",
      "epoch: 2 step: 944, loss is 0.006052596960216761\n",
      "epoch: 2 step: 945, loss is 0.0438477024435997\n",
      "epoch: 2 step: 946, loss is 0.18105755746364594\n",
      "epoch: 2 step: 947, loss is 0.14973066747188568\n",
      "epoch: 2 step: 948, loss is 0.030814947560429573\n",
      "epoch: 2 step: 949, loss is 0.05345255136489868\n",
      "epoch: 2 step: 950, loss is 0.011426635086536407\n",
      "epoch: 2 step: 951, loss is 0.17333218455314636\n",
      "epoch: 2 step: 952, loss is 0.15913686156272888\n",
      "epoch: 2 step: 953, loss is 0.11077272891998291\n",
      "epoch: 2 step: 954, loss is 0.025968702509999275\n",
      "epoch: 2 step: 955, loss is 0.06251344084739685\n",
      "epoch: 2 step: 956, loss is 0.14017045497894287\n",
      "epoch: 2 step: 957, loss is 0.02593245543539524\n",
      "epoch: 2 step: 958, loss is 0.01762818917632103\n",
      "epoch: 2 step: 959, loss is 0.0470043383538723\n",
      "epoch: 2 step: 960, loss is 0.05318726226687431\n",
      "epoch: 2 step: 961, loss is 0.1181190088391304\n",
      "epoch: 2 step: 962, loss is 0.06753663718700409\n",
      "epoch: 2 step: 963, loss is 0.07411788403987885\n",
      "epoch: 2 step: 964, loss is 0.07886801660060883\n",
      "epoch: 2 step: 965, loss is 0.05646524950861931\n",
      "epoch: 2 step: 966, loss is 0.26053228974342346\n",
      "epoch: 2 step: 967, loss is 0.14454303681850433\n",
      "epoch: 2 step: 968, loss is 0.023345058783888817\n",
      "epoch: 2 step: 969, loss is 0.07557429373264313\n",
      "epoch: 2 step: 970, loss is 0.02721826173365116\n",
      "epoch: 2 step: 971, loss is 0.02853047475218773\n",
      "epoch: 2 step: 972, loss is 0.07278462499380112\n",
      "epoch: 2 step: 973, loss is 0.05218823626637459\n",
      "epoch: 2 step: 974, loss is 0.04163863882422447\n",
      "epoch: 2 step: 975, loss is 0.022777676582336426\n",
      "epoch: 2 step: 976, loss is 0.0549175851047039\n",
      "epoch: 2 step: 977, loss is 0.556486189365387\n",
      "epoch: 2 step: 978, loss is 0.2708244323730469\n",
      "epoch: 2 step: 979, loss is 0.041083164513111115\n",
      "epoch: 2 step: 980, loss is 0.017237588763237\n",
      "epoch: 2 step: 981, loss is 0.253808856010437\n",
      "epoch: 2 step: 982, loss is 0.06221415102481842\n",
      "epoch: 2 step: 983, loss is 0.020005622878670692\n",
      "epoch: 2 step: 984, loss is 0.022263303399086\n",
      "epoch: 2 step: 985, loss is 0.05957747623324394\n",
      "epoch: 2 step: 986, loss is 0.0199925284832716\n",
      "epoch: 2 step: 987, loss is 0.060041967779397964\n",
      "epoch: 2 step: 988, loss is 0.07067574560642242\n",
      "epoch: 2 step: 989, loss is 0.015095102600753307\n",
      "epoch: 2 step: 990, loss is 0.12043381482362747\n",
      "epoch: 2 step: 991, loss is 0.13388244807720184\n",
      "epoch: 2 step: 992, loss is 0.09424997866153717\n",
      "epoch: 2 step: 993, loss is 0.054925914853811264\n",
      "epoch: 2 step: 994, loss is 0.024168331176042557\n",
      "epoch: 2 step: 995, loss is 0.06510383635759354\n",
      "epoch: 2 step: 996, loss is 0.04673558846116066\n",
      "epoch: 2 step: 997, loss is 0.5256257653236389\n",
      "epoch: 2 step: 998, loss is 0.057773131877183914\n",
      "epoch: 2 step: 999, loss is 0.04786946624517441\n",
      "epoch: 2 step: 1000, loss is 0.006384499836713076\n",
      "epoch: 2 step: 1001, loss is 0.48267093300819397\n",
      "epoch: 2 step: 1002, loss is 0.06059626117348671\n",
      "epoch: 2 step: 1003, loss is 0.06801462918519974\n",
      "epoch: 2 step: 1004, loss is 0.08214071393013\n",
      "epoch: 2 step: 1005, loss is 0.22758999466896057\n",
      "epoch: 2 step: 1006, loss is 0.1696440428495407\n",
      "epoch: 2 step: 1007, loss is 0.20854468643665314\n",
      "epoch: 2 step: 1008, loss is 0.03246176242828369\n",
      "epoch: 2 step: 1009, loss is 0.05595390498638153\n",
      "epoch: 2 step: 1010, loss is 0.14450250566005707\n",
      "epoch: 2 step: 1011, loss is 0.08214335143566132\n",
      "epoch: 2 step: 1012, loss is 0.08180355280637741\n",
      "epoch: 2 step: 1013, loss is 0.05524510517716408\n",
      "epoch: 2 step: 1014, loss is 0.16990628838539124\n",
      "epoch: 2 step: 1015, loss is 0.05617964640259743\n",
      "epoch: 2 step: 1016, loss is 0.07482873648405075\n",
      "epoch: 2 step: 1017, loss is 0.20944395661354065\n",
      "epoch: 2 step: 1018, loss is 0.21696209907531738\n",
      "epoch: 2 step: 1019, loss is 0.13002616167068481\n",
      "epoch: 2 step: 1020, loss is 0.06587696075439453\n",
      "epoch: 2 step: 1021, loss is 0.062080517411231995\n",
      "epoch: 2 step: 1022, loss is 0.006547034718096256\n",
      "epoch: 2 step: 1023, loss is 0.037727948278188705\n",
      "epoch: 2 step: 1024, loss is 0.021525656804442406\n",
      "epoch: 2 step: 1025, loss is 0.009429247118532658\n",
      "epoch: 2 step: 1026, loss is 0.11852153390645981\n",
      "epoch: 2 step: 1027, loss is 0.004264813847839832\n",
      "epoch: 2 step: 1028, loss is 0.01855078898370266\n",
      "epoch: 2 step: 1029, loss is 0.2226543426513672\n",
      "epoch: 2 step: 1030, loss is 0.09832106530666351\n",
      "epoch: 2 step: 1031, loss is 0.023184774443507195\n",
      "epoch: 2 step: 1032, loss is 0.07479363679885864\n",
      "epoch: 2 step: 1033, loss is 0.010485013946890831\n",
      "epoch: 2 step: 1034, loss is 0.008390800096094608\n",
      "epoch: 2 step: 1035, loss is 0.11543096601963043\n",
      "epoch: 2 step: 1036, loss is 0.07713603228330612\n",
      "epoch: 2 step: 1037, loss is 0.06559101492166519\n",
      "epoch: 2 step: 1038, loss is 0.12308388948440552\n",
      "epoch: 2 step: 1039, loss is 0.020121261477470398\n",
      "epoch: 2 step: 1040, loss is 0.005328237544745207\n",
      "epoch: 2 step: 1041, loss is 0.016592085361480713\n",
      "epoch: 2 step: 1042, loss is 0.07417669892311096\n",
      "epoch: 2 step: 1043, loss is 0.023397529497742653\n",
      "epoch: 2 step: 1044, loss is 0.06948022544384003\n",
      "epoch: 2 step: 1045, loss is 0.011222302913665771\n",
      "epoch: 2 step: 1046, loss is 0.012372974306344986\n",
      "epoch: 2 step: 1047, loss is 0.26594722270965576\n",
      "epoch: 2 step: 1048, loss is 0.06058121472597122\n",
      "epoch: 2 step: 1049, loss is 0.0779058039188385\n",
      "epoch: 2 step: 1050, loss is 0.07050841301679611\n",
      "epoch: 2 step: 1051, loss is 0.026457715779542923\n",
      "epoch: 2 step: 1052, loss is 0.047913726419210434\n",
      "epoch: 2 step: 1053, loss is 0.023306485265493393\n",
      "epoch: 2 step: 1054, loss is 0.036645323038101196\n",
      "epoch: 2 step: 1055, loss is 0.12279579043388367\n",
      "epoch: 2 step: 1056, loss is 0.009290331974625587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 1057, loss is 0.09453054517507553\n",
      "epoch: 2 step: 1058, loss is 0.03336300328373909\n",
      "epoch: 2 step: 1059, loss is 0.0747152641415596\n",
      "epoch: 2 step: 1060, loss is 0.4572548568248749\n",
      "epoch: 2 step: 1061, loss is 0.06824425607919693\n",
      "epoch: 2 step: 1062, loss is 0.026360362768173218\n",
      "epoch: 2 step: 1063, loss is 0.05106913670897484\n",
      "epoch: 2 step: 1064, loss is 0.01670732907950878\n",
      "epoch: 2 step: 1065, loss is 0.02957598678767681\n",
      "epoch: 2 step: 1066, loss is 0.050829529762268066\n",
      "epoch: 2 step: 1067, loss is 0.06265132874250412\n",
      "epoch: 2 step: 1068, loss is 0.015490812249481678\n",
      "epoch: 2 step: 1069, loss is 0.057416267693042755\n",
      "epoch: 2 step: 1070, loss is 0.05104035511612892\n",
      "epoch: 2 step: 1071, loss is 0.06300531327724457\n",
      "epoch: 2 step: 1072, loss is 0.03201613575220108\n",
      "epoch: 2 step: 1073, loss is 0.022769061848521233\n",
      "epoch: 2 step: 1074, loss is 0.14753110706806183\n",
      "epoch: 2 step: 1075, loss is 0.10923277586698532\n",
      "epoch: 2 step: 1076, loss is 0.01872766576707363\n",
      "epoch: 2 step: 1077, loss is 0.033120885491371155\n",
      "epoch: 2 step: 1078, loss is 0.008699080906808376\n",
      "epoch: 2 step: 1079, loss is 0.012815584428608418\n",
      "epoch: 2 step: 1080, loss is 0.05644524469971657\n",
      "epoch: 2 step: 1081, loss is 0.002038717968389392\n",
      "epoch: 2 step: 1082, loss is 0.0026925704441964626\n",
      "epoch: 2 step: 1083, loss is 0.0644206553697586\n",
      "epoch: 2 step: 1084, loss is 0.0315743163228035\n",
      "epoch: 2 step: 1085, loss is 0.0022480979096144438\n",
      "epoch: 2 step: 1086, loss is 0.004212834406644106\n",
      "epoch: 2 step: 1087, loss is 0.1521570235490799\n",
      "epoch: 2 step: 1088, loss is 0.08861912786960602\n",
      "epoch: 2 step: 1089, loss is 0.15868963301181793\n",
      "epoch: 2 step: 1090, loss is 0.017635969445109367\n",
      "epoch: 2 step: 1091, loss is 0.08152876794338226\n",
      "epoch: 2 step: 1092, loss is 0.11568266898393631\n",
      "epoch: 2 step: 1093, loss is 0.004875841084867716\n",
      "epoch: 2 step: 1094, loss is 0.1387086808681488\n",
      "epoch: 2 step: 1095, loss is 0.10749046504497528\n",
      "epoch: 2 step: 1096, loss is 0.2240271270275116\n",
      "epoch: 2 step: 1097, loss is 0.024816006422042847\n",
      "epoch: 2 step: 1098, loss is 0.008846449665725231\n",
      "epoch: 2 step: 1099, loss is 0.01250859908759594\n",
      "epoch: 2 step: 1100, loss is 0.07428481429815292\n",
      "epoch: 2 step: 1101, loss is 0.09204408526420593\n",
      "epoch: 2 step: 1102, loss is 0.016939476132392883\n",
      "epoch: 2 step: 1103, loss is 0.03641509637236595\n",
      "epoch: 2 step: 1104, loss is 0.011545438319444656\n",
      "epoch: 2 step: 1105, loss is 0.1393560916185379\n",
      "epoch: 2 step: 1106, loss is 0.006906803697347641\n",
      "epoch: 2 step: 1107, loss is 0.00622133444994688\n",
      "epoch: 2 step: 1108, loss is 0.20481997728347778\n",
      "epoch: 2 step: 1109, loss is 0.09351454675197601\n",
      "epoch: 2 step: 1110, loss is 0.010610299184918404\n",
      "epoch: 2 step: 1111, loss is 0.10467567294836044\n",
      "epoch: 2 step: 1112, loss is 0.05106863006949425\n",
      "epoch: 2 step: 1113, loss is 0.19310566782951355\n",
      "epoch: 2 step: 1114, loss is 0.09059792757034302\n",
      "epoch: 2 step: 1115, loss is 0.034342922270298004\n",
      "epoch: 2 step: 1116, loss is 0.027805736288428307\n",
      "epoch: 2 step: 1117, loss is 0.05080448463559151\n",
      "epoch: 2 step: 1118, loss is 0.023015225306153297\n",
      "epoch: 2 step: 1119, loss is 0.13162623345851898\n",
      "epoch: 2 step: 1120, loss is 0.03705573454499245\n",
      "epoch: 2 step: 1121, loss is 0.05803403630852699\n",
      "epoch: 2 step: 1122, loss is 0.13783445954322815\n",
      "epoch: 2 step: 1123, loss is 0.0027811869513243437\n",
      "epoch: 2 step: 1124, loss is 0.04145069420337677\n",
      "epoch: 2 step: 1125, loss is 0.14325512945652008\n",
      "epoch: 2 step: 1126, loss is 0.00839714240282774\n",
      "epoch: 2 step: 1127, loss is 0.2842468023300171\n",
      "epoch: 2 step: 1128, loss is 0.11341822892427444\n",
      "epoch: 2 step: 1129, loss is 0.01459616981446743\n",
      "epoch: 2 step: 1130, loss is 0.018161311745643616\n",
      "epoch: 2 step: 1131, loss is 0.013772416859865189\n",
      "epoch: 2 step: 1132, loss is 0.02427610382437706\n",
      "epoch: 2 step: 1133, loss is 0.09044364094734192\n",
      "epoch: 2 step: 1134, loss is 0.004858429078012705\n",
      "epoch: 2 step: 1135, loss is 0.061922863125801086\n",
      "epoch: 2 step: 1136, loss is 0.004198614973574877\n",
      "epoch: 2 step: 1137, loss is 0.0070427036844193935\n",
      "epoch: 2 step: 1138, loss is 0.012647871859371662\n",
      "epoch: 2 step: 1139, loss is 0.0919058620929718\n",
      "epoch: 2 step: 1140, loss is 0.09593671560287476\n",
      "epoch: 2 step: 1141, loss is 0.522936999797821\n",
      "epoch: 2 step: 1142, loss is 0.04889564961194992\n",
      "epoch: 2 step: 1143, loss is 0.11800796538591385\n",
      "epoch: 2 step: 1144, loss is 0.24277769029140472\n",
      "epoch: 2 step: 1145, loss is 0.0573093444108963\n",
      "epoch: 2 step: 1146, loss is 0.08484644442796707\n",
      "epoch: 2 step: 1147, loss is 0.045187968760728836\n",
      "epoch: 2 step: 1148, loss is 0.0057192277163267136\n",
      "epoch: 2 step: 1149, loss is 0.03579118102788925\n",
      "epoch: 2 step: 1150, loss is 0.07956793159246445\n",
      "epoch: 2 step: 1151, loss is 0.05557440593838692\n",
      "epoch: 2 step: 1152, loss is 0.1254224181175232\n",
      "epoch: 2 step: 1153, loss is 0.013017367571592331\n",
      "epoch: 2 step: 1154, loss is 0.10495846718549728\n",
      "epoch: 2 step: 1155, loss is 0.06314614415168762\n",
      "epoch: 2 step: 1156, loss is 0.1413920521736145\n",
      "epoch: 2 step: 1157, loss is 0.008157121948897839\n",
      "epoch: 2 step: 1158, loss is 0.15215428173542023\n",
      "epoch: 2 step: 1159, loss is 0.06906772404909134\n",
      "epoch: 2 step: 1160, loss is 0.041183680295944214\n",
      "epoch: 2 step: 1161, loss is 0.025474634021520615\n",
      "epoch: 2 step: 1162, loss is 0.12443588674068451\n",
      "epoch: 2 step: 1163, loss is 0.030349573120474815\n",
      "epoch: 2 step: 1164, loss is 0.25117769837379456\n",
      "epoch: 2 step: 1165, loss is 0.01198236458003521\n",
      "epoch: 2 step: 1166, loss is 0.08238163590431213\n",
      "epoch: 2 step: 1167, loss is 0.08133617788553238\n",
      "epoch: 2 step: 1168, loss is 0.03237215429544449\n",
      "epoch: 2 step: 1169, loss is 0.22733914852142334\n",
      "epoch: 2 step: 1170, loss is 0.037799105048179626\n",
      "epoch: 2 step: 1171, loss is 0.016165973618626595\n",
      "epoch: 2 step: 1172, loss is 0.00676541356369853\n",
      "epoch: 2 step: 1173, loss is 0.08909721672534943\n",
      "epoch: 2 step: 1174, loss is 0.07605241239070892\n",
      "epoch: 2 step: 1175, loss is 0.14362940192222595\n",
      "epoch: 2 step: 1176, loss is 0.00994623638689518\n",
      "epoch: 2 step: 1177, loss is 0.10929805785417557\n",
      "epoch: 2 step: 1178, loss is 0.07023410499095917\n",
      "epoch: 2 step: 1179, loss is 0.588677167892456\n",
      "epoch: 2 step: 1180, loss is 0.24768655002117157\n",
      "epoch: 2 step: 1181, loss is 0.07775575667619705\n",
      "epoch: 2 step: 1182, loss is 0.05538290739059448\n",
      "epoch: 2 step: 1183, loss is 0.04965182766318321\n",
      "epoch: 2 step: 1184, loss is 0.30611467361450195\n",
      "epoch: 2 step: 1185, loss is 0.03140774741768837\n",
      "epoch: 2 step: 1186, loss is 0.028980150818824768\n",
      "epoch: 2 step: 1187, loss is 0.00620062742382288\n",
      "epoch: 2 step: 1188, loss is 0.0190699752420187\n",
      "epoch: 2 step: 1189, loss is 0.06866919249296188\n",
      "epoch: 2 step: 1190, loss is 0.005757360253483057\n",
      "epoch: 2 step: 1191, loss is 0.05922135338187218\n",
      "epoch: 2 step: 1192, loss is 0.050467781722545624\n",
      "epoch: 2 step: 1193, loss is 0.13890859484672546\n",
      "epoch: 2 step: 1194, loss is 0.037371765822172165\n",
      "epoch: 2 step: 1195, loss is 0.019492115825414658\n",
      "epoch: 2 step: 1196, loss is 0.04486190527677536\n",
      "epoch: 2 step: 1197, loss is 0.15785233676433563\n",
      "epoch: 2 step: 1198, loss is 0.013574284501373768\n",
      "epoch: 2 step: 1199, loss is 0.07486604899168015\n",
      "epoch: 2 step: 1200, loss is 0.03538321703672409\n",
      "epoch: 2 step: 1201, loss is 0.03632183000445366\n",
      "epoch: 2 step: 1202, loss is 0.041161321103572845\n",
      "epoch: 2 step: 1203, loss is 0.0143750524148345\n",
      "epoch: 2 step: 1204, loss is 0.03587253764271736\n",
      "epoch: 2 step: 1205, loss is 0.1395494043827057\n",
      "epoch: 2 step: 1206, loss is 0.16670429706573486\n",
      "epoch: 2 step: 1207, loss is 0.08930173516273499\n",
      "epoch: 2 step: 1208, loss is 0.18711547553539276\n",
      "epoch: 2 step: 1209, loss is 0.006627307739108801\n",
      "epoch: 2 step: 1210, loss is 0.34109216928482056\n",
      "epoch: 2 step: 1211, loss is 0.18111315369606018\n",
      "epoch: 2 step: 1212, loss is 0.013548733666539192\n",
      "epoch: 2 step: 1213, loss is 0.05159064382314682\n",
      "epoch: 2 step: 1214, loss is 0.01809723861515522\n",
      "epoch: 2 step: 1215, loss is 0.008891030214726925\n",
      "epoch: 2 step: 1216, loss is 0.01315259374678135\n",
      "epoch: 2 step: 1217, loss is 0.11369677633047104\n",
      "epoch: 2 step: 1218, loss is 0.17204134166240692\n",
      "epoch: 2 step: 1219, loss is 0.005580658558756113\n",
      "epoch: 2 step: 1220, loss is 0.007680172566324472\n",
      "epoch: 2 step: 1221, loss is 0.07808007299900055\n",
      "epoch: 2 step: 1222, loss is 0.03188426047563553\n",
      "epoch: 2 step: 1223, loss is 0.16845519840717316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 1224, loss is 0.007985036820173264\n",
      "epoch: 2 step: 1225, loss is 0.14100146293640137\n",
      "epoch: 2 step: 1226, loss is 0.1307525932788849\n",
      "epoch: 2 step: 1227, loss is 0.2446032613515854\n",
      "epoch: 2 step: 1228, loss is 0.01641307771205902\n",
      "epoch: 2 step: 1229, loss is 0.10535530000925064\n",
      "epoch: 2 step: 1230, loss is 0.05393057316541672\n",
      "epoch: 2 step: 1231, loss is 0.04035932198166847\n",
      "epoch: 2 step: 1232, loss is 0.15281923115253448\n",
      "epoch: 2 step: 1233, loss is 0.02055741287767887\n",
      "epoch: 2 step: 1234, loss is 0.15283088386058807\n",
      "epoch: 2 step: 1235, loss is 0.004679478704929352\n",
      "epoch: 2 step: 1236, loss is 0.013644294813275337\n",
      "epoch: 2 step: 1237, loss is 0.2065839320421219\n",
      "epoch: 2 step: 1238, loss is 0.040374740958213806\n",
      "epoch: 2 step: 1239, loss is 0.033328041434288025\n",
      "epoch: 2 step: 1240, loss is 0.08611977100372314\n",
      "epoch: 2 step: 1241, loss is 0.03903932496905327\n",
      "epoch: 2 step: 1242, loss is 0.08192469924688339\n",
      "epoch: 2 step: 1243, loss is 0.005587873049080372\n",
      "epoch: 2 step: 1244, loss is 0.016478627920150757\n",
      "epoch: 2 step: 1245, loss is 0.02065248414874077\n",
      "epoch: 2 step: 1246, loss is 0.10028332471847534\n",
      "epoch: 2 step: 1247, loss is 0.01166254747658968\n",
      "epoch: 2 step: 1248, loss is 0.004489346407353878\n",
      "epoch: 2 step: 1249, loss is 0.10254476219415665\n",
      "epoch: 2 step: 1250, loss is 0.005981071386486292\n",
      "epoch: 2 step: 1251, loss is 0.03675733506679535\n",
      "epoch: 2 step: 1252, loss is 0.0372513048350811\n",
      "epoch: 2 step: 1253, loss is 0.10231886804103851\n",
      "epoch: 2 step: 1254, loss is 0.0520273819565773\n",
      "epoch: 2 step: 1255, loss is 0.022649625316262245\n",
      "epoch: 2 step: 1256, loss is 0.022980567067861557\n",
      "epoch: 2 step: 1257, loss is 0.038848619908094406\n",
      "epoch: 2 step: 1258, loss is 0.032586488872766495\n",
      "epoch: 2 step: 1259, loss is 0.009123534895479679\n",
      "epoch: 2 step: 1260, loss is 0.003235754556953907\n",
      "epoch: 2 step: 1261, loss is 0.17110002040863037\n",
      "epoch: 2 step: 1262, loss is 0.14804691076278687\n",
      "epoch: 2 step: 1263, loss is 0.3311002850532532\n",
      "epoch: 2 step: 1264, loss is 0.1290888786315918\n",
      "epoch: 2 step: 1265, loss is 0.004931451287120581\n",
      "epoch: 2 step: 1266, loss is 0.008228383027017117\n",
      "epoch: 2 step: 1267, loss is 0.004434454720467329\n",
      "epoch: 2 step: 1268, loss is 0.10232437402009964\n",
      "epoch: 2 step: 1269, loss is 0.010330778546631336\n",
      "epoch: 2 step: 1270, loss is 0.09953434020280838\n",
      "epoch: 2 step: 1271, loss is 0.00819858442991972\n",
      "epoch: 2 step: 1272, loss is 0.02042737603187561\n",
      "epoch: 2 step: 1273, loss is 0.005130171775817871\n",
      "epoch: 2 step: 1274, loss is 0.11143822968006134\n",
      "epoch: 2 step: 1275, loss is 0.050423745065927505\n",
      "epoch: 2 step: 1276, loss is 0.03391969949007034\n",
      "epoch: 2 step: 1277, loss is 0.03932693973183632\n",
      "epoch: 2 step: 1278, loss is 0.013537956401705742\n",
      "epoch: 2 step: 1279, loss is 0.006386506836861372\n",
      "epoch: 2 step: 1280, loss is 0.10991287231445312\n",
      "epoch: 2 step: 1281, loss is 0.004141631070524454\n",
      "epoch: 2 step: 1282, loss is 0.011090430431067944\n",
      "epoch: 2 step: 1283, loss is 0.09761649370193481\n",
      "epoch: 2 step: 1284, loss is 0.11558747291564941\n",
      "epoch: 2 step: 1285, loss is 0.2305130511522293\n",
      "epoch: 2 step: 1286, loss is 0.10689246654510498\n",
      "epoch: 2 step: 1287, loss is 0.04412543773651123\n",
      "epoch: 2 step: 1288, loss is 0.00826217420399189\n",
      "epoch: 2 step: 1289, loss is 0.006947744637727737\n",
      "epoch: 2 step: 1290, loss is 0.004742790944874287\n",
      "epoch: 2 step: 1291, loss is 0.17826703190803528\n",
      "epoch: 2 step: 1292, loss is 0.09221288561820984\n",
      "epoch: 2 step: 1293, loss is 0.005967909470200539\n",
      "epoch: 2 step: 1294, loss is 0.20747369527816772\n",
      "epoch: 2 step: 1295, loss is 0.005548514891415834\n",
      "epoch: 2 step: 1296, loss is 0.07113440334796906\n",
      "epoch: 2 step: 1297, loss is 0.29448580741882324\n",
      "epoch: 2 step: 1298, loss is 0.1473841369152069\n",
      "epoch: 2 step: 1299, loss is 0.024087494239211082\n",
      "epoch: 2 step: 1300, loss is 0.07348638027906418\n",
      "epoch: 2 step: 1301, loss is 0.03672010079026222\n",
      "epoch: 2 step: 1302, loss is 0.05290147662162781\n",
      "epoch: 2 step: 1303, loss is 0.05405942723155022\n",
      "epoch: 2 step: 1304, loss is 0.01106263417750597\n",
      "epoch: 2 step: 1305, loss is 0.24155940115451813\n",
      "epoch: 2 step: 1306, loss is 0.015596602112054825\n",
      "epoch: 2 step: 1307, loss is 0.07038307934999466\n",
      "epoch: 2 step: 1308, loss is 0.1870936155319214\n",
      "epoch: 2 step: 1309, loss is 0.10211732983589172\n",
      "epoch: 2 step: 1310, loss is 0.11264524608850479\n",
      "epoch: 2 step: 1311, loss is 0.004783114418387413\n",
      "epoch: 2 step: 1312, loss is 0.2035505771636963\n",
      "epoch: 2 step: 1313, loss is 0.0068878959864377975\n",
      "epoch: 2 step: 1314, loss is 0.034190889447927475\n",
      "epoch: 2 step: 1315, loss is 0.011874188669025898\n",
      "epoch: 2 step: 1316, loss is 0.0021256471518427134\n",
      "epoch: 2 step: 1317, loss is 0.06784321367740631\n",
      "epoch: 2 step: 1318, loss is 0.02225520648062229\n",
      "epoch: 2 step: 1319, loss is 0.022410603240132332\n",
      "epoch: 2 step: 1320, loss is 0.21776419878005981\n",
      "epoch: 2 step: 1321, loss is 0.24943028390407562\n",
      "epoch: 2 step: 1322, loss is 0.09299340844154358\n",
      "epoch: 2 step: 1323, loss is 0.15780487656593323\n",
      "epoch: 2 step: 1324, loss is 0.013852966018021107\n",
      "epoch: 2 step: 1325, loss is 0.088695228099823\n",
      "epoch: 2 step: 1326, loss is 0.04829247295856476\n",
      "epoch: 2 step: 1327, loss is 0.003957731183618307\n",
      "epoch: 2 step: 1328, loss is 0.21716518700122833\n",
      "epoch: 2 step: 1329, loss is 0.1613435000181198\n",
      "epoch: 2 step: 1330, loss is 0.09673305600881577\n",
      "epoch: 2 step: 1331, loss is 0.006209660321474075\n",
      "epoch: 2 step: 1332, loss is 0.06929483264684677\n",
      "epoch: 2 step: 1333, loss is 0.14846964180469513\n",
      "epoch: 2 step: 1334, loss is 0.0073374612256884575\n",
      "epoch: 2 step: 1335, loss is 0.01101620215922594\n",
      "epoch: 2 step: 1336, loss is 0.4005107283592224\n",
      "epoch: 2 step: 1337, loss is 0.07216033339500427\n",
      "epoch: 2 step: 1338, loss is 0.005607940722256899\n",
      "epoch: 2 step: 1339, loss is 0.018650086596608162\n",
      "epoch: 2 step: 1340, loss is 0.19979366660118103\n",
      "epoch: 2 step: 1341, loss is 0.008820021525025368\n",
      "epoch: 2 step: 1342, loss is 0.048793986439704895\n",
      "epoch: 2 step: 1343, loss is 0.0019832258112728596\n",
      "epoch: 2 step: 1344, loss is 0.03919258341193199\n",
      "epoch: 2 step: 1345, loss is 0.01344291865825653\n",
      "epoch: 2 step: 1346, loss is 0.028901737183332443\n",
      "epoch: 2 step: 1347, loss is 0.15491890907287598\n",
      "epoch: 2 step: 1348, loss is 0.13707897067070007\n",
      "epoch: 2 step: 1349, loss is 0.028075311332941055\n",
      "epoch: 2 step: 1350, loss is 0.013520969077944756\n",
      "epoch: 2 step: 1351, loss is 0.0124805998057127\n",
      "epoch: 2 step: 1352, loss is 0.08580057322978973\n",
      "epoch: 2 step: 1353, loss is 0.03206346184015274\n",
      "epoch: 2 step: 1354, loss is 0.1154467985033989\n",
      "epoch: 2 step: 1355, loss is 0.006286995019763708\n",
      "epoch: 2 step: 1356, loss is 0.04740520939230919\n",
      "epoch: 2 step: 1357, loss is 0.12286681681871414\n",
      "epoch: 2 step: 1358, loss is 0.015360052697360516\n",
      "epoch: 2 step: 1359, loss is 0.00815547164529562\n",
      "epoch: 2 step: 1360, loss is 0.20569264888763428\n",
      "epoch: 2 step: 1361, loss is 0.0026943415869027376\n",
      "epoch: 2 step: 1362, loss is 0.005617799703031778\n",
      "epoch: 2 step: 1363, loss is 0.0660356730222702\n",
      "epoch: 2 step: 1364, loss is 0.046908728778362274\n",
      "epoch: 2 step: 1365, loss is 0.03246796503663063\n",
      "epoch: 2 step: 1366, loss is 0.05680030956864357\n",
      "epoch: 2 step: 1367, loss is 0.01529949065297842\n",
      "epoch: 2 step: 1368, loss is 0.05067723989486694\n",
      "epoch: 2 step: 1369, loss is 0.010800275020301342\n",
      "epoch: 2 step: 1370, loss is 0.037470389157533646\n",
      "epoch: 2 step: 1371, loss is 0.06046414002776146\n",
      "epoch: 2 step: 1372, loss is 0.011649884283542633\n",
      "epoch: 2 step: 1373, loss is 0.015530673786997795\n",
      "epoch: 2 step: 1374, loss is 0.01162161398679018\n",
      "epoch: 2 step: 1375, loss is 0.06846686452627182\n",
      "epoch: 2 step: 1376, loss is 0.07460550218820572\n",
      "epoch: 2 step: 1377, loss is 0.5398586392402649\n",
      "epoch: 2 step: 1378, loss is 0.006828080862760544\n",
      "epoch: 2 step: 1379, loss is 0.15926198661327362\n",
      "epoch: 2 step: 1380, loss is 0.45383220911026\n",
      "epoch: 2 step: 1381, loss is 0.0979447290301323\n",
      "epoch: 2 step: 1382, loss is 0.08861640095710754\n",
      "epoch: 2 step: 1383, loss is 0.03733972832560539\n",
      "epoch: 2 step: 1384, loss is 0.008723292499780655\n",
      "epoch: 2 step: 1385, loss is 0.0077871596440672874\n",
      "epoch: 2 step: 1386, loss is 0.011407650075852871\n",
      "epoch: 2 step: 1387, loss is 0.05010826513171196\n",
      "epoch: 2 step: 1388, loss is 0.1144808828830719\n",
      "epoch: 2 step: 1389, loss is 0.1300036907196045\n",
      "epoch: 2 step: 1390, loss is 0.005993611179292202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 1391, loss is 0.02429046481847763\n",
      "epoch: 2 step: 1392, loss is 0.09285969287157059\n",
      "epoch: 2 step: 1393, loss is 0.12569549679756165\n",
      "epoch: 2 step: 1394, loss is 0.02329159900546074\n",
      "epoch: 2 step: 1395, loss is 0.03239411115646362\n",
      "epoch: 2 step: 1396, loss is 0.13658714294433594\n",
      "epoch: 2 step: 1397, loss is 0.25627562403678894\n",
      "epoch: 2 step: 1398, loss is 0.003992569167166948\n",
      "epoch: 2 step: 1399, loss is 0.021011747419834137\n",
      "epoch: 2 step: 1400, loss is 0.040371377021074295\n",
      "epoch: 2 step: 1401, loss is 0.015484564006328583\n",
      "epoch: 2 step: 1402, loss is 0.1370585709810257\n",
      "epoch: 2 step: 1403, loss is 0.2257220596075058\n",
      "epoch: 2 step: 1404, loss is 0.12070724368095398\n",
      "epoch: 2 step: 1405, loss is 0.0056209503673017025\n",
      "epoch: 2 step: 1406, loss is 0.061457403004169464\n",
      "epoch: 2 step: 1407, loss is 0.23974403738975525\n",
      "epoch: 2 step: 1408, loss is 0.1294303685426712\n",
      "epoch: 2 step: 1409, loss is 0.1379520446062088\n",
      "epoch: 2 step: 1410, loss is 0.016550391912460327\n",
      "epoch: 2 step: 1411, loss is 0.04727895185351372\n",
      "epoch: 2 step: 1412, loss is 0.010639689862728119\n",
      "epoch: 2 step: 1413, loss is 0.10887794941663742\n",
      "epoch: 2 step: 1414, loss is 0.06480840593576431\n",
      "epoch: 2 step: 1415, loss is 0.09589783847332001\n",
      "epoch: 2 step: 1416, loss is 0.004078452475368977\n",
      "epoch: 2 step: 1417, loss is 0.08189636468887329\n",
      "epoch: 2 step: 1418, loss is 0.10129960626363754\n",
      "epoch: 2 step: 1419, loss is 0.10513439029455185\n",
      "epoch: 2 step: 1420, loss is 0.017463505268096924\n",
      "epoch: 2 step: 1421, loss is 0.08131148666143417\n",
      "epoch: 2 step: 1422, loss is 0.016583574935793877\n",
      "epoch: 2 step: 1423, loss is 0.18861991167068481\n",
      "epoch: 2 step: 1424, loss is 0.013174204155802727\n",
      "epoch: 2 step: 1425, loss is 0.06008937954902649\n",
      "epoch: 2 step: 1426, loss is 0.19918383657932281\n",
      "epoch: 2 step: 1427, loss is 0.07195550948381424\n",
      "epoch: 2 step: 1428, loss is 0.005006704013794661\n",
      "epoch: 2 step: 1429, loss is 0.026422636583447456\n",
      "epoch: 2 step: 1430, loss is 0.1207285076379776\n",
      "epoch: 2 step: 1431, loss is 0.030578574165701866\n",
      "epoch: 2 step: 1432, loss is 0.15198801457881927\n",
      "epoch: 2 step: 1433, loss is 0.007655126508325338\n",
      "epoch: 2 step: 1434, loss is 0.0015541171887889504\n",
      "epoch: 2 step: 1435, loss is 0.02039925381541252\n",
      "epoch: 2 step: 1436, loss is 0.047343797981739044\n",
      "epoch: 2 step: 1437, loss is 0.08692089468240738\n",
      "epoch: 2 step: 1438, loss is 0.2659878730773926\n",
      "epoch: 2 step: 1439, loss is 0.028891760855913162\n",
      "epoch: 2 step: 1440, loss is 0.07111956179141998\n",
      "epoch: 2 step: 1441, loss is 0.03658454865217209\n",
      "epoch: 2 step: 1442, loss is 0.00669217761605978\n",
      "epoch: 2 step: 1443, loss is 0.09236712753772736\n",
      "epoch: 2 step: 1444, loss is 0.024987393990159035\n",
      "epoch: 2 step: 1445, loss is 0.060072161257267\n",
      "epoch: 2 step: 1446, loss is 0.18100741505622864\n",
      "epoch: 2 step: 1447, loss is 0.12593583762645721\n",
      "epoch: 2 step: 1448, loss is 0.24917951226234436\n",
      "epoch: 2 step: 1449, loss is 0.0890481248497963\n",
      "epoch: 2 step: 1450, loss is 0.11215032637119293\n",
      "epoch: 2 step: 1451, loss is 0.03793368861079216\n",
      "epoch: 2 step: 1452, loss is 0.06581805646419525\n",
      "epoch: 2 step: 1453, loss is 0.019332071766257286\n",
      "epoch: 2 step: 1454, loss is 0.13024842739105225\n",
      "epoch: 2 step: 1455, loss is 0.1130327582359314\n",
      "epoch: 2 step: 1456, loss is 0.0015433517983183265\n",
      "epoch: 2 step: 1457, loss is 0.015224477276206017\n",
      "epoch: 2 step: 1458, loss is 0.025805290788412094\n",
      "epoch: 2 step: 1459, loss is 0.017153458669781685\n",
      "epoch: 2 step: 1460, loss is 0.007884486578404903\n",
      "epoch: 2 step: 1461, loss is 0.003928298596292734\n",
      "epoch: 2 step: 1462, loss is 0.06546451896429062\n",
      "epoch: 2 step: 1463, loss is 0.032652419060468674\n",
      "epoch: 2 step: 1464, loss is 0.1270732581615448\n",
      "epoch: 2 step: 1465, loss is 0.10205879807472229\n",
      "epoch: 2 step: 1466, loss is 0.18144534528255463\n",
      "epoch: 2 step: 1467, loss is 0.06215408816933632\n",
      "epoch: 2 step: 1468, loss is 0.09319417923688889\n",
      "epoch: 2 step: 1469, loss is 0.17455628514289856\n",
      "epoch: 2 step: 1470, loss is 0.2179824262857437\n",
      "epoch: 2 step: 1471, loss is 0.38887467980384827\n",
      "epoch: 2 step: 1472, loss is 0.13283976912498474\n",
      "epoch: 2 step: 1473, loss is 0.007839443162083626\n",
      "epoch: 2 step: 1474, loss is 0.06605057418346405\n",
      "epoch: 2 step: 1475, loss is 0.18395766615867615\n",
      "epoch: 2 step: 1476, loss is 0.06061088666319847\n",
      "epoch: 2 step: 1477, loss is 0.03373835235834122\n",
      "epoch: 2 step: 1478, loss is 0.003229615744203329\n",
      "epoch: 2 step: 1479, loss is 0.06238624081015587\n",
      "epoch: 2 step: 1480, loss is 0.09642880409955978\n",
      "epoch: 2 step: 1481, loss is 0.0071297017857432365\n",
      "epoch: 2 step: 1482, loss is 0.06258607655763626\n",
      "epoch: 2 step: 1483, loss is 0.08718682080507278\n",
      "epoch: 2 step: 1484, loss is 0.13856394588947296\n",
      "epoch: 2 step: 1485, loss is 0.25318726897239685\n",
      "epoch: 2 step: 1486, loss is 0.05187110975384712\n",
      "epoch: 2 step: 1487, loss is 0.012874562293291092\n",
      "epoch: 2 step: 1488, loss is 0.026883265003561974\n",
      "epoch: 2 step: 1489, loss is 0.31438082456588745\n",
      "epoch: 2 step: 1490, loss is 0.03300685063004494\n",
      "epoch: 2 step: 1491, loss is 0.013003842905163765\n",
      "epoch: 2 step: 1492, loss is 0.03688594326376915\n",
      "epoch: 2 step: 1493, loss is 0.10404343903064728\n",
      "epoch: 2 step: 1494, loss is 0.12199468910694122\n",
      "epoch: 2 step: 1495, loss is 0.08291097730398178\n",
      "epoch: 2 step: 1496, loss is 0.11878975480794907\n",
      "epoch: 2 step: 1497, loss is 0.04496920108795166\n",
      "epoch: 2 step: 1498, loss is 0.010933540761470795\n",
      "epoch: 2 step: 1499, loss is 0.025953324511647224\n",
      "epoch: 2 step: 1500, loss is 0.07414398342370987\n",
      "epoch: 2 step: 1501, loss is 0.18322721123695374\n",
      "epoch: 2 step: 1502, loss is 0.003904531942680478\n",
      "epoch: 2 step: 1503, loss is 0.028862588107585907\n",
      "epoch: 2 step: 1504, loss is 0.011668484658002853\n",
      "epoch: 2 step: 1505, loss is 0.3126104474067688\n",
      "epoch: 2 step: 1506, loss is 0.07044602185487747\n",
      "epoch: 2 step: 1507, loss is 0.013073897920548916\n",
      "epoch: 2 step: 1508, loss is 0.05941486731171608\n",
      "epoch: 2 step: 1509, loss is 0.0215437151491642\n",
      "epoch: 2 step: 1510, loss is 0.016978798434138298\n",
      "epoch: 2 step: 1511, loss is 0.010061337612569332\n",
      "epoch: 2 step: 1512, loss is 0.004103768151253462\n",
      "epoch: 2 step: 1513, loss is 0.04456198215484619\n",
      "epoch: 2 step: 1514, loss is 0.005982396192848682\n",
      "epoch: 2 step: 1515, loss is 0.40805870294570923\n",
      "epoch: 2 step: 1516, loss is 0.012562577612698078\n",
      "epoch: 2 step: 1517, loss is 0.23093663156032562\n",
      "epoch: 2 step: 1518, loss is 0.02248971536755562\n",
      "epoch: 2 step: 1519, loss is 0.004339235369116068\n",
      "epoch: 2 step: 1520, loss is 0.09635598212480545\n",
      "epoch: 2 step: 1521, loss is 0.09667885303497314\n",
      "epoch: 2 step: 1522, loss is 0.010932186618447304\n",
      "epoch: 2 step: 1523, loss is 0.02573423646390438\n",
      "epoch: 2 step: 1524, loss is 0.04992677643895149\n",
      "epoch: 2 step: 1525, loss is 0.036999426782131195\n",
      "epoch: 2 step: 1526, loss is 0.014596393331885338\n",
      "epoch: 2 step: 1527, loss is 0.03478620946407318\n",
      "epoch: 2 step: 1528, loss is 0.2918667495250702\n",
      "epoch: 2 step: 1529, loss is 0.027151059359312057\n",
      "epoch: 2 step: 1530, loss is 0.1526397317647934\n",
      "epoch: 2 step: 1531, loss is 0.058076705783605576\n",
      "epoch: 2 step: 1532, loss is 0.03962841257452965\n",
      "epoch: 2 step: 1533, loss is 0.009588559158146381\n",
      "epoch: 2 step: 1534, loss is 0.09136898070573807\n",
      "epoch: 2 step: 1535, loss is 0.06547918170690536\n",
      "epoch: 2 step: 1536, loss is 0.24793004989624023\n",
      "epoch: 2 step: 1537, loss is 0.05224822834134102\n",
      "epoch: 2 step: 1538, loss is 0.010412544012069702\n",
      "epoch: 2 step: 1539, loss is 0.0770774781703949\n",
      "epoch: 2 step: 1540, loss is 0.026711855083703995\n",
      "epoch: 2 step: 1541, loss is 0.1011059433221817\n",
      "epoch: 2 step: 1542, loss is 0.09807004779577255\n",
      "epoch: 2 step: 1543, loss is 0.23103868961334229\n",
      "epoch: 2 step: 1544, loss is 0.1984938383102417\n",
      "epoch: 2 step: 1545, loss is 0.07236430048942566\n",
      "epoch: 2 step: 1546, loss is 0.19504398107528687\n",
      "epoch: 2 step: 1547, loss is 0.07639428228139877\n",
      "epoch: 2 step: 1548, loss is 0.006659039296209812\n",
      "epoch: 2 step: 1549, loss is 0.013567951507866383\n",
      "epoch: 2 step: 1550, loss is 0.19499695301055908\n",
      "epoch: 2 step: 1551, loss is 0.0149761363863945\n",
      "epoch: 2 step: 1552, loss is 0.011686926707625389\n",
      "epoch: 2 step: 1553, loss is 0.05650026351213455\n",
      "epoch: 2 step: 1554, loss is 0.48660194873809814\n",
      "epoch: 2 step: 1555, loss is 0.05136730894446373\n",
      "epoch: 2 step: 1556, loss is 0.03594237193465233\n",
      "epoch: 2 step: 1557, loss is 0.021701263263821602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 1558, loss is 0.12062034010887146\n",
      "epoch: 2 step: 1559, loss is 0.007049111649394035\n",
      "epoch: 2 step: 1560, loss is 0.005630567669868469\n",
      "epoch: 2 step: 1561, loss is 0.10317407548427582\n",
      "epoch: 2 step: 1562, loss is 0.034628935158252716\n",
      "epoch: 2 step: 1563, loss is 0.04208095371723175\n",
      "epoch: 2 step: 1564, loss is 0.1315487027168274\n",
      "epoch: 2 step: 1565, loss is 0.05681318789720535\n",
      "epoch: 2 step: 1566, loss is 0.02816067263484001\n",
      "epoch: 2 step: 1567, loss is 0.02045663632452488\n",
      "epoch: 2 step: 1568, loss is 0.002819869900122285\n",
      "epoch: 2 step: 1569, loss is 0.01915883645415306\n",
      "epoch: 2 step: 1570, loss is 0.03584790229797363\n",
      "epoch: 2 step: 1571, loss is 0.16702690720558167\n",
      "epoch: 2 step: 1572, loss is 0.1455332189798355\n",
      "epoch: 2 step: 1573, loss is 0.08014070242643356\n",
      "epoch: 2 step: 1574, loss is 0.05818450078368187\n",
      "epoch: 2 step: 1575, loss is 0.016207728534936905\n",
      "epoch: 2 step: 1576, loss is 0.10976430028676987\n",
      "epoch: 2 step: 1577, loss is 0.010233614593744278\n",
      "epoch: 2 step: 1578, loss is 0.009996404871344566\n",
      "epoch: 2 step: 1579, loss is 0.1038157269358635\n",
      "epoch: 2 step: 1580, loss is 0.045284755527973175\n",
      "epoch: 2 step: 1581, loss is 0.004257120192050934\n",
      "epoch: 2 step: 1582, loss is 0.02570551075041294\n",
      "epoch: 2 step: 1583, loss is 0.01601163111627102\n",
      "epoch: 2 step: 1584, loss is 0.03196461498737335\n",
      "epoch: 2 step: 1585, loss is 0.0028224920388311148\n",
      "epoch: 2 step: 1586, loss is 0.012911390513181686\n",
      "epoch: 2 step: 1587, loss is 0.010096129961311817\n",
      "epoch: 2 step: 1588, loss is 0.20290599763393402\n",
      "epoch: 2 step: 1589, loss is 0.03698084503412247\n",
      "epoch: 2 step: 1590, loss is 0.04490846022963524\n",
      "epoch: 2 step: 1591, loss is 0.0026639087591320276\n",
      "epoch: 2 step: 1592, loss is 0.11172161996364594\n",
      "epoch: 2 step: 1593, loss is 0.271637886762619\n",
      "epoch: 2 step: 1594, loss is 0.03742460906505585\n",
      "epoch: 2 step: 1595, loss is 0.0005350965657271445\n",
      "epoch: 2 step: 1596, loss is 0.19469693303108215\n",
      "epoch: 2 step: 1597, loss is 0.01007837150245905\n",
      "epoch: 2 step: 1598, loss is 0.08433666825294495\n",
      "epoch: 2 step: 1599, loss is 0.21712951362133026\n",
      "epoch: 2 step: 1600, loss is 0.2661532461643219\n",
      "epoch: 2 step: 1601, loss is 0.14168526232242584\n",
      "epoch: 2 step: 1602, loss is 0.015542150475084782\n",
      "epoch: 2 step: 1603, loss is 0.06856778264045715\n",
      "epoch: 2 step: 1604, loss is 0.11857256293296814\n",
      "epoch: 2 step: 1605, loss is 0.004326488822698593\n",
      "epoch: 2 step: 1606, loss is 0.007147899363189936\n",
      "epoch: 2 step: 1607, loss is 0.01525714248418808\n",
      "epoch: 2 step: 1608, loss is 0.10113582760095596\n",
      "epoch: 2 step: 1609, loss is 0.09425126016139984\n",
      "epoch: 2 step: 1610, loss is 0.1306212991476059\n",
      "epoch: 2 step: 1611, loss is 0.10675771534442902\n",
      "epoch: 2 step: 1612, loss is 0.09261985123157501\n",
      "epoch: 2 step: 1613, loss is 0.01739528402686119\n",
      "epoch: 2 step: 1614, loss is 0.030295422300696373\n",
      "epoch: 2 step: 1615, loss is 0.018418066203594208\n",
      "epoch: 2 step: 1616, loss is 0.09263940155506134\n",
      "epoch: 2 step: 1617, loss is 0.06551661342382431\n",
      "epoch: 2 step: 1618, loss is 0.015039630234241486\n",
      "epoch: 2 step: 1619, loss is 0.012559070251882076\n",
      "epoch: 2 step: 1620, loss is 0.20486488938331604\n",
      "epoch: 2 step: 1621, loss is 0.00965016707777977\n",
      "epoch: 2 step: 1622, loss is 0.09438042342662811\n",
      "epoch: 2 step: 1623, loss is 0.14308899641036987\n",
      "epoch: 2 step: 1624, loss is 0.0698108896613121\n",
      "epoch: 2 step: 1625, loss is 0.033865779638290405\n",
      "epoch: 2 step: 1626, loss is 0.27635857462882996\n",
      "epoch: 2 step: 1627, loss is 0.019538959488272667\n",
      "epoch: 2 step: 1628, loss is 0.007594055496156216\n",
      "epoch: 2 step: 1629, loss is 0.015120275318622589\n",
      "epoch: 2 step: 1630, loss is 0.009155238047242165\n",
      "epoch: 2 step: 1631, loss is 0.001974682789295912\n",
      "epoch: 2 step: 1632, loss is 0.02967921644449234\n",
      "epoch: 2 step: 1633, loss is 0.014808760955929756\n",
      "epoch: 2 step: 1634, loss is 0.02182815410196781\n",
      "epoch: 2 step: 1635, loss is 0.11519806832075119\n",
      "epoch: 2 step: 1636, loss is 0.025563370436429977\n",
      "epoch: 2 step: 1637, loss is 0.042112138122320175\n",
      "epoch: 2 step: 1638, loss is 0.004910342860966921\n",
      "epoch: 2 step: 1639, loss is 0.007004998158663511\n",
      "epoch: 2 step: 1640, loss is 0.003348018741235137\n",
      "epoch: 2 step: 1641, loss is 0.0760544091463089\n",
      "epoch: 2 step: 1642, loss is 0.0014726463705301285\n",
      "epoch: 2 step: 1643, loss is 0.03198517486453056\n",
      "epoch: 2 step: 1644, loss is 0.042569417506456375\n",
      "epoch: 2 step: 1645, loss is 0.06856700032949448\n",
      "epoch: 2 step: 1646, loss is 0.022028757259249687\n",
      "epoch: 2 step: 1647, loss is 0.1781979203224182\n",
      "epoch: 2 step: 1648, loss is 0.1398371160030365\n",
      "epoch: 2 step: 1649, loss is 0.023029418662190437\n",
      "epoch: 2 step: 1650, loss is 0.034728314727544785\n",
      "epoch: 2 step: 1651, loss is 0.07688616216182709\n",
      "epoch: 2 step: 1652, loss is 0.0049107931554317474\n",
      "epoch: 2 step: 1653, loss is 0.049264296889305115\n",
      "epoch: 2 step: 1654, loss is 0.1014295145869255\n",
      "epoch: 2 step: 1655, loss is 0.20854967832565308\n",
      "epoch: 2 step: 1656, loss is 0.003291236236691475\n",
      "epoch: 2 step: 1657, loss is 0.12407355010509491\n",
      "epoch: 2 step: 1658, loss is 0.022621052339673042\n",
      "epoch: 2 step: 1659, loss is 0.02853604592382908\n",
      "epoch: 2 step: 1660, loss is 0.06896840035915375\n",
      "epoch: 2 step: 1661, loss is 0.11381334811449051\n",
      "epoch: 2 step: 1662, loss is 0.026186542585492134\n",
      "epoch: 2 step: 1663, loss is 0.037753768265247345\n",
      "epoch: 2 step: 1664, loss is 0.004858783446252346\n",
      "epoch: 2 step: 1665, loss is 0.013086442835628986\n",
      "epoch: 2 step: 1666, loss is 0.059770356863737106\n",
      "epoch: 2 step: 1667, loss is 0.0015621574129909277\n",
      "epoch: 2 step: 1668, loss is 0.023778866976499557\n",
      "epoch: 2 step: 1669, loss is 0.02779700979590416\n",
      "epoch: 2 step: 1670, loss is 0.08095493167638779\n",
      "epoch: 2 step: 1671, loss is 0.038381483405828476\n",
      "epoch: 2 step: 1672, loss is 0.009945917874574661\n",
      "epoch: 2 step: 1673, loss is 0.11496473103761673\n",
      "epoch: 2 step: 1674, loss is 0.12872417271137238\n",
      "epoch: 2 step: 1675, loss is 0.09753607958555222\n",
      "epoch: 2 step: 1676, loss is 0.05844661965966225\n",
      "epoch: 2 step: 1677, loss is 0.15186135470867157\n",
      "epoch: 2 step: 1678, loss is 0.08693020045757294\n",
      "epoch: 2 step: 1679, loss is 0.04660702124238014\n",
      "epoch: 2 step: 1680, loss is 0.151047021150589\n",
      "epoch: 2 step: 1681, loss is 0.00590518768876791\n",
      "epoch: 2 step: 1682, loss is 0.03723609820008278\n",
      "epoch: 2 step: 1683, loss is 0.14209173619747162\n",
      "epoch: 2 step: 1684, loss is 0.028656281530857086\n",
      "epoch: 2 step: 1685, loss is 0.14231790602207184\n",
      "epoch: 2 step: 1686, loss is 0.02284737303853035\n",
      "epoch: 2 step: 1687, loss is 0.12942887842655182\n",
      "epoch: 2 step: 1688, loss is 0.002560714026913047\n",
      "epoch: 2 step: 1689, loss is 0.24151866137981415\n",
      "epoch: 2 step: 1690, loss is 0.25097131729125977\n",
      "epoch: 2 step: 1691, loss is 0.03292962536215782\n",
      "epoch: 2 step: 1692, loss is 0.06496462970972061\n",
      "epoch: 2 step: 1693, loss is 0.023426979780197144\n",
      "epoch: 2 step: 1694, loss is 0.03406452387571335\n",
      "epoch: 2 step: 1695, loss is 0.0017345339292660356\n",
      "epoch: 2 step: 1696, loss is 0.08274692296981812\n",
      "epoch: 2 step: 1697, loss is 0.0956122949719429\n",
      "epoch: 2 step: 1698, loss is 0.057460084557533264\n",
      "epoch: 2 step: 1699, loss is 0.006845576222985983\n",
      "epoch: 2 step: 1700, loss is 0.023484377190470695\n",
      "epoch: 2 step: 1701, loss is 0.013761093840003014\n",
      "epoch: 2 step: 1702, loss is 0.030865656211972237\n",
      "epoch: 2 step: 1703, loss is 0.031150657683610916\n",
      "epoch: 2 step: 1704, loss is 0.032343652099370956\n",
      "epoch: 2 step: 1705, loss is 0.02875473164021969\n",
      "epoch: 2 step: 1706, loss is 0.0074279350228607655\n",
      "epoch: 2 step: 1707, loss is 0.07530459016561508\n",
      "epoch: 2 step: 1708, loss is 0.003970863297581673\n",
      "epoch: 2 step: 1709, loss is 0.012947160750627518\n",
      "epoch: 2 step: 1710, loss is 0.010458809323608875\n",
      "epoch: 2 step: 1711, loss is 0.11394042521715164\n",
      "epoch: 2 step: 1712, loss is 0.06846445053815842\n",
      "epoch: 2 step: 1713, loss is 0.023904375731945038\n",
      "epoch: 2 step: 1714, loss is 0.18289893865585327\n",
      "epoch: 2 step: 1715, loss is 0.08500530570745468\n",
      "epoch: 2 step: 1716, loss is 0.08505823463201523\n",
      "epoch: 2 step: 1717, loss is 0.004884775727987289\n",
      "epoch: 2 step: 1718, loss is 0.016462285071611404\n",
      "epoch: 2 step: 1719, loss is 0.0380091518163681\n",
      "epoch: 2 step: 1720, loss is 0.07715843617916107\n",
      "epoch: 2 step: 1721, loss is 0.00543416291475296\n",
      "epoch: 2 step: 1722, loss is 0.11264624446630478\n",
      "epoch: 2 step: 1723, loss is 0.031749941408634186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 1724, loss is 0.038743313401937485\n",
      "epoch: 2 step: 1725, loss is 0.03816317394375801\n",
      "epoch: 2 step: 1726, loss is 0.0860442966222763\n",
      "epoch: 2 step: 1727, loss is 0.11296427994966507\n",
      "epoch: 2 step: 1728, loss is 0.005649796221405268\n",
      "epoch: 2 step: 1729, loss is 0.006399298552423716\n",
      "epoch: 2 step: 1730, loss is 0.0054839421063661575\n",
      "epoch: 2 step: 1731, loss is 0.026632923632860184\n",
      "epoch: 2 step: 1732, loss is 0.10421785712242126\n",
      "epoch: 2 step: 1733, loss is 0.047595538198947906\n",
      "epoch: 2 step: 1734, loss is 0.00811278261244297\n",
      "epoch: 2 step: 1735, loss is 0.024281244724988937\n",
      "epoch: 2 step: 1736, loss is 0.0008875280618667603\n",
      "epoch: 2 step: 1737, loss is 0.015875935554504395\n",
      "epoch: 2 step: 1738, loss is 0.037762053310871124\n",
      "epoch: 2 step: 1739, loss is 0.08297013491392136\n",
      "epoch: 2 step: 1740, loss is 0.032573189586400986\n",
      "epoch: 2 step: 1741, loss is 0.22966381907463074\n",
      "epoch: 2 step: 1742, loss is 0.024701200425624847\n",
      "epoch: 2 step: 1743, loss is 0.19444631040096283\n",
      "epoch: 2 step: 1744, loss is 0.15891508758068085\n",
      "epoch: 2 step: 1745, loss is 0.005976211279630661\n",
      "epoch: 2 step: 1746, loss is 0.2316017597913742\n",
      "epoch: 2 step: 1747, loss is 0.009536020457744598\n",
      "epoch: 2 step: 1748, loss is 0.05531679466366768\n",
      "epoch: 2 step: 1749, loss is 0.0059636239893734455\n",
      "epoch: 2 step: 1750, loss is 0.20586952567100525\n",
      "epoch: 2 step: 1751, loss is 0.15964077413082123\n",
      "epoch: 2 step: 1752, loss is 0.054875124245882034\n",
      "epoch: 2 step: 1753, loss is 0.004007075447589159\n",
      "epoch: 2 step: 1754, loss is 0.0899251326918602\n",
      "epoch: 2 step: 1755, loss is 0.0014894497580826283\n",
      "epoch: 2 step: 1756, loss is 0.13230066001415253\n",
      "epoch: 2 step: 1757, loss is 0.017263885587453842\n",
      "epoch: 2 step: 1758, loss is 0.07598116248846054\n",
      "epoch: 2 step: 1759, loss is 0.02382366731762886\n",
      "epoch: 2 step: 1760, loss is 0.02418428845703602\n",
      "epoch: 2 step: 1761, loss is 0.07161001116037369\n",
      "epoch: 2 step: 1762, loss is 0.000913495197892189\n",
      "epoch: 2 step: 1763, loss is 0.07164070755243301\n",
      "epoch: 2 step: 1764, loss is 0.008532104082405567\n",
      "epoch: 2 step: 1765, loss is 0.060214485973119736\n",
      "epoch: 2 step: 1766, loss is 0.17824509739875793\n",
      "epoch: 2 step: 1767, loss is 0.018025947734713554\n",
      "epoch: 2 step: 1768, loss is 0.008490757085382938\n",
      "epoch: 2 step: 1769, loss is 0.0061166612431406975\n",
      "epoch: 2 step: 1770, loss is 0.12203790992498398\n",
      "epoch: 2 step: 1771, loss is 0.0065870871767401695\n",
      "epoch: 2 step: 1772, loss is 0.11457119137048721\n",
      "epoch: 2 step: 1773, loss is 0.08709222078323364\n",
      "epoch: 2 step: 1774, loss is 0.14543911814689636\n",
      "epoch: 2 step: 1775, loss is 0.11414806544780731\n",
      "epoch: 2 step: 1776, loss is 0.00130512285977602\n",
      "epoch: 2 step: 1777, loss is 0.003200120758265257\n",
      "epoch: 2 step: 1778, loss is 0.006009699311107397\n",
      "epoch: 2 step: 1779, loss is 0.01696060039103031\n",
      "epoch: 2 step: 1780, loss is 0.10086879879236221\n",
      "epoch: 2 step: 1781, loss is 0.004146043211221695\n",
      "epoch: 2 step: 1782, loss is 0.10584784299135208\n",
      "epoch: 2 step: 1783, loss is 0.08682440966367722\n",
      "epoch: 2 step: 1784, loss is 0.005909934174269438\n",
      "epoch: 2 step: 1785, loss is 0.010345258750021458\n",
      "epoch: 2 step: 1786, loss is 0.34615543484687805\n",
      "epoch: 2 step: 1787, loss is 0.08914581686258316\n",
      "epoch: 2 step: 1788, loss is 0.0165490061044693\n",
      "epoch: 2 step: 1789, loss is 0.09148652851581573\n",
      "epoch: 2 step: 1790, loss is 0.03777577728033066\n",
      "epoch: 2 step: 1791, loss is 0.3790813982486725\n",
      "epoch: 2 step: 1792, loss is 0.08137920498847961\n",
      "epoch: 2 step: 1793, loss is 0.10865497589111328\n",
      "epoch: 2 step: 1794, loss is 0.007458361331373453\n",
      "epoch: 2 step: 1795, loss is 0.017400462180376053\n",
      "epoch: 2 step: 1796, loss is 0.2615315616130829\n",
      "epoch: 2 step: 1797, loss is 0.1164463683962822\n",
      "epoch: 2 step: 1798, loss is 0.2619277238845825\n",
      "epoch: 2 step: 1799, loss is 0.06801588088274002\n",
      "epoch: 2 step: 1800, loss is 0.004867522977292538\n",
      "epoch: 2 step: 1801, loss is 0.34961998462677\n",
      "epoch: 2 step: 1802, loss is 0.011127347126603127\n",
      "epoch: 2 step: 1803, loss is 0.017615310847759247\n",
      "epoch: 2 step: 1804, loss is 0.015851067379117012\n",
      "epoch: 2 step: 1805, loss is 0.11072544753551483\n",
      "epoch: 2 step: 1806, loss is 0.09754132479429245\n",
      "epoch: 2 step: 1807, loss is 0.05663439258933067\n",
      "epoch: 2 step: 1808, loss is 0.0636734738945961\n",
      "epoch: 2 step: 1809, loss is 0.07838554680347443\n",
      "epoch: 2 step: 1810, loss is 0.02747652307152748\n",
      "epoch: 2 step: 1811, loss is 0.02861013635993004\n",
      "epoch: 2 step: 1812, loss is 0.08298953622579575\n",
      "epoch: 2 step: 1813, loss is 0.2570113241672516\n",
      "epoch: 2 step: 1814, loss is 0.01747073419392109\n",
      "epoch: 2 step: 1815, loss is 0.1999058574438095\n",
      "epoch: 2 step: 1816, loss is 0.1760515421628952\n",
      "epoch: 2 step: 1817, loss is 0.01847974956035614\n",
      "epoch: 2 step: 1818, loss is 0.003805598709732294\n",
      "epoch: 2 step: 1819, loss is 0.07188738882541656\n",
      "epoch: 2 step: 1820, loss is 0.09811121225357056\n",
      "epoch: 2 step: 1821, loss is 0.049811333417892456\n",
      "epoch: 2 step: 1822, loss is 0.023981163278222084\n",
      "epoch: 2 step: 1823, loss is 0.11040647327899933\n",
      "epoch: 2 step: 1824, loss is 0.08008355647325516\n",
      "epoch: 2 step: 1825, loss is 0.013851656578481197\n",
      "epoch: 2 step: 1826, loss is 0.02493003010749817\n",
      "epoch: 2 step: 1827, loss is 0.04145676642656326\n",
      "epoch: 2 step: 1828, loss is 0.01358792744576931\n",
      "epoch: 2 step: 1829, loss is 0.09986138343811035\n",
      "epoch: 2 step: 1830, loss is 0.05049732327461243\n",
      "epoch: 2 step: 1831, loss is 0.02060529962182045\n",
      "epoch: 2 step: 1832, loss is 0.22645637392997742\n",
      "epoch: 2 step: 1833, loss is 0.008249503560364246\n",
      "epoch: 2 step: 1834, loss is 0.26091328263282776\n",
      "epoch: 2 step: 1835, loss is 0.1127609834074974\n",
      "epoch: 2 step: 1836, loss is 0.015590880066156387\n",
      "epoch: 2 step: 1837, loss is 0.014311355538666248\n",
      "epoch: 2 step: 1838, loss is 0.007500378880649805\n",
      "epoch: 2 step: 1839, loss is 0.03822219744324684\n",
      "epoch: 2 step: 1840, loss is 0.11990805715322495\n",
      "epoch: 2 step: 1841, loss is 0.012872499413788319\n",
      "epoch: 2 step: 1842, loss is 0.11030524969100952\n",
      "epoch: 2 step: 1843, loss is 0.0330953449010849\n",
      "epoch: 2 step: 1844, loss is 0.01623045839369297\n",
      "epoch: 2 step: 1845, loss is 0.27627894282341003\n",
      "epoch: 2 step: 1846, loss is 0.10547322034835815\n",
      "epoch: 2 step: 1847, loss is 0.12819811701774597\n",
      "epoch: 2 step: 1848, loss is 0.05890306085348129\n",
      "epoch: 2 step: 1849, loss is 0.13539761304855347\n",
      "epoch: 2 step: 1850, loss is 0.023877492174506187\n",
      "epoch: 2 step: 1851, loss is 0.003949687350541353\n",
      "epoch: 2 step: 1852, loss is 0.08357404172420502\n",
      "epoch: 2 step: 1853, loss is 0.008501145988702774\n",
      "epoch: 2 step: 1854, loss is 0.13478738069534302\n",
      "epoch: 2 step: 1855, loss is 0.0017991370987147093\n",
      "epoch: 2 step: 1856, loss is 0.2067987471818924\n",
      "epoch: 2 step: 1857, loss is 0.07026800513267517\n",
      "epoch: 2 step: 1858, loss is 0.14438335597515106\n",
      "epoch: 2 step: 1859, loss is 0.07192455977201462\n",
      "epoch: 2 step: 1860, loss is 0.0892328992486\n",
      "epoch: 2 step: 1861, loss is 0.11893245577812195\n",
      "epoch: 2 step: 1862, loss is 0.07603466510772705\n",
      "epoch: 2 step: 1863, loss is 0.19111482799053192\n",
      "epoch: 2 step: 1864, loss is 0.016827337443828583\n",
      "epoch: 2 step: 1865, loss is 0.01691843383014202\n",
      "epoch: 2 step: 1866, loss is 0.06228223070502281\n",
      "epoch: 2 step: 1867, loss is 0.10457678884267807\n",
      "epoch: 2 step: 1868, loss is 0.018024960532784462\n",
      "epoch: 2 step: 1869, loss is 0.06865984201431274\n",
      "epoch: 2 step: 1870, loss is 0.0014167388435453176\n",
      "epoch: 2 step: 1871, loss is 0.037342894822359085\n",
      "epoch: 2 step: 1872, loss is 0.2007012516260147\n",
      "epoch: 2 step: 1873, loss is 0.03303436189889908\n",
      "epoch: 2 step: 1874, loss is 0.023034201934933662\n",
      "epoch: 2 step: 1875, loss is 0.022252826020121574\n",
      "epoch: 3 step: 1, loss is 0.21764987707138062\n",
      "epoch: 3 step: 2, loss is 0.03433407098054886\n",
      "epoch: 3 step: 3, loss is 0.00906304083764553\n",
      "epoch: 3 step: 4, loss is 0.05363375321030617\n",
      "epoch: 3 step: 5, loss is 0.29559803009033203\n",
      "epoch: 3 step: 6, loss is 0.008680415339767933\n",
      "epoch: 3 step: 7, loss is 0.04311428591609001\n",
      "epoch: 3 step: 8, loss is 0.04530143365263939\n",
      "epoch: 3 step: 9, loss is 0.03805358707904816\n",
      "epoch: 3 step: 10, loss is 0.15474815666675568\n",
      "epoch: 3 step: 11, loss is 0.028148652985692024\n",
      "epoch: 3 step: 12, loss is 0.14500144124031067\n",
      "epoch: 3 step: 13, loss is 0.012670079246163368\n",
      "epoch: 3 step: 14, loss is 0.02211534045636654\n",
      "epoch: 3 step: 15, loss is 0.012330788187682629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 16, loss is 0.008904987946152687\n",
      "epoch: 3 step: 17, loss is 0.20084117352962494\n",
      "epoch: 3 step: 18, loss is 0.0703972801566124\n",
      "epoch: 3 step: 19, loss is 0.023613939061760902\n",
      "epoch: 3 step: 20, loss is 0.009736377745866776\n",
      "epoch: 3 step: 21, loss is 0.012269101105630398\n",
      "epoch: 3 step: 22, loss is 0.05652427300810814\n",
      "epoch: 3 step: 23, loss is 0.010658868588507175\n",
      "epoch: 3 step: 24, loss is 0.04868178814649582\n",
      "epoch: 3 step: 25, loss is 0.299113929271698\n",
      "epoch: 3 step: 26, loss is 0.02045808546245098\n",
      "epoch: 3 step: 27, loss is 0.18632365763187408\n",
      "epoch: 3 step: 28, loss is 0.012018066830933094\n",
      "epoch: 3 step: 29, loss is 0.009944603778421879\n",
      "epoch: 3 step: 30, loss is 0.16987136006355286\n",
      "epoch: 3 step: 31, loss is 0.041755951941013336\n",
      "epoch: 3 step: 32, loss is 0.007938344031572342\n",
      "epoch: 3 step: 33, loss is 0.007771275471895933\n",
      "epoch: 3 step: 34, loss is 0.013964819721877575\n",
      "epoch: 3 step: 35, loss is 0.006664827931672335\n",
      "epoch: 3 step: 36, loss is 0.011564277112483978\n",
      "epoch: 3 step: 37, loss is 0.004120239056646824\n",
      "epoch: 3 step: 38, loss is 0.042301420122385025\n",
      "epoch: 3 step: 39, loss is 0.0435541495680809\n",
      "epoch: 3 step: 40, loss is 0.21419641375541687\n",
      "epoch: 3 step: 41, loss is 0.0024631412234157324\n",
      "epoch: 3 step: 42, loss is 0.018274448812007904\n",
      "epoch: 3 step: 43, loss is 0.02667737938463688\n",
      "epoch: 3 step: 44, loss is 0.07767144590616226\n",
      "epoch: 3 step: 45, loss is 0.0018476841505616903\n",
      "epoch: 3 step: 46, loss is 0.03518778830766678\n",
      "epoch: 3 step: 47, loss is 0.028972981497645378\n",
      "epoch: 3 step: 48, loss is 0.04228559881448746\n",
      "epoch: 3 step: 49, loss is 0.08948992937803268\n",
      "epoch: 3 step: 50, loss is 0.021136658266186714\n",
      "epoch: 3 step: 51, loss is 0.1832035481929779\n",
      "epoch: 3 step: 52, loss is 0.14340373873710632\n",
      "epoch: 3 step: 53, loss is 0.002249079756438732\n",
      "epoch: 3 step: 54, loss is 0.01077988650649786\n",
      "epoch: 3 step: 55, loss is 0.03202712908387184\n",
      "epoch: 3 step: 56, loss is 0.13450686633586884\n",
      "epoch: 3 step: 57, loss is 0.0068392762914299965\n",
      "epoch: 3 step: 58, loss is 0.04973491281270981\n",
      "epoch: 3 step: 59, loss is 0.008952301926910877\n",
      "epoch: 3 step: 60, loss is 0.02149401232600212\n",
      "epoch: 3 step: 61, loss is 0.05715401843190193\n",
      "epoch: 3 step: 62, loss is 0.03351416811347008\n",
      "epoch: 3 step: 63, loss is 0.0025360353756695986\n",
      "epoch: 3 step: 64, loss is 0.01014692708849907\n",
      "epoch: 3 step: 65, loss is 0.11554305255413055\n",
      "epoch: 3 step: 66, loss is 0.029908185824751854\n",
      "epoch: 3 step: 67, loss is 0.05861302837729454\n",
      "epoch: 3 step: 68, loss is 0.05355413258075714\n",
      "epoch: 3 step: 69, loss is 0.07597960531711578\n",
      "epoch: 3 step: 70, loss is 0.001354767708107829\n",
      "epoch: 3 step: 71, loss is 0.08918973803520203\n",
      "epoch: 3 step: 72, loss is 0.03636038303375244\n",
      "epoch: 3 step: 73, loss is 0.27459415793418884\n",
      "epoch: 3 step: 74, loss is 0.05170227959752083\n",
      "epoch: 3 step: 75, loss is 0.004051379859447479\n",
      "epoch: 3 step: 76, loss is 0.0044264825992286205\n",
      "epoch: 3 step: 77, loss is 0.01644410565495491\n",
      "epoch: 3 step: 78, loss is 0.024263259023427963\n",
      "epoch: 3 step: 79, loss is 0.059586577117443085\n",
      "epoch: 3 step: 80, loss is 0.0024303284008055925\n",
      "epoch: 3 step: 81, loss is 0.02122209221124649\n",
      "epoch: 3 step: 82, loss is 0.051275305449962616\n",
      "epoch: 3 step: 83, loss is 0.02446722611784935\n",
      "epoch: 3 step: 84, loss is 0.2228095680475235\n",
      "epoch: 3 step: 85, loss is 0.011754036881029606\n",
      "epoch: 3 step: 86, loss is 0.002149646170437336\n",
      "epoch: 3 step: 87, loss is 0.05111852288246155\n",
      "epoch: 3 step: 88, loss is 0.04365672916173935\n",
      "epoch: 3 step: 89, loss is 0.05205744877457619\n",
      "epoch: 3 step: 90, loss is 0.008924361318349838\n",
      "epoch: 3 step: 91, loss is 0.025209274142980576\n",
      "epoch: 3 step: 92, loss is 0.0018483223393559456\n",
      "epoch: 3 step: 93, loss is 0.008112461306154728\n",
      "epoch: 3 step: 94, loss is 0.01793912798166275\n",
      "epoch: 3 step: 95, loss is 0.009563065133988857\n",
      "epoch: 3 step: 96, loss is 0.25326478481292725\n",
      "epoch: 3 step: 97, loss is 0.0010227874154224992\n",
      "epoch: 3 step: 98, loss is 0.004944348242133856\n",
      "epoch: 3 step: 99, loss is 0.05157694220542908\n",
      "epoch: 3 step: 100, loss is 0.04084314405918121\n",
      "epoch: 3 step: 101, loss is 0.006746917497366667\n",
      "epoch: 3 step: 102, loss is 0.03186556324362755\n",
      "epoch: 3 step: 103, loss is 0.01339606661349535\n",
      "epoch: 3 step: 104, loss is 0.015361298806965351\n",
      "epoch: 3 step: 105, loss is 0.011155619286000729\n",
      "epoch: 3 step: 106, loss is 0.017916854470968246\n",
      "epoch: 3 step: 107, loss is 0.07609927654266357\n",
      "epoch: 3 step: 108, loss is 0.00798532273620367\n",
      "epoch: 3 step: 109, loss is 0.24034380912780762\n",
      "epoch: 3 step: 110, loss is 0.17840257287025452\n",
      "epoch: 3 step: 111, loss is 0.050956495106220245\n",
      "epoch: 3 step: 112, loss is 0.007630571722984314\n",
      "epoch: 3 step: 113, loss is 0.0053230468183755875\n",
      "epoch: 3 step: 114, loss is 0.07931724190711975\n",
      "epoch: 3 step: 115, loss is 0.1534467190504074\n",
      "epoch: 3 step: 116, loss is 0.08844513446092606\n",
      "epoch: 3 step: 117, loss is 0.07279370725154877\n",
      "epoch: 3 step: 118, loss is 0.010235527530312538\n",
      "epoch: 3 step: 119, loss is 0.02045498415827751\n",
      "epoch: 3 step: 120, loss is 0.007250996772199869\n",
      "epoch: 3 step: 121, loss is 0.22750303149223328\n",
      "epoch: 3 step: 122, loss is 0.009606699459254742\n",
      "epoch: 3 step: 123, loss is 0.04255388304591179\n",
      "epoch: 3 step: 124, loss is 0.05454420670866966\n",
      "epoch: 3 step: 125, loss is 0.0641871839761734\n",
      "epoch: 3 step: 126, loss is 0.08779323846101761\n",
      "epoch: 3 step: 127, loss is 0.00540182227268815\n",
      "epoch: 3 step: 128, loss is 0.14459943771362305\n",
      "epoch: 3 step: 129, loss is 0.07135690003633499\n",
      "epoch: 3 step: 130, loss is 0.011907271109521389\n",
      "epoch: 3 step: 131, loss is 0.5108987092971802\n",
      "epoch: 3 step: 132, loss is 0.09559749811887741\n",
      "epoch: 3 step: 133, loss is 0.005323190242052078\n",
      "epoch: 3 step: 134, loss is 0.003164134221151471\n",
      "epoch: 3 step: 135, loss is 0.00846097618341446\n",
      "epoch: 3 step: 136, loss is 0.07646487653255463\n",
      "epoch: 3 step: 137, loss is 0.05048173666000366\n",
      "epoch: 3 step: 138, loss is 0.023857062682509422\n",
      "epoch: 3 step: 139, loss is 0.09495687484741211\n",
      "epoch: 3 step: 140, loss is 0.06660149991512299\n",
      "epoch: 3 step: 141, loss is 0.00703389523550868\n",
      "epoch: 3 step: 142, loss is 0.003982135094702244\n",
      "epoch: 3 step: 143, loss is 0.03298856317996979\n",
      "epoch: 3 step: 144, loss is 0.005856587551534176\n",
      "epoch: 3 step: 145, loss is 0.030713465064764023\n",
      "epoch: 3 step: 146, loss is 0.006883949972689152\n",
      "epoch: 3 step: 147, loss is 0.05382080003619194\n",
      "epoch: 3 step: 148, loss is 0.14212310314178467\n",
      "epoch: 3 step: 149, loss is 0.04941783845424652\n",
      "epoch: 3 step: 150, loss is 0.013372404500842094\n",
      "epoch: 3 step: 151, loss is 0.005119094159454107\n",
      "epoch: 3 step: 152, loss is 0.032321758568286896\n",
      "epoch: 3 step: 153, loss is 0.024177975952625275\n",
      "epoch: 3 step: 154, loss is 0.014554958790540695\n",
      "epoch: 3 step: 155, loss is 0.11572931706905365\n",
      "epoch: 3 step: 156, loss is 0.007683095056563616\n",
      "epoch: 3 step: 157, loss is 0.0504530593752861\n",
      "epoch: 3 step: 158, loss is 0.005978148430585861\n",
      "epoch: 3 step: 159, loss is 0.16037733852863312\n",
      "epoch: 3 step: 160, loss is 0.002625834196805954\n",
      "epoch: 3 step: 161, loss is 0.07483493536710739\n",
      "epoch: 3 step: 162, loss is 0.038298752158880234\n",
      "epoch: 3 step: 163, loss is 0.14236360788345337\n",
      "epoch: 3 step: 164, loss is 0.017856553196907043\n",
      "epoch: 3 step: 165, loss is 0.14702804386615753\n",
      "epoch: 3 step: 166, loss is 0.01919182576239109\n",
      "epoch: 3 step: 167, loss is 0.007303356193006039\n",
      "epoch: 3 step: 168, loss is 0.0055048358626663685\n",
      "epoch: 3 step: 169, loss is 0.0022231326438486576\n",
      "epoch: 3 step: 170, loss is 0.03000311367213726\n",
      "epoch: 3 step: 171, loss is 0.08361123502254486\n",
      "epoch: 3 step: 172, loss is 0.05040411278605461\n",
      "epoch: 3 step: 173, loss is 0.04824897646903992\n",
      "epoch: 3 step: 174, loss is 0.11071693152189255\n",
      "epoch: 3 step: 175, loss is 0.02588990144431591\n",
      "epoch: 3 step: 176, loss is 0.0011403122916817665\n",
      "epoch: 3 step: 177, loss is 0.0603751577436924\n",
      "epoch: 3 step: 178, loss is 0.1024932935833931\n",
      "epoch: 3 step: 179, loss is 0.00653821462765336\n",
      "epoch: 3 step: 180, loss is 0.028651608154177666\n",
      "epoch: 3 step: 181, loss is 0.04339319467544556\n",
      "epoch: 3 step: 182, loss is 0.03906680643558502\n",
      "epoch: 3 step: 183, loss is 0.004948408808559179\n",
      "epoch: 3 step: 184, loss is 0.002663114108145237\n",
      "epoch: 3 step: 185, loss is 0.038440264761447906\n",
      "epoch: 3 step: 186, loss is 0.2232133150100708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 187, loss is 0.0593191534280777\n",
      "epoch: 3 step: 188, loss is 0.010105395689606667\n",
      "epoch: 3 step: 189, loss is 0.036223191767930984\n",
      "epoch: 3 step: 190, loss is 0.11464429646730423\n",
      "epoch: 3 step: 191, loss is 0.006010795012116432\n",
      "epoch: 3 step: 192, loss is 0.008855925872921944\n",
      "epoch: 3 step: 193, loss is 0.00866687297821045\n",
      "epoch: 3 step: 194, loss is 0.0034404562320560217\n",
      "epoch: 3 step: 195, loss is 0.022849809378385544\n",
      "epoch: 3 step: 196, loss is 0.0008766853134147823\n",
      "epoch: 3 step: 197, loss is 0.009483834728598595\n",
      "epoch: 3 step: 198, loss is 0.15887793898582458\n",
      "epoch: 3 step: 199, loss is 0.0543137788772583\n",
      "epoch: 3 step: 200, loss is 0.0072625079192221165\n",
      "epoch: 3 step: 201, loss is 0.002864337060600519\n",
      "epoch: 3 step: 202, loss is 0.05831833928823471\n",
      "epoch: 3 step: 203, loss is 0.001093456638045609\n",
      "epoch: 3 step: 204, loss is 0.0024385093711316586\n",
      "epoch: 3 step: 205, loss is 0.0009007089538499713\n",
      "epoch: 3 step: 206, loss is 0.003421127563342452\n",
      "epoch: 3 step: 207, loss is 0.21437032520771027\n",
      "epoch: 3 step: 208, loss is 0.03485792130231857\n",
      "epoch: 3 step: 209, loss is 0.04767632111907005\n",
      "epoch: 3 step: 210, loss is 0.01903020776808262\n",
      "epoch: 3 step: 211, loss is 0.1394498497247696\n",
      "epoch: 3 step: 212, loss is 0.020312804728746414\n",
      "epoch: 3 step: 213, loss is 0.05058787018060684\n",
      "epoch: 3 step: 214, loss is 0.11514890938997269\n",
      "epoch: 3 step: 215, loss is 0.020093213766813278\n",
      "epoch: 3 step: 216, loss is 0.006806786172091961\n",
      "epoch: 3 step: 217, loss is 0.07309173047542572\n",
      "epoch: 3 step: 218, loss is 0.0008522751159034669\n",
      "epoch: 3 step: 219, loss is 0.004577072337269783\n",
      "epoch: 3 step: 220, loss is 0.005291484761983156\n",
      "epoch: 3 step: 221, loss is 0.0035283660981804132\n",
      "epoch: 3 step: 222, loss is 0.1433912217617035\n",
      "epoch: 3 step: 223, loss is 0.0010671458439901471\n",
      "epoch: 3 step: 224, loss is 0.07895176112651825\n",
      "epoch: 3 step: 225, loss is 0.002506667748093605\n",
      "epoch: 3 step: 226, loss is 0.12961335480213165\n",
      "epoch: 3 step: 227, loss is 0.016002150252461433\n",
      "epoch: 3 step: 228, loss is 0.0005263218772597611\n",
      "epoch: 3 step: 229, loss is 0.0010914215818047523\n",
      "epoch: 3 step: 230, loss is 0.002104682382196188\n",
      "epoch: 3 step: 231, loss is 0.053838446736335754\n",
      "epoch: 3 step: 232, loss is 0.1344737559556961\n",
      "epoch: 3 step: 233, loss is 0.08967914432287216\n",
      "epoch: 3 step: 234, loss is 0.10234833508729935\n",
      "epoch: 3 step: 235, loss is 0.045519307255744934\n",
      "epoch: 3 step: 236, loss is 0.011213131248950958\n",
      "epoch: 3 step: 237, loss is 0.05590388923883438\n",
      "epoch: 3 step: 238, loss is 0.02345825918018818\n",
      "epoch: 3 step: 239, loss is 0.04521948844194412\n",
      "epoch: 3 step: 240, loss is 0.028448397293686867\n",
      "epoch: 3 step: 241, loss is 0.048339903354644775\n",
      "epoch: 3 step: 242, loss is 0.0010852024424821138\n",
      "epoch: 3 step: 243, loss is 0.1488724648952484\n",
      "epoch: 3 step: 244, loss is 0.05146517977118492\n",
      "epoch: 3 step: 245, loss is 0.030301833525300026\n",
      "epoch: 3 step: 246, loss is 0.010152095928788185\n",
      "epoch: 3 step: 247, loss is 0.001325537683442235\n",
      "epoch: 3 step: 248, loss is 0.04437856376171112\n",
      "epoch: 3 step: 249, loss is 0.26464909315109253\n",
      "epoch: 3 step: 250, loss is 0.038084421306848526\n",
      "epoch: 3 step: 251, loss is 0.2450435757637024\n",
      "epoch: 3 step: 252, loss is 0.3094238042831421\n",
      "epoch: 3 step: 253, loss is 0.06060069799423218\n",
      "epoch: 3 step: 254, loss is 0.0400395430624485\n",
      "epoch: 3 step: 255, loss is 0.09597030282020569\n",
      "epoch: 3 step: 256, loss is 0.1660834401845932\n",
      "epoch: 3 step: 257, loss is 0.07258082181215286\n",
      "epoch: 3 step: 258, loss is 0.004732137080281973\n",
      "epoch: 3 step: 259, loss is 0.03785638138651848\n",
      "epoch: 3 step: 260, loss is 0.07563962042331696\n",
      "epoch: 3 step: 261, loss is 0.00478751678019762\n",
      "epoch: 3 step: 262, loss is 0.022294608876109123\n",
      "epoch: 3 step: 263, loss is 0.0861155316233635\n",
      "epoch: 3 step: 264, loss is 0.08399946987628937\n",
      "epoch: 3 step: 265, loss is 0.039604272693395615\n",
      "epoch: 3 step: 266, loss is 0.058676827698946\n",
      "epoch: 3 step: 267, loss is 0.00930760893970728\n",
      "epoch: 3 step: 268, loss is 0.001378387794829905\n",
      "epoch: 3 step: 269, loss is 0.035403452813625336\n",
      "epoch: 3 step: 270, loss is 0.0008895081118680537\n",
      "epoch: 3 step: 271, loss is 0.012814764864742756\n",
      "epoch: 3 step: 272, loss is 0.014960737898945808\n",
      "epoch: 3 step: 273, loss is 0.05167923867702484\n",
      "epoch: 3 step: 274, loss is 0.013224483467638493\n",
      "epoch: 3 step: 275, loss is 0.21573571860790253\n",
      "epoch: 3 step: 276, loss is 0.009954369626939297\n",
      "epoch: 3 step: 277, loss is 0.0776006281375885\n",
      "epoch: 3 step: 278, loss is 0.0027291683945804834\n",
      "epoch: 3 step: 279, loss is 0.0012705812696367502\n",
      "epoch: 3 step: 280, loss is 0.016652856022119522\n",
      "epoch: 3 step: 281, loss is 0.08573365211486816\n",
      "epoch: 3 step: 282, loss is 0.026182685047388077\n",
      "epoch: 3 step: 283, loss is 0.08671760559082031\n",
      "epoch: 3 step: 284, loss is 0.017518453299999237\n",
      "epoch: 3 step: 285, loss is 0.008318555541336536\n",
      "epoch: 3 step: 286, loss is 0.0006938777514733374\n",
      "epoch: 3 step: 287, loss is 0.03747832775115967\n",
      "epoch: 3 step: 288, loss is 0.0013667575549334288\n",
      "epoch: 3 step: 289, loss is 0.0524369515478611\n",
      "epoch: 3 step: 290, loss is 0.052836932241916656\n",
      "epoch: 3 step: 291, loss is 0.10206463932991028\n",
      "epoch: 3 step: 292, loss is 0.1415795534849167\n",
      "epoch: 3 step: 293, loss is 0.0639428123831749\n",
      "epoch: 3 step: 294, loss is 0.01954990066587925\n",
      "epoch: 3 step: 295, loss is 0.0031097042374312878\n",
      "epoch: 3 step: 296, loss is 0.03503894805908203\n",
      "epoch: 3 step: 297, loss is 0.0012838204856961966\n",
      "epoch: 3 step: 298, loss is 0.03525581583380699\n",
      "epoch: 3 step: 299, loss is 0.01987096667289734\n",
      "epoch: 3 step: 300, loss is 0.003855218179523945\n",
      "epoch: 3 step: 301, loss is 0.03878604620695114\n",
      "epoch: 3 step: 302, loss is 0.09411755949258804\n",
      "epoch: 3 step: 303, loss is 0.026222355663776398\n",
      "epoch: 3 step: 304, loss is 0.006176769733428955\n",
      "epoch: 3 step: 305, loss is 0.06764770299196243\n",
      "epoch: 3 step: 306, loss is 0.04431331530213356\n",
      "epoch: 3 step: 307, loss is 0.016830895096063614\n",
      "epoch: 3 step: 308, loss is 0.057129867374897\n",
      "epoch: 3 step: 309, loss is 0.07019931077957153\n",
      "epoch: 3 step: 310, loss is 0.008364013396203518\n",
      "epoch: 3 step: 311, loss is 0.01386761199682951\n",
      "epoch: 3 step: 312, loss is 0.05553167685866356\n",
      "epoch: 3 step: 313, loss is 0.005703205242753029\n",
      "epoch: 3 step: 314, loss is 0.003701284294947982\n",
      "epoch: 3 step: 315, loss is 0.011778214946389198\n",
      "epoch: 3 step: 316, loss is 0.12557083368301392\n",
      "epoch: 3 step: 317, loss is 0.04967287927865982\n",
      "epoch: 3 step: 318, loss is 0.24450884759426117\n",
      "epoch: 3 step: 319, loss is 0.05372553691267967\n",
      "epoch: 3 step: 320, loss is 0.023219222202897072\n",
      "epoch: 3 step: 321, loss is 0.003914026543498039\n",
      "epoch: 3 step: 322, loss is 0.08420882374048233\n",
      "epoch: 3 step: 323, loss is 0.03714112192392349\n",
      "epoch: 3 step: 324, loss is 0.0019452788401395082\n",
      "epoch: 3 step: 325, loss is 0.003766806097701192\n",
      "epoch: 3 step: 326, loss is 0.3457917869091034\n",
      "epoch: 3 step: 327, loss is 0.012483672238886356\n",
      "epoch: 3 step: 328, loss is 0.043714456260204315\n",
      "epoch: 3 step: 329, loss is 0.017222510650753975\n",
      "epoch: 3 step: 330, loss is 0.04332922026515007\n",
      "epoch: 3 step: 331, loss is 0.09770019352436066\n",
      "epoch: 3 step: 332, loss is 0.011930938810110092\n",
      "epoch: 3 step: 333, loss is 0.06179250776767731\n",
      "epoch: 3 step: 334, loss is 0.028942210599780083\n",
      "epoch: 3 step: 335, loss is 0.13311821222305298\n",
      "epoch: 3 step: 336, loss is 0.02734331041574478\n",
      "epoch: 3 step: 337, loss is 0.030940081924200058\n",
      "epoch: 3 step: 338, loss is 0.09486643224954605\n",
      "epoch: 3 step: 339, loss is 0.019831307232379913\n",
      "epoch: 3 step: 340, loss is 0.0022674687206745148\n",
      "epoch: 3 step: 341, loss is 0.02304370328783989\n",
      "epoch: 3 step: 342, loss is 0.03805242106318474\n",
      "epoch: 3 step: 343, loss is 0.056157760322093964\n",
      "epoch: 3 step: 344, loss is 0.04883941262960434\n",
      "epoch: 3 step: 345, loss is 0.017620433121919632\n",
      "epoch: 3 step: 346, loss is 0.03288034349679947\n",
      "epoch: 3 step: 347, loss is 0.07522857189178467\n",
      "epoch: 3 step: 348, loss is 0.13068944215774536\n",
      "epoch: 3 step: 349, loss is 0.11314086616039276\n",
      "epoch: 3 step: 350, loss is 0.02225985750555992\n",
      "epoch: 3 step: 351, loss is 0.08888652920722961\n",
      "epoch: 3 step: 352, loss is 0.14389179646968842\n",
      "epoch: 3 step: 353, loss is 0.08163616806268692\n",
      "epoch: 3 step: 354, loss is 0.022602520883083344\n",
      "epoch: 3 step: 355, loss is 0.008888042531907558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 356, loss is 0.08719835430383682\n",
      "epoch: 3 step: 357, loss is 0.003888990031555295\n",
      "epoch: 3 step: 358, loss is 0.0009092214168049395\n",
      "epoch: 3 step: 359, loss is 0.02265855111181736\n",
      "epoch: 3 step: 360, loss is 0.01238956954330206\n",
      "epoch: 3 step: 361, loss is 0.023229407146573067\n",
      "epoch: 3 step: 362, loss is 0.01119456347078085\n",
      "epoch: 3 step: 363, loss is 0.07198729366064072\n",
      "epoch: 3 step: 364, loss is 0.013415966182947159\n",
      "epoch: 3 step: 365, loss is 0.017721647396683693\n",
      "epoch: 3 step: 366, loss is 0.10017717629671097\n",
      "epoch: 3 step: 367, loss is 0.03930380567908287\n",
      "epoch: 3 step: 368, loss is 0.1350565105676651\n",
      "epoch: 3 step: 369, loss is 0.09167785942554474\n",
      "epoch: 3 step: 370, loss is 0.011778945103287697\n",
      "epoch: 3 step: 371, loss is 0.01948527805507183\n",
      "epoch: 3 step: 372, loss is 0.28171899914741516\n",
      "epoch: 3 step: 373, loss is 0.004246617667376995\n",
      "epoch: 3 step: 374, loss is 0.035837750881910324\n",
      "epoch: 3 step: 375, loss is 0.019772784784436226\n",
      "epoch: 3 step: 376, loss is 0.042280592024326324\n",
      "epoch: 3 step: 377, loss is 0.0967998281121254\n",
      "epoch: 3 step: 378, loss is 0.05673369765281677\n",
      "epoch: 3 step: 379, loss is 0.053357794880867004\n",
      "epoch: 3 step: 380, loss is 0.1067061573266983\n",
      "epoch: 3 step: 381, loss is 0.022784002125263214\n",
      "epoch: 3 step: 382, loss is 0.0010163680417463183\n",
      "epoch: 3 step: 383, loss is 0.007443481124937534\n",
      "epoch: 3 step: 384, loss is 0.0018052172381430864\n",
      "epoch: 3 step: 385, loss is 0.004035955294966698\n",
      "epoch: 3 step: 386, loss is 0.00987153872847557\n",
      "epoch: 3 step: 387, loss is 0.025346383452415466\n",
      "epoch: 3 step: 388, loss is 0.10005534440279007\n",
      "epoch: 3 step: 389, loss is 0.02341054007411003\n",
      "epoch: 3 step: 390, loss is 0.02461482584476471\n",
      "epoch: 3 step: 391, loss is 0.04863078147172928\n",
      "epoch: 3 step: 392, loss is 0.028645077720284462\n",
      "epoch: 3 step: 393, loss is 0.010230140760540962\n",
      "epoch: 3 step: 394, loss is 0.004816098138689995\n",
      "epoch: 3 step: 395, loss is 0.02028937265276909\n",
      "epoch: 3 step: 396, loss is 0.01688726246356964\n",
      "epoch: 3 step: 397, loss is 0.006038918159902096\n",
      "epoch: 3 step: 398, loss is 0.18493647873401642\n",
      "epoch: 3 step: 399, loss is 0.15973006188869476\n",
      "epoch: 3 step: 400, loss is 0.004193970467895269\n",
      "epoch: 3 step: 401, loss is 0.061002712696790695\n",
      "epoch: 3 step: 402, loss is 0.02934361808001995\n",
      "epoch: 3 step: 403, loss is 0.012228131294250488\n",
      "epoch: 3 step: 404, loss is 0.037907857447862625\n",
      "epoch: 3 step: 405, loss is 0.005063090939074755\n",
      "epoch: 3 step: 406, loss is 0.08455052971839905\n",
      "epoch: 3 step: 407, loss is 0.036640360951423645\n",
      "epoch: 3 step: 408, loss is 0.02539467252790928\n",
      "epoch: 3 step: 409, loss is 0.0016771625960245728\n",
      "epoch: 3 step: 410, loss is 0.04130217432975769\n",
      "epoch: 3 step: 411, loss is 0.09012943506240845\n",
      "epoch: 3 step: 412, loss is 0.14449700713157654\n",
      "epoch: 3 step: 413, loss is 0.12438485026359558\n",
      "epoch: 3 step: 414, loss is 0.02411358430981636\n",
      "epoch: 3 step: 415, loss is 0.010867421515285969\n",
      "epoch: 3 step: 416, loss is 0.012681746855378151\n",
      "epoch: 3 step: 417, loss is 0.04340061545372009\n",
      "epoch: 3 step: 418, loss is 0.22014625370502472\n",
      "epoch: 3 step: 419, loss is 0.011412185616791248\n",
      "epoch: 3 step: 420, loss is 0.006513303145766258\n",
      "epoch: 3 step: 421, loss is 0.0337582603096962\n",
      "epoch: 3 step: 422, loss is 0.005607629660516977\n",
      "epoch: 3 step: 423, loss is 0.06692423671483994\n",
      "epoch: 3 step: 424, loss is 0.030158214271068573\n",
      "epoch: 3 step: 425, loss is 0.04555456340312958\n",
      "epoch: 3 step: 426, loss is 0.02150135301053524\n",
      "epoch: 3 step: 427, loss is 0.024238670244812965\n",
      "epoch: 3 step: 428, loss is 0.13779038190841675\n",
      "epoch: 3 step: 429, loss is 0.11365211755037308\n",
      "epoch: 3 step: 430, loss is 0.006255257409065962\n",
      "epoch: 3 step: 431, loss is 0.01681608147919178\n",
      "epoch: 3 step: 432, loss is 0.0007591360481455922\n",
      "epoch: 3 step: 433, loss is 0.0011561980936676264\n",
      "epoch: 3 step: 434, loss is 0.0016884085489436984\n",
      "epoch: 3 step: 435, loss is 0.028672564774751663\n",
      "epoch: 3 step: 436, loss is 0.022533714771270752\n",
      "epoch: 3 step: 437, loss is 0.22737759351730347\n",
      "epoch: 3 step: 438, loss is 0.002596148056909442\n",
      "epoch: 3 step: 439, loss is 0.0048281545750796795\n",
      "epoch: 3 step: 440, loss is 0.02025561034679413\n",
      "epoch: 3 step: 441, loss is 0.002156831556931138\n",
      "epoch: 3 step: 442, loss is 0.028473077341914177\n",
      "epoch: 3 step: 443, loss is 0.09569673985242844\n",
      "epoch: 3 step: 444, loss is 0.08024289458990097\n",
      "epoch: 3 step: 445, loss is 0.018121469765901566\n",
      "epoch: 3 step: 446, loss is 0.0008763482910580933\n",
      "epoch: 3 step: 447, loss is 0.013204000890254974\n",
      "epoch: 3 step: 448, loss is 0.014594677835702896\n",
      "epoch: 3 step: 449, loss is 0.13076476752758026\n",
      "epoch: 3 step: 450, loss is 0.1997065246105194\n",
      "epoch: 3 step: 451, loss is 0.11585306376218796\n",
      "epoch: 3 step: 452, loss is 0.2706780731678009\n",
      "epoch: 3 step: 453, loss is 0.021327683702111244\n",
      "epoch: 3 step: 454, loss is 0.008229399099946022\n",
      "epoch: 3 step: 455, loss is 0.09517430514097214\n",
      "epoch: 3 step: 456, loss is 0.001525853993371129\n",
      "epoch: 3 step: 457, loss is 0.006414805073291063\n",
      "epoch: 3 step: 458, loss is 0.12753060460090637\n",
      "epoch: 3 step: 459, loss is 0.007832130417227745\n",
      "epoch: 3 step: 460, loss is 0.007406437303870916\n",
      "epoch: 3 step: 461, loss is 0.0017353242728859186\n",
      "epoch: 3 step: 462, loss is 0.35327982902526855\n",
      "epoch: 3 step: 463, loss is 0.03852476552128792\n",
      "epoch: 3 step: 464, loss is 0.003509632544592023\n",
      "epoch: 3 step: 465, loss is 0.30028608441352844\n",
      "epoch: 3 step: 466, loss is 0.020368807017803192\n",
      "epoch: 3 step: 467, loss is 0.009022556245326996\n",
      "epoch: 3 step: 468, loss is 0.01718394085764885\n",
      "epoch: 3 step: 469, loss is 0.016198284924030304\n",
      "epoch: 3 step: 470, loss is 0.030299555510282516\n",
      "epoch: 3 step: 471, loss is 0.0037016929127275944\n",
      "epoch: 3 step: 472, loss is 0.05479733645915985\n",
      "epoch: 3 step: 473, loss is 0.037531137466430664\n",
      "epoch: 3 step: 474, loss is 0.003784917062148452\n",
      "epoch: 3 step: 475, loss is 0.010415631346404552\n",
      "epoch: 3 step: 476, loss is 0.021245356649160385\n",
      "epoch: 3 step: 477, loss is 0.021686002612113953\n",
      "epoch: 3 step: 478, loss is 0.0008110770140774548\n",
      "epoch: 3 step: 479, loss is 0.15569618344306946\n",
      "epoch: 3 step: 480, loss is 0.02879757061600685\n",
      "epoch: 3 step: 481, loss is 0.005810243543237448\n",
      "epoch: 3 step: 482, loss is 0.012343261390924454\n",
      "epoch: 3 step: 483, loss is 0.004900376778095961\n",
      "epoch: 3 step: 484, loss is 0.12265003472566605\n",
      "epoch: 3 step: 485, loss is 0.011324736289680004\n",
      "epoch: 3 step: 486, loss is 0.008682534098625183\n",
      "epoch: 3 step: 487, loss is 0.038010869175195694\n",
      "epoch: 3 step: 488, loss is 0.06280618160963058\n",
      "epoch: 3 step: 489, loss is 0.2068391740322113\n",
      "epoch: 3 step: 490, loss is 0.23738062381744385\n",
      "epoch: 3 step: 491, loss is 0.05039029195904732\n",
      "epoch: 3 step: 492, loss is 0.030924895778298378\n",
      "epoch: 3 step: 493, loss is 0.0028370553627610207\n",
      "epoch: 3 step: 494, loss is 0.0501692034304142\n",
      "epoch: 3 step: 495, loss is 0.02035551890730858\n",
      "epoch: 3 step: 496, loss is 0.12895464897155762\n",
      "epoch: 3 step: 497, loss is 0.0025279626715928316\n",
      "epoch: 3 step: 498, loss is 0.0797194242477417\n",
      "epoch: 3 step: 499, loss is 0.16046904027462006\n",
      "epoch: 3 step: 500, loss is 0.16391615569591522\n",
      "epoch: 3 step: 501, loss is 0.014122286811470985\n",
      "epoch: 3 step: 502, loss is 0.05089610069990158\n",
      "epoch: 3 step: 503, loss is 0.011826363392174244\n",
      "epoch: 3 step: 504, loss is 0.036783624440431595\n",
      "epoch: 3 step: 505, loss is 0.004474759567528963\n",
      "epoch: 3 step: 506, loss is 0.010489782318472862\n",
      "epoch: 3 step: 507, loss is 0.04047073423862457\n",
      "epoch: 3 step: 508, loss is 0.04281794652342796\n",
      "epoch: 3 step: 509, loss is 0.0029571843333542347\n",
      "epoch: 3 step: 510, loss is 0.005996982101351023\n",
      "epoch: 3 step: 511, loss is 0.09036637097597122\n",
      "epoch: 3 step: 512, loss is 0.038279350847005844\n",
      "epoch: 3 step: 513, loss is 0.008315627463161945\n",
      "epoch: 3 step: 514, loss is 0.15033356845378876\n",
      "epoch: 3 step: 515, loss is 0.004746232181787491\n",
      "epoch: 3 step: 516, loss is 0.029902884736657143\n",
      "epoch: 3 step: 517, loss is 0.0014353886945173144\n",
      "epoch: 3 step: 518, loss is 0.0024452561046928167\n",
      "epoch: 3 step: 519, loss is 0.03179863095283508\n",
      "epoch: 3 step: 520, loss is 0.009490887634456158\n",
      "epoch: 3 step: 521, loss is 0.03680292144417763\n",
      "epoch: 3 step: 522, loss is 0.013287169858813286\n",
      "epoch: 3 step: 523, loss is 0.1149725615978241\n",
      "epoch: 3 step: 524, loss is 0.05535174533724785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 525, loss is 0.0011522475397214293\n",
      "epoch: 3 step: 526, loss is 0.018132084980607033\n",
      "epoch: 3 step: 527, loss is 0.02293003350496292\n",
      "epoch: 3 step: 528, loss is 0.009992138482630253\n",
      "epoch: 3 step: 529, loss is 0.0738181471824646\n",
      "epoch: 3 step: 530, loss is 0.025641130283474922\n",
      "epoch: 3 step: 531, loss is 0.07333719730377197\n",
      "epoch: 3 step: 532, loss is 0.07590556889772415\n",
      "epoch: 3 step: 533, loss is 0.10494723170995712\n",
      "epoch: 3 step: 534, loss is 0.018519550561904907\n",
      "epoch: 3 step: 535, loss is 0.10347054898738861\n",
      "epoch: 3 step: 536, loss is 0.005554915871471167\n",
      "epoch: 3 step: 537, loss is 0.05729171633720398\n",
      "epoch: 3 step: 538, loss is 0.021637465804815292\n",
      "epoch: 3 step: 539, loss is 0.0008996253018267453\n",
      "epoch: 3 step: 540, loss is 0.004727370571345091\n",
      "epoch: 3 step: 541, loss is 0.11981534957885742\n",
      "epoch: 3 step: 542, loss is 0.0008457758813165128\n",
      "epoch: 3 step: 543, loss is 0.05920987203717232\n",
      "epoch: 3 step: 544, loss is 0.02306975983083248\n",
      "epoch: 3 step: 545, loss is 0.042085204273462296\n",
      "epoch: 3 step: 546, loss is 0.015826266258955002\n",
      "epoch: 3 step: 547, loss is 0.542700469493866\n",
      "epoch: 3 step: 548, loss is 0.033199846744537354\n",
      "epoch: 3 step: 549, loss is 0.025256123393774033\n",
      "epoch: 3 step: 550, loss is 0.001579377567395568\n",
      "epoch: 3 step: 551, loss is 0.020198741927742958\n",
      "epoch: 3 step: 552, loss is 0.06049780547618866\n",
      "epoch: 3 step: 553, loss is 0.005380481015890837\n",
      "epoch: 3 step: 554, loss is 0.013899105601012707\n",
      "epoch: 3 step: 555, loss is 0.0026013583410531282\n",
      "epoch: 3 step: 556, loss is 0.2964249849319458\n",
      "epoch: 3 step: 557, loss is 0.02237182855606079\n",
      "epoch: 3 step: 558, loss is 0.17775318026542664\n",
      "epoch: 3 step: 559, loss is 0.08786368370056152\n",
      "epoch: 3 step: 560, loss is 0.00076552719110623\n",
      "epoch: 3 step: 561, loss is 0.01348854973912239\n",
      "epoch: 3 step: 562, loss is 0.08953490108251572\n",
      "epoch: 3 step: 563, loss is 0.012134896591305733\n",
      "epoch: 3 step: 564, loss is 0.028323883190751076\n",
      "epoch: 3 step: 565, loss is 0.09115458279848099\n",
      "epoch: 3 step: 566, loss is 0.0190025195479393\n",
      "epoch: 3 step: 567, loss is 0.0815565213561058\n",
      "epoch: 3 step: 568, loss is 0.0026011038571596146\n",
      "epoch: 3 step: 569, loss is 0.03142741695046425\n",
      "epoch: 3 step: 570, loss is 0.0016215945361182094\n",
      "epoch: 3 step: 571, loss is 0.039434920996427536\n",
      "epoch: 3 step: 572, loss is 0.0031482945196330547\n",
      "epoch: 3 step: 573, loss is 0.024057835340499878\n",
      "epoch: 3 step: 574, loss is 0.032059092074632645\n",
      "epoch: 3 step: 575, loss is 0.13595816493034363\n",
      "epoch: 3 step: 576, loss is 0.01142698060721159\n",
      "epoch: 3 step: 577, loss is 0.0838823989033699\n",
      "epoch: 3 step: 578, loss is 0.020462684333324432\n",
      "epoch: 3 step: 579, loss is 0.011923526413738728\n",
      "epoch: 3 step: 580, loss is 0.02649829164147377\n",
      "epoch: 3 step: 581, loss is 0.003954223822802305\n",
      "epoch: 3 step: 582, loss is 0.018357036635279655\n",
      "epoch: 3 step: 583, loss is 0.013047956861555576\n",
      "epoch: 3 step: 584, loss is 0.011601435020565987\n",
      "epoch: 3 step: 585, loss is 0.09521070867776871\n",
      "epoch: 3 step: 586, loss is 0.06412201374769211\n",
      "epoch: 3 step: 587, loss is 0.06426973640918732\n",
      "epoch: 3 step: 588, loss is 0.2578648328781128\n",
      "epoch: 3 step: 589, loss is 0.022225745022296906\n",
      "epoch: 3 step: 590, loss is 0.013689899817109108\n",
      "epoch: 3 step: 591, loss is 0.028209242969751358\n",
      "epoch: 3 step: 592, loss is 0.005347912199795246\n",
      "epoch: 3 step: 593, loss is 0.10135189443826675\n",
      "epoch: 3 step: 594, loss is 0.01381152868270874\n",
      "epoch: 3 step: 595, loss is 0.01897260546684265\n",
      "epoch: 3 step: 596, loss is 0.0092080133035779\n",
      "epoch: 3 step: 597, loss is 0.06511064618825912\n",
      "epoch: 3 step: 598, loss is 0.007975921034812927\n",
      "epoch: 3 step: 599, loss is 0.015665188431739807\n",
      "epoch: 3 step: 600, loss is 0.019719146192073822\n",
      "epoch: 3 step: 601, loss is 0.027977073565125465\n",
      "epoch: 3 step: 602, loss is 0.08098001778125763\n",
      "epoch: 3 step: 603, loss is 0.007843113504350185\n",
      "epoch: 3 step: 604, loss is 0.18558165431022644\n",
      "epoch: 3 step: 605, loss is 0.013018572703003883\n",
      "epoch: 3 step: 606, loss is 0.002255793660879135\n",
      "epoch: 3 step: 607, loss is 0.01488663163036108\n",
      "epoch: 3 step: 608, loss is 0.04842584580183029\n",
      "epoch: 3 step: 609, loss is 0.038186632096767426\n",
      "epoch: 3 step: 610, loss is 0.0755804032087326\n",
      "epoch: 3 step: 611, loss is 0.006581885740160942\n",
      "epoch: 3 step: 612, loss is 0.07251372933387756\n",
      "epoch: 3 step: 613, loss is 0.029240097850561142\n",
      "epoch: 3 step: 614, loss is 0.007289366330951452\n",
      "epoch: 3 step: 615, loss is 0.007328871637582779\n",
      "epoch: 3 step: 616, loss is 0.023601900786161423\n",
      "epoch: 3 step: 617, loss is 0.015931256115436554\n",
      "epoch: 3 step: 618, loss is 0.03608328476548195\n",
      "epoch: 3 step: 619, loss is 0.0012345114955678582\n",
      "epoch: 3 step: 620, loss is 0.027242418378591537\n",
      "epoch: 3 step: 621, loss is 0.007601819932460785\n",
      "epoch: 3 step: 622, loss is 0.007960187271237373\n",
      "epoch: 3 step: 623, loss is 0.17362549901008606\n",
      "epoch: 3 step: 624, loss is 0.001962156966328621\n",
      "epoch: 3 step: 625, loss is 0.029445700347423553\n",
      "epoch: 3 step: 626, loss is 0.0018438197439536452\n",
      "epoch: 3 step: 627, loss is 0.044497523456811905\n",
      "epoch: 3 step: 628, loss is 0.015464555472135544\n",
      "epoch: 3 step: 629, loss is 0.042796291410923004\n",
      "epoch: 3 step: 630, loss is 0.010192818939685822\n",
      "epoch: 3 step: 631, loss is 0.12649039924144745\n",
      "epoch: 3 step: 632, loss is 0.030804242938756943\n",
      "epoch: 3 step: 633, loss is 0.001678703585639596\n",
      "epoch: 3 step: 634, loss is 0.08230334520339966\n",
      "epoch: 3 step: 635, loss is 0.04726821556687355\n",
      "epoch: 3 step: 636, loss is 0.01682755909860134\n",
      "epoch: 3 step: 637, loss is 0.038929104804992676\n",
      "epoch: 3 step: 638, loss is 0.023305146023631096\n",
      "epoch: 3 step: 639, loss is 0.0019243685528635979\n",
      "epoch: 3 step: 640, loss is 0.006810576189309359\n",
      "epoch: 3 step: 641, loss is 0.014376197010278702\n",
      "epoch: 3 step: 642, loss is 0.000485294935060665\n",
      "epoch: 3 step: 643, loss is 0.006066495086997747\n",
      "epoch: 3 step: 644, loss is 0.08615358918905258\n",
      "epoch: 3 step: 645, loss is 0.108849436044693\n",
      "epoch: 3 step: 646, loss is 0.007217229809612036\n",
      "epoch: 3 step: 647, loss is 0.0023229706566780806\n",
      "epoch: 3 step: 648, loss is 0.15209221839904785\n",
      "epoch: 3 step: 649, loss is 0.03221229091286659\n",
      "epoch: 3 step: 650, loss is 0.020166713744401932\n",
      "epoch: 3 step: 651, loss is 0.0036264024674892426\n",
      "epoch: 3 step: 652, loss is 0.00040644800174050033\n",
      "epoch: 3 step: 653, loss is 0.004615110345184803\n",
      "epoch: 3 step: 654, loss is 0.06810878962278366\n",
      "epoch: 3 step: 655, loss is 0.0035383440554142\n",
      "epoch: 3 step: 656, loss is 0.0106009216979146\n",
      "epoch: 3 step: 657, loss is 0.07163530588150024\n",
      "epoch: 3 step: 658, loss is 0.08964841812849045\n",
      "epoch: 3 step: 659, loss is 0.0022821479942649603\n",
      "epoch: 3 step: 660, loss is 0.054736729711294174\n",
      "epoch: 3 step: 661, loss is 0.001820848905481398\n",
      "epoch: 3 step: 662, loss is 0.0016783776227384806\n",
      "epoch: 3 step: 663, loss is 0.004670559428632259\n",
      "epoch: 3 step: 664, loss is 0.028998766094446182\n",
      "epoch: 3 step: 665, loss is 0.11581989377737045\n",
      "epoch: 3 step: 666, loss is 0.002749214181676507\n",
      "epoch: 3 step: 667, loss is 0.009211893193423748\n",
      "epoch: 3 step: 668, loss is 0.07906834036111832\n",
      "epoch: 3 step: 669, loss is 0.015448549762368202\n",
      "epoch: 3 step: 670, loss is 0.07060535997152328\n",
      "epoch: 3 step: 671, loss is 0.009339582175016403\n",
      "epoch: 3 step: 672, loss is 0.06099618226289749\n",
      "epoch: 3 step: 673, loss is 0.0005004362319596112\n",
      "epoch: 3 step: 674, loss is 0.008411637507379055\n",
      "epoch: 3 step: 675, loss is 0.008241578936576843\n",
      "epoch: 3 step: 676, loss is 0.01056059543043375\n",
      "epoch: 3 step: 677, loss is 0.1291990876197815\n",
      "epoch: 3 step: 678, loss is 0.0272667296230793\n",
      "epoch: 3 step: 679, loss is 0.009928148239850998\n",
      "epoch: 3 step: 680, loss is 0.029003385454416275\n",
      "epoch: 3 step: 681, loss is 0.0158646572381258\n",
      "epoch: 3 step: 682, loss is 0.03692367300391197\n",
      "epoch: 3 step: 683, loss is 0.004023730754852295\n",
      "epoch: 3 step: 684, loss is 0.0015945215709507465\n",
      "epoch: 3 step: 685, loss is 0.008888645097613335\n",
      "epoch: 3 step: 686, loss is 0.02533574216067791\n",
      "epoch: 3 step: 687, loss is 0.0029029936995357275\n",
      "epoch: 3 step: 688, loss is 0.08241782337427139\n",
      "epoch: 3 step: 689, loss is 0.012223664671182632\n",
      "epoch: 3 step: 690, loss is 0.1798228919506073\n",
      "epoch: 3 step: 691, loss is 0.004087992012500763\n",
      "epoch: 3 step: 692, loss is 0.0250334981828928\n",
      "epoch: 3 step: 693, loss is 0.033159710466861725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 694, loss is 0.002050956478342414\n",
      "epoch: 3 step: 695, loss is 0.12108449637889862\n",
      "epoch: 3 step: 696, loss is 0.03251552954316139\n",
      "epoch: 3 step: 697, loss is 0.11784782260656357\n",
      "epoch: 3 step: 698, loss is 0.004368404857814312\n",
      "epoch: 3 step: 699, loss is 0.35002002120018005\n",
      "epoch: 3 step: 700, loss is 0.012749060057103634\n",
      "epoch: 3 step: 701, loss is 0.2392352670431137\n",
      "epoch: 3 step: 702, loss is 0.004950101487338543\n",
      "epoch: 3 step: 703, loss is 0.09969451278448105\n",
      "epoch: 3 step: 704, loss is 0.002791594248265028\n",
      "epoch: 3 step: 705, loss is 0.019230270758271217\n",
      "epoch: 3 step: 706, loss is 0.011963766999542713\n",
      "epoch: 3 step: 707, loss is 0.009889857843518257\n",
      "epoch: 3 step: 708, loss is 0.017394371330738068\n",
      "epoch: 3 step: 709, loss is 0.030909067019820213\n",
      "epoch: 3 step: 710, loss is 0.32991668581962585\n",
      "epoch: 3 step: 711, loss is 0.12192342430353165\n",
      "epoch: 3 step: 712, loss is 0.03669475018978119\n",
      "epoch: 3 step: 713, loss is 0.032548580318689346\n",
      "epoch: 3 step: 714, loss is 0.021428070962429047\n",
      "epoch: 3 step: 715, loss is 0.12247937172651291\n",
      "epoch: 3 step: 716, loss is 0.0019330516224727035\n",
      "epoch: 3 step: 717, loss is 0.0540790893137455\n",
      "epoch: 3 step: 718, loss is 0.03463113680481911\n",
      "epoch: 3 step: 719, loss is 0.00033958713174797595\n",
      "epoch: 3 step: 720, loss is 0.08314248919487\n",
      "epoch: 3 step: 721, loss is 0.10916375368833542\n",
      "epoch: 3 step: 722, loss is 0.10269353538751602\n",
      "epoch: 3 step: 723, loss is 0.018981710076332092\n",
      "epoch: 3 step: 724, loss is 0.1060396134853363\n",
      "epoch: 3 step: 725, loss is 0.0027536917477846146\n",
      "epoch: 3 step: 726, loss is 0.001942272880114615\n",
      "epoch: 3 step: 727, loss is 0.009696368128061295\n",
      "epoch: 3 step: 728, loss is 0.0019903122447431087\n",
      "epoch: 3 step: 729, loss is 0.018320133909583092\n",
      "epoch: 3 step: 730, loss is 0.014232554472982883\n",
      "epoch: 3 step: 731, loss is 0.0069503472186625\n",
      "epoch: 3 step: 732, loss is 0.009415771812200546\n",
      "epoch: 3 step: 733, loss is 0.10369253158569336\n",
      "epoch: 3 step: 734, loss is 0.019230876117944717\n",
      "epoch: 3 step: 735, loss is 0.00441243639215827\n",
      "epoch: 3 step: 736, loss is 0.07206786423921585\n",
      "epoch: 3 step: 737, loss is 0.029846154153347015\n",
      "epoch: 3 step: 738, loss is 0.0021389946341514587\n",
      "epoch: 3 step: 739, loss is 0.0986841693520546\n",
      "epoch: 3 step: 740, loss is 0.023287421092391014\n",
      "epoch: 3 step: 741, loss is 0.0018750050803646445\n",
      "epoch: 3 step: 742, loss is 0.010376127436757088\n",
      "epoch: 3 step: 743, loss is 0.00925545021891594\n",
      "epoch: 3 step: 744, loss is 0.035632919520139694\n",
      "epoch: 3 step: 745, loss is 0.04470411315560341\n",
      "epoch: 3 step: 746, loss is 0.002799971029162407\n",
      "epoch: 3 step: 747, loss is 0.0023162662982940674\n",
      "epoch: 3 step: 748, loss is 0.0023233466781675816\n",
      "epoch: 3 step: 749, loss is 0.0007824485073797405\n",
      "epoch: 3 step: 750, loss is 0.010034669190645218\n",
      "epoch: 3 step: 751, loss is 0.010723160579800606\n",
      "epoch: 3 step: 752, loss is 0.010660705156624317\n",
      "epoch: 3 step: 753, loss is 0.004871085286140442\n",
      "epoch: 3 step: 754, loss is 0.010255801491439342\n",
      "epoch: 3 step: 755, loss is 0.002072135917842388\n",
      "epoch: 3 step: 756, loss is 0.18685626983642578\n",
      "epoch: 3 step: 757, loss is 0.004656082484871149\n",
      "epoch: 3 step: 758, loss is 0.013925479725003242\n",
      "epoch: 3 step: 759, loss is 0.01702577993273735\n",
      "epoch: 3 step: 760, loss is 0.010164646431803703\n",
      "epoch: 3 step: 761, loss is 0.1097746342420578\n",
      "epoch: 3 step: 762, loss is 0.0012898758286610246\n",
      "epoch: 3 step: 763, loss is 0.07258599251508713\n",
      "epoch: 3 step: 764, loss is 0.0034609977155923843\n",
      "epoch: 3 step: 765, loss is 0.4784303903579712\n",
      "epoch: 3 step: 766, loss is 0.23437491059303284\n",
      "epoch: 3 step: 767, loss is 0.00736526632681489\n",
      "epoch: 3 step: 768, loss is 0.0394614078104496\n",
      "epoch: 3 step: 769, loss is 0.013720962218940258\n",
      "epoch: 3 step: 770, loss is 0.14299269020557404\n",
      "epoch: 3 step: 771, loss is 0.0026785852387547493\n",
      "epoch: 3 step: 772, loss is 0.008268424309790134\n",
      "epoch: 3 step: 773, loss is 0.0670277550816536\n",
      "epoch: 3 step: 774, loss is 0.005007761064916849\n",
      "epoch: 3 step: 775, loss is 0.1182669922709465\n",
      "epoch: 3 step: 776, loss is 0.2501985430717468\n",
      "epoch: 3 step: 777, loss is 0.013543027453124523\n",
      "epoch: 3 step: 778, loss is 0.0020132549107074738\n",
      "epoch: 3 step: 779, loss is 0.0077839139848947525\n",
      "epoch: 3 step: 780, loss is 0.005173080135136843\n",
      "epoch: 3 step: 781, loss is 0.0203852616250515\n",
      "epoch: 3 step: 782, loss is 0.014227212406694889\n",
      "epoch: 3 step: 783, loss is 0.15010197460651398\n",
      "epoch: 3 step: 784, loss is 0.14192187786102295\n",
      "epoch: 3 step: 785, loss is 0.007457529194653034\n",
      "epoch: 3 step: 786, loss is 0.006242550443857908\n",
      "epoch: 3 step: 787, loss is 0.0015790603356435895\n",
      "epoch: 3 step: 788, loss is 0.0042345644906163216\n",
      "epoch: 3 step: 789, loss is 0.0006658467464148998\n",
      "epoch: 3 step: 790, loss is 0.037682581692934036\n",
      "epoch: 3 step: 791, loss is 0.08550576865673065\n",
      "epoch: 3 step: 792, loss is 0.004630222916603088\n",
      "epoch: 3 step: 793, loss is 0.017456604167819023\n",
      "epoch: 3 step: 794, loss is 0.04362039268016815\n",
      "epoch: 3 step: 795, loss is 0.031220337375998497\n",
      "epoch: 3 step: 796, loss is 0.003565350081771612\n",
      "epoch: 3 step: 797, loss is 0.004958268720656633\n",
      "epoch: 3 step: 798, loss is 0.0008174572139978409\n",
      "epoch: 3 step: 799, loss is 0.003453973215073347\n",
      "epoch: 3 step: 800, loss is 0.130743145942688\n",
      "epoch: 3 step: 801, loss is 0.015382702462375164\n",
      "epoch: 3 step: 802, loss is 0.0033836527727544308\n",
      "epoch: 3 step: 803, loss is 0.109171062707901\n",
      "epoch: 3 step: 804, loss is 0.14985598623752594\n",
      "epoch: 3 step: 805, loss is 0.03995993360877037\n",
      "epoch: 3 step: 806, loss is 0.002850805176422\n",
      "epoch: 3 step: 807, loss is 0.11141439527273178\n",
      "epoch: 3 step: 808, loss is 0.0019737088587135077\n",
      "epoch: 3 step: 809, loss is 0.014503095299005508\n",
      "epoch: 3 step: 810, loss is 0.06860899925231934\n",
      "epoch: 3 step: 811, loss is 0.018347598612308502\n",
      "epoch: 3 step: 812, loss is 0.022861989215016365\n",
      "epoch: 3 step: 813, loss is 0.1284140944480896\n",
      "epoch: 3 step: 814, loss is 0.014593137428164482\n",
      "epoch: 3 step: 815, loss is 0.01656731590628624\n",
      "epoch: 3 step: 816, loss is 0.12843658030033112\n",
      "epoch: 3 step: 817, loss is 0.06618940085172653\n",
      "epoch: 3 step: 818, loss is 0.03663766384124756\n",
      "epoch: 3 step: 819, loss is 0.013073286041617393\n",
      "epoch: 3 step: 820, loss is 0.047132257372140884\n",
      "epoch: 3 step: 821, loss is 0.009707649238407612\n",
      "epoch: 3 step: 822, loss is 0.06843190640211105\n",
      "epoch: 3 step: 823, loss is 0.0704108402132988\n",
      "epoch: 3 step: 824, loss is 0.18856798112392426\n",
      "epoch: 3 step: 825, loss is 0.015687281265854836\n",
      "epoch: 3 step: 826, loss is 0.07176295667886734\n",
      "epoch: 3 step: 827, loss is 0.09365007281303406\n",
      "epoch: 3 step: 828, loss is 0.31151264905929565\n",
      "epoch: 3 step: 829, loss is 0.10097797960042953\n",
      "epoch: 3 step: 830, loss is 0.007537484634667635\n",
      "epoch: 3 step: 831, loss is 0.014879542402923107\n",
      "epoch: 3 step: 832, loss is 0.07332490384578705\n",
      "epoch: 3 step: 833, loss is 0.006871591322124004\n",
      "epoch: 3 step: 834, loss is 0.00800352357327938\n",
      "epoch: 3 step: 835, loss is 0.1595095992088318\n",
      "epoch: 3 step: 836, loss is 0.007241012994199991\n",
      "epoch: 3 step: 837, loss is 0.11846673488616943\n",
      "epoch: 3 step: 838, loss is 0.0472753681242466\n",
      "epoch: 3 step: 839, loss is 0.0032355771400034428\n",
      "epoch: 3 step: 840, loss is 0.02648039348423481\n",
      "epoch: 3 step: 841, loss is 0.013095860369503498\n",
      "epoch: 3 step: 842, loss is 0.05360697954893112\n",
      "epoch: 3 step: 843, loss is 0.11844015121459961\n",
      "epoch: 3 step: 844, loss is 0.13371263444423676\n",
      "epoch: 3 step: 845, loss is 0.023724595084786415\n",
      "epoch: 3 step: 846, loss is 0.01754278503358364\n",
      "epoch: 3 step: 847, loss is 0.0071194963529706\n",
      "epoch: 3 step: 848, loss is 0.00398271344602108\n",
      "epoch: 3 step: 849, loss is 0.0067189703695476055\n",
      "epoch: 3 step: 850, loss is 0.009206220507621765\n",
      "epoch: 3 step: 851, loss is 0.0025475197471678257\n",
      "epoch: 3 step: 852, loss is 0.017717070877552032\n",
      "epoch: 3 step: 853, loss is 0.03965071961283684\n",
      "epoch: 3 step: 854, loss is 0.10684703290462494\n",
      "epoch: 3 step: 855, loss is 0.02976658195257187\n",
      "epoch: 3 step: 856, loss is 0.20336346328258514\n",
      "epoch: 3 step: 857, loss is 0.15872922539710999\n",
      "epoch: 3 step: 858, loss is 0.08773926645517349\n",
      "epoch: 3 step: 859, loss is 0.004553341772407293\n",
      "epoch: 3 step: 860, loss is 0.005347733851522207\n",
      "epoch: 3 step: 861, loss is 0.08734194934368134\n",
      "epoch: 3 step: 862, loss is 0.265497088432312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 863, loss is 0.012169083580374718\n",
      "epoch: 3 step: 864, loss is 0.027065757662057877\n",
      "epoch: 3 step: 865, loss is 0.004736743867397308\n",
      "epoch: 3 step: 866, loss is 0.008374649100005627\n",
      "epoch: 3 step: 867, loss is 0.028478382155299187\n",
      "epoch: 3 step: 868, loss is 0.024729225784540176\n",
      "epoch: 3 step: 869, loss is 0.019141197204589844\n",
      "epoch: 3 step: 870, loss is 0.09439858049154282\n",
      "epoch: 3 step: 871, loss is 0.018855493515729904\n",
      "epoch: 3 step: 872, loss is 0.014216831885278225\n",
      "epoch: 3 step: 873, loss is 0.03111816570162773\n",
      "epoch: 3 step: 874, loss is 0.013531232252717018\n",
      "epoch: 3 step: 875, loss is 0.005609484389424324\n",
      "epoch: 3 step: 876, loss is 0.004999857861548662\n",
      "epoch: 3 step: 877, loss is 0.014747096225619316\n",
      "epoch: 3 step: 878, loss is 0.05069372057914734\n",
      "epoch: 3 step: 879, loss is 0.09593917429447174\n",
      "epoch: 3 step: 880, loss is 0.03307744860649109\n",
      "epoch: 3 step: 881, loss is 0.019503796473145485\n",
      "epoch: 3 step: 882, loss is 0.03213246911764145\n",
      "epoch: 3 step: 883, loss is 0.040768492966890335\n",
      "epoch: 3 step: 884, loss is 0.003970364108681679\n",
      "epoch: 3 step: 885, loss is 0.0009754166239872575\n",
      "epoch: 3 step: 886, loss is 0.21119597554206848\n",
      "epoch: 3 step: 887, loss is 0.0015729295555502176\n",
      "epoch: 3 step: 888, loss is 0.049116965383291245\n",
      "epoch: 3 step: 889, loss is 0.035163961350917816\n",
      "epoch: 3 step: 890, loss is 0.2750698924064636\n",
      "epoch: 3 step: 891, loss is 0.2699275612831116\n",
      "epoch: 3 step: 892, loss is 0.12211258709430695\n",
      "epoch: 3 step: 893, loss is 0.008604364469647408\n",
      "epoch: 3 step: 894, loss is 0.2468939572572708\n",
      "epoch: 3 step: 895, loss is 0.006655480712652206\n",
      "epoch: 3 step: 896, loss is 0.014156913384795189\n",
      "epoch: 3 step: 897, loss is 0.10896028578281403\n",
      "epoch: 3 step: 898, loss is 0.1673281341791153\n",
      "epoch: 3 step: 899, loss is 0.0031726525630801916\n",
      "epoch: 3 step: 900, loss is 0.02981634996831417\n",
      "epoch: 3 step: 901, loss is 0.05194031447172165\n",
      "epoch: 3 step: 902, loss is 0.04102780297398567\n",
      "epoch: 3 step: 903, loss is 0.06001130864024162\n",
      "epoch: 3 step: 904, loss is 0.03468819335103035\n",
      "epoch: 3 step: 905, loss is 0.0923900380730629\n",
      "epoch: 3 step: 906, loss is 0.014884073287248611\n",
      "epoch: 3 step: 907, loss is 0.019577864557504654\n",
      "epoch: 3 step: 908, loss is 0.0985444113612175\n",
      "epoch: 3 step: 909, loss is 0.25348955392837524\n",
      "epoch: 3 step: 910, loss is 0.009920241311192513\n",
      "epoch: 3 step: 911, loss is 0.03506052494049072\n",
      "epoch: 3 step: 912, loss is 0.0196766909211874\n",
      "epoch: 3 step: 913, loss is 0.004962506704032421\n",
      "epoch: 3 step: 914, loss is 0.02592550404369831\n",
      "epoch: 3 step: 915, loss is 0.017414472997188568\n",
      "epoch: 3 step: 916, loss is 0.1316925436258316\n",
      "epoch: 3 step: 917, loss is 0.019312119111418724\n",
      "epoch: 3 step: 918, loss is 0.016534216701984406\n",
      "epoch: 3 step: 919, loss is 0.14852894842624664\n",
      "epoch: 3 step: 920, loss is 0.02184775285422802\n",
      "epoch: 3 step: 921, loss is 0.17352594435214996\n",
      "epoch: 3 step: 922, loss is 0.01881842315196991\n",
      "epoch: 3 step: 923, loss is 0.0166012030094862\n",
      "epoch: 3 step: 924, loss is 0.03563002124428749\n",
      "epoch: 3 step: 925, loss is 0.016467273235321045\n",
      "epoch: 3 step: 926, loss is 0.03297770023345947\n",
      "epoch: 3 step: 927, loss is 0.07085704803466797\n",
      "epoch: 3 step: 928, loss is 0.0016093699960038066\n",
      "epoch: 3 step: 929, loss is 0.029115330427885056\n",
      "epoch: 3 step: 930, loss is 0.013908944092690945\n",
      "epoch: 3 step: 931, loss is 0.05065743625164032\n",
      "epoch: 3 step: 932, loss is 0.03448097035288811\n",
      "epoch: 3 step: 933, loss is 0.022839725017547607\n",
      "epoch: 3 step: 934, loss is 0.08847509324550629\n",
      "epoch: 3 step: 935, loss is 0.01067300047725439\n",
      "epoch: 3 step: 936, loss is 0.03249279037117958\n",
      "epoch: 3 step: 937, loss is 0.012312788516283035\n",
      "epoch: 3 step: 938, loss is 0.013778571970760822\n",
      "epoch: 3 step: 939, loss is 0.004140371456742287\n",
      "epoch: 3 step: 940, loss is 0.06499839574098587\n",
      "epoch: 3 step: 941, loss is 0.03661585599184036\n",
      "epoch: 3 step: 942, loss is 0.013474355451762676\n",
      "epoch: 3 step: 943, loss is 0.048663027584552765\n",
      "epoch: 3 step: 944, loss is 0.013330037705600262\n",
      "epoch: 3 step: 945, loss is 0.010012393817305565\n",
      "epoch: 3 step: 946, loss is 0.02425178326666355\n",
      "epoch: 3 step: 947, loss is 0.008714774623513222\n",
      "epoch: 3 step: 948, loss is 0.027161769568920135\n",
      "epoch: 3 step: 949, loss is 0.004128431901335716\n",
      "epoch: 3 step: 950, loss is 0.007536607328802347\n",
      "epoch: 3 step: 951, loss is 0.30430513620376587\n",
      "epoch: 3 step: 952, loss is 0.10806851834058762\n",
      "epoch: 3 step: 953, loss is 0.0035951838362962008\n",
      "epoch: 3 step: 954, loss is 0.10251570492982864\n",
      "epoch: 3 step: 955, loss is 0.002282531699165702\n",
      "epoch: 3 step: 956, loss is 0.014064200222492218\n",
      "epoch: 3 step: 957, loss is 0.0014003133401274681\n",
      "epoch: 3 step: 958, loss is 0.05949409678578377\n",
      "epoch: 3 step: 959, loss is 0.011574272066354752\n",
      "epoch: 3 step: 960, loss is 0.2997898459434509\n",
      "epoch: 3 step: 961, loss is 0.19272056221961975\n",
      "epoch: 3 step: 962, loss is 0.120505191385746\n",
      "epoch: 3 step: 963, loss is 0.0010552552994340658\n",
      "epoch: 3 step: 964, loss is 0.02070826292037964\n",
      "epoch: 3 step: 965, loss is 0.025351133197546005\n",
      "epoch: 3 step: 966, loss is 0.01349505316466093\n",
      "epoch: 3 step: 967, loss is 0.09717413783073425\n",
      "epoch: 3 step: 968, loss is 0.02277177758514881\n",
      "epoch: 3 step: 969, loss is 0.06252492219209671\n",
      "epoch: 3 step: 970, loss is 0.00438974192366004\n",
      "epoch: 3 step: 971, loss is 0.0073793684132397175\n",
      "epoch: 3 step: 972, loss is 0.004397312179207802\n",
      "epoch: 3 step: 973, loss is 0.10123050957918167\n",
      "epoch: 3 step: 974, loss is 0.511134684085846\n",
      "epoch: 3 step: 975, loss is 0.003821195336058736\n",
      "epoch: 3 step: 976, loss is 0.029335372149944305\n",
      "epoch: 3 step: 977, loss is 0.002068602479994297\n",
      "epoch: 3 step: 978, loss is 0.02776212990283966\n",
      "epoch: 3 step: 979, loss is 0.15971852838993073\n",
      "epoch: 3 step: 980, loss is 0.05122184753417969\n",
      "epoch: 3 step: 981, loss is 0.010973561555147171\n",
      "epoch: 3 step: 982, loss is 0.024763276800513268\n",
      "epoch: 3 step: 983, loss is 0.04721548408269882\n",
      "epoch: 3 step: 984, loss is 0.032617926597595215\n",
      "epoch: 3 step: 985, loss is 0.062163155525922775\n",
      "epoch: 3 step: 986, loss is 0.057169109582901\n",
      "epoch: 3 step: 987, loss is 0.14861010015010834\n",
      "epoch: 3 step: 988, loss is 0.039876170456409454\n",
      "epoch: 3 step: 989, loss is 0.017450984567403793\n",
      "epoch: 3 step: 990, loss is 0.27553093433380127\n",
      "epoch: 3 step: 991, loss is 0.13017436861991882\n",
      "epoch: 3 step: 992, loss is 0.02318575605750084\n",
      "epoch: 3 step: 993, loss is 0.00523081561550498\n",
      "epoch: 3 step: 994, loss is 0.01583288051187992\n",
      "epoch: 3 step: 995, loss is 0.05507759004831314\n",
      "epoch: 3 step: 996, loss is 0.010161690413951874\n",
      "epoch: 3 step: 997, loss is 0.029237831011414528\n",
      "epoch: 3 step: 998, loss is 0.14027319848537445\n",
      "epoch: 3 step: 999, loss is 0.036656834185123444\n",
      "epoch: 3 step: 1000, loss is 0.01036970317363739\n",
      "epoch: 3 step: 1001, loss is 0.022071518003940582\n",
      "epoch: 3 step: 1002, loss is 0.011694133281707764\n",
      "epoch: 3 step: 1003, loss is 0.2260168045759201\n",
      "epoch: 3 step: 1004, loss is 0.005871432367712259\n",
      "epoch: 3 step: 1005, loss is 0.05493691563606262\n",
      "epoch: 3 step: 1006, loss is 0.03246283903717995\n",
      "epoch: 3 step: 1007, loss is 0.06216791272163391\n",
      "epoch: 3 step: 1008, loss is 0.1943252980709076\n",
      "epoch: 3 step: 1009, loss is 0.021722782403230667\n",
      "epoch: 3 step: 1010, loss is 0.0018975401762872934\n",
      "epoch: 3 step: 1011, loss is 0.019406303763389587\n",
      "epoch: 3 step: 1012, loss is 0.053155798465013504\n",
      "epoch: 3 step: 1013, loss is 0.08096428960561752\n",
      "epoch: 3 step: 1014, loss is 0.08491646498441696\n",
      "epoch: 3 step: 1015, loss is 0.019441679120063782\n",
      "epoch: 3 step: 1016, loss is 0.0021001354325562716\n",
      "epoch: 3 step: 1017, loss is 0.07948623597621918\n",
      "epoch: 3 step: 1018, loss is 0.006385335233062506\n",
      "epoch: 3 step: 1019, loss is 0.025195466354489326\n",
      "epoch: 3 step: 1020, loss is 0.010727297514677048\n",
      "epoch: 3 step: 1021, loss is 0.010880852118134499\n",
      "epoch: 3 step: 1022, loss is 0.1489255279302597\n",
      "epoch: 3 step: 1023, loss is 0.013756532222032547\n",
      "epoch: 3 step: 1024, loss is 0.02588421292603016\n",
      "epoch: 3 step: 1025, loss is 0.006697524804621935\n",
      "epoch: 3 step: 1026, loss is 0.05716617777943611\n",
      "epoch: 3 step: 1027, loss is 0.011806518770754337\n",
      "epoch: 3 step: 1028, loss is 0.000792915525380522\n",
      "epoch: 3 step: 1029, loss is 0.03260697051882744\n",
      "epoch: 3 step: 1030, loss is 0.005233370698988438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 1031, loss is 0.023113377392292023\n",
      "epoch: 3 step: 1032, loss is 0.004892958793789148\n",
      "epoch: 3 step: 1033, loss is 0.03307705000042915\n",
      "epoch: 3 step: 1034, loss is 0.015418396331369877\n",
      "epoch: 3 step: 1035, loss is 0.053294554352760315\n",
      "epoch: 3 step: 1036, loss is 0.0030667977407574654\n",
      "epoch: 3 step: 1037, loss is 0.029897183179855347\n",
      "epoch: 3 step: 1038, loss is 0.03559374436736107\n",
      "epoch: 3 step: 1039, loss is 0.0016729396302253008\n",
      "epoch: 3 step: 1040, loss is 0.008713174611330032\n",
      "epoch: 3 step: 1041, loss is 0.14519087970256805\n",
      "epoch: 3 step: 1042, loss is 0.012705262750387192\n",
      "epoch: 3 step: 1043, loss is 0.03273238614201546\n",
      "epoch: 3 step: 1044, loss is 0.012530092149972916\n",
      "epoch: 3 step: 1045, loss is 0.07295408099889755\n",
      "epoch: 3 step: 1046, loss is 0.017365431413054466\n",
      "epoch: 3 step: 1047, loss is 0.03617807850241661\n",
      "epoch: 3 step: 1048, loss is 0.006026587449014187\n",
      "epoch: 3 step: 1049, loss is 0.016833478584885597\n",
      "epoch: 3 step: 1050, loss is 0.002954930765554309\n",
      "epoch: 3 step: 1051, loss is 0.020099014043807983\n",
      "epoch: 3 step: 1052, loss is 0.017616886645555496\n",
      "epoch: 3 step: 1053, loss is 0.0038921739906072617\n",
      "epoch: 3 step: 1054, loss is 0.11941846460103989\n",
      "epoch: 3 step: 1055, loss is 0.05112786591053009\n",
      "epoch: 3 step: 1056, loss is 0.0913989320397377\n",
      "epoch: 3 step: 1057, loss is 0.0012837726389989257\n",
      "epoch: 3 step: 1058, loss is 0.005631458479911089\n",
      "epoch: 3 step: 1059, loss is 0.02023073472082615\n",
      "epoch: 3 step: 1060, loss is 0.0017341701313853264\n",
      "epoch: 3 step: 1061, loss is 0.014585336670279503\n",
      "epoch: 3 step: 1062, loss is 0.00367665640078485\n",
      "epoch: 3 step: 1063, loss is 0.02917182259261608\n",
      "epoch: 3 step: 1064, loss is 0.013415678404271603\n",
      "epoch: 3 step: 1065, loss is 0.04901732876896858\n",
      "epoch: 3 step: 1066, loss is 0.10571444779634476\n",
      "epoch: 3 step: 1067, loss is 0.003677105065435171\n",
      "epoch: 3 step: 1068, loss is 0.11885244399309158\n",
      "epoch: 3 step: 1069, loss is 0.01032707467675209\n",
      "epoch: 3 step: 1070, loss is 0.041473113000392914\n",
      "epoch: 3 step: 1071, loss is 0.017628854140639305\n",
      "epoch: 3 step: 1072, loss is 0.05593757703900337\n",
      "epoch: 3 step: 1073, loss is 0.00973835214972496\n",
      "epoch: 3 step: 1074, loss is 0.005832612980157137\n",
      "epoch: 3 step: 1075, loss is 0.15512296557426453\n",
      "epoch: 3 step: 1076, loss is 0.09718742966651917\n",
      "epoch: 3 step: 1077, loss is 0.08857325464487076\n",
      "epoch: 3 step: 1078, loss is 0.2232542335987091\n",
      "epoch: 3 step: 1079, loss is 0.13349634408950806\n",
      "epoch: 3 step: 1080, loss is 0.05089263245463371\n",
      "epoch: 3 step: 1081, loss is 0.002609656658023596\n",
      "epoch: 3 step: 1082, loss is 0.12968066334724426\n",
      "epoch: 3 step: 1083, loss is 0.00121357012540102\n",
      "epoch: 3 step: 1084, loss is 0.02696051634848118\n",
      "epoch: 3 step: 1085, loss is 0.7957178354263306\n",
      "epoch: 3 step: 1086, loss is 0.046703923493623734\n",
      "epoch: 3 step: 1087, loss is 0.004084230400621891\n",
      "epoch: 3 step: 1088, loss is 0.006200012750923634\n",
      "epoch: 3 step: 1089, loss is 0.015721116214990616\n",
      "epoch: 3 step: 1090, loss is 0.00995307695120573\n",
      "epoch: 3 step: 1091, loss is 0.09753962606191635\n",
      "epoch: 3 step: 1092, loss is 0.03522834554314613\n",
      "epoch: 3 step: 1093, loss is 0.38365063071250916\n",
      "epoch: 3 step: 1094, loss is 0.1836269348859787\n",
      "epoch: 3 step: 1095, loss is 0.029014000669121742\n",
      "epoch: 3 step: 1096, loss is 0.048329878598451614\n",
      "epoch: 3 step: 1097, loss is 0.18535122275352478\n",
      "epoch: 3 step: 1098, loss is 0.05913979932665825\n",
      "epoch: 3 step: 1099, loss is 0.10450122505426407\n",
      "epoch: 3 step: 1100, loss is 0.017189865931868553\n",
      "epoch: 3 step: 1101, loss is 0.033956144005060196\n",
      "epoch: 3 step: 1102, loss is 0.3254094421863556\n",
      "epoch: 3 step: 1103, loss is 0.015260577201843262\n",
      "epoch: 3 step: 1104, loss is 0.024161677807569504\n",
      "epoch: 3 step: 1105, loss is 0.014787059277296066\n",
      "epoch: 3 step: 1106, loss is 0.030142925679683685\n",
      "epoch: 3 step: 1107, loss is 0.0033107511699199677\n",
      "epoch: 3 step: 1108, loss is 0.04233907535672188\n",
      "epoch: 3 step: 1109, loss is 0.012306047603487968\n",
      "epoch: 3 step: 1110, loss is 0.037776172161102295\n",
      "epoch: 3 step: 1111, loss is 0.06048355996608734\n",
      "epoch: 3 step: 1112, loss is 0.04872727021574974\n",
      "epoch: 3 step: 1113, loss is 0.20379307866096497\n",
      "epoch: 3 step: 1114, loss is 0.029654160141944885\n",
      "epoch: 3 step: 1115, loss is 0.0033104359172284603\n",
      "epoch: 3 step: 1116, loss is 0.0030974457040429115\n",
      "epoch: 3 step: 1117, loss is 0.07696452736854553\n",
      "epoch: 3 step: 1118, loss is 0.1214914321899414\n",
      "epoch: 3 step: 1119, loss is 0.02081385999917984\n",
      "epoch: 3 step: 1120, loss is 0.048264291137456894\n",
      "epoch: 3 step: 1121, loss is 0.020825207233428955\n",
      "epoch: 3 step: 1122, loss is 0.02184581570327282\n",
      "epoch: 3 step: 1123, loss is 0.02696852758526802\n",
      "epoch: 3 step: 1124, loss is 0.04964136332273483\n",
      "epoch: 3 step: 1125, loss is 0.27902787923812866\n",
      "epoch: 3 step: 1126, loss is 0.015607244335114956\n",
      "epoch: 3 step: 1127, loss is 0.04181533306837082\n",
      "epoch: 3 step: 1128, loss is 0.2295471578836441\n",
      "epoch: 3 step: 1129, loss is 0.022175170481204987\n",
      "epoch: 3 step: 1130, loss is 0.03750840947031975\n",
      "epoch: 3 step: 1131, loss is 0.2146114856004715\n",
      "epoch: 3 step: 1132, loss is 0.010318833403289318\n",
      "epoch: 3 step: 1133, loss is 0.28265929222106934\n",
      "epoch: 3 step: 1134, loss is 0.07823578268289566\n",
      "epoch: 3 step: 1135, loss is 0.05234713852405548\n",
      "epoch: 3 step: 1136, loss is 0.0020787687972187996\n",
      "epoch: 3 step: 1137, loss is 0.001356088207103312\n",
      "epoch: 3 step: 1138, loss is 0.0031239588279277086\n",
      "epoch: 3 step: 1139, loss is 0.16188035905361176\n",
      "epoch: 3 step: 1140, loss is 0.0024479359854012728\n",
      "epoch: 3 step: 1141, loss is 0.12010114639997482\n",
      "epoch: 3 step: 1142, loss is 0.017992299050092697\n",
      "epoch: 3 step: 1143, loss is 0.04910670593380928\n",
      "epoch: 3 step: 1144, loss is 0.01921335607767105\n",
      "epoch: 3 step: 1145, loss is 0.007722554262727499\n",
      "epoch: 3 step: 1146, loss is 0.0019400062737986445\n",
      "epoch: 3 step: 1147, loss is 0.09416788816452026\n",
      "epoch: 3 step: 1148, loss is 0.011627197265625\n",
      "epoch: 3 step: 1149, loss is 0.1077895313501358\n",
      "epoch: 3 step: 1150, loss is 0.0070751868188381195\n",
      "epoch: 3 step: 1151, loss is 0.13376475870609283\n",
      "epoch: 3 step: 1152, loss is 0.04681960120797157\n",
      "epoch: 3 step: 1153, loss is 0.08508782088756561\n",
      "epoch: 3 step: 1154, loss is 0.002382440958172083\n",
      "epoch: 3 step: 1155, loss is 0.1268424391746521\n",
      "epoch: 3 step: 1156, loss is 0.03043089248239994\n",
      "epoch: 3 step: 1157, loss is 0.014753852970898151\n",
      "epoch: 3 step: 1158, loss is 0.002172291511669755\n",
      "epoch: 3 step: 1159, loss is 0.060627344995737076\n",
      "epoch: 3 step: 1160, loss is 0.008605862967669964\n",
      "epoch: 3 step: 1161, loss is 0.056855808943510056\n",
      "epoch: 3 step: 1162, loss is 0.13371853530406952\n",
      "epoch: 3 step: 1163, loss is 0.02089572884142399\n",
      "epoch: 3 step: 1164, loss is 0.030106164515018463\n",
      "epoch: 3 step: 1165, loss is 0.0056392475962638855\n",
      "epoch: 3 step: 1166, loss is 0.0013880595797672868\n",
      "epoch: 3 step: 1167, loss is 0.0131524121388793\n",
      "epoch: 3 step: 1168, loss is 0.013899118639528751\n",
      "epoch: 3 step: 1169, loss is 0.0014912779442965984\n",
      "epoch: 3 step: 1170, loss is 0.017533516511321068\n",
      "epoch: 3 step: 1171, loss is 0.003027615835890174\n",
      "epoch: 3 step: 1172, loss is 0.0184906218200922\n",
      "epoch: 3 step: 1173, loss is 0.21151325106620789\n",
      "epoch: 3 step: 1174, loss is 0.006832704413682222\n",
      "epoch: 3 step: 1175, loss is 0.008843494579195976\n",
      "epoch: 3 step: 1176, loss is 0.032401252537965775\n",
      "epoch: 3 step: 1177, loss is 0.1434365212917328\n",
      "epoch: 3 step: 1178, loss is 0.190978541970253\n",
      "epoch: 3 step: 1179, loss is 0.01455372292548418\n",
      "epoch: 3 step: 1180, loss is 0.02205706760287285\n",
      "epoch: 3 step: 1181, loss is 0.0029108577873557806\n",
      "epoch: 3 step: 1182, loss is 0.05844522640109062\n",
      "epoch: 3 step: 1183, loss is 0.08841170370578766\n",
      "epoch: 3 step: 1184, loss is 0.10627581924200058\n",
      "epoch: 3 step: 1185, loss is 0.01930837891995907\n",
      "epoch: 3 step: 1186, loss is 0.13964657485485077\n",
      "epoch: 3 step: 1187, loss is 0.14607970416545868\n",
      "epoch: 3 step: 1188, loss is 0.2702212929725647\n",
      "epoch: 3 step: 1189, loss is 0.001143777510151267\n",
      "epoch: 3 step: 1190, loss is 0.12566311657428741\n",
      "epoch: 3 step: 1191, loss is 0.05087701231241226\n",
      "epoch: 3 step: 1192, loss is 0.344187468290329\n",
      "epoch: 3 step: 1193, loss is 0.08198612928390503\n",
      "epoch: 3 step: 1194, loss is 0.0017394291935488582\n",
      "epoch: 3 step: 1195, loss is 0.40261566638946533\n",
      "epoch: 3 step: 1196, loss is 0.001988045172765851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 1197, loss is 0.0729764923453331\n",
      "epoch: 3 step: 1198, loss is 0.01582738384604454\n",
      "epoch: 3 step: 1199, loss is 0.02404762990772724\n",
      "epoch: 3 step: 1200, loss is 0.04056992754340172\n",
      "epoch: 3 step: 1201, loss is 0.014479330740869045\n",
      "epoch: 3 step: 1202, loss is 0.030480243265628815\n",
      "epoch: 3 step: 1203, loss is 0.05877063795924187\n",
      "epoch: 3 step: 1204, loss is 0.022941485047340393\n",
      "epoch: 3 step: 1205, loss is 0.0032377876341342926\n",
      "epoch: 3 step: 1206, loss is 0.19588930904865265\n",
      "epoch: 3 step: 1207, loss is 0.11713964492082596\n",
      "epoch: 3 step: 1208, loss is 0.00784203503280878\n",
      "epoch: 3 step: 1209, loss is 0.011575596407055855\n",
      "epoch: 3 step: 1210, loss is 0.009985392913222313\n",
      "epoch: 3 step: 1211, loss is 0.0920400321483612\n",
      "epoch: 3 step: 1212, loss is 0.07684590667486191\n",
      "epoch: 3 step: 1213, loss is 0.1966320276260376\n",
      "epoch: 3 step: 1214, loss is 0.10343854874372482\n",
      "epoch: 3 step: 1215, loss is 0.007185620721429586\n",
      "epoch: 3 step: 1216, loss is 0.12004410475492477\n",
      "epoch: 3 step: 1217, loss is 0.13983340561389923\n",
      "epoch: 3 step: 1218, loss is 0.14401839673519135\n",
      "epoch: 3 step: 1219, loss is 0.009317249990999699\n",
      "epoch: 3 step: 1220, loss is 0.033406153321266174\n",
      "epoch: 3 step: 1221, loss is 0.05447078123688698\n",
      "epoch: 3 step: 1222, loss is 0.019137978553771973\n",
      "epoch: 3 step: 1223, loss is 0.094875268638134\n",
      "epoch: 3 step: 1224, loss is 0.020548809319734573\n",
      "epoch: 3 step: 1225, loss is 0.021608082577586174\n",
      "epoch: 3 step: 1226, loss is 0.08809870481491089\n",
      "epoch: 3 step: 1227, loss is 0.005727119743824005\n",
      "epoch: 3 step: 1228, loss is 0.19038231670856476\n",
      "epoch: 3 step: 1229, loss is 0.036275625228881836\n",
      "epoch: 3 step: 1230, loss is 0.013833577744662762\n",
      "epoch: 3 step: 1231, loss is 0.061487261205911636\n",
      "epoch: 3 step: 1232, loss is 0.008495932444930077\n",
      "epoch: 3 step: 1233, loss is 0.004130588844418526\n",
      "epoch: 3 step: 1234, loss is 0.015579959377646446\n",
      "epoch: 3 step: 1235, loss is 0.02763340063393116\n",
      "epoch: 3 step: 1236, loss is 0.04762979596853256\n",
      "epoch: 3 step: 1237, loss is 0.01640002802014351\n",
      "epoch: 3 step: 1238, loss is 0.03281518444418907\n",
      "epoch: 3 step: 1239, loss is 0.0014746080851182342\n",
      "epoch: 3 step: 1240, loss is 0.009503418579697609\n",
      "epoch: 3 step: 1241, loss is 0.005736896768212318\n",
      "epoch: 3 step: 1242, loss is 0.0035300494637340307\n",
      "epoch: 3 step: 1243, loss is 0.1549820452928543\n",
      "epoch: 3 step: 1244, loss is 0.031557846814394\n",
      "epoch: 3 step: 1245, loss is 0.009755157865583897\n",
      "epoch: 3 step: 1246, loss is 0.00811774656176567\n",
      "epoch: 3 step: 1247, loss is 0.019760901108384132\n",
      "epoch: 3 step: 1248, loss is 0.029327375814318657\n",
      "epoch: 3 step: 1249, loss is 0.006712484173476696\n",
      "epoch: 3 step: 1250, loss is 0.11814580857753754\n",
      "epoch: 3 step: 1251, loss is 0.20295311510562897\n",
      "epoch: 3 step: 1252, loss is 0.004292102996259928\n",
      "epoch: 3 step: 1253, loss is 0.01270344853401184\n",
      "epoch: 3 step: 1254, loss is 0.014545237645506859\n",
      "epoch: 3 step: 1255, loss is 0.03993182256817818\n",
      "epoch: 3 step: 1256, loss is 0.0018525876803323627\n",
      "epoch: 3 step: 1257, loss is 0.03727275878190994\n",
      "epoch: 3 step: 1258, loss is 0.010111280716955662\n",
      "epoch: 3 step: 1259, loss is 0.0020208540372550488\n",
      "epoch: 3 step: 1260, loss is 0.1129206120967865\n",
      "epoch: 3 step: 1261, loss is 0.043718453496694565\n",
      "epoch: 3 step: 1262, loss is 0.002438850700855255\n",
      "epoch: 3 step: 1263, loss is 0.26714685559272766\n",
      "epoch: 3 step: 1264, loss is 0.10094541311264038\n",
      "epoch: 3 step: 1265, loss is 0.1988600492477417\n",
      "epoch: 3 step: 1266, loss is 0.0008999028359539807\n",
      "epoch: 3 step: 1267, loss is 0.0029275116976350546\n",
      "epoch: 3 step: 1268, loss is 0.024426782503724098\n",
      "epoch: 3 step: 1269, loss is 0.016118023544549942\n",
      "epoch: 3 step: 1270, loss is 0.08646013587713242\n",
      "epoch: 3 step: 1271, loss is 0.24434998631477356\n",
      "epoch: 3 step: 1272, loss is 0.08019421249628067\n",
      "epoch: 3 step: 1273, loss is 0.017722852528095245\n",
      "epoch: 3 step: 1274, loss is 0.016681941226124763\n",
      "epoch: 3 step: 1275, loss is 0.011273221112787724\n",
      "epoch: 3 step: 1276, loss is 0.04601408541202545\n",
      "epoch: 3 step: 1277, loss is 0.002817840315401554\n",
      "epoch: 3 step: 1278, loss is 0.06000204384326935\n",
      "epoch: 3 step: 1279, loss is 0.005065650679171085\n",
      "epoch: 3 step: 1280, loss is 0.0013081275392323732\n",
      "epoch: 3 step: 1281, loss is 0.006756079848855734\n",
      "epoch: 3 step: 1282, loss is 0.04314449802041054\n",
      "epoch: 3 step: 1283, loss is 0.002707625040784478\n",
      "epoch: 3 step: 1284, loss is 0.17421799898147583\n",
      "epoch: 3 step: 1285, loss is 0.0018933198880404234\n",
      "epoch: 3 step: 1286, loss is 0.03812546283006668\n",
      "epoch: 3 step: 1287, loss is 0.0969235748052597\n",
      "epoch: 3 step: 1288, loss is 0.01103600487112999\n",
      "epoch: 3 step: 1289, loss is 0.02146029844880104\n",
      "epoch: 3 step: 1290, loss is 0.04388486221432686\n",
      "epoch: 3 step: 1291, loss is 0.012142511084675789\n",
      "epoch: 3 step: 1292, loss is 0.05221381038427353\n",
      "epoch: 3 step: 1293, loss is 0.07248244434595108\n",
      "epoch: 3 step: 1294, loss is 0.014284338802099228\n",
      "epoch: 3 step: 1295, loss is 0.0331033430993557\n",
      "epoch: 3 step: 1296, loss is 0.020718181505799294\n",
      "epoch: 3 step: 1297, loss is 0.009707612916827202\n",
      "epoch: 3 step: 1298, loss is 0.00783075112849474\n",
      "epoch: 3 step: 1299, loss is 0.1442873179912567\n",
      "epoch: 3 step: 1300, loss is 0.0037796939723193645\n",
      "epoch: 3 step: 1301, loss is 0.07290977239608765\n",
      "epoch: 3 step: 1302, loss is 0.0008588943746872246\n",
      "epoch: 3 step: 1303, loss is 0.018640464171767235\n",
      "epoch: 3 step: 1304, loss is 0.0014179887948557734\n",
      "epoch: 3 step: 1305, loss is 0.2754502594470978\n",
      "epoch: 3 step: 1306, loss is 0.1379256695508957\n",
      "epoch: 3 step: 1307, loss is 0.13629810512065887\n",
      "epoch: 3 step: 1308, loss is 0.03218097612261772\n",
      "epoch: 3 step: 1309, loss is 0.05784878507256508\n",
      "epoch: 3 step: 1310, loss is 0.003415103070437908\n",
      "epoch: 3 step: 1311, loss is 0.09951186180114746\n",
      "epoch: 3 step: 1312, loss is 0.08228036761283875\n",
      "epoch: 3 step: 1313, loss is 0.0014319309266284108\n",
      "epoch: 3 step: 1314, loss is 0.19271866977214813\n",
      "epoch: 3 step: 1315, loss is 0.004969360772520304\n",
      "epoch: 3 step: 1316, loss is 0.044299546629190445\n",
      "epoch: 3 step: 1317, loss is 0.036477863788604736\n",
      "epoch: 3 step: 1318, loss is 0.08755993843078613\n",
      "epoch: 3 step: 1319, loss is 0.0014664360787719488\n",
      "epoch: 3 step: 1320, loss is 0.003346448065713048\n",
      "epoch: 3 step: 1321, loss is 0.03274338319897652\n",
      "epoch: 3 step: 1322, loss is 0.008606761693954468\n",
      "epoch: 3 step: 1323, loss is 0.12135357409715652\n",
      "epoch: 3 step: 1324, loss is 0.003912327345460653\n",
      "epoch: 3 step: 1325, loss is 0.01108550000935793\n",
      "epoch: 3 step: 1326, loss is 0.00149741570930928\n",
      "epoch: 3 step: 1327, loss is 0.1677597165107727\n",
      "epoch: 3 step: 1328, loss is 0.11540079861879349\n",
      "epoch: 3 step: 1329, loss is 0.01116461493074894\n",
      "epoch: 3 step: 1330, loss is 0.0017972959904000163\n",
      "epoch: 3 step: 1331, loss is 0.013147389516234398\n",
      "epoch: 3 step: 1332, loss is 0.006335707381367683\n",
      "epoch: 3 step: 1333, loss is 0.0043722535483539104\n",
      "epoch: 3 step: 1334, loss is 0.014539189636707306\n",
      "epoch: 3 step: 1335, loss is 0.008648556657135487\n",
      "epoch: 3 step: 1336, loss is 0.0334463007748127\n",
      "epoch: 3 step: 1337, loss is 0.010054446756839752\n",
      "epoch: 3 step: 1338, loss is 0.004144689068198204\n",
      "epoch: 3 step: 1339, loss is 0.2242281436920166\n",
      "epoch: 3 step: 1340, loss is 0.006643656641244888\n",
      "epoch: 3 step: 1341, loss is 0.080961212515831\n",
      "epoch: 3 step: 1342, loss is 0.07008933275938034\n",
      "epoch: 3 step: 1343, loss is 0.03850874304771423\n",
      "epoch: 3 step: 1344, loss is 0.014924369752407074\n",
      "epoch: 3 step: 1345, loss is 0.05142999812960625\n",
      "epoch: 3 step: 1346, loss is 0.0019312006188556552\n",
      "epoch: 3 step: 1347, loss is 0.164067342877388\n",
      "epoch: 3 step: 1348, loss is 0.15336129069328308\n",
      "epoch: 3 step: 1349, loss is 0.13606299459934235\n",
      "epoch: 3 step: 1350, loss is 0.013729874044656754\n",
      "epoch: 3 step: 1351, loss is 0.016350913792848587\n",
      "epoch: 3 step: 1352, loss is 0.03718782216310501\n",
      "epoch: 3 step: 1353, loss is 0.016466351225972176\n",
      "epoch: 3 step: 1354, loss is 0.0012781122932210565\n",
      "epoch: 3 step: 1355, loss is 0.014721098355948925\n",
      "epoch: 3 step: 1356, loss is 0.07316667586565018\n",
      "epoch: 3 step: 1357, loss is 0.000899146543815732\n",
      "epoch: 3 step: 1358, loss is 0.004025094676762819\n",
      "epoch: 3 step: 1359, loss is 0.002321051899343729\n",
      "epoch: 3 step: 1360, loss is 0.02682088501751423\n",
      "epoch: 3 step: 1361, loss is 0.04502548277378082\n",
      "epoch: 3 step: 1362, loss is 0.019711310043931007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 1363, loss is 0.0345168374478817\n",
      "epoch: 3 step: 1364, loss is 0.015400988981127739\n",
      "epoch: 3 step: 1365, loss is 0.039238207042217255\n",
      "epoch: 3 step: 1366, loss is 0.015817338600754738\n",
      "epoch: 3 step: 1367, loss is 0.042094796895980835\n",
      "epoch: 3 step: 1368, loss is 0.0039023403078317642\n",
      "epoch: 3 step: 1369, loss is 0.007192249875515699\n",
      "epoch: 3 step: 1370, loss is 0.17948345839977264\n",
      "epoch: 3 step: 1371, loss is 0.001689885277301073\n",
      "epoch: 3 step: 1372, loss is 0.15158288180828094\n",
      "epoch: 3 step: 1373, loss is 0.06602249294519424\n",
      "epoch: 3 step: 1374, loss is 0.014239093288779259\n",
      "epoch: 3 step: 1375, loss is 0.08012425154447556\n",
      "epoch: 3 step: 1376, loss is 0.22731739282608032\n",
      "epoch: 3 step: 1377, loss is 0.04134378209710121\n",
      "epoch: 3 step: 1378, loss is 0.015380761586129665\n",
      "epoch: 3 step: 1379, loss is 0.006925356108695269\n",
      "epoch: 3 step: 1380, loss is 0.0455930121243\n",
      "epoch: 3 step: 1381, loss is 0.018887298181653023\n",
      "epoch: 3 step: 1382, loss is 0.0004641139239538461\n",
      "epoch: 3 step: 1383, loss is 0.015783676877617836\n",
      "epoch: 3 step: 1384, loss is 0.008227495476603508\n",
      "epoch: 3 step: 1385, loss is 0.028905754908919334\n",
      "epoch: 3 step: 1386, loss is 0.044083308428525925\n",
      "epoch: 3 step: 1387, loss is 0.0009605454979464412\n",
      "epoch: 3 step: 1388, loss is 0.0026244441978633404\n",
      "epoch: 3 step: 1389, loss is 0.01918787881731987\n",
      "epoch: 3 step: 1390, loss is 0.06644731760025024\n",
      "epoch: 3 step: 1391, loss is 0.0018906648037955165\n",
      "epoch: 3 step: 1392, loss is 0.15278352797031403\n",
      "epoch: 3 step: 1393, loss is 0.06649142503738403\n",
      "epoch: 3 step: 1394, loss is 0.04564285650849342\n",
      "epoch: 3 step: 1395, loss is 0.0708157941699028\n",
      "epoch: 3 step: 1396, loss is 0.004294100683182478\n",
      "epoch: 3 step: 1397, loss is 0.011945003643631935\n",
      "epoch: 3 step: 1398, loss is 0.002163763390854001\n",
      "epoch: 3 step: 1399, loss is 0.025072362273931503\n",
      "epoch: 3 step: 1400, loss is 0.043712083250284195\n",
      "epoch: 3 step: 1401, loss is 0.18551687896251678\n",
      "epoch: 3 step: 1402, loss is 0.06803060322999954\n",
      "epoch: 3 step: 1403, loss is 0.0391789935529232\n",
      "epoch: 3 step: 1404, loss is 0.11409558355808258\n",
      "epoch: 3 step: 1405, loss is 0.0325859934091568\n",
      "epoch: 3 step: 1406, loss is 0.006044595967978239\n",
      "epoch: 3 step: 1407, loss is 0.0003860170254483819\n",
      "epoch: 3 step: 1408, loss is 0.0032576716039329767\n",
      "epoch: 3 step: 1409, loss is 0.2349679321050644\n",
      "epoch: 3 step: 1410, loss is 0.028218291699886322\n",
      "epoch: 3 step: 1411, loss is 0.03960930556058884\n",
      "epoch: 3 step: 1412, loss is 0.00804290920495987\n",
      "epoch: 3 step: 1413, loss is 0.06271369755268097\n",
      "epoch: 3 step: 1414, loss is 0.031579017639160156\n",
      "epoch: 3 step: 1415, loss is 0.015806429088115692\n",
      "epoch: 3 step: 1416, loss is 0.016987968236207962\n",
      "epoch: 3 step: 1417, loss is 0.15865880250930786\n",
      "epoch: 3 step: 1418, loss is 0.11192020773887634\n",
      "epoch: 3 step: 1419, loss is 0.0034315306693315506\n",
      "epoch: 3 step: 1420, loss is 0.09139012545347214\n",
      "epoch: 3 step: 1421, loss is 0.07482784986495972\n",
      "epoch: 3 step: 1422, loss is 0.08666356652975082\n",
      "epoch: 3 step: 1423, loss is 0.00542420195415616\n",
      "epoch: 3 step: 1424, loss is 0.0043157851323485374\n",
      "epoch: 3 step: 1425, loss is 0.001887305174022913\n",
      "epoch: 3 step: 1426, loss is 0.0006239907816052437\n",
      "epoch: 3 step: 1427, loss is 0.0033848625607788563\n",
      "epoch: 3 step: 1428, loss is 0.1445964127779007\n",
      "epoch: 3 step: 1429, loss is 0.014795035123825073\n",
      "epoch: 3 step: 1430, loss is 0.17577709257602692\n",
      "epoch: 3 step: 1431, loss is 0.029285261407494545\n",
      "epoch: 3 step: 1432, loss is 0.016137033700942993\n",
      "epoch: 3 step: 1433, loss is 0.055152423679828644\n",
      "epoch: 3 step: 1434, loss is 0.007899036630988121\n",
      "epoch: 3 step: 1435, loss is 0.019197365269064903\n",
      "epoch: 3 step: 1436, loss is 0.001111096702516079\n",
      "epoch: 3 step: 1437, loss is 0.0031553138978779316\n",
      "epoch: 3 step: 1438, loss is 0.01380616519600153\n",
      "epoch: 3 step: 1439, loss is 0.017765356227755547\n",
      "epoch: 3 step: 1440, loss is 0.09534608572721481\n",
      "epoch: 3 step: 1441, loss is 0.031125318259000778\n",
      "epoch: 3 step: 1442, loss is 0.0013030990958213806\n",
      "epoch: 3 step: 1443, loss is 0.05701896920800209\n",
      "epoch: 3 step: 1444, loss is 0.014532690867781639\n",
      "epoch: 3 step: 1445, loss is 0.10632908344268799\n",
      "epoch: 3 step: 1446, loss is 0.018740272149443626\n",
      "epoch: 3 step: 1447, loss is 0.07579662650823593\n",
      "epoch: 3 step: 1448, loss is 0.12539252638816833\n",
      "epoch: 3 step: 1449, loss is 0.043332528322935104\n",
      "epoch: 3 step: 1450, loss is 0.006975497584789991\n",
      "epoch: 3 step: 1451, loss is 0.08294220268726349\n",
      "epoch: 3 step: 1452, loss is 0.10446115583181381\n",
      "epoch: 3 step: 1453, loss is 0.013641317375004292\n",
      "epoch: 3 step: 1454, loss is 0.3682510256767273\n",
      "epoch: 3 step: 1455, loss is 0.008897675201296806\n",
      "epoch: 3 step: 1456, loss is 0.08629464358091354\n",
      "epoch: 3 step: 1457, loss is 0.19850273430347443\n",
      "epoch: 3 step: 1458, loss is 0.04938456416130066\n",
      "epoch: 3 step: 1459, loss is 0.1152799129486084\n",
      "epoch: 3 step: 1460, loss is 0.130022794008255\n",
      "epoch: 3 step: 1461, loss is 0.02361484244465828\n",
      "epoch: 3 step: 1462, loss is 0.01170955691486597\n",
      "epoch: 3 step: 1463, loss is 0.1529330313205719\n",
      "epoch: 3 step: 1464, loss is 0.05462821200489998\n",
      "epoch: 3 step: 1465, loss is 0.002575990743935108\n",
      "epoch: 3 step: 1466, loss is 0.13845568895339966\n",
      "epoch: 3 step: 1467, loss is 0.1657724380493164\n",
      "epoch: 3 step: 1468, loss is 0.03841445967555046\n",
      "epoch: 3 step: 1469, loss is 0.016192549839615822\n",
      "epoch: 3 step: 1470, loss is 0.007833864539861679\n",
      "epoch: 3 step: 1471, loss is 0.006572868209332228\n",
      "epoch: 3 step: 1472, loss is 0.050271693617105484\n",
      "epoch: 3 step: 1473, loss is 0.06831271946430206\n",
      "epoch: 3 step: 1474, loss is 0.004787097219377756\n",
      "epoch: 3 step: 1475, loss is 0.04052502661943436\n",
      "epoch: 3 step: 1476, loss is 0.019538745284080505\n",
      "epoch: 3 step: 1477, loss is 0.03444491699337959\n",
      "epoch: 3 step: 1478, loss is 0.022973597049713135\n",
      "epoch: 3 step: 1479, loss is 0.029124479740858078\n",
      "epoch: 3 step: 1480, loss is 0.06941310316324234\n",
      "epoch: 3 step: 1481, loss is 0.05189485847949982\n",
      "epoch: 3 step: 1482, loss is 0.13367103040218353\n",
      "epoch: 3 step: 1483, loss is 0.003648222889751196\n",
      "epoch: 3 step: 1484, loss is 0.022347521036863327\n",
      "epoch: 3 step: 1485, loss is 0.005739668384194374\n",
      "epoch: 3 step: 1486, loss is 0.01701456494629383\n",
      "epoch: 3 step: 1487, loss is 0.14030417799949646\n",
      "epoch: 3 step: 1488, loss is 0.006713655777275562\n",
      "epoch: 3 step: 1489, loss is 0.0075861550867557526\n",
      "epoch: 3 step: 1490, loss is 0.014312567189335823\n",
      "epoch: 3 step: 1491, loss is 0.013634375296533108\n",
      "epoch: 3 step: 1492, loss is 0.015322958119213581\n",
      "epoch: 3 step: 1493, loss is 0.005685214418917894\n",
      "epoch: 3 step: 1494, loss is 0.0035930927842855453\n",
      "epoch: 3 step: 1495, loss is 0.310364693403244\n",
      "epoch: 3 step: 1496, loss is 0.09519422799348831\n",
      "epoch: 3 step: 1497, loss is 0.003470784518867731\n",
      "epoch: 3 step: 1498, loss is 0.006293698213994503\n",
      "epoch: 3 step: 1499, loss is 0.009688188321888447\n",
      "epoch: 3 step: 1500, loss is 0.010349326767027378\n",
      "epoch: 3 step: 1501, loss is 0.02974865958094597\n",
      "epoch: 3 step: 1502, loss is 0.0074043190106749535\n",
      "epoch: 3 step: 1503, loss is 0.0036583691835403442\n",
      "epoch: 3 step: 1504, loss is 0.021624214947223663\n",
      "epoch: 3 step: 1505, loss is 0.026778141036629677\n",
      "epoch: 3 step: 1506, loss is 0.02603217586874962\n",
      "epoch: 3 step: 1507, loss is 0.07452496886253357\n",
      "epoch: 3 step: 1508, loss is 0.19191667437553406\n",
      "epoch: 3 step: 1509, loss is 0.0989980399608612\n",
      "epoch: 3 step: 1510, loss is 0.016413945704698563\n",
      "epoch: 3 step: 1511, loss is 0.017423905432224274\n",
      "epoch: 3 step: 1512, loss is 0.006566008552908897\n",
      "epoch: 3 step: 1513, loss is 0.025242600589990616\n",
      "epoch: 3 step: 1514, loss is 0.007628669962286949\n",
      "epoch: 3 step: 1515, loss is 0.04354340583086014\n",
      "epoch: 3 step: 1516, loss is 0.005188598297536373\n",
      "epoch: 3 step: 1517, loss is 0.07260522991418839\n",
      "epoch: 3 step: 1518, loss is 0.002965848194435239\n",
      "epoch: 3 step: 1519, loss is 0.10499278455972672\n",
      "epoch: 3 step: 1520, loss is 0.08580420166254044\n",
      "epoch: 3 step: 1521, loss is 0.018850214779376984\n",
      "epoch: 3 step: 1522, loss is 0.04897898808121681\n",
      "epoch: 3 step: 1523, loss is 0.008833796717226505\n",
      "epoch: 3 step: 1524, loss is 0.04178081825375557\n",
      "epoch: 3 step: 1525, loss is 0.0010438022436574101\n",
      "epoch: 3 step: 1526, loss is 0.00671855453401804\n",
      "epoch: 3 step: 1527, loss is 0.007893244735896587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 1528, loss is 0.009669545106589794\n",
      "epoch: 3 step: 1529, loss is 0.007543014362454414\n",
      "epoch: 3 step: 1530, loss is 0.056907013058662415\n",
      "epoch: 3 step: 1531, loss is 0.028495769947767258\n",
      "epoch: 3 step: 1532, loss is 0.11821209639310837\n",
      "epoch: 3 step: 1533, loss is 0.000716587936040014\n",
      "epoch: 3 step: 1534, loss is 0.04466623067855835\n",
      "epoch: 3 step: 1535, loss is 0.09647464752197266\n",
      "epoch: 3 step: 1536, loss is 0.03613824397325516\n",
      "epoch: 3 step: 1537, loss is 0.10730661451816559\n",
      "epoch: 3 step: 1538, loss is 0.0011261163745075464\n",
      "epoch: 3 step: 1539, loss is 0.013621038757264614\n",
      "epoch: 3 step: 1540, loss is 0.014330153353512287\n",
      "epoch: 3 step: 1541, loss is 0.021173866465687752\n",
      "epoch: 3 step: 1542, loss is 0.020755061879754066\n",
      "epoch: 3 step: 1543, loss is 0.18003666400909424\n",
      "epoch: 3 step: 1544, loss is 0.0029281924944370985\n",
      "epoch: 3 step: 1545, loss is 0.033185310661792755\n",
      "epoch: 3 step: 1546, loss is 0.0031530330888926983\n",
      "epoch: 3 step: 1547, loss is 0.009182036854326725\n",
      "epoch: 3 step: 1548, loss is 0.00652120029553771\n",
      "epoch: 3 step: 1549, loss is 0.0005036747315898538\n",
      "epoch: 3 step: 1550, loss is 0.17518733441829681\n",
      "epoch: 3 step: 1551, loss is 0.002573049161583185\n",
      "epoch: 3 step: 1552, loss is 0.0004717050469480455\n",
      "epoch: 3 step: 1553, loss is 0.009542579762637615\n",
      "epoch: 3 step: 1554, loss is 0.0060205417685210705\n",
      "epoch: 3 step: 1555, loss is 0.05941224843263626\n",
      "epoch: 3 step: 1556, loss is 0.344546377658844\n",
      "epoch: 3 step: 1557, loss is 0.008875589817762375\n",
      "epoch: 3 step: 1558, loss is 0.042745888233184814\n",
      "epoch: 3 step: 1559, loss is 0.16484874486923218\n",
      "epoch: 3 step: 1560, loss is 0.0011188071221113205\n",
      "epoch: 3 step: 1561, loss is 0.04776699095964432\n",
      "epoch: 3 step: 1562, loss is 0.002970906440168619\n",
      "epoch: 3 step: 1563, loss is 0.10600613057613373\n",
      "epoch: 3 step: 1564, loss is 0.034763678908348083\n",
      "epoch: 3 step: 1565, loss is 0.004564638249576092\n",
      "epoch: 3 step: 1566, loss is 0.028410304337739944\n",
      "epoch: 3 step: 1567, loss is 0.08306876569986343\n",
      "epoch: 3 step: 1568, loss is 0.09666716307401657\n",
      "epoch: 3 step: 1569, loss is 0.007486932445317507\n",
      "epoch: 3 step: 1570, loss is 0.06367287039756775\n",
      "epoch: 3 step: 1571, loss is 0.01805173233151436\n",
      "epoch: 3 step: 1572, loss is 0.0018828223692253232\n",
      "epoch: 3 step: 1573, loss is 0.054832592606544495\n",
      "epoch: 3 step: 1574, loss is 0.06652608513832092\n",
      "epoch: 3 step: 1575, loss is 0.0018354631029069424\n",
      "epoch: 3 step: 1576, loss is 0.02627534046769142\n",
      "epoch: 3 step: 1577, loss is 0.019200336188077927\n",
      "epoch: 3 step: 1578, loss is 0.019673766568303108\n",
      "epoch: 3 step: 1579, loss is 0.0020352955907583237\n",
      "epoch: 3 step: 1580, loss is 0.13692378997802734\n",
      "epoch: 3 step: 1581, loss is 0.020065054297447205\n",
      "epoch: 3 step: 1582, loss is 0.0479484423995018\n",
      "epoch: 3 step: 1583, loss is 0.11831095814704895\n",
      "epoch: 3 step: 1584, loss is 0.08084923028945923\n",
      "epoch: 3 step: 1585, loss is 0.0026386927347630262\n",
      "epoch: 3 step: 1586, loss is 0.013537213206291199\n",
      "epoch: 3 step: 1587, loss is 0.009329822845757008\n",
      "epoch: 3 step: 1588, loss is 0.010803415440022945\n",
      "epoch: 3 step: 1589, loss is 0.005512758158147335\n",
      "epoch: 3 step: 1590, loss is 0.09923447668552399\n",
      "epoch: 3 step: 1591, loss is 0.12289736419916153\n",
      "epoch: 3 step: 1592, loss is 0.01674308255314827\n",
      "epoch: 3 step: 1593, loss is 0.008258042857050896\n",
      "epoch: 3 step: 1594, loss is 0.1263144463300705\n",
      "epoch: 3 step: 1595, loss is 0.006996750365942717\n",
      "epoch: 3 step: 1596, loss is 0.11699405312538147\n",
      "epoch: 3 step: 1597, loss is 0.14565807580947876\n",
      "epoch: 3 step: 1598, loss is 0.026038506999611855\n",
      "epoch: 3 step: 1599, loss is 0.015325454995036125\n",
      "epoch: 3 step: 1600, loss is 0.0017680888995528221\n",
      "epoch: 3 step: 1601, loss is 0.1606699824333191\n",
      "epoch: 3 step: 1602, loss is 0.021960310637950897\n",
      "epoch: 3 step: 1603, loss is 0.038302917033433914\n",
      "epoch: 3 step: 1604, loss is 0.10939369350671768\n",
      "epoch: 3 step: 1605, loss is 0.02702423743903637\n",
      "epoch: 3 step: 1606, loss is 0.003994915634393692\n",
      "epoch: 3 step: 1607, loss is 0.012877287343144417\n",
      "epoch: 3 step: 1608, loss is 0.002739518415182829\n",
      "epoch: 3 step: 1609, loss is 0.0023048713337630033\n",
      "epoch: 3 step: 1610, loss is 0.0048264372162520885\n",
      "epoch: 3 step: 1611, loss is 0.02486976608633995\n",
      "epoch: 3 step: 1612, loss is 0.0029105432331562042\n",
      "epoch: 3 step: 1613, loss is 0.012199115939438343\n",
      "epoch: 3 step: 1614, loss is 0.016362188383936882\n",
      "epoch: 3 step: 1615, loss is 0.0044578383676707745\n",
      "epoch: 3 step: 1616, loss is 0.027248946949839592\n",
      "epoch: 3 step: 1617, loss is 0.005623332690447569\n",
      "epoch: 3 step: 1618, loss is 0.05258909985423088\n",
      "epoch: 3 step: 1619, loss is 0.04423888400197029\n",
      "epoch: 3 step: 1620, loss is 0.015924740582704544\n",
      "epoch: 3 step: 1621, loss is 0.4638028144836426\n",
      "epoch: 3 step: 1622, loss is 0.0022554488386958838\n",
      "epoch: 3 step: 1623, loss is 0.14974460005760193\n",
      "epoch: 3 step: 1624, loss is 0.13882917165756226\n",
      "epoch: 3 step: 1625, loss is 0.047167014330625534\n",
      "epoch: 3 step: 1626, loss is 0.00494470726698637\n",
      "epoch: 3 step: 1627, loss is 0.0024830857291817665\n",
      "epoch: 3 step: 1628, loss is 0.0013501590583473444\n",
      "epoch: 3 step: 1629, loss is 0.009748243726789951\n",
      "epoch: 3 step: 1630, loss is 0.009268276393413544\n",
      "epoch: 3 step: 1631, loss is 0.09457316994667053\n",
      "epoch: 3 step: 1632, loss is 0.12873347103595734\n",
      "epoch: 3 step: 1633, loss is 0.24789400398731232\n",
      "epoch: 3 step: 1634, loss is 0.0036610697861760855\n",
      "epoch: 3 step: 1635, loss is 0.005296753719449043\n",
      "epoch: 3 step: 1636, loss is 0.16440221667289734\n",
      "epoch: 3 step: 1637, loss is 0.006938690319657326\n",
      "epoch: 3 step: 1638, loss is 0.0025144077371805906\n",
      "epoch: 3 step: 1639, loss is 0.0011602985905483365\n",
      "epoch: 3 step: 1640, loss is 0.004414334427565336\n",
      "epoch: 3 step: 1641, loss is 0.015056177973747253\n",
      "epoch: 3 step: 1642, loss is 0.0012792907655239105\n",
      "epoch: 3 step: 1643, loss is 0.025427820160984993\n",
      "epoch: 3 step: 1644, loss is 0.11778930574655533\n",
      "epoch: 3 step: 1645, loss is 0.0775333121418953\n",
      "epoch: 3 step: 1646, loss is 0.007177815772593021\n",
      "epoch: 3 step: 1647, loss is 0.2874697744846344\n",
      "epoch: 3 step: 1648, loss is 0.15600614249706268\n",
      "epoch: 3 step: 1649, loss is 0.12590593099594116\n",
      "epoch: 3 step: 1650, loss is 0.0013714184751734138\n",
      "epoch: 3 step: 1651, loss is 0.20737065374851227\n",
      "epoch: 3 step: 1652, loss is 0.10615306347608566\n",
      "epoch: 3 step: 1653, loss is 0.1997217983007431\n",
      "epoch: 3 step: 1654, loss is 0.1606733202934265\n",
      "epoch: 3 step: 1655, loss is 0.030859047546982765\n",
      "epoch: 3 step: 1656, loss is 0.16774149239063263\n",
      "epoch: 3 step: 1657, loss is 0.03514984995126724\n",
      "epoch: 3 step: 1658, loss is 0.13431575894355774\n",
      "epoch: 3 step: 1659, loss is 0.018960895016789436\n",
      "epoch: 3 step: 1660, loss is 0.015854155644774437\n",
      "epoch: 3 step: 1661, loss is 0.028665419667959213\n",
      "epoch: 3 step: 1662, loss is 0.033288367092609406\n",
      "epoch: 3 step: 1663, loss is 0.06485948711633682\n",
      "epoch: 3 step: 1664, loss is 0.007946962490677834\n",
      "epoch: 3 step: 1665, loss is 0.12585827708244324\n",
      "epoch: 3 step: 1666, loss is 0.017714641988277435\n",
      "epoch: 3 step: 1667, loss is 0.05066461116075516\n",
      "epoch: 3 step: 1668, loss is 0.05142439529299736\n",
      "epoch: 3 step: 1669, loss is 0.013792600482702255\n",
      "epoch: 3 step: 1670, loss is 0.06716850399971008\n",
      "epoch: 3 step: 1671, loss is 0.012491343542933464\n",
      "epoch: 3 step: 1672, loss is 0.01693897508084774\n",
      "epoch: 3 step: 1673, loss is 0.010760590434074402\n",
      "epoch: 3 step: 1674, loss is 0.07294115424156189\n",
      "epoch: 3 step: 1675, loss is 0.0751885250210762\n",
      "epoch: 3 step: 1676, loss is 0.02229858934879303\n",
      "epoch: 3 step: 1677, loss is 0.006532265339046717\n",
      "epoch: 3 step: 1678, loss is 0.011727269738912582\n",
      "epoch: 3 step: 1679, loss is 0.0047378852032125\n",
      "epoch: 3 step: 1680, loss is 0.22827932238578796\n",
      "epoch: 3 step: 1681, loss is 0.017085814848542213\n",
      "epoch: 3 step: 1682, loss is 0.05857067555189133\n",
      "epoch: 3 step: 1683, loss is 0.08014088869094849\n",
      "epoch: 3 step: 1684, loss is 0.004632591735571623\n",
      "epoch: 3 step: 1685, loss is 0.002818223088979721\n",
      "epoch: 3 step: 1686, loss is 0.07905077189207077\n",
      "epoch: 3 step: 1687, loss is 0.03347790986299515\n",
      "epoch: 3 step: 1688, loss is 0.07456767559051514\n",
      "epoch: 3 step: 1689, loss is 0.0649321973323822\n",
      "epoch: 3 step: 1690, loss is 0.01841047592461109\n",
      "epoch: 3 step: 1691, loss is 0.03114374913275242\n",
      "epoch: 3 step: 1692, loss is 0.036640945822000504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 1693, loss is 0.002762154908850789\n",
      "epoch: 3 step: 1694, loss is 0.003173189703375101\n",
      "epoch: 3 step: 1695, loss is 0.021416278555989265\n",
      "epoch: 3 step: 1696, loss is 0.01761319302022457\n",
      "epoch: 3 step: 1697, loss is 0.010139654390513897\n",
      "epoch: 3 step: 1698, loss is 0.023537306115031242\n",
      "epoch: 3 step: 1699, loss is 0.0009254860924556851\n",
      "epoch: 3 step: 1700, loss is 0.09299944341182709\n",
      "epoch: 3 step: 1701, loss is 0.029986001551151276\n",
      "epoch: 3 step: 1702, loss is 0.0384180061519146\n",
      "epoch: 3 step: 1703, loss is 0.027422046288847923\n",
      "epoch: 3 step: 1704, loss is 0.04285063222050667\n",
      "epoch: 3 step: 1705, loss is 0.006645862013101578\n",
      "epoch: 3 step: 1706, loss is 0.016760151833295822\n",
      "epoch: 3 step: 1707, loss is 0.00831962563097477\n",
      "epoch: 3 step: 1708, loss is 0.010973154567182064\n",
      "epoch: 3 step: 1709, loss is 0.01320075336843729\n",
      "epoch: 3 step: 1710, loss is 0.05209879204630852\n",
      "epoch: 3 step: 1711, loss is 0.2842482626438141\n",
      "epoch: 3 step: 1712, loss is 0.07628907263278961\n",
      "epoch: 3 step: 1713, loss is 0.01391555741429329\n",
      "epoch: 3 step: 1714, loss is 0.00445764372125268\n",
      "epoch: 3 step: 1715, loss is 0.11412985622882843\n",
      "epoch: 3 step: 1716, loss is 0.10586929321289062\n",
      "epoch: 3 step: 1717, loss is 0.18017134070396423\n",
      "epoch: 3 step: 1718, loss is 0.01494438387453556\n",
      "epoch: 3 step: 1719, loss is 0.003137351246550679\n",
      "epoch: 3 step: 1720, loss is 0.04688774049282074\n",
      "epoch: 3 step: 1721, loss is 0.004969280678778887\n",
      "epoch: 3 step: 1722, loss is 0.1555795967578888\n",
      "epoch: 3 step: 1723, loss is 0.013617574237287045\n",
      "epoch: 3 step: 1724, loss is 0.023406963795423508\n",
      "epoch: 3 step: 1725, loss is 0.02169026806950569\n",
      "epoch: 3 step: 1726, loss is 0.00838338304311037\n",
      "epoch: 3 step: 1727, loss is 0.001644436502829194\n",
      "epoch: 3 step: 1728, loss is 0.1108650490641594\n",
      "epoch: 3 step: 1729, loss is 0.011117726564407349\n",
      "epoch: 3 step: 1730, loss is 0.003760639578104019\n",
      "epoch: 3 step: 1731, loss is 0.05906543508172035\n",
      "epoch: 3 step: 1732, loss is 0.07223404198884964\n",
      "epoch: 3 step: 1733, loss is 0.03922557458281517\n",
      "epoch: 3 step: 1734, loss is 0.002312894444912672\n",
      "epoch: 3 step: 1735, loss is 0.003491695737466216\n",
      "epoch: 3 step: 1736, loss is 0.016676709055900574\n",
      "epoch: 3 step: 1737, loss is 0.01906880922615528\n",
      "epoch: 3 step: 1738, loss is 0.05686995014548302\n",
      "epoch: 3 step: 1739, loss is 0.011938639916479588\n",
      "epoch: 3 step: 1740, loss is 0.0074352240189909935\n",
      "epoch: 3 step: 1741, loss is 0.003395096166059375\n",
      "epoch: 3 step: 1742, loss is 0.0009720945381559432\n",
      "epoch: 3 step: 1743, loss is 0.04068465158343315\n",
      "epoch: 3 step: 1744, loss is 0.00825973879545927\n",
      "epoch: 3 step: 1745, loss is 0.013822277076542377\n",
      "epoch: 3 step: 1746, loss is 0.0015852369833737612\n",
      "epoch: 3 step: 1747, loss is 0.6774293780326843\n",
      "epoch: 3 step: 1748, loss is 0.005608319770544767\n",
      "epoch: 3 step: 1749, loss is 0.010638927109539509\n",
      "epoch: 3 step: 1750, loss is 0.0074720680713653564\n",
      "epoch: 3 step: 1751, loss is 0.010534447617828846\n",
      "epoch: 3 step: 1752, loss is 0.027113262563943863\n",
      "epoch: 3 step: 1753, loss is 0.002443908713757992\n",
      "epoch: 3 step: 1754, loss is 0.11285349726676941\n",
      "epoch: 3 step: 1755, loss is 0.011054664850234985\n",
      "epoch: 3 step: 1756, loss is 0.00134945975150913\n",
      "epoch: 3 step: 1757, loss is 0.07595973461866379\n",
      "epoch: 3 step: 1758, loss is 0.005827213171869516\n",
      "epoch: 3 step: 1759, loss is 0.003528309753164649\n",
      "epoch: 3 step: 1760, loss is 0.042825888842344284\n",
      "epoch: 3 step: 1761, loss is 0.004761500749737024\n",
      "epoch: 3 step: 1762, loss is 0.045015882700681686\n",
      "epoch: 3 step: 1763, loss is 0.0011135635431855917\n",
      "epoch: 3 step: 1764, loss is 0.004647203255444765\n",
      "epoch: 3 step: 1765, loss is 0.046877969056367874\n",
      "epoch: 3 step: 1766, loss is 0.0037066000513732433\n",
      "epoch: 3 step: 1767, loss is 0.09121408313512802\n",
      "epoch: 3 step: 1768, loss is 0.00436319038271904\n",
      "epoch: 3 step: 1769, loss is 0.23382872343063354\n",
      "epoch: 3 step: 1770, loss is 0.06457078456878662\n",
      "epoch: 3 step: 1771, loss is 0.00541682867333293\n",
      "epoch: 3 step: 1772, loss is 0.07802343368530273\n",
      "epoch: 3 step: 1773, loss is 0.03832622617483139\n",
      "epoch: 3 step: 1774, loss is 0.04500705003738403\n",
      "epoch: 3 step: 1775, loss is 0.004262721631675959\n",
      "epoch: 3 step: 1776, loss is 0.034504491835832596\n",
      "epoch: 3 step: 1777, loss is 0.07231128960847855\n",
      "epoch: 3 step: 1778, loss is 0.003894704394042492\n",
      "epoch: 3 step: 1779, loss is 0.06266527622938156\n",
      "epoch: 3 step: 1780, loss is 0.0035350823309272528\n",
      "epoch: 3 step: 1781, loss is 0.0026775645092129707\n",
      "epoch: 3 step: 1782, loss is 0.03239681199193001\n",
      "epoch: 3 step: 1783, loss is 0.02588771842420101\n",
      "epoch: 3 step: 1784, loss is 0.05609370023012161\n",
      "epoch: 3 step: 1785, loss is 0.11235128343105316\n",
      "epoch: 3 step: 1786, loss is 0.08699008822441101\n",
      "epoch: 3 step: 1787, loss is 0.02474016137421131\n",
      "epoch: 3 step: 1788, loss is 0.07232625037431717\n",
      "epoch: 3 step: 1789, loss is 0.0031504768412560225\n",
      "epoch: 3 step: 1790, loss is 0.12832364439964294\n",
      "epoch: 3 step: 1791, loss is 0.10844399034976959\n",
      "epoch: 3 step: 1792, loss is 0.006579954642802477\n",
      "epoch: 3 step: 1793, loss is 0.0009318670490756631\n",
      "epoch: 3 step: 1794, loss is 0.15430976450443268\n",
      "epoch: 3 step: 1795, loss is 0.22730423510074615\n",
      "epoch: 3 step: 1796, loss is 0.1213051900267601\n",
      "epoch: 3 step: 1797, loss is 0.14719010889530182\n",
      "epoch: 3 step: 1798, loss is 0.03575017303228378\n",
      "epoch: 3 step: 1799, loss is 0.006425194442272186\n",
      "epoch: 3 step: 1800, loss is 0.041338011622428894\n",
      "epoch: 3 step: 1801, loss is 0.18514615297317505\n",
      "epoch: 3 step: 1802, loss is 0.026115894317626953\n",
      "epoch: 3 step: 1803, loss is 0.12301832437515259\n",
      "epoch: 3 step: 1804, loss is 0.05898946523666382\n",
      "epoch: 3 step: 1805, loss is 0.03190775588154793\n",
      "epoch: 3 step: 1806, loss is 0.2193462997674942\n",
      "epoch: 3 step: 1807, loss is 0.05422189086675644\n",
      "epoch: 3 step: 1808, loss is 0.002740028779953718\n",
      "epoch: 3 step: 1809, loss is 0.05726765841245651\n",
      "epoch: 3 step: 1810, loss is 0.009091230109333992\n",
      "epoch: 3 step: 1811, loss is 0.13719777762889862\n",
      "epoch: 3 step: 1812, loss is 0.10881558060646057\n",
      "epoch: 3 step: 1813, loss is 0.08771961182355881\n",
      "epoch: 3 step: 1814, loss is 0.006658453494310379\n",
      "epoch: 3 step: 1815, loss is 0.015721790492534637\n",
      "epoch: 3 step: 1816, loss is 0.05669640749692917\n",
      "epoch: 3 step: 1817, loss is 0.20318818092346191\n",
      "epoch: 3 step: 1818, loss is 0.008051110431551933\n",
      "epoch: 3 step: 1819, loss is 0.01048371847718954\n",
      "epoch: 3 step: 1820, loss is 0.00666068447753787\n",
      "epoch: 3 step: 1821, loss is 0.038909900933504105\n",
      "epoch: 3 step: 1822, loss is 0.013310037553310394\n",
      "epoch: 3 step: 1823, loss is 0.024559225887060165\n",
      "epoch: 3 step: 1824, loss is 0.0029612984508275986\n",
      "epoch: 3 step: 1825, loss is 0.13297805190086365\n",
      "epoch: 3 step: 1826, loss is 0.008534167893230915\n",
      "epoch: 3 step: 1827, loss is 0.03326669707894325\n",
      "epoch: 3 step: 1828, loss is 0.07736741751432419\n",
      "epoch: 3 step: 1829, loss is 0.005236170720309019\n",
      "epoch: 3 step: 1830, loss is 0.03435599431395531\n",
      "epoch: 3 step: 1831, loss is 0.09518907964229584\n",
      "epoch: 3 step: 1832, loss is 0.08931464701890945\n",
      "epoch: 3 step: 1833, loss is 0.005058357026427984\n",
      "epoch: 3 step: 1834, loss is 0.00108611723408103\n",
      "epoch: 3 step: 1835, loss is 0.006483473815023899\n",
      "epoch: 3 step: 1836, loss is 0.012330078519880772\n",
      "epoch: 3 step: 1837, loss is 0.22577480971813202\n",
      "epoch: 3 step: 1838, loss is 0.0017332943389192224\n",
      "epoch: 3 step: 1839, loss is 0.02523690089583397\n",
      "epoch: 3 step: 1840, loss is 0.0503326915204525\n",
      "epoch: 3 step: 1841, loss is 0.034588977694511414\n",
      "epoch: 3 step: 1842, loss is 0.09392847120761871\n",
      "epoch: 3 step: 1843, loss is 0.00917143002152443\n",
      "epoch: 3 step: 1844, loss is 0.10735055059194565\n",
      "epoch: 3 step: 1845, loss is 0.02724102884531021\n",
      "epoch: 3 step: 1846, loss is 0.01392356213182211\n",
      "epoch: 3 step: 1847, loss is 0.007997710257768631\n",
      "epoch: 3 step: 1848, loss is 0.010950948111712933\n",
      "epoch: 3 step: 1849, loss is 0.021206306293606758\n",
      "epoch: 3 step: 1850, loss is 0.10600123554468155\n",
      "epoch: 3 step: 1851, loss is 0.01142409723252058\n",
      "epoch: 3 step: 1852, loss is 0.0029889377765357494\n",
      "epoch: 3 step: 1853, loss is 0.009708244353532791\n",
      "epoch: 3 step: 1854, loss is 0.02160787209868431\n",
      "epoch: 3 step: 1855, loss is 0.04075852036476135\n",
      "epoch: 3 step: 1856, loss is 0.13000982999801636\n",
      "epoch: 3 step: 1857, loss is 0.06077989935874939\n",
      "epoch: 3 step: 1858, loss is 0.10149896889925003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 1859, loss is 0.01913340575993061\n",
      "epoch: 3 step: 1860, loss is 0.21614806354045868\n",
      "epoch: 3 step: 1861, loss is 0.016927994787693024\n",
      "epoch: 3 step: 1862, loss is 0.007258661091327667\n",
      "epoch: 3 step: 1863, loss is 0.008358855731785297\n",
      "epoch: 3 step: 1864, loss is 0.002981549361720681\n",
      "epoch: 3 step: 1865, loss is 0.08712785691022873\n",
      "epoch: 3 step: 1866, loss is 0.004603252746164799\n",
      "epoch: 3 step: 1867, loss is 0.013816999271512032\n",
      "epoch: 3 step: 1868, loss is 0.007030083332210779\n",
      "epoch: 3 step: 1869, loss is 0.014169061556458473\n",
      "epoch: 3 step: 1870, loss is 0.001195832621306181\n",
      "epoch: 3 step: 1871, loss is 0.0035345552023500204\n",
      "epoch: 3 step: 1872, loss is 0.10886839032173157\n",
      "epoch: 3 step: 1873, loss is 0.017693763598799706\n",
      "epoch: 3 step: 1874, loss is 0.05277327075600624\n",
      "epoch: 3 step: 1875, loss is 0.0021735639311373234\n",
      "epoch: 4 step: 1, loss is 0.0009688090067356825\n",
      "epoch: 4 step: 2, loss is 0.006628854665905237\n",
      "epoch: 4 step: 3, loss is 0.04616662487387657\n",
      "epoch: 4 step: 4, loss is 0.02122068591415882\n",
      "epoch: 4 step: 5, loss is 0.0039375401102006435\n",
      "epoch: 4 step: 6, loss is 0.03636356443166733\n",
      "epoch: 4 step: 7, loss is 0.04268413409590721\n",
      "epoch: 4 step: 8, loss is 0.09579430520534515\n",
      "epoch: 4 step: 9, loss is 0.041188765317201614\n",
      "epoch: 4 step: 10, loss is 0.002058204961940646\n",
      "epoch: 4 step: 11, loss is 0.01565951108932495\n",
      "epoch: 4 step: 12, loss is 0.10993935912847519\n",
      "epoch: 4 step: 13, loss is 0.0014077418018132448\n",
      "epoch: 4 step: 14, loss is 0.01226935163140297\n",
      "epoch: 4 step: 15, loss is 0.01938900165259838\n",
      "epoch: 4 step: 16, loss is 0.0005772353615611792\n",
      "epoch: 4 step: 17, loss is 0.008129262365400791\n",
      "epoch: 4 step: 18, loss is 0.09872506558895111\n",
      "epoch: 4 step: 19, loss is 0.0036480012349784374\n",
      "epoch: 4 step: 20, loss is 0.0014622369781136513\n",
      "epoch: 4 step: 21, loss is 0.0022573249880224466\n",
      "epoch: 4 step: 22, loss is 0.005016502924263477\n",
      "epoch: 4 step: 23, loss is 0.0019883448258042336\n",
      "epoch: 4 step: 24, loss is 0.0007476345053873956\n",
      "epoch: 4 step: 25, loss is 0.0008183607133105397\n",
      "epoch: 4 step: 26, loss is 0.004317860119044781\n",
      "epoch: 4 step: 27, loss is 0.012204071506857872\n",
      "epoch: 4 step: 28, loss is 0.044903017580509186\n",
      "epoch: 4 step: 29, loss is 0.0722334161400795\n",
      "epoch: 4 step: 30, loss is 0.018260294571518898\n",
      "epoch: 4 step: 31, loss is 0.01782451756298542\n",
      "epoch: 4 step: 32, loss is 0.010179677978157997\n",
      "epoch: 4 step: 33, loss is 0.0855865478515625\n",
      "epoch: 4 step: 34, loss is 0.11350398510694504\n",
      "epoch: 4 step: 35, loss is 0.0027802432887256145\n",
      "epoch: 4 step: 36, loss is 0.0016493218718096614\n",
      "epoch: 4 step: 37, loss is 0.0037851925007998943\n",
      "epoch: 4 step: 38, loss is 0.0015691915759816766\n",
      "epoch: 4 step: 39, loss is 0.012850702740252018\n",
      "epoch: 4 step: 40, loss is 0.030205145478248596\n",
      "epoch: 4 step: 41, loss is 0.005589257925748825\n",
      "epoch: 4 step: 42, loss is 0.02314932458102703\n",
      "epoch: 4 step: 43, loss is 0.03931530937552452\n",
      "epoch: 4 step: 44, loss is 0.14484143257141113\n",
      "epoch: 4 step: 45, loss is 0.00014262949116528034\n",
      "epoch: 4 step: 46, loss is 0.0014518448151648045\n",
      "epoch: 4 step: 47, loss is 0.009770316071808338\n",
      "epoch: 4 step: 48, loss is 0.006839722394943237\n",
      "epoch: 4 step: 49, loss is 0.0021786256693303585\n",
      "epoch: 4 step: 50, loss is 0.09951717406511307\n",
      "epoch: 4 step: 51, loss is 0.14695823192596436\n",
      "epoch: 4 step: 52, loss is 0.04777102917432785\n",
      "epoch: 4 step: 53, loss is 0.0008718853932805359\n",
      "epoch: 4 step: 54, loss is 0.20609933137893677\n",
      "epoch: 4 step: 55, loss is 0.006897156126797199\n",
      "epoch: 4 step: 56, loss is 0.0009369023609906435\n",
      "epoch: 4 step: 57, loss is 0.03078421950340271\n",
      "epoch: 4 step: 58, loss is 0.07602039724588394\n",
      "epoch: 4 step: 59, loss is 0.0031305921729654074\n",
      "epoch: 4 step: 60, loss is 0.0932311862707138\n",
      "epoch: 4 step: 61, loss is 0.017231108620762825\n",
      "epoch: 4 step: 62, loss is 0.0029007713310420513\n",
      "epoch: 4 step: 63, loss is 0.14689220488071442\n",
      "epoch: 4 step: 64, loss is 0.007083659525960684\n",
      "epoch: 4 step: 65, loss is 0.03263307362794876\n",
      "epoch: 4 step: 66, loss is 0.21911507844924927\n",
      "epoch: 4 step: 67, loss is 0.006483215838670731\n",
      "epoch: 4 step: 68, loss is 0.08482293784618378\n",
      "epoch: 4 step: 69, loss is 0.09585466980934143\n",
      "epoch: 4 step: 70, loss is 0.000646912376396358\n",
      "epoch: 4 step: 71, loss is 0.002752993255853653\n",
      "epoch: 4 step: 72, loss is 0.047310855239629745\n",
      "epoch: 4 step: 73, loss is 0.04068934544920921\n",
      "epoch: 4 step: 74, loss is 0.14259782433509827\n",
      "epoch: 4 step: 75, loss is 0.05193430930376053\n",
      "epoch: 4 step: 76, loss is 0.07924500107765198\n",
      "epoch: 4 step: 77, loss is 0.0012541648466140032\n",
      "epoch: 4 step: 78, loss is 0.009996682405471802\n",
      "epoch: 4 step: 79, loss is 0.06342247873544693\n",
      "epoch: 4 step: 80, loss is 0.015695026144385338\n",
      "epoch: 4 step: 81, loss is 0.08155293762683868\n",
      "epoch: 4 step: 82, loss is 0.002569120842963457\n",
      "epoch: 4 step: 83, loss is 0.004293837118893862\n",
      "epoch: 4 step: 84, loss is 0.0024259709753096104\n",
      "epoch: 4 step: 85, loss is 0.01997351087629795\n",
      "epoch: 4 step: 86, loss is 0.024581022560596466\n",
      "epoch: 4 step: 87, loss is 0.038147933781147\n",
      "epoch: 4 step: 88, loss is 0.1266450136899948\n",
      "epoch: 4 step: 89, loss is 0.013431181199848652\n",
      "epoch: 4 step: 90, loss is 0.004436308518052101\n",
      "epoch: 4 step: 91, loss is 0.022238444536924362\n",
      "epoch: 4 step: 92, loss is 0.04252757132053375\n",
      "epoch: 4 step: 93, loss is 0.03906626999378204\n",
      "epoch: 4 step: 94, loss is 0.0013832852710038424\n",
      "epoch: 4 step: 95, loss is 0.02361118048429489\n",
      "epoch: 4 step: 96, loss is 0.05269866809248924\n",
      "epoch: 4 step: 97, loss is 0.006194298155605793\n",
      "epoch: 4 step: 98, loss is 0.0005680760950781405\n",
      "epoch: 4 step: 99, loss is 0.009309547953307629\n",
      "epoch: 4 step: 100, loss is 0.03492024540901184\n",
      "epoch: 4 step: 101, loss is 0.01137455552816391\n",
      "epoch: 4 step: 102, loss is 0.002487834542989731\n",
      "epoch: 4 step: 103, loss is 0.03727994114160538\n",
      "epoch: 4 step: 104, loss is 0.01570727862417698\n",
      "epoch: 4 step: 105, loss is 0.08942611515522003\n",
      "epoch: 4 step: 106, loss is 0.0009715835913084447\n",
      "epoch: 4 step: 107, loss is 0.15401896834373474\n",
      "epoch: 4 step: 108, loss is 0.21513022482395172\n",
      "epoch: 4 step: 109, loss is 0.0005291985580697656\n",
      "epoch: 4 step: 110, loss is 0.0016321773873642087\n",
      "epoch: 4 step: 111, loss is 0.0005839608493261039\n",
      "epoch: 4 step: 112, loss is 0.022349810227751732\n",
      "epoch: 4 step: 113, loss is 0.0047492654994130135\n",
      "epoch: 4 step: 114, loss is 0.0081928176805377\n",
      "epoch: 4 step: 115, loss is 0.012752440758049488\n",
      "epoch: 4 step: 116, loss is 0.0067196376621723175\n",
      "epoch: 4 step: 117, loss is 0.00407119607552886\n",
      "epoch: 4 step: 118, loss is 0.0171612910926342\n",
      "epoch: 4 step: 119, loss is 0.004877876024693251\n",
      "epoch: 4 step: 120, loss is 0.0021669859997928143\n",
      "epoch: 4 step: 121, loss is 0.009066066704690456\n",
      "epoch: 4 step: 122, loss is 0.04208904877305031\n",
      "epoch: 4 step: 123, loss is 0.003304549027234316\n",
      "epoch: 4 step: 124, loss is 0.012285650707781315\n",
      "epoch: 4 step: 125, loss is 0.008843613788485527\n",
      "epoch: 4 step: 126, loss is 0.018756262958049774\n",
      "epoch: 4 step: 127, loss is 0.024556711316108704\n",
      "epoch: 4 step: 128, loss is 0.015052210539579391\n",
      "epoch: 4 step: 129, loss is 0.09487947076559067\n",
      "epoch: 4 step: 130, loss is 0.024009326472878456\n",
      "epoch: 4 step: 131, loss is 0.01724841445684433\n",
      "epoch: 4 step: 132, loss is 0.02937675081193447\n",
      "epoch: 4 step: 133, loss is 0.019153298810124397\n",
      "epoch: 4 step: 134, loss is 0.0489751435816288\n",
      "epoch: 4 step: 135, loss is 0.10487781465053558\n",
      "epoch: 4 step: 136, loss is 0.09627144783735275\n",
      "epoch: 4 step: 137, loss is 0.0009347763261757791\n",
      "epoch: 4 step: 138, loss is 0.02029438316822052\n",
      "epoch: 4 step: 139, loss is 0.0008317473693750799\n",
      "epoch: 4 step: 140, loss is 0.0034826130140572786\n",
      "epoch: 4 step: 141, loss is 0.006097596604377031\n",
      "epoch: 4 step: 142, loss is 0.01960722543299198\n",
      "epoch: 4 step: 143, loss is 0.051220547407865524\n",
      "epoch: 4 step: 144, loss is 0.03361060842871666\n",
      "epoch: 4 step: 145, loss is 0.016668614000082016\n",
      "epoch: 4 step: 146, loss is 0.014347395859658718\n",
      "epoch: 4 step: 147, loss is 0.005512481089681387\n",
      "epoch: 4 step: 148, loss is 0.0003613752778619528\n",
      "epoch: 4 step: 149, loss is 0.060131411999464035\n",
      "epoch: 4 step: 150, loss is 0.030864737927913666\n",
      "epoch: 4 step: 151, loss is 0.00219197035767138\n",
      "epoch: 4 step: 152, loss is 0.04098234325647354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 153, loss is 0.015951817855238914\n",
      "epoch: 4 step: 154, loss is 0.0024226470850408077\n",
      "epoch: 4 step: 155, loss is 0.06180746480822563\n",
      "epoch: 4 step: 156, loss is 0.007644919212907553\n",
      "epoch: 4 step: 157, loss is 0.08124355971813202\n",
      "epoch: 4 step: 158, loss is 0.0023931304458528757\n",
      "epoch: 4 step: 159, loss is 0.21206533908843994\n",
      "epoch: 4 step: 160, loss is 0.016203101724386215\n",
      "epoch: 4 step: 161, loss is 0.032730888575315475\n",
      "epoch: 4 step: 162, loss is 0.09698725491762161\n",
      "epoch: 4 step: 163, loss is 0.20014998316764832\n",
      "epoch: 4 step: 164, loss is 0.03763018175959587\n",
      "epoch: 4 step: 165, loss is 0.028933150693774223\n",
      "epoch: 4 step: 166, loss is 0.15659768879413605\n",
      "epoch: 4 step: 167, loss is 0.010622537694871426\n",
      "epoch: 4 step: 168, loss is 0.00167875736951828\n",
      "epoch: 4 step: 169, loss is 0.20553795993328094\n",
      "epoch: 4 step: 170, loss is 0.003453122917562723\n",
      "epoch: 4 step: 171, loss is 0.002550029894337058\n",
      "epoch: 4 step: 172, loss is 0.006081949919462204\n",
      "epoch: 4 step: 173, loss is 0.012089691124856472\n",
      "epoch: 4 step: 174, loss is 0.029799383133649826\n",
      "epoch: 4 step: 175, loss is 0.006474927067756653\n",
      "epoch: 4 step: 176, loss is 0.05018910393118858\n",
      "epoch: 4 step: 177, loss is 0.0009463528986088932\n",
      "epoch: 4 step: 178, loss is 0.02390359714627266\n",
      "epoch: 4 step: 179, loss is 0.02732827514410019\n",
      "epoch: 4 step: 180, loss is 0.0022392687387764454\n",
      "epoch: 4 step: 181, loss is 0.023000018671154976\n",
      "epoch: 4 step: 182, loss is 0.010991566814482212\n",
      "epoch: 4 step: 183, loss is 0.0049056801944971085\n",
      "epoch: 4 step: 184, loss is 0.0035540671087801456\n",
      "epoch: 4 step: 185, loss is 0.005798468831926584\n",
      "epoch: 4 step: 186, loss is 0.003684393363073468\n",
      "epoch: 4 step: 187, loss is 0.10140358656644821\n",
      "epoch: 4 step: 188, loss is 0.003392178798094392\n",
      "epoch: 4 step: 189, loss is 0.014152106828987598\n",
      "epoch: 4 step: 190, loss is 0.11516464501619339\n",
      "epoch: 4 step: 191, loss is 0.12357742339372635\n",
      "epoch: 4 step: 192, loss is 0.21625147759914398\n",
      "epoch: 4 step: 193, loss is 0.02145891822874546\n",
      "epoch: 4 step: 194, loss is 0.05502491071820259\n",
      "epoch: 4 step: 195, loss is 0.0021107830107212067\n",
      "epoch: 4 step: 196, loss is 0.07718393206596375\n",
      "epoch: 4 step: 197, loss is 0.15436844527721405\n",
      "epoch: 4 step: 198, loss is 0.043947044759988785\n",
      "epoch: 4 step: 199, loss is 0.0046297903172671795\n",
      "epoch: 4 step: 200, loss is 0.02538863755762577\n",
      "epoch: 4 step: 201, loss is 0.13699635863304138\n",
      "epoch: 4 step: 202, loss is 0.018840201199054718\n",
      "epoch: 4 step: 203, loss is 0.0066088223829865456\n",
      "epoch: 4 step: 204, loss is 0.02231963537633419\n",
      "epoch: 4 step: 205, loss is 0.03539254143834114\n",
      "epoch: 4 step: 206, loss is 0.010707160457968712\n",
      "epoch: 4 step: 207, loss is 0.13124363124370575\n",
      "epoch: 4 step: 208, loss is 0.046288613229990005\n",
      "epoch: 4 step: 209, loss is 0.046266891062259674\n",
      "epoch: 4 step: 210, loss is 0.0005791699513792992\n",
      "epoch: 4 step: 211, loss is 0.03959738835692406\n",
      "epoch: 4 step: 212, loss is 0.024558981880545616\n",
      "epoch: 4 step: 213, loss is 0.007214119657874107\n",
      "epoch: 4 step: 214, loss is 0.006921032443642616\n",
      "epoch: 4 step: 215, loss is 0.021494250744581223\n",
      "epoch: 4 step: 216, loss is 0.0891781896352768\n",
      "epoch: 4 step: 217, loss is 0.016229825094342232\n",
      "epoch: 4 step: 218, loss is 0.14223645627498627\n",
      "epoch: 4 step: 219, loss is 0.26528578996658325\n",
      "epoch: 4 step: 220, loss is 0.0615670308470726\n",
      "epoch: 4 step: 221, loss is 0.0585489384829998\n",
      "epoch: 4 step: 222, loss is 0.018811596557497978\n",
      "epoch: 4 step: 223, loss is 0.00041644825250841677\n",
      "epoch: 4 step: 224, loss is 0.0002582574379630387\n",
      "epoch: 4 step: 225, loss is 0.041695889085531235\n",
      "epoch: 4 step: 226, loss is 0.25841623544692993\n",
      "epoch: 4 step: 227, loss is 0.003493553726002574\n",
      "epoch: 4 step: 228, loss is 0.004432540386915207\n",
      "epoch: 4 step: 229, loss is 0.02947237528860569\n",
      "epoch: 4 step: 230, loss is 0.04068978130817413\n",
      "epoch: 4 step: 231, loss is 0.03260861709713936\n",
      "epoch: 4 step: 232, loss is 0.15142032504081726\n",
      "epoch: 4 step: 233, loss is 0.006499163806438446\n",
      "epoch: 4 step: 234, loss is 0.010195031762123108\n",
      "epoch: 4 step: 235, loss is 0.002540874993428588\n",
      "epoch: 4 step: 236, loss is 0.008640940301120281\n",
      "epoch: 4 step: 237, loss is 0.07818281650543213\n",
      "epoch: 4 step: 238, loss is 0.04521479457616806\n",
      "epoch: 4 step: 239, loss is 0.10613588988780975\n",
      "epoch: 4 step: 240, loss is 0.002599752740934491\n",
      "epoch: 4 step: 241, loss is 0.0891166403889656\n",
      "epoch: 4 step: 242, loss is 0.01429755985736847\n",
      "epoch: 4 step: 243, loss is 0.008896210230886936\n",
      "epoch: 4 step: 244, loss is 0.029500141739845276\n",
      "epoch: 4 step: 245, loss is 0.006154556293040514\n",
      "epoch: 4 step: 246, loss is 0.008160345256328583\n",
      "epoch: 4 step: 247, loss is 0.0718684047460556\n",
      "epoch: 4 step: 248, loss is 0.04058323800563812\n",
      "epoch: 4 step: 249, loss is 0.001976183382794261\n",
      "epoch: 4 step: 250, loss is 0.10023166984319687\n",
      "epoch: 4 step: 251, loss is 0.16838113963603973\n",
      "epoch: 4 step: 252, loss is 0.019770756363868713\n",
      "epoch: 4 step: 253, loss is 0.01635568030178547\n",
      "epoch: 4 step: 254, loss is 0.02272498793900013\n",
      "epoch: 4 step: 255, loss is 0.08774804323911667\n",
      "epoch: 4 step: 256, loss is 0.009466242976486683\n",
      "epoch: 4 step: 257, loss is 0.09193327277898788\n",
      "epoch: 4 step: 258, loss is 0.008039808832108974\n",
      "epoch: 4 step: 259, loss is 0.010612809099256992\n",
      "epoch: 4 step: 260, loss is 0.09401819109916687\n",
      "epoch: 4 step: 261, loss is 0.014207692816853523\n",
      "epoch: 4 step: 262, loss is 0.011838876642286777\n",
      "epoch: 4 step: 263, loss is 0.010411630384624004\n",
      "epoch: 4 step: 264, loss is 0.14661282300949097\n",
      "epoch: 4 step: 265, loss is 0.001954377628862858\n",
      "epoch: 4 step: 266, loss is 0.00448044715449214\n",
      "epoch: 4 step: 267, loss is 0.0041479929350316525\n",
      "epoch: 4 step: 268, loss is 0.007589238230139017\n",
      "epoch: 4 step: 269, loss is 0.00880538858473301\n",
      "epoch: 4 step: 270, loss is 0.029548943042755127\n",
      "epoch: 4 step: 271, loss is 0.04748795926570892\n",
      "epoch: 4 step: 272, loss is 0.11678170412778854\n",
      "epoch: 4 step: 273, loss is 0.030603652819991112\n",
      "epoch: 4 step: 274, loss is 0.19281192123889923\n",
      "epoch: 4 step: 275, loss is 0.23106376826763153\n",
      "epoch: 4 step: 276, loss is 0.05394555255770683\n",
      "epoch: 4 step: 277, loss is 0.0016749614151194692\n",
      "epoch: 4 step: 278, loss is 0.004116520285606384\n",
      "epoch: 4 step: 279, loss is 0.017814384773373604\n",
      "epoch: 4 step: 280, loss is 0.009905258193612099\n",
      "epoch: 4 step: 281, loss is 0.0009938818402588367\n",
      "epoch: 4 step: 282, loss is 0.043013643473386765\n",
      "epoch: 4 step: 283, loss is 0.01138211041688919\n",
      "epoch: 4 step: 284, loss is 0.005054485984146595\n",
      "epoch: 4 step: 285, loss is 0.06503941863775253\n",
      "epoch: 4 step: 286, loss is 0.0019955865573138\n",
      "epoch: 4 step: 287, loss is 0.0021930711809545755\n",
      "epoch: 4 step: 288, loss is 0.005690896883606911\n",
      "epoch: 4 step: 289, loss is 0.001518823904916644\n",
      "epoch: 4 step: 290, loss is 0.0035734225530177355\n",
      "epoch: 4 step: 291, loss is 0.018817542120814323\n",
      "epoch: 4 step: 292, loss is 0.0638221949338913\n",
      "epoch: 4 step: 293, loss is 0.0008823348907753825\n",
      "epoch: 4 step: 294, loss is 0.021780356764793396\n",
      "epoch: 4 step: 295, loss is 0.009913825429975986\n",
      "epoch: 4 step: 296, loss is 0.0024010830093175173\n",
      "epoch: 4 step: 297, loss is 0.0643359050154686\n",
      "epoch: 4 step: 298, loss is 0.018395986407995224\n",
      "epoch: 4 step: 299, loss is 0.05419447645545006\n",
      "epoch: 4 step: 300, loss is 0.12563398480415344\n",
      "epoch: 4 step: 301, loss is 0.000745249621104449\n",
      "epoch: 4 step: 302, loss is 0.0009580577607266605\n",
      "epoch: 4 step: 303, loss is 0.16453975439071655\n",
      "epoch: 4 step: 304, loss is 0.004537865519523621\n",
      "epoch: 4 step: 305, loss is 0.04353361949324608\n",
      "epoch: 4 step: 306, loss is 0.023815184831619263\n",
      "epoch: 4 step: 307, loss is 0.08329374343156815\n",
      "epoch: 4 step: 308, loss is 0.057862263172864914\n",
      "epoch: 4 step: 309, loss is 0.0022323806770145893\n",
      "epoch: 4 step: 310, loss is 0.00425620935857296\n",
      "epoch: 4 step: 311, loss is 0.07612113654613495\n",
      "epoch: 4 step: 312, loss is 0.214805468916893\n",
      "epoch: 4 step: 313, loss is 0.014297141693532467\n",
      "epoch: 4 step: 314, loss is 0.0026912717148661613\n",
      "epoch: 4 step: 315, loss is 0.01150474976748228\n",
      "epoch: 4 step: 316, loss is 0.006260716821998358\n",
      "epoch: 4 step: 317, loss is 0.09306363761425018\n",
      "epoch: 4 step: 318, loss is 0.016392948105931282\n",
      "epoch: 4 step: 319, loss is 0.004691019654273987\n",
      "epoch: 4 step: 320, loss is 0.017278986051678658\n",
      "epoch: 4 step: 321, loss is 0.07246935367584229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 322, loss is 0.12305539846420288\n",
      "epoch: 4 step: 323, loss is 0.009084193967282772\n",
      "epoch: 4 step: 324, loss is 0.0027794886846095324\n",
      "epoch: 4 step: 325, loss is 0.00926072709262371\n",
      "epoch: 4 step: 326, loss is 0.0035127904266119003\n",
      "epoch: 4 step: 327, loss is 0.022502481937408447\n",
      "epoch: 4 step: 328, loss is 0.004293086472898722\n",
      "epoch: 4 step: 329, loss is 0.01727486401796341\n",
      "epoch: 4 step: 330, loss is 0.04903808608651161\n",
      "epoch: 4 step: 331, loss is 0.00031552824657410383\n",
      "epoch: 4 step: 332, loss is 0.016666775569319725\n",
      "epoch: 4 step: 333, loss is 0.009342676028609276\n",
      "epoch: 4 step: 334, loss is 0.007404887117445469\n",
      "epoch: 4 step: 335, loss is 0.010400618426501751\n",
      "epoch: 4 step: 336, loss is 0.0020155024249106646\n",
      "epoch: 4 step: 337, loss is 0.18645162880420685\n",
      "epoch: 4 step: 338, loss is 0.0019886645022779703\n",
      "epoch: 4 step: 339, loss is 0.008993188850581646\n",
      "epoch: 4 step: 340, loss is 0.012878146022558212\n",
      "epoch: 4 step: 341, loss is 0.008723611012101173\n",
      "epoch: 4 step: 342, loss is 0.039410337805747986\n",
      "epoch: 4 step: 343, loss is 0.002644130727276206\n",
      "epoch: 4 step: 344, loss is 6.347870657918975e-05\n",
      "epoch: 4 step: 345, loss is 0.02325833961367607\n",
      "epoch: 4 step: 346, loss is 0.14868828654289246\n",
      "epoch: 4 step: 347, loss is 0.1161370575428009\n",
      "epoch: 4 step: 348, loss is 0.027291666716337204\n",
      "epoch: 4 step: 349, loss is 0.09539297968149185\n",
      "epoch: 4 step: 350, loss is 0.04897183179855347\n",
      "epoch: 4 step: 351, loss is 0.001273919129744172\n",
      "epoch: 4 step: 352, loss is 0.0023990345653146505\n",
      "epoch: 4 step: 353, loss is 0.0009832107461988926\n",
      "epoch: 4 step: 354, loss is 0.008630579337477684\n",
      "epoch: 4 step: 355, loss is 0.05771905183792114\n",
      "epoch: 4 step: 356, loss is 0.010165289975702763\n",
      "epoch: 4 step: 357, loss is 0.025856375694274902\n",
      "epoch: 4 step: 358, loss is 0.05504351481795311\n",
      "epoch: 4 step: 359, loss is 0.00281101418659091\n",
      "epoch: 4 step: 360, loss is 0.005793627817183733\n",
      "epoch: 4 step: 361, loss is 0.001397572923451662\n",
      "epoch: 4 step: 362, loss is 0.030950725078582764\n",
      "epoch: 4 step: 363, loss is 0.0006599050830118358\n",
      "epoch: 4 step: 364, loss is 0.002270161174237728\n",
      "epoch: 4 step: 365, loss is 0.0003651072911452502\n",
      "epoch: 4 step: 366, loss is 0.0010900881607085466\n",
      "epoch: 4 step: 367, loss is 0.002888629911467433\n",
      "epoch: 4 step: 368, loss is 0.0064659141935408115\n",
      "epoch: 4 step: 369, loss is 0.06826157122850418\n",
      "epoch: 4 step: 370, loss is 0.009308031760156155\n",
      "epoch: 4 step: 371, loss is 0.1766032576560974\n",
      "epoch: 4 step: 372, loss is 0.012281246483325958\n",
      "epoch: 4 step: 373, loss is 0.023158246651291847\n",
      "epoch: 4 step: 374, loss is 0.003533093724399805\n",
      "epoch: 4 step: 375, loss is 0.016027575358748436\n",
      "epoch: 4 step: 376, loss is 0.05811899155378342\n",
      "epoch: 4 step: 377, loss is 0.005063350778073072\n",
      "epoch: 4 step: 378, loss is 0.003971216734498739\n",
      "epoch: 4 step: 379, loss is 0.003843535203486681\n",
      "epoch: 4 step: 380, loss is 0.18695083260536194\n",
      "epoch: 4 step: 381, loss is 0.22886359691619873\n",
      "epoch: 4 step: 382, loss is 0.010409081354737282\n",
      "epoch: 4 step: 383, loss is 0.047767359763383865\n",
      "epoch: 4 step: 384, loss is 0.046987809240818024\n",
      "epoch: 4 step: 385, loss is 0.008880190551280975\n",
      "epoch: 4 step: 386, loss is 0.023013761267066002\n",
      "epoch: 4 step: 387, loss is 0.015592885203659534\n",
      "epoch: 4 step: 388, loss is 0.023296279832720757\n",
      "epoch: 4 step: 389, loss is 0.00045307702384889126\n",
      "epoch: 4 step: 390, loss is 0.006597451865673065\n",
      "epoch: 4 step: 391, loss is 0.03121773712337017\n",
      "epoch: 4 step: 392, loss is 0.008882984519004822\n",
      "epoch: 4 step: 393, loss is 0.0008852847968228161\n",
      "epoch: 4 step: 394, loss is 0.011910531669855118\n",
      "epoch: 4 step: 395, loss is 0.004648350179195404\n",
      "epoch: 4 step: 396, loss is 0.014151660725474358\n",
      "epoch: 4 step: 397, loss is 0.004062495660036802\n",
      "epoch: 4 step: 398, loss is 0.012686014175415039\n",
      "epoch: 4 step: 399, loss is 0.0021179234609007835\n",
      "epoch: 4 step: 400, loss is 0.27183693647384644\n",
      "epoch: 4 step: 401, loss is 0.005959771107882261\n",
      "epoch: 4 step: 402, loss is 0.00915762223303318\n",
      "epoch: 4 step: 403, loss is 0.0046645598486065865\n",
      "epoch: 4 step: 404, loss is 0.0452655665576458\n",
      "epoch: 4 step: 405, loss is 0.0007576945936307311\n",
      "epoch: 4 step: 406, loss is 0.025649722665548325\n",
      "epoch: 4 step: 407, loss is 0.09468609094619751\n",
      "epoch: 4 step: 408, loss is 0.0011649143416434526\n",
      "epoch: 4 step: 409, loss is 0.019870003685355186\n",
      "epoch: 4 step: 410, loss is 0.0426802933216095\n",
      "epoch: 4 step: 411, loss is 0.0028637819923460484\n",
      "epoch: 4 step: 412, loss is 0.03509603440761566\n",
      "epoch: 4 step: 413, loss is 0.027683807536959648\n",
      "epoch: 4 step: 414, loss is 0.0018609630642458797\n",
      "epoch: 4 step: 415, loss is 0.2098970264196396\n",
      "epoch: 4 step: 416, loss is 0.19082720577716827\n",
      "epoch: 4 step: 417, loss is 0.031552642583847046\n",
      "epoch: 4 step: 418, loss is 0.002556770108640194\n",
      "epoch: 4 step: 419, loss is 0.003050741972401738\n",
      "epoch: 4 step: 420, loss is 0.03279633820056915\n",
      "epoch: 4 step: 421, loss is 0.018501587212085724\n",
      "epoch: 4 step: 422, loss is 0.008374425582587719\n",
      "epoch: 4 step: 423, loss is 0.1080913320183754\n",
      "epoch: 4 step: 424, loss is 0.04592905938625336\n",
      "epoch: 4 step: 425, loss is 0.009732149541378021\n",
      "epoch: 4 step: 426, loss is 0.0545189194381237\n",
      "epoch: 4 step: 427, loss is 0.09307238459587097\n",
      "epoch: 4 step: 428, loss is 0.008836637251079082\n",
      "epoch: 4 step: 429, loss is 0.025352977216243744\n",
      "epoch: 4 step: 430, loss is 0.0033280756324529648\n",
      "epoch: 4 step: 431, loss is 0.006404678802937269\n",
      "epoch: 4 step: 432, loss is 0.11378174275159836\n",
      "epoch: 4 step: 433, loss is 0.07553137093782425\n",
      "epoch: 4 step: 434, loss is 0.19342847168445587\n",
      "epoch: 4 step: 435, loss is 0.0017759273760020733\n",
      "epoch: 4 step: 436, loss is 0.002896806923672557\n",
      "epoch: 4 step: 437, loss is 0.1411462426185608\n",
      "epoch: 4 step: 438, loss is 0.015526090748608112\n",
      "epoch: 4 step: 439, loss is 0.0017647187924012542\n",
      "epoch: 4 step: 440, loss is 0.0019372758688405156\n",
      "epoch: 4 step: 441, loss is 0.012163356877863407\n",
      "epoch: 4 step: 442, loss is 0.005840505473315716\n",
      "epoch: 4 step: 443, loss is 0.026986150071024895\n",
      "epoch: 4 step: 444, loss is 0.009195110760629177\n",
      "epoch: 4 step: 445, loss is 0.03449796512722969\n",
      "epoch: 4 step: 446, loss is 0.0017675842391327024\n",
      "epoch: 4 step: 447, loss is 0.006528838071972132\n",
      "epoch: 4 step: 448, loss is 0.0035536419600248337\n",
      "epoch: 4 step: 449, loss is 0.08158361166715622\n",
      "epoch: 4 step: 450, loss is 0.000784530071541667\n",
      "epoch: 4 step: 451, loss is 0.009966946206986904\n",
      "epoch: 4 step: 452, loss is 0.0021083445753902197\n",
      "epoch: 4 step: 453, loss is 0.005666612181812525\n",
      "epoch: 4 step: 454, loss is 0.03099612519145012\n",
      "epoch: 4 step: 455, loss is 0.06422469019889832\n",
      "epoch: 4 step: 456, loss is 0.0041380757465958595\n",
      "epoch: 4 step: 457, loss is 0.13761262595653534\n",
      "epoch: 4 step: 458, loss is 0.002295517362654209\n",
      "epoch: 4 step: 459, loss is 0.005881978664547205\n",
      "epoch: 4 step: 460, loss is 0.0004728941712528467\n",
      "epoch: 4 step: 461, loss is 0.013483318500220776\n",
      "epoch: 4 step: 462, loss is 0.019235624000430107\n",
      "epoch: 4 step: 463, loss is 0.017830315977334976\n",
      "epoch: 4 step: 464, loss is 0.010987184010446072\n",
      "epoch: 4 step: 465, loss is 0.008532826788723469\n",
      "epoch: 4 step: 466, loss is 0.015126246958971024\n",
      "epoch: 4 step: 467, loss is 0.20543131232261658\n",
      "epoch: 4 step: 468, loss is 0.00018575636204332113\n",
      "epoch: 4 step: 469, loss is 0.018182069063186646\n",
      "epoch: 4 step: 470, loss is 0.011088977567851543\n",
      "epoch: 4 step: 471, loss is 0.003347975667566061\n",
      "epoch: 4 step: 472, loss is 0.0002774055756162852\n",
      "epoch: 4 step: 473, loss is 0.11144597083330154\n",
      "epoch: 4 step: 474, loss is 0.006301407236605883\n",
      "epoch: 4 step: 475, loss is 0.12924592196941376\n",
      "epoch: 4 step: 476, loss is 0.0016383701004087925\n",
      "epoch: 4 step: 477, loss is 0.002029701368883252\n",
      "epoch: 4 step: 478, loss is 0.001895775320008397\n",
      "epoch: 4 step: 479, loss is 0.019331341609358788\n",
      "epoch: 4 step: 480, loss is 0.013496296480298042\n",
      "epoch: 4 step: 481, loss is 0.0072800652123987675\n",
      "epoch: 4 step: 482, loss is 0.001408407581038773\n",
      "epoch: 4 step: 483, loss is 0.027101019397377968\n",
      "epoch: 4 step: 484, loss is 0.027108564972877502\n",
      "epoch: 4 step: 485, loss is 0.0997903123497963\n",
      "epoch: 4 step: 486, loss is 0.012683337554335594\n",
      "epoch: 4 step: 487, loss is 0.0010652488563209772\n",
      "epoch: 4 step: 488, loss is 0.0006337116938084364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 489, loss is 0.0416727215051651\n",
      "epoch: 4 step: 490, loss is 0.14325708150863647\n",
      "epoch: 4 step: 491, loss is 0.000777382985688746\n",
      "epoch: 4 step: 492, loss is 0.008207730017602444\n",
      "epoch: 4 step: 493, loss is 0.017072215676307678\n",
      "epoch: 4 step: 494, loss is 0.02096734382212162\n",
      "epoch: 4 step: 495, loss is 0.015711361542344093\n",
      "epoch: 4 step: 496, loss is 0.22252261638641357\n",
      "epoch: 4 step: 497, loss is 0.008228563703596592\n",
      "epoch: 4 step: 498, loss is 0.062197137624025345\n",
      "epoch: 4 step: 499, loss is 0.0020218784920871258\n",
      "epoch: 4 step: 500, loss is 0.00035750254755839705\n",
      "epoch: 4 step: 501, loss is 0.031635601073503494\n",
      "epoch: 4 step: 502, loss is 0.002376256510615349\n",
      "epoch: 4 step: 503, loss is 0.00594861526042223\n",
      "epoch: 4 step: 504, loss is 0.03918089717626572\n",
      "epoch: 4 step: 505, loss is 0.013136623427271843\n",
      "epoch: 4 step: 506, loss is 0.058385349810123444\n",
      "epoch: 4 step: 507, loss is 0.020429691299796104\n",
      "epoch: 4 step: 508, loss is 0.013565268367528915\n",
      "epoch: 4 step: 509, loss is 0.0007233970682136714\n",
      "epoch: 4 step: 510, loss is 0.022230040282011032\n",
      "epoch: 4 step: 511, loss is 0.1479092538356781\n",
      "epoch: 4 step: 512, loss is 0.1799689084291458\n",
      "epoch: 4 step: 513, loss is 0.012078627943992615\n",
      "epoch: 4 step: 514, loss is 0.00725535536184907\n",
      "epoch: 4 step: 515, loss is 0.04109346121549606\n",
      "epoch: 4 step: 516, loss is 0.08200027048587799\n",
      "epoch: 4 step: 517, loss is 0.01573270931839943\n",
      "epoch: 4 step: 518, loss is 0.047974761575460434\n",
      "epoch: 4 step: 519, loss is 0.07476401329040527\n",
      "epoch: 4 step: 520, loss is 0.048182688653469086\n",
      "epoch: 4 step: 521, loss is 0.04700193554162979\n",
      "epoch: 4 step: 522, loss is 0.002159765223041177\n",
      "epoch: 4 step: 523, loss is 0.03901144862174988\n",
      "epoch: 4 step: 524, loss is 0.03809145838022232\n",
      "epoch: 4 step: 525, loss is 0.16910696029663086\n",
      "epoch: 4 step: 526, loss is 0.005648068152368069\n",
      "epoch: 4 step: 527, loss is 0.01863204687833786\n",
      "epoch: 4 step: 528, loss is 0.022315988317131996\n",
      "epoch: 4 step: 529, loss is 0.0019207604927942157\n",
      "epoch: 4 step: 530, loss is 0.13738331198692322\n",
      "epoch: 4 step: 531, loss is 0.002450678963214159\n",
      "epoch: 4 step: 532, loss is 0.16121812164783478\n",
      "epoch: 4 step: 533, loss is 0.07997340708971024\n",
      "epoch: 4 step: 534, loss is 0.021158359944820404\n",
      "epoch: 4 step: 535, loss is 0.13629701733589172\n",
      "epoch: 4 step: 536, loss is 0.03727171570062637\n",
      "epoch: 4 step: 537, loss is 0.002814721083268523\n",
      "epoch: 4 step: 538, loss is 0.03167269378900528\n",
      "epoch: 4 step: 539, loss is 0.004892438650131226\n",
      "epoch: 4 step: 540, loss is 0.07095512002706528\n",
      "epoch: 4 step: 541, loss is 0.007184075191617012\n",
      "epoch: 4 step: 542, loss is 0.0032327312510460615\n",
      "epoch: 4 step: 543, loss is 0.002422400750219822\n",
      "epoch: 4 step: 544, loss is 0.003950717858970165\n",
      "epoch: 4 step: 545, loss is 0.003649973776191473\n",
      "epoch: 4 step: 546, loss is 0.010194993577897549\n",
      "epoch: 4 step: 547, loss is 0.11283998191356659\n",
      "epoch: 4 step: 548, loss is 0.03514739125967026\n",
      "epoch: 4 step: 549, loss is 0.007444534916430712\n",
      "epoch: 4 step: 550, loss is 0.006081374362111092\n",
      "epoch: 4 step: 551, loss is 0.055322155356407166\n",
      "epoch: 4 step: 552, loss is 0.0033466683235019445\n",
      "epoch: 4 step: 553, loss is 0.02607574872672558\n",
      "epoch: 4 step: 554, loss is 0.000545995426364243\n",
      "epoch: 4 step: 555, loss is 0.0037366640754044056\n",
      "epoch: 4 step: 556, loss is 0.012420330196619034\n",
      "epoch: 4 step: 557, loss is 0.0014067795127630234\n",
      "epoch: 4 step: 558, loss is 0.0834255963563919\n",
      "epoch: 4 step: 559, loss is 0.05579957738518715\n",
      "epoch: 4 step: 560, loss is 0.06998389214277267\n",
      "epoch: 4 step: 561, loss is 0.057351142168045044\n",
      "epoch: 4 step: 562, loss is 0.001287420280277729\n",
      "epoch: 4 step: 563, loss is 0.04266076162457466\n",
      "epoch: 4 step: 564, loss is 0.04276688024401665\n",
      "epoch: 4 step: 565, loss is 0.09004785120487213\n",
      "epoch: 4 step: 566, loss is 0.002521838992834091\n",
      "epoch: 4 step: 567, loss is 0.007221424486488104\n",
      "epoch: 4 step: 568, loss is 0.014934196136891842\n",
      "epoch: 4 step: 569, loss is 0.004224252887070179\n",
      "epoch: 4 step: 570, loss is 0.0006121272454038262\n",
      "epoch: 4 step: 571, loss is 0.011307054199278355\n",
      "epoch: 4 step: 572, loss is 0.020783355459570885\n",
      "epoch: 4 step: 573, loss is 0.00565062602981925\n",
      "epoch: 4 step: 574, loss is 0.017169887199997902\n",
      "epoch: 4 step: 575, loss is 0.0174990464001894\n",
      "epoch: 4 step: 576, loss is 0.02791731059551239\n",
      "epoch: 4 step: 577, loss is 0.001071355422027409\n",
      "epoch: 4 step: 578, loss is 0.03403361141681671\n",
      "epoch: 4 step: 579, loss is 0.03227496147155762\n",
      "epoch: 4 step: 580, loss is 0.014923918060958385\n",
      "epoch: 4 step: 581, loss is 0.09583090245723724\n",
      "epoch: 4 step: 582, loss is 0.017097819596529007\n",
      "epoch: 4 step: 583, loss is 0.005218879319727421\n",
      "epoch: 4 step: 584, loss is 0.002702609170228243\n",
      "epoch: 4 step: 585, loss is 0.033537812530994415\n",
      "epoch: 4 step: 586, loss is 0.00466272933408618\n",
      "epoch: 4 step: 587, loss is 0.012122781947255135\n",
      "epoch: 4 step: 588, loss is 0.030487418174743652\n",
      "epoch: 4 step: 589, loss is 0.08194450289011002\n",
      "epoch: 4 step: 590, loss is 0.11844740808010101\n",
      "epoch: 4 step: 591, loss is 0.0039919414557516575\n",
      "epoch: 4 step: 592, loss is 0.012410301715135574\n",
      "epoch: 4 step: 593, loss is 0.34435558319091797\n",
      "epoch: 4 step: 594, loss is 0.03315455839037895\n",
      "epoch: 4 step: 595, loss is 0.08214685320854187\n",
      "epoch: 4 step: 596, loss is 0.037603769451379776\n",
      "epoch: 4 step: 597, loss is 0.0015571190742775798\n",
      "epoch: 4 step: 598, loss is 0.1036079153418541\n",
      "epoch: 4 step: 599, loss is 0.018848806619644165\n",
      "epoch: 4 step: 600, loss is 0.006264164112508297\n",
      "epoch: 4 step: 601, loss is 0.2522175908088684\n",
      "epoch: 4 step: 602, loss is 0.006914782337844372\n",
      "epoch: 4 step: 603, loss is 0.002915139077231288\n",
      "epoch: 4 step: 604, loss is 0.0006084207561798394\n",
      "epoch: 4 step: 605, loss is 0.0010448835091665387\n",
      "epoch: 4 step: 606, loss is 0.08248891681432724\n",
      "epoch: 4 step: 607, loss is 0.0019202005350962281\n",
      "epoch: 4 step: 608, loss is 0.1021316796541214\n",
      "epoch: 4 step: 609, loss is 0.02734173648059368\n",
      "epoch: 4 step: 610, loss is 0.07667884230613708\n",
      "epoch: 4 step: 611, loss is 0.004494913853704929\n",
      "epoch: 4 step: 612, loss is 0.0624481700360775\n",
      "epoch: 4 step: 613, loss is 0.005575314164161682\n",
      "epoch: 4 step: 614, loss is 0.005626986268907785\n",
      "epoch: 4 step: 615, loss is 0.011909394524991512\n",
      "epoch: 4 step: 616, loss is 0.00875728391110897\n",
      "epoch: 4 step: 617, loss is 0.009267686866223812\n",
      "epoch: 4 step: 618, loss is 0.014633474871516228\n",
      "epoch: 4 step: 619, loss is 0.010048196651041508\n",
      "epoch: 4 step: 620, loss is 0.029865572229027748\n",
      "epoch: 4 step: 621, loss is 0.01244335900992155\n",
      "epoch: 4 step: 622, loss is 0.4072358310222626\n",
      "epoch: 4 step: 623, loss is 0.006638919934630394\n",
      "epoch: 4 step: 624, loss is 0.12511736154556274\n",
      "epoch: 4 step: 625, loss is 0.08855650573968887\n",
      "epoch: 4 step: 626, loss is 0.0785282626748085\n",
      "epoch: 4 step: 627, loss is 0.02823524735867977\n",
      "epoch: 4 step: 628, loss is 0.005962112452834845\n",
      "epoch: 4 step: 629, loss is 0.0037299031391739845\n",
      "epoch: 4 step: 630, loss is 0.008466134779155254\n",
      "epoch: 4 step: 631, loss is 0.155340313911438\n",
      "epoch: 4 step: 632, loss is 0.004862414672970772\n",
      "epoch: 4 step: 633, loss is 0.006872573867440224\n",
      "epoch: 4 step: 634, loss is 0.013431702740490437\n",
      "epoch: 4 step: 635, loss is 0.010919294320046902\n",
      "epoch: 4 step: 636, loss is 0.005166069138795137\n",
      "epoch: 4 step: 637, loss is 0.0053977398201823235\n",
      "epoch: 4 step: 638, loss is 0.0020310685504227877\n",
      "epoch: 4 step: 639, loss is 0.1849428117275238\n",
      "epoch: 4 step: 640, loss is 0.015822991728782654\n",
      "epoch: 4 step: 641, loss is 0.053610917180776596\n",
      "epoch: 4 step: 642, loss is 0.030376309528946877\n",
      "epoch: 4 step: 643, loss is 0.02051394246518612\n",
      "epoch: 4 step: 644, loss is 0.014634731225669384\n",
      "epoch: 4 step: 645, loss is 0.03001049906015396\n",
      "epoch: 4 step: 646, loss is 0.008130610920488834\n",
      "epoch: 4 step: 647, loss is 0.05427102744579315\n",
      "epoch: 4 step: 648, loss is 0.004069031216204166\n",
      "epoch: 4 step: 649, loss is 0.0036171223036944866\n",
      "epoch: 4 step: 650, loss is 0.3161032199859619\n",
      "epoch: 4 step: 651, loss is 0.0015482077142223716\n",
      "epoch: 4 step: 652, loss is 0.0047368332743644714\n",
      "epoch: 4 step: 653, loss is 0.052049439400434494\n",
      "epoch: 4 step: 654, loss is 0.061972081661224365\n",
      "epoch: 4 step: 655, loss is 0.0020461080130189657\n",
      "epoch: 4 step: 656, loss is 0.00624395115301013\n",
      "epoch: 4 step: 657, loss is 0.04145127907395363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 658, loss is 0.0016031155828386545\n",
      "epoch: 4 step: 659, loss is 0.0009768788004294038\n",
      "epoch: 4 step: 660, loss is 0.013629172928631306\n",
      "epoch: 4 step: 661, loss is 0.09498357027769089\n",
      "epoch: 4 step: 662, loss is 0.02377483993768692\n",
      "epoch: 4 step: 663, loss is 0.008656351827085018\n",
      "epoch: 4 step: 664, loss is 0.04431646689772606\n",
      "epoch: 4 step: 665, loss is 0.005744133144617081\n",
      "epoch: 4 step: 666, loss is 0.004733630456030369\n",
      "epoch: 4 step: 667, loss is 0.007517945021390915\n",
      "epoch: 4 step: 668, loss is 0.01651299186050892\n",
      "epoch: 4 step: 669, loss is 0.07324598729610443\n",
      "epoch: 4 step: 670, loss is 0.05847065895795822\n",
      "epoch: 4 step: 671, loss is 0.04850155487656593\n",
      "epoch: 4 step: 672, loss is 0.004331760108470917\n",
      "epoch: 4 step: 673, loss is 0.002187502570450306\n",
      "epoch: 4 step: 674, loss is 0.0654989704489708\n",
      "epoch: 4 step: 675, loss is 0.030580490827560425\n",
      "epoch: 4 step: 676, loss is 0.003270187880843878\n",
      "epoch: 4 step: 677, loss is 0.028081422671675682\n",
      "epoch: 4 step: 678, loss is 0.0044944146648049355\n",
      "epoch: 4 step: 679, loss is 0.004063218832015991\n",
      "epoch: 4 step: 680, loss is 0.0009846994653344154\n",
      "epoch: 4 step: 681, loss is 0.01919468119740486\n",
      "epoch: 4 step: 682, loss is 0.047205764800310135\n",
      "epoch: 4 step: 683, loss is 0.010961035266518593\n",
      "epoch: 4 step: 684, loss is 0.0031764493323862553\n",
      "epoch: 4 step: 685, loss is 0.0006363372667692602\n",
      "epoch: 4 step: 686, loss is 0.0031796216499060392\n",
      "epoch: 4 step: 687, loss is 0.05153333395719528\n",
      "epoch: 4 step: 688, loss is 0.011769788339734077\n",
      "epoch: 4 step: 689, loss is 0.11870620399713516\n",
      "epoch: 4 step: 690, loss is 0.0018643663497641683\n",
      "epoch: 4 step: 691, loss is 0.002920831786468625\n",
      "epoch: 4 step: 692, loss is 0.04437803104519844\n",
      "epoch: 4 step: 693, loss is 0.0002779725182335824\n",
      "epoch: 4 step: 694, loss is 0.012207910418510437\n",
      "epoch: 4 step: 695, loss is 0.009275779128074646\n",
      "epoch: 4 step: 696, loss is 0.10632660239934921\n",
      "epoch: 4 step: 697, loss is 0.006914630997925997\n",
      "epoch: 4 step: 698, loss is 0.03539877384901047\n",
      "epoch: 4 step: 699, loss is 0.05424370616674423\n",
      "epoch: 4 step: 700, loss is 0.034376442432403564\n",
      "epoch: 4 step: 701, loss is 0.04715798795223236\n",
      "epoch: 4 step: 702, loss is 0.01827322691679001\n",
      "epoch: 4 step: 703, loss is 0.017289848998188972\n",
      "epoch: 4 step: 704, loss is 0.0431269146502018\n",
      "epoch: 4 step: 705, loss is 0.02573409117758274\n",
      "epoch: 4 step: 706, loss is 0.03498107194900513\n",
      "epoch: 4 step: 707, loss is 0.0010467029642313719\n",
      "epoch: 4 step: 708, loss is 0.05123068392276764\n",
      "epoch: 4 step: 709, loss is 0.007150063291192055\n",
      "epoch: 4 step: 710, loss is 0.011217785999178886\n",
      "epoch: 4 step: 711, loss is 0.12327994406223297\n",
      "epoch: 4 step: 712, loss is 0.007677269633859396\n",
      "epoch: 4 step: 713, loss is 0.006256151013076305\n",
      "epoch: 4 step: 714, loss is 0.0018911621300503612\n",
      "epoch: 4 step: 715, loss is 0.018885351717472076\n",
      "epoch: 4 step: 716, loss is 0.004354123491793871\n",
      "epoch: 4 step: 717, loss is 0.0004414716095197946\n",
      "epoch: 4 step: 718, loss is 0.004731097258627415\n",
      "epoch: 4 step: 719, loss is 0.003302170429378748\n",
      "epoch: 4 step: 720, loss is 0.06255403161048889\n",
      "epoch: 4 step: 721, loss is 0.008627635426819324\n",
      "epoch: 4 step: 722, loss is 0.06179659813642502\n",
      "epoch: 4 step: 723, loss is 0.002658243291079998\n",
      "epoch: 4 step: 724, loss is 0.10453905165195465\n",
      "epoch: 4 step: 725, loss is 0.011126616969704628\n",
      "epoch: 4 step: 726, loss is 0.005062302574515343\n",
      "epoch: 4 step: 727, loss is 0.0028807823546230793\n",
      "epoch: 4 step: 728, loss is 0.00030555567354895175\n",
      "epoch: 4 step: 729, loss is 0.09869445115327835\n",
      "epoch: 4 step: 730, loss is 0.0014119886327534914\n",
      "epoch: 4 step: 731, loss is 0.012974200770258904\n",
      "epoch: 4 step: 732, loss is 0.040642786771059036\n",
      "epoch: 4 step: 733, loss is 0.03298844024538994\n",
      "epoch: 4 step: 734, loss is 0.008990802802145481\n",
      "epoch: 4 step: 735, loss is 0.1269247978925705\n",
      "epoch: 4 step: 736, loss is 0.001755882054567337\n",
      "epoch: 4 step: 737, loss is 0.05740058794617653\n",
      "epoch: 4 step: 738, loss is 0.0004983550170436502\n",
      "epoch: 4 step: 739, loss is 0.009824986569583416\n",
      "epoch: 4 step: 740, loss is 0.0018079157453030348\n",
      "epoch: 4 step: 741, loss is 0.0017056718934327364\n",
      "epoch: 4 step: 742, loss is 0.010677190497517586\n",
      "epoch: 4 step: 743, loss is 0.006024553906172514\n",
      "epoch: 4 step: 744, loss is 0.15244615077972412\n",
      "epoch: 4 step: 745, loss is 8.11706850072369e-05\n",
      "epoch: 4 step: 746, loss is 0.027609122917056084\n",
      "epoch: 4 step: 747, loss is 0.014238422736525536\n",
      "epoch: 4 step: 748, loss is 0.0009698935318738222\n",
      "epoch: 4 step: 749, loss is 0.03095821663737297\n",
      "epoch: 4 step: 750, loss is 0.22821252048015594\n",
      "epoch: 4 step: 751, loss is 0.008783001452684402\n",
      "epoch: 4 step: 752, loss is 0.0008067918242886662\n",
      "epoch: 4 step: 753, loss is 0.005163448862731457\n",
      "epoch: 4 step: 754, loss is 0.07987762242555618\n",
      "epoch: 4 step: 755, loss is 0.24350614845752716\n",
      "epoch: 4 step: 756, loss is 0.04410484433174133\n",
      "epoch: 4 step: 757, loss is 0.024203283712267876\n",
      "epoch: 4 step: 758, loss is 0.00031421679886989295\n",
      "epoch: 4 step: 759, loss is 0.06630945950746536\n",
      "epoch: 4 step: 760, loss is 0.04157274216413498\n",
      "epoch: 4 step: 761, loss is 0.010999378748238087\n",
      "epoch: 4 step: 762, loss is 0.001737778540700674\n",
      "epoch: 4 step: 763, loss is 0.04245191067457199\n",
      "epoch: 4 step: 764, loss is 0.008474104106426239\n",
      "epoch: 4 step: 765, loss is 0.08932176232337952\n",
      "epoch: 4 step: 766, loss is 0.01860439032316208\n",
      "epoch: 4 step: 767, loss is 0.1572432667016983\n",
      "epoch: 4 step: 768, loss is 0.12587228417396545\n",
      "epoch: 4 step: 769, loss is 0.05959537252783775\n",
      "epoch: 4 step: 770, loss is 0.11660726368427277\n",
      "epoch: 4 step: 771, loss is 0.07210404425859451\n",
      "epoch: 4 step: 772, loss is 0.09116905927658081\n",
      "epoch: 4 step: 773, loss is 0.005081975366920233\n",
      "epoch: 4 step: 774, loss is 0.005276064854115248\n",
      "epoch: 4 step: 775, loss is 0.03785041719675064\n",
      "epoch: 4 step: 776, loss is 0.0016691922210156918\n",
      "epoch: 4 step: 777, loss is 0.06390167772769928\n",
      "epoch: 4 step: 778, loss is 0.017065294086933136\n",
      "epoch: 4 step: 779, loss is 0.004322471097111702\n",
      "epoch: 4 step: 780, loss is 0.15969058871269226\n",
      "epoch: 4 step: 781, loss is 0.1850155144929886\n",
      "epoch: 4 step: 782, loss is 0.01033933088183403\n",
      "epoch: 4 step: 783, loss is 0.00848292000591755\n",
      "epoch: 4 step: 784, loss is 0.004015552811324596\n",
      "epoch: 4 step: 785, loss is 0.16961200535297394\n",
      "epoch: 4 step: 786, loss is 0.0004708085907623172\n",
      "epoch: 4 step: 787, loss is 0.014855275861918926\n",
      "epoch: 4 step: 788, loss is 0.11314702779054642\n",
      "epoch: 4 step: 789, loss is 0.008334318175911903\n",
      "epoch: 4 step: 790, loss is 0.018014369532465935\n",
      "epoch: 4 step: 791, loss is 0.0008193663088604808\n",
      "epoch: 4 step: 792, loss is 0.036690451204776764\n",
      "epoch: 4 step: 793, loss is 0.19094057381153107\n",
      "epoch: 4 step: 794, loss is 0.006821875926107168\n",
      "epoch: 4 step: 795, loss is 0.18269123136997223\n",
      "epoch: 4 step: 796, loss is 0.002656205790117383\n",
      "epoch: 4 step: 797, loss is 0.02823362685739994\n",
      "epoch: 4 step: 798, loss is 0.0534946508705616\n",
      "epoch: 4 step: 799, loss is 0.0018923080060631037\n",
      "epoch: 4 step: 800, loss is 0.022267993539571762\n",
      "epoch: 4 step: 801, loss is 0.009221837855875492\n",
      "epoch: 4 step: 802, loss is 0.2891555428504944\n",
      "epoch: 4 step: 803, loss is 0.08908841013908386\n",
      "epoch: 4 step: 804, loss is 0.0005011879839003086\n",
      "epoch: 4 step: 805, loss is 0.06623955070972443\n",
      "epoch: 4 step: 806, loss is 0.0020154688972979784\n",
      "epoch: 4 step: 807, loss is 0.03429585322737694\n",
      "epoch: 4 step: 808, loss is 0.002846561837941408\n",
      "epoch: 4 step: 809, loss is 0.04552685469388962\n",
      "epoch: 4 step: 810, loss is 0.10960865020751953\n",
      "epoch: 4 step: 811, loss is 0.006399436853826046\n",
      "epoch: 4 step: 812, loss is 0.01253820676356554\n",
      "epoch: 4 step: 813, loss is 0.011269002221524715\n",
      "epoch: 4 step: 814, loss is 0.0002575425896793604\n",
      "epoch: 4 step: 815, loss is 0.010391317307949066\n",
      "epoch: 4 step: 816, loss is 0.02322445623576641\n",
      "epoch: 4 step: 817, loss is 0.002324081724509597\n",
      "epoch: 4 step: 818, loss is 0.011018889956176281\n",
      "epoch: 4 step: 819, loss is 0.031026313081383705\n",
      "epoch: 4 step: 820, loss is 0.022349659353494644\n",
      "epoch: 4 step: 821, loss is 0.007289351429790258\n",
      "epoch: 4 step: 822, loss is 0.041994817554950714\n",
      "epoch: 4 step: 823, loss is 0.09616554528474808\n",
      "epoch: 4 step: 824, loss is 0.022555343806743622\n",
      "epoch: 4 step: 825, loss is 0.011235719546675682\n",
      "epoch: 4 step: 826, loss is 0.27173659205436707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 827, loss is 0.0012000621063634753\n",
      "epoch: 4 step: 828, loss is 0.012512085027992725\n",
      "epoch: 4 step: 829, loss is 0.0015658399788662791\n",
      "epoch: 4 step: 830, loss is 0.09782927483320236\n",
      "epoch: 4 step: 831, loss is 0.027103859931230545\n",
      "epoch: 4 step: 832, loss is 0.004308253526687622\n",
      "epoch: 4 step: 833, loss is 0.0017967908643186092\n",
      "epoch: 4 step: 834, loss is 0.02028365060687065\n",
      "epoch: 4 step: 835, loss is 0.0007632660563103855\n",
      "epoch: 4 step: 836, loss is 0.2425384670495987\n",
      "epoch: 4 step: 837, loss is 0.011522420682013035\n",
      "epoch: 4 step: 838, loss is 0.07134237140417099\n",
      "epoch: 4 step: 839, loss is 0.024866901338100433\n",
      "epoch: 4 step: 840, loss is 0.00021153557463549078\n",
      "epoch: 4 step: 841, loss is 0.023104768246412277\n",
      "epoch: 4 step: 842, loss is 0.019419126212596893\n",
      "epoch: 4 step: 843, loss is 0.15195608139038086\n",
      "epoch: 4 step: 844, loss is 0.004964482970535755\n",
      "epoch: 4 step: 845, loss is 0.02430838532745838\n",
      "epoch: 4 step: 846, loss is 0.0007399226306006312\n",
      "epoch: 4 step: 847, loss is 0.2714160084724426\n",
      "epoch: 4 step: 848, loss is 0.06061798706650734\n",
      "epoch: 4 step: 849, loss is 0.059891607612371445\n",
      "epoch: 4 step: 850, loss is 0.00947227980941534\n",
      "epoch: 4 step: 851, loss is 0.005602499004453421\n",
      "epoch: 4 step: 852, loss is 0.0895102322101593\n",
      "epoch: 4 step: 853, loss is 0.1322704553604126\n",
      "epoch: 4 step: 854, loss is 0.1426246166229248\n",
      "epoch: 4 step: 855, loss is 0.0461505763232708\n",
      "epoch: 4 step: 856, loss is 0.2587279975414276\n",
      "epoch: 4 step: 857, loss is 0.0006471347296610475\n",
      "epoch: 4 step: 858, loss is 0.04708678647875786\n",
      "epoch: 4 step: 859, loss is 0.005931808613240719\n",
      "epoch: 4 step: 860, loss is 0.006700499914586544\n",
      "epoch: 4 step: 861, loss is 0.0062265899032354355\n",
      "epoch: 4 step: 862, loss is 0.0033294130116701126\n",
      "epoch: 4 step: 863, loss is 0.06902572512626648\n",
      "epoch: 4 step: 864, loss is 0.05981551855802536\n",
      "epoch: 4 step: 865, loss is 0.009863554500043392\n",
      "epoch: 4 step: 866, loss is 0.008579179644584656\n",
      "epoch: 4 step: 867, loss is 0.006089289207011461\n",
      "epoch: 4 step: 868, loss is 0.04858657345175743\n",
      "epoch: 4 step: 869, loss is 0.028706850484013557\n",
      "epoch: 4 step: 870, loss is 0.0030137731228023767\n",
      "epoch: 4 step: 871, loss is 0.041004352271556854\n",
      "epoch: 4 step: 872, loss is 0.005510951858013868\n",
      "epoch: 4 step: 873, loss is 0.1458507478237152\n",
      "epoch: 4 step: 874, loss is 0.04057583957910538\n",
      "epoch: 4 step: 875, loss is 0.01166281383484602\n",
      "epoch: 4 step: 876, loss is 0.0004393731942400336\n",
      "epoch: 4 step: 877, loss is 0.0003079565067309886\n",
      "epoch: 4 step: 878, loss is 0.021150600165128708\n",
      "epoch: 4 step: 879, loss is 0.0007329686195589602\n",
      "epoch: 4 step: 880, loss is 0.008903684094548225\n",
      "epoch: 4 step: 881, loss is 0.005244506057351828\n",
      "epoch: 4 step: 882, loss is 0.022814927622675896\n",
      "epoch: 4 step: 883, loss is 0.021994447335600853\n",
      "epoch: 4 step: 884, loss is 0.006874663755297661\n",
      "epoch: 4 step: 885, loss is 0.04574483633041382\n",
      "epoch: 4 step: 886, loss is 0.00913409236818552\n",
      "epoch: 4 step: 887, loss is 0.06571248173713684\n",
      "epoch: 4 step: 888, loss is 0.19108107686042786\n",
      "epoch: 4 step: 889, loss is 0.0075621153227984905\n",
      "epoch: 4 step: 890, loss is 0.0028275512158870697\n",
      "epoch: 4 step: 891, loss is 0.0014393202727660537\n",
      "epoch: 4 step: 892, loss is 0.011395898647606373\n",
      "epoch: 4 step: 893, loss is 0.0018231795402243733\n",
      "epoch: 4 step: 894, loss is 0.004191682208329439\n",
      "epoch: 4 step: 895, loss is 0.02434939332306385\n",
      "epoch: 4 step: 896, loss is 0.16463229060173035\n",
      "epoch: 4 step: 897, loss is 0.23395368456840515\n",
      "epoch: 4 step: 898, loss is 0.07398593425750732\n",
      "epoch: 4 step: 899, loss is 0.0034392988309264183\n",
      "epoch: 4 step: 900, loss is 0.10452771186828613\n",
      "epoch: 4 step: 901, loss is 0.008828382939100266\n",
      "epoch: 4 step: 902, loss is 0.004056483507156372\n",
      "epoch: 4 step: 903, loss is 0.007509499788284302\n",
      "epoch: 4 step: 904, loss is 0.014569015242159367\n",
      "epoch: 4 step: 905, loss is 0.013232204131782055\n",
      "epoch: 4 step: 906, loss is 0.021172793582081795\n",
      "epoch: 4 step: 907, loss is 0.028941377997398376\n",
      "epoch: 4 step: 908, loss is 0.006216727662831545\n",
      "epoch: 4 step: 909, loss is 0.04502423480153084\n",
      "epoch: 4 step: 910, loss is 0.0066327485255897045\n",
      "epoch: 4 step: 911, loss is 0.006628368049860001\n",
      "epoch: 4 step: 912, loss is 0.0018070831429213285\n",
      "epoch: 4 step: 913, loss is 0.03653842583298683\n",
      "epoch: 4 step: 914, loss is 0.18010519444942474\n",
      "epoch: 4 step: 915, loss is 0.05724218860268593\n",
      "epoch: 4 step: 916, loss is 0.007262732367962599\n",
      "epoch: 4 step: 917, loss is 0.042447201907634735\n",
      "epoch: 4 step: 918, loss is 0.0028564981184899807\n",
      "epoch: 4 step: 919, loss is 0.1297338604927063\n",
      "epoch: 4 step: 920, loss is 0.2686966359615326\n",
      "epoch: 4 step: 921, loss is 0.013376383110880852\n",
      "epoch: 4 step: 922, loss is 0.03945649415254593\n",
      "epoch: 4 step: 923, loss is 0.012297488749027252\n",
      "epoch: 4 step: 924, loss is 0.02690841816365719\n",
      "epoch: 4 step: 925, loss is 0.003226079512387514\n",
      "epoch: 4 step: 926, loss is 0.00975020881742239\n",
      "epoch: 4 step: 927, loss is 0.002454897854477167\n",
      "epoch: 4 step: 928, loss is 0.0016280192648991942\n",
      "epoch: 4 step: 929, loss is 0.029177619144320488\n",
      "epoch: 4 step: 930, loss is 0.0022669138852506876\n",
      "epoch: 4 step: 931, loss is 0.025761401280760765\n",
      "epoch: 4 step: 932, loss is 0.022185832262039185\n",
      "epoch: 4 step: 933, loss is 0.023169372230768204\n",
      "epoch: 4 step: 934, loss is 0.005541169084608555\n",
      "epoch: 4 step: 935, loss is 0.0049978080205619335\n",
      "epoch: 4 step: 936, loss is 0.0354052409529686\n",
      "epoch: 4 step: 937, loss is 0.03516155481338501\n",
      "epoch: 4 step: 938, loss is 0.03970415145158768\n",
      "epoch: 4 step: 939, loss is 0.003545437939465046\n",
      "epoch: 4 step: 940, loss is 0.06104244291782379\n",
      "epoch: 4 step: 941, loss is 0.028051523491740227\n",
      "epoch: 4 step: 942, loss is 0.04612929746508598\n",
      "epoch: 4 step: 943, loss is 0.058264780789613724\n",
      "epoch: 4 step: 944, loss is 0.07777681201696396\n",
      "epoch: 4 step: 945, loss is 0.0005564736784435809\n",
      "epoch: 4 step: 946, loss is 0.0067869448103010654\n",
      "epoch: 4 step: 947, loss is 0.0936691164970398\n",
      "epoch: 4 step: 948, loss is 0.00131236482411623\n",
      "epoch: 4 step: 949, loss is 0.24227489531040192\n",
      "epoch: 4 step: 950, loss is 0.00035416657919995487\n",
      "epoch: 4 step: 951, loss is 0.0058658309280872345\n",
      "epoch: 4 step: 952, loss is 0.24570170044898987\n",
      "epoch: 4 step: 953, loss is 0.12363136559724808\n",
      "epoch: 4 step: 954, loss is 0.01680987887084484\n",
      "epoch: 4 step: 955, loss is 0.008561139926314354\n",
      "epoch: 4 step: 956, loss is 0.0019513374427333474\n",
      "epoch: 4 step: 957, loss is 0.08748466521501541\n",
      "epoch: 4 step: 958, loss is 0.07294470816850662\n",
      "epoch: 4 step: 959, loss is 0.007075436878949404\n",
      "epoch: 4 step: 960, loss is 0.04938298091292381\n",
      "epoch: 4 step: 961, loss is 0.054945044219493866\n",
      "epoch: 4 step: 962, loss is 0.004516360815614462\n",
      "epoch: 4 step: 963, loss is 0.001620967173948884\n",
      "epoch: 4 step: 964, loss is 0.0009323613485321403\n",
      "epoch: 4 step: 965, loss is 0.009480088017880917\n",
      "epoch: 4 step: 966, loss is 0.03936396911740303\n",
      "epoch: 4 step: 967, loss is 0.11126939207315445\n",
      "epoch: 4 step: 968, loss is 0.007713218219578266\n",
      "epoch: 4 step: 969, loss is 0.012805239297449589\n",
      "epoch: 4 step: 970, loss is 0.015410155989229679\n",
      "epoch: 4 step: 971, loss is 0.004901805426925421\n",
      "epoch: 4 step: 972, loss is 0.005988031160086393\n",
      "epoch: 4 step: 973, loss is 0.018083760514855385\n",
      "epoch: 4 step: 974, loss is 0.019822992384433746\n",
      "epoch: 4 step: 975, loss is 0.02193959429860115\n",
      "epoch: 4 step: 976, loss is 0.001242713420651853\n",
      "epoch: 4 step: 977, loss is 0.012923586182296276\n",
      "epoch: 4 step: 978, loss is 0.08685776591300964\n",
      "epoch: 4 step: 979, loss is 0.025116879492998123\n",
      "epoch: 4 step: 980, loss is 0.23190583288669586\n",
      "epoch: 4 step: 981, loss is 0.012736763805150986\n",
      "epoch: 4 step: 982, loss is 0.0007421803893521428\n",
      "epoch: 4 step: 983, loss is 0.0007742240559309721\n",
      "epoch: 4 step: 984, loss is 0.14392074942588806\n",
      "epoch: 4 step: 985, loss is 0.007845582440495491\n",
      "epoch: 4 step: 986, loss is 0.06737402826547623\n",
      "epoch: 4 step: 987, loss is 0.02554936707019806\n",
      "epoch: 4 step: 988, loss is 0.0003794016956817359\n",
      "epoch: 4 step: 989, loss is 0.02083422802388668\n",
      "epoch: 4 step: 990, loss is 0.06384597718715668\n",
      "epoch: 4 step: 991, loss is 0.008296341635286808\n",
      "epoch: 4 step: 992, loss is 0.034349117428064346\n",
      "epoch: 4 step: 993, loss is 0.12184196710586548\n",
      "epoch: 4 step: 994, loss is 0.03379271179437637\n",
      "epoch: 4 step: 995, loss is 0.003171605756506324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 996, loss is 0.00037630347651429474\n",
      "epoch: 4 step: 997, loss is 0.01806575432419777\n",
      "epoch: 4 step: 998, loss is 0.004239814355969429\n",
      "epoch: 4 step: 999, loss is 0.007025907281786203\n",
      "epoch: 4 step: 1000, loss is 0.020364316180348396\n",
      "epoch: 4 step: 1001, loss is 0.013064137659966946\n",
      "epoch: 4 step: 1002, loss is 0.002091325121000409\n",
      "epoch: 4 step: 1003, loss is 0.04118282347917557\n",
      "epoch: 4 step: 1004, loss is 0.047219932079315186\n",
      "epoch: 4 step: 1005, loss is 0.0355449914932251\n",
      "epoch: 4 step: 1006, loss is 0.16448378562927246\n",
      "epoch: 4 step: 1007, loss is 0.006638612598180771\n",
      "epoch: 4 step: 1008, loss is 0.01058273483067751\n",
      "epoch: 4 step: 1009, loss is 0.061637286096811295\n",
      "epoch: 4 step: 1010, loss is 0.02009451389312744\n",
      "epoch: 4 step: 1011, loss is 0.04530620574951172\n",
      "epoch: 4 step: 1012, loss is 0.0038695528637617826\n",
      "epoch: 4 step: 1013, loss is 0.009149436838924885\n",
      "epoch: 4 step: 1014, loss is 0.04196906089782715\n",
      "epoch: 4 step: 1015, loss is 0.009591237641870975\n",
      "epoch: 4 step: 1016, loss is 0.004535925108939409\n",
      "epoch: 4 step: 1017, loss is 0.002792675280943513\n",
      "epoch: 4 step: 1018, loss is 0.00968067068606615\n",
      "epoch: 4 step: 1019, loss is 0.08302676677703857\n",
      "epoch: 4 step: 1020, loss is 0.011573286727070808\n",
      "epoch: 4 step: 1021, loss is 0.0049550291150808334\n",
      "epoch: 4 step: 1022, loss is 0.002578957471996546\n",
      "epoch: 4 step: 1023, loss is 0.0033280171919614077\n",
      "epoch: 4 step: 1024, loss is 0.020281832665205002\n",
      "epoch: 4 step: 1025, loss is 0.012493685819208622\n",
      "epoch: 4 step: 1026, loss is 0.09701092541217804\n",
      "epoch: 4 step: 1027, loss is 0.021865759044885635\n",
      "epoch: 4 step: 1028, loss is 0.08634678274393082\n",
      "epoch: 4 step: 1029, loss is 0.22646640241146088\n",
      "epoch: 4 step: 1030, loss is 0.17857012152671814\n",
      "epoch: 4 step: 1031, loss is 0.01570139266550541\n",
      "epoch: 4 step: 1032, loss is 0.04967141151428223\n",
      "epoch: 4 step: 1033, loss is 0.025929734110832214\n",
      "epoch: 4 step: 1034, loss is 0.00028967985417693853\n",
      "epoch: 4 step: 1035, loss is 0.0008061805856414139\n",
      "epoch: 4 step: 1036, loss is 0.0008515722583979368\n",
      "epoch: 4 step: 1037, loss is 0.021565314382314682\n",
      "epoch: 4 step: 1038, loss is 0.032963477075099945\n",
      "epoch: 4 step: 1039, loss is 0.006314561236649752\n",
      "epoch: 4 step: 1040, loss is 0.014984113164246082\n",
      "epoch: 4 step: 1041, loss is 0.08660336583852768\n",
      "epoch: 4 step: 1042, loss is 0.001334435073658824\n",
      "epoch: 4 step: 1043, loss is 0.018380217254161835\n",
      "epoch: 4 step: 1044, loss is 0.0011739721521735191\n",
      "epoch: 4 step: 1045, loss is 0.04847072809934616\n",
      "epoch: 4 step: 1046, loss is 0.0035122998524457216\n",
      "epoch: 4 step: 1047, loss is 0.004343688488006592\n",
      "epoch: 4 step: 1048, loss is 0.0047391923144459724\n",
      "epoch: 4 step: 1049, loss is 0.010148798115551472\n",
      "epoch: 4 step: 1050, loss is 0.0020572722423821688\n",
      "epoch: 4 step: 1051, loss is 0.006187111604958773\n",
      "epoch: 4 step: 1052, loss is 0.011431793682277203\n",
      "epoch: 4 step: 1053, loss is 0.12923063337802887\n",
      "epoch: 4 step: 1054, loss is 0.00015653279842808843\n",
      "epoch: 4 step: 1055, loss is 0.06014449521899223\n",
      "epoch: 4 step: 1056, loss is 0.0009603899670764804\n",
      "epoch: 4 step: 1057, loss is 0.001815789146348834\n",
      "epoch: 4 step: 1058, loss is 0.0019876619335263968\n",
      "epoch: 4 step: 1059, loss is 0.0004772570391651243\n",
      "epoch: 4 step: 1060, loss is 0.0006761636468581855\n",
      "epoch: 4 step: 1061, loss is 0.007885183207690716\n",
      "epoch: 4 step: 1062, loss is 0.004910336807370186\n",
      "epoch: 4 step: 1063, loss is 0.012887697666883469\n",
      "epoch: 4 step: 1064, loss is 0.03660605475306511\n",
      "epoch: 4 step: 1065, loss is 0.0416899099946022\n",
      "epoch: 4 step: 1066, loss is 0.01431391853839159\n",
      "epoch: 4 step: 1067, loss is 0.12178681045770645\n",
      "epoch: 4 step: 1068, loss is 0.0008125650347210467\n",
      "epoch: 4 step: 1069, loss is 0.2372424155473709\n",
      "epoch: 4 step: 1070, loss is 0.009566856548190117\n",
      "epoch: 4 step: 1071, loss is 0.06819659471511841\n",
      "epoch: 4 step: 1072, loss is 0.1417146772146225\n",
      "epoch: 4 step: 1073, loss is 0.012348118238151073\n",
      "epoch: 4 step: 1074, loss is 0.1424475461244583\n",
      "epoch: 4 step: 1075, loss is 0.010528872720897198\n",
      "epoch: 4 step: 1076, loss is 0.005906976759433746\n",
      "epoch: 4 step: 1077, loss is 0.03018208220601082\n",
      "epoch: 4 step: 1078, loss is 0.06412943452596664\n",
      "epoch: 4 step: 1079, loss is 0.027934661135077477\n",
      "epoch: 4 step: 1080, loss is 0.02403545007109642\n",
      "epoch: 4 step: 1081, loss is 0.012054883874952793\n",
      "epoch: 4 step: 1082, loss is 0.00031005515484139323\n",
      "epoch: 4 step: 1083, loss is 0.0007905896636657417\n",
      "epoch: 4 step: 1084, loss is 0.0021734903566539288\n",
      "epoch: 4 step: 1085, loss is 0.00030657523893751204\n",
      "epoch: 4 step: 1086, loss is 0.02294141799211502\n",
      "epoch: 4 step: 1087, loss is 0.011253190226852894\n",
      "epoch: 4 step: 1088, loss is 0.11485068500041962\n",
      "epoch: 4 step: 1089, loss is 0.013616379350423813\n",
      "epoch: 4 step: 1090, loss is 0.03646664693951607\n",
      "epoch: 4 step: 1091, loss is 0.0006982150953263044\n",
      "epoch: 4 step: 1092, loss is 0.004929688293486834\n",
      "epoch: 4 step: 1093, loss is 0.0326327420771122\n",
      "epoch: 4 step: 1094, loss is 0.04786606878042221\n",
      "epoch: 4 step: 1095, loss is 0.1367063820362091\n",
      "epoch: 4 step: 1096, loss is 0.0016834086272865534\n",
      "epoch: 4 step: 1097, loss is 0.03208032250404358\n",
      "epoch: 4 step: 1098, loss is 0.0010264202719554305\n",
      "epoch: 4 step: 1099, loss is 0.002029591705650091\n",
      "epoch: 4 step: 1100, loss is 0.00046061069588176906\n",
      "epoch: 4 step: 1101, loss is 0.10992173105478287\n",
      "epoch: 4 step: 1102, loss is 0.04244738072156906\n",
      "epoch: 4 step: 1103, loss is 0.0007593353511765599\n",
      "epoch: 4 step: 1104, loss is 0.03476143255829811\n",
      "epoch: 4 step: 1105, loss is 0.03826897591352463\n",
      "epoch: 4 step: 1106, loss is 0.0008558750851079822\n",
      "epoch: 4 step: 1107, loss is 0.005204429849982262\n",
      "epoch: 4 step: 1108, loss is 0.02206394262611866\n",
      "epoch: 4 step: 1109, loss is 0.0035440281499177217\n",
      "epoch: 4 step: 1110, loss is 0.0025984891690313816\n",
      "epoch: 4 step: 1111, loss is 0.00556499557569623\n",
      "epoch: 4 step: 1112, loss is 0.022588983178138733\n",
      "epoch: 4 step: 1113, loss is 0.02331201359629631\n",
      "epoch: 4 step: 1114, loss is 0.01606447994709015\n",
      "epoch: 4 step: 1115, loss is 0.0007278176490217447\n",
      "epoch: 4 step: 1116, loss is 0.008060385473072529\n",
      "epoch: 4 step: 1117, loss is 0.007997504435479641\n",
      "epoch: 4 step: 1118, loss is 0.09384921938180923\n",
      "epoch: 4 step: 1119, loss is 0.0022343057207763195\n",
      "epoch: 4 step: 1120, loss is 0.03438776358962059\n",
      "epoch: 4 step: 1121, loss is 0.0003990650293417275\n",
      "epoch: 4 step: 1122, loss is 0.2200806438922882\n",
      "epoch: 4 step: 1123, loss is 0.01601487025618553\n",
      "epoch: 4 step: 1124, loss is 0.007898311130702496\n",
      "epoch: 4 step: 1125, loss is 0.012351755052804947\n",
      "epoch: 4 step: 1126, loss is 0.008765487931668758\n",
      "epoch: 4 step: 1127, loss is 0.13528726994991302\n",
      "epoch: 4 step: 1128, loss is 0.012156695127487183\n",
      "epoch: 4 step: 1129, loss is 0.014672412537038326\n",
      "epoch: 4 step: 1130, loss is 0.4671250581741333\n",
      "epoch: 4 step: 1131, loss is 0.06657113134860992\n",
      "epoch: 4 step: 1132, loss is 0.005135691724717617\n",
      "epoch: 4 step: 1133, loss is 0.0037384494207799435\n",
      "epoch: 4 step: 1134, loss is 0.002578836865723133\n",
      "epoch: 4 step: 1135, loss is 0.012710202485322952\n",
      "epoch: 4 step: 1136, loss is 0.0010733857052400708\n",
      "epoch: 4 step: 1137, loss is 0.07967716455459595\n",
      "epoch: 4 step: 1138, loss is 0.035260677337646484\n",
      "epoch: 4 step: 1139, loss is 0.010658372193574905\n",
      "epoch: 4 step: 1140, loss is 0.00030716348555870354\n",
      "epoch: 4 step: 1141, loss is 0.009843138977885246\n",
      "epoch: 4 step: 1142, loss is 0.06679169833660126\n",
      "epoch: 4 step: 1143, loss is 0.0831107422709465\n",
      "epoch: 4 step: 1144, loss is 0.0009682482341304421\n",
      "epoch: 4 step: 1145, loss is 0.017381178215146065\n",
      "epoch: 4 step: 1146, loss is 0.0027408935129642487\n",
      "epoch: 4 step: 1147, loss is 0.028895055875182152\n",
      "epoch: 4 step: 1148, loss is 0.13492465019226074\n",
      "epoch: 4 step: 1149, loss is 0.0010321690933778882\n",
      "epoch: 4 step: 1150, loss is 0.03678438812494278\n",
      "epoch: 4 step: 1151, loss is 0.009012333117425442\n",
      "epoch: 4 step: 1152, loss is 0.0018064528703689575\n",
      "epoch: 4 step: 1153, loss is 0.005135732237249613\n",
      "epoch: 4 step: 1154, loss is 0.0020631481893360615\n",
      "epoch: 4 step: 1155, loss is 0.13811810314655304\n",
      "epoch: 4 step: 1156, loss is 0.01943236216902733\n",
      "epoch: 4 step: 1157, loss is 0.03212486580014229\n",
      "epoch: 4 step: 1158, loss is 0.0026824793312698603\n",
      "epoch: 4 step: 1159, loss is 0.0007907578255981207\n",
      "epoch: 4 step: 1160, loss is 0.0011960223782807589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 1161, loss is 0.04188224673271179\n",
      "epoch: 4 step: 1162, loss is 0.0013844312634319067\n",
      "epoch: 4 step: 1163, loss is 0.17451155185699463\n",
      "epoch: 4 step: 1164, loss is 0.0014810520224273205\n",
      "epoch: 4 step: 1165, loss is 0.008665423840284348\n",
      "epoch: 4 step: 1166, loss is 0.047966163605451584\n",
      "epoch: 4 step: 1167, loss is 0.0012920689769089222\n",
      "epoch: 4 step: 1168, loss is 0.004473067820072174\n",
      "epoch: 4 step: 1169, loss is 0.15541917085647583\n",
      "epoch: 4 step: 1170, loss is 0.048989735543727875\n",
      "epoch: 4 step: 1171, loss is 0.03732892498373985\n",
      "epoch: 4 step: 1172, loss is 0.07764159888029099\n",
      "epoch: 4 step: 1173, loss is 0.030280759558081627\n",
      "epoch: 4 step: 1174, loss is 0.02762811817228794\n",
      "epoch: 4 step: 1175, loss is 0.1815388798713684\n",
      "epoch: 4 step: 1176, loss is 0.04411732405424118\n",
      "epoch: 4 step: 1177, loss is 0.0021223039366304874\n",
      "epoch: 4 step: 1178, loss is 0.03158334642648697\n",
      "epoch: 4 step: 1179, loss is 0.03611871227622032\n",
      "epoch: 4 step: 1180, loss is 0.0009988642996177077\n",
      "epoch: 4 step: 1181, loss is 0.00402512401342392\n",
      "epoch: 4 step: 1182, loss is 0.0005202110623940825\n",
      "epoch: 4 step: 1183, loss is 0.0228444654494524\n",
      "epoch: 4 step: 1184, loss is 0.03784270957112312\n",
      "epoch: 4 step: 1185, loss is 0.01089340541511774\n",
      "epoch: 4 step: 1186, loss is 0.001494026742875576\n",
      "epoch: 4 step: 1187, loss is 0.014487219974398613\n",
      "epoch: 4 step: 1188, loss is 0.028310969471931458\n",
      "epoch: 4 step: 1189, loss is 0.10323631763458252\n",
      "epoch: 4 step: 1190, loss is 0.06135805696249008\n",
      "epoch: 4 step: 1191, loss is 0.12584014236927032\n",
      "epoch: 4 step: 1192, loss is 0.015047465451061726\n",
      "epoch: 4 step: 1193, loss is 0.003970836289227009\n",
      "epoch: 4 step: 1194, loss is 0.12029226124286652\n",
      "epoch: 4 step: 1195, loss is 0.18651676177978516\n",
      "epoch: 4 step: 1196, loss is 0.032906707376241684\n",
      "epoch: 4 step: 1197, loss is 0.03487806394696236\n",
      "epoch: 4 step: 1198, loss is 0.0014631171943619847\n",
      "epoch: 4 step: 1199, loss is 0.0033715185709297657\n",
      "epoch: 4 step: 1200, loss is 0.06270118802785873\n",
      "epoch: 4 step: 1201, loss is 0.01903028041124344\n",
      "epoch: 4 step: 1202, loss is 0.16073182225227356\n",
      "epoch: 4 step: 1203, loss is 0.09849841147661209\n",
      "epoch: 4 step: 1204, loss is 0.020481755957007408\n",
      "epoch: 4 step: 1205, loss is 0.0011368254199624062\n",
      "epoch: 4 step: 1206, loss is 0.03185734525322914\n",
      "epoch: 4 step: 1207, loss is 0.005312446504831314\n",
      "epoch: 4 step: 1208, loss is 0.003115834202617407\n",
      "epoch: 4 step: 1209, loss is 0.008030850440263748\n",
      "epoch: 4 step: 1210, loss is 0.0007276306278072298\n",
      "epoch: 4 step: 1211, loss is 0.00233326549641788\n",
      "epoch: 4 step: 1212, loss is 0.06715890020132065\n",
      "epoch: 4 step: 1213, loss is 0.018714549019932747\n",
      "epoch: 4 step: 1214, loss is 0.06293057650327682\n",
      "epoch: 4 step: 1215, loss is 0.030260125175118446\n",
      "epoch: 4 step: 1216, loss is 0.00099121720995754\n",
      "epoch: 4 step: 1217, loss is 0.0010720511199906468\n",
      "epoch: 4 step: 1218, loss is 0.14411790668964386\n",
      "epoch: 4 step: 1219, loss is 0.12057165801525116\n",
      "epoch: 4 step: 1220, loss is 0.059872813522815704\n",
      "epoch: 4 step: 1221, loss is 0.0014309233520179987\n",
      "epoch: 4 step: 1222, loss is 0.009923763573169708\n",
      "epoch: 4 step: 1223, loss is 0.012762712314724922\n",
      "epoch: 4 step: 1224, loss is 0.08836209028959274\n",
      "epoch: 4 step: 1225, loss is 0.006777849514037371\n",
      "epoch: 4 step: 1226, loss is 0.01619173027575016\n",
      "epoch: 4 step: 1227, loss is 0.004541461355984211\n",
      "epoch: 4 step: 1228, loss is 0.00031718844547867775\n",
      "epoch: 4 step: 1229, loss is 0.02124055102467537\n",
      "epoch: 4 step: 1230, loss is 0.02578643709421158\n",
      "epoch: 4 step: 1231, loss is 0.0011820525396615267\n",
      "epoch: 4 step: 1232, loss is 0.008357160724699497\n",
      "epoch: 4 step: 1233, loss is 0.05476674810051918\n",
      "epoch: 4 step: 1234, loss is 0.05324643477797508\n",
      "epoch: 4 step: 1235, loss is 0.006591159850358963\n",
      "epoch: 4 step: 1236, loss is 0.017214061692357063\n",
      "epoch: 4 step: 1237, loss is 0.06600647419691086\n",
      "epoch: 4 step: 1238, loss is 0.005508047994226217\n",
      "epoch: 4 step: 1239, loss is 0.003060037735849619\n",
      "epoch: 4 step: 1240, loss is 0.008201994001865387\n",
      "epoch: 4 step: 1241, loss is 0.07238458842039108\n",
      "epoch: 4 step: 1242, loss is 0.0011187982745468616\n",
      "epoch: 4 step: 1243, loss is 0.002863225992769003\n",
      "epoch: 4 step: 1244, loss is 0.0639367625117302\n",
      "epoch: 4 step: 1245, loss is 0.0022253249771893024\n",
      "epoch: 4 step: 1246, loss is 0.01920543611049652\n",
      "epoch: 4 step: 1247, loss is 0.01537561509758234\n",
      "epoch: 4 step: 1248, loss is 0.0005916763911955059\n",
      "epoch: 4 step: 1249, loss is 0.007214135956019163\n",
      "epoch: 4 step: 1250, loss is 0.005604286212474108\n",
      "epoch: 4 step: 1251, loss is 0.0035143233835697174\n",
      "epoch: 4 step: 1252, loss is 0.003149518510326743\n",
      "epoch: 4 step: 1253, loss is 0.12190024554729462\n",
      "epoch: 4 step: 1254, loss is 0.07939274609088898\n",
      "epoch: 4 step: 1255, loss is 0.003046538680791855\n",
      "epoch: 4 step: 1256, loss is 0.004326162859797478\n",
      "epoch: 4 step: 1257, loss is 0.32447734475135803\n",
      "epoch: 4 step: 1258, loss is 0.0026018100325018167\n",
      "epoch: 4 step: 1259, loss is 0.1005224660038948\n",
      "epoch: 4 step: 1260, loss is 0.001203593099489808\n",
      "epoch: 4 step: 1261, loss is 0.024907667189836502\n",
      "epoch: 4 step: 1262, loss is 0.007643293123692274\n",
      "epoch: 4 step: 1263, loss is 0.00387673475779593\n",
      "epoch: 4 step: 1264, loss is 0.11888149380683899\n",
      "epoch: 4 step: 1265, loss is 0.06675229221582413\n",
      "epoch: 4 step: 1266, loss is 0.0007905370439402759\n",
      "epoch: 4 step: 1267, loss is 0.013220568187534809\n",
      "epoch: 4 step: 1268, loss is 0.0038493352476507425\n",
      "epoch: 4 step: 1269, loss is 0.01698056235909462\n",
      "epoch: 4 step: 1270, loss is 0.008866027928888798\n",
      "epoch: 4 step: 1271, loss is 0.14577333629131317\n",
      "epoch: 4 step: 1272, loss is 0.05120333284139633\n",
      "epoch: 4 step: 1273, loss is 0.1543421745300293\n",
      "epoch: 4 step: 1274, loss is 0.0009320586686953902\n",
      "epoch: 4 step: 1275, loss is 0.002720852615311742\n",
      "epoch: 4 step: 1276, loss is 0.022852670401334763\n",
      "epoch: 4 step: 1277, loss is 0.006357918959110975\n",
      "epoch: 4 step: 1278, loss is 0.007092362269759178\n",
      "epoch: 4 step: 1279, loss is 0.007973544299602509\n",
      "epoch: 4 step: 1280, loss is 0.006013547070324421\n",
      "epoch: 4 step: 1281, loss is 0.09605386108160019\n",
      "epoch: 4 step: 1282, loss is 0.012553371489048004\n",
      "epoch: 4 step: 1283, loss is 0.01313074305653572\n",
      "epoch: 4 step: 1284, loss is 0.11164852976799011\n",
      "epoch: 4 step: 1285, loss is 0.019716555252671242\n",
      "epoch: 4 step: 1286, loss is 0.006264223717153072\n",
      "epoch: 4 step: 1287, loss is 0.005623305216431618\n",
      "epoch: 4 step: 1288, loss is 0.002370978007093072\n",
      "epoch: 4 step: 1289, loss is 0.0002679571625776589\n",
      "epoch: 4 step: 1290, loss is 0.02754666656255722\n",
      "epoch: 4 step: 1291, loss is 0.024265633895993233\n",
      "epoch: 4 step: 1292, loss is 0.010314648039638996\n",
      "epoch: 4 step: 1293, loss is 0.007474326528608799\n",
      "epoch: 4 step: 1294, loss is 0.02864324115216732\n",
      "epoch: 4 step: 1295, loss is 0.061840035021305084\n",
      "epoch: 4 step: 1296, loss is 0.004707247018814087\n",
      "epoch: 4 step: 1297, loss is 0.067591592669487\n",
      "epoch: 4 step: 1298, loss is 0.008530069142580032\n",
      "epoch: 4 step: 1299, loss is 0.001263759913854301\n",
      "epoch: 4 step: 1300, loss is 0.03586115315556526\n",
      "epoch: 4 step: 1301, loss is 0.017553003504872322\n",
      "epoch: 4 step: 1302, loss is 0.0013634462375193834\n",
      "epoch: 4 step: 1303, loss is 0.006418206263333559\n",
      "epoch: 4 step: 1304, loss is 0.0034760180860757828\n",
      "epoch: 4 step: 1305, loss is 0.006308280397206545\n",
      "epoch: 4 step: 1306, loss is 0.004296224098652601\n",
      "epoch: 4 step: 1307, loss is 0.2067573517560959\n",
      "epoch: 4 step: 1308, loss is 0.174149751663208\n",
      "epoch: 4 step: 1309, loss is 0.13833242654800415\n",
      "epoch: 4 step: 1310, loss is 0.010602190159261227\n",
      "epoch: 4 step: 1311, loss is 0.004401043988764286\n",
      "epoch: 4 step: 1312, loss is 0.013037717901170254\n",
      "epoch: 4 step: 1313, loss is 0.0017436310881748796\n",
      "epoch: 4 step: 1314, loss is 0.012075577862560749\n",
      "epoch: 4 step: 1315, loss is 0.0015886978944763541\n",
      "epoch: 4 step: 1316, loss is 0.08537834137678146\n",
      "epoch: 4 step: 1317, loss is 0.007993177510797977\n",
      "epoch: 4 step: 1318, loss is 0.0141943683847785\n",
      "epoch: 4 step: 1319, loss is 0.03662458434700966\n",
      "epoch: 4 step: 1320, loss is 0.04149002954363823\n",
      "epoch: 4 step: 1321, loss is 0.03897727280855179\n",
      "epoch: 4 step: 1322, loss is 0.12656967341899872\n",
      "epoch: 4 step: 1323, loss is 0.00026981320115737617\n",
      "epoch: 4 step: 1324, loss is 0.20335474610328674\n",
      "epoch: 4 step: 1325, loss is 0.01719312183558941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 1326, loss is 0.039417561143636703\n",
      "epoch: 4 step: 1327, loss is 0.0001569181913509965\n",
      "epoch: 4 step: 1328, loss is 0.0017189689679071307\n",
      "epoch: 4 step: 1329, loss is 0.10900619626045227\n",
      "epoch: 4 step: 1330, loss is 0.0007530613802373409\n",
      "epoch: 4 step: 1331, loss is 0.004372572060674429\n",
      "epoch: 4 step: 1332, loss is 0.06370512396097183\n",
      "epoch: 4 step: 1333, loss is 0.15992455184459686\n",
      "epoch: 4 step: 1334, loss is 0.03563816472887993\n",
      "epoch: 4 step: 1335, loss is 0.011209500953555107\n",
      "epoch: 4 step: 1336, loss is 0.0703616589307785\n",
      "epoch: 4 step: 1337, loss is 0.0021728172432631254\n",
      "epoch: 4 step: 1338, loss is 0.021014157682657242\n",
      "epoch: 4 step: 1339, loss is 0.004711231682449579\n",
      "epoch: 4 step: 1340, loss is 0.09903356432914734\n",
      "epoch: 4 step: 1341, loss is 0.03419603779911995\n",
      "epoch: 4 step: 1342, loss is 0.020314296707510948\n",
      "epoch: 4 step: 1343, loss is 0.009031981229782104\n",
      "epoch: 4 step: 1344, loss is 0.03557363152503967\n",
      "epoch: 4 step: 1345, loss is 0.007035430055111647\n",
      "epoch: 4 step: 1346, loss is 0.00946308858692646\n",
      "epoch: 4 step: 1347, loss is 0.02178197167813778\n",
      "epoch: 4 step: 1348, loss is 0.0011273714480921626\n",
      "epoch: 4 step: 1349, loss is 0.13167673349380493\n",
      "epoch: 4 step: 1350, loss is 0.00296210334636271\n",
      "epoch: 4 step: 1351, loss is 0.04979902133345604\n",
      "epoch: 4 step: 1352, loss is 0.14302273094654083\n",
      "epoch: 4 step: 1353, loss is 0.00039170734817162156\n",
      "epoch: 4 step: 1354, loss is 0.043664634227752686\n",
      "epoch: 4 step: 1355, loss is 0.0025047874078154564\n",
      "epoch: 4 step: 1356, loss is 0.0164774302393198\n",
      "epoch: 4 step: 1357, loss is 0.06278350949287415\n",
      "epoch: 4 step: 1358, loss is 0.0030744760297238827\n",
      "epoch: 4 step: 1359, loss is 0.0023486739955842495\n",
      "epoch: 4 step: 1360, loss is 0.10168370604515076\n",
      "epoch: 4 step: 1361, loss is 0.00998824741691351\n",
      "epoch: 4 step: 1362, loss is 0.004588022828102112\n",
      "epoch: 4 step: 1363, loss is 0.027271384373307228\n",
      "epoch: 4 step: 1364, loss is 0.005419278051704168\n",
      "epoch: 4 step: 1365, loss is 0.0019194387132301927\n",
      "epoch: 4 step: 1366, loss is 0.013440552167594433\n",
      "epoch: 4 step: 1367, loss is 0.11303019523620605\n",
      "epoch: 4 step: 1368, loss is 0.005350269377231598\n",
      "epoch: 4 step: 1369, loss is 0.048664893954992294\n",
      "epoch: 4 step: 1370, loss is 0.014549629762768745\n",
      "epoch: 4 step: 1371, loss is 0.24734194576740265\n",
      "epoch: 4 step: 1372, loss is 0.002974279224872589\n",
      "epoch: 4 step: 1373, loss is 0.002785824239253998\n",
      "epoch: 4 step: 1374, loss is 0.050765834748744965\n",
      "epoch: 4 step: 1375, loss is 0.001008946797810495\n",
      "epoch: 4 step: 1376, loss is 0.012185387313365936\n",
      "epoch: 4 step: 1377, loss is 0.0013793663820251822\n",
      "epoch: 4 step: 1378, loss is 0.11370404809713364\n",
      "epoch: 4 step: 1379, loss is 0.10086529701948166\n",
      "epoch: 4 step: 1380, loss is 0.007113310042768717\n",
      "epoch: 4 step: 1381, loss is 0.09011391550302505\n",
      "epoch: 4 step: 1382, loss is 0.00019711186178028584\n",
      "epoch: 4 step: 1383, loss is 0.07376141101121902\n",
      "epoch: 4 step: 1384, loss is 0.0017650768859311938\n",
      "epoch: 4 step: 1385, loss is 0.004091829061508179\n",
      "epoch: 4 step: 1386, loss is 0.027593057602643967\n",
      "epoch: 4 step: 1387, loss is 0.030966196209192276\n",
      "epoch: 4 step: 1388, loss is 0.13862337172031403\n",
      "epoch: 4 step: 1389, loss is 0.13935934007167816\n",
      "epoch: 4 step: 1390, loss is 0.0077841030433773994\n",
      "epoch: 4 step: 1391, loss is 0.004509505350142717\n",
      "epoch: 4 step: 1392, loss is 0.1445758193731308\n",
      "epoch: 4 step: 1393, loss is 0.010689949616789818\n",
      "epoch: 4 step: 1394, loss is 0.005588996224105358\n",
      "epoch: 4 step: 1395, loss is 0.003973931539803743\n",
      "epoch: 4 step: 1396, loss is 0.013420367613434792\n",
      "epoch: 4 step: 1397, loss is 0.3236621618270874\n",
      "epoch: 4 step: 1398, loss is 0.0017302690539509058\n",
      "epoch: 4 step: 1399, loss is 0.002114135306328535\n",
      "epoch: 4 step: 1400, loss is 0.03914608806371689\n",
      "epoch: 4 step: 1401, loss is 0.008818055503070354\n",
      "epoch: 4 step: 1402, loss is 0.015906641259789467\n",
      "epoch: 4 step: 1403, loss is 0.02029726654291153\n",
      "epoch: 4 step: 1404, loss is 0.058870598673820496\n",
      "epoch: 4 step: 1405, loss is 0.008008704520761967\n",
      "epoch: 4 step: 1406, loss is 0.29782798886299133\n",
      "epoch: 4 step: 1407, loss is 0.00222108606249094\n",
      "epoch: 4 step: 1408, loss is 0.007381509989500046\n",
      "epoch: 4 step: 1409, loss is 0.002325061708688736\n",
      "epoch: 4 step: 1410, loss is 0.11093403398990631\n",
      "epoch: 4 step: 1411, loss is 0.05839727446436882\n",
      "epoch: 4 step: 1412, loss is 0.1309337466955185\n",
      "epoch: 4 step: 1413, loss is 0.018633168190717697\n",
      "epoch: 4 step: 1414, loss is 0.01346709206700325\n",
      "epoch: 4 step: 1415, loss is 0.052258625626564026\n",
      "epoch: 4 step: 1416, loss is 0.008631831035017967\n",
      "epoch: 4 step: 1417, loss is 0.06906644254922867\n",
      "epoch: 4 step: 1418, loss is 0.08417615294456482\n",
      "epoch: 4 step: 1419, loss is 0.0009063938050530851\n",
      "epoch: 4 step: 1420, loss is 0.003048031823709607\n",
      "epoch: 4 step: 1421, loss is 0.009137318469583988\n",
      "epoch: 4 step: 1422, loss is 0.008291047997772694\n",
      "epoch: 4 step: 1423, loss is 0.01727566495537758\n",
      "epoch: 4 step: 1424, loss is 0.01539951004087925\n",
      "epoch: 4 step: 1425, loss is 0.027959974482655525\n",
      "epoch: 4 step: 1426, loss is 0.01677231676876545\n",
      "epoch: 4 step: 1427, loss is 0.0019408621592447162\n",
      "epoch: 4 step: 1428, loss is 0.019672993570566177\n",
      "epoch: 4 step: 1429, loss is 0.013840272091329098\n",
      "epoch: 4 step: 1430, loss is 0.03699331358075142\n",
      "epoch: 4 step: 1431, loss is 0.0012073884718120098\n",
      "epoch: 4 step: 1432, loss is 0.0026887068524956703\n",
      "epoch: 4 step: 1433, loss is 0.011104603298008442\n",
      "epoch: 4 step: 1434, loss is 0.0026289571542292833\n",
      "epoch: 4 step: 1435, loss is 0.022938808426260948\n",
      "epoch: 4 step: 1436, loss is 0.03557135909795761\n",
      "epoch: 4 step: 1437, loss is 0.02133471705019474\n",
      "epoch: 4 step: 1438, loss is 0.0009126634104177356\n",
      "epoch: 4 step: 1439, loss is 0.00633648969233036\n",
      "epoch: 4 step: 1440, loss is 0.176009863615036\n",
      "epoch: 4 step: 1441, loss is 0.00047374452697113156\n",
      "epoch: 4 step: 1442, loss is 0.0009492058889009058\n",
      "epoch: 4 step: 1443, loss is 0.002670415211468935\n",
      "epoch: 4 step: 1444, loss is 0.003827541833743453\n",
      "epoch: 4 step: 1445, loss is 0.02308819815516472\n",
      "epoch: 4 step: 1446, loss is 0.023385262116789818\n",
      "epoch: 4 step: 1447, loss is 0.0011087849270552397\n",
      "epoch: 4 step: 1448, loss is 0.01134585402905941\n",
      "epoch: 4 step: 1449, loss is 0.045707471668720245\n",
      "epoch: 4 step: 1450, loss is 0.01597224362194538\n",
      "epoch: 4 step: 1451, loss is 0.0008022711263038218\n",
      "epoch: 4 step: 1452, loss is 0.14689992368221283\n",
      "epoch: 4 step: 1453, loss is 0.1465723067522049\n",
      "epoch: 4 step: 1454, loss is 0.0015450853388756514\n",
      "epoch: 4 step: 1455, loss is 0.0037093160208314657\n",
      "epoch: 4 step: 1456, loss is 0.0127788744866848\n",
      "epoch: 4 step: 1457, loss is 0.002743609482422471\n",
      "epoch: 4 step: 1458, loss is 0.04933370649814606\n",
      "epoch: 4 step: 1459, loss is 0.018494537100195885\n",
      "epoch: 4 step: 1460, loss is 0.003771840361878276\n",
      "epoch: 4 step: 1461, loss is 0.03114999644458294\n",
      "epoch: 4 step: 1462, loss is 0.011494269594550133\n",
      "epoch: 4 step: 1463, loss is 0.019074352458119392\n",
      "epoch: 4 step: 1464, loss is 0.2669612467288971\n",
      "epoch: 4 step: 1465, loss is 0.03781507536768913\n",
      "epoch: 4 step: 1466, loss is 0.0837055891752243\n",
      "epoch: 4 step: 1467, loss is 0.04369674623012543\n",
      "epoch: 4 step: 1468, loss is 0.057051464915275574\n",
      "epoch: 4 step: 1469, loss is 0.02821454219520092\n",
      "epoch: 4 step: 1470, loss is 0.028828801587224007\n",
      "epoch: 4 step: 1471, loss is 0.2965850830078125\n",
      "epoch: 4 step: 1472, loss is 0.05312112346291542\n",
      "epoch: 4 step: 1473, loss is 0.01768343336880207\n",
      "epoch: 4 step: 1474, loss is 0.008139525540173054\n",
      "epoch: 4 step: 1475, loss is 0.005911779589951038\n",
      "epoch: 4 step: 1476, loss is 0.004616752732545137\n",
      "epoch: 4 step: 1477, loss is 0.0016672216588631272\n",
      "epoch: 4 step: 1478, loss is 0.006969484034925699\n",
      "epoch: 4 step: 1479, loss is 0.0039034115616232157\n",
      "epoch: 4 step: 1480, loss is 0.06021743267774582\n",
      "epoch: 4 step: 1481, loss is 0.014078116044402122\n",
      "epoch: 4 step: 1482, loss is 0.037139177322387695\n",
      "epoch: 4 step: 1483, loss is 0.0032563405111432076\n",
      "epoch: 4 step: 1484, loss is 0.035224445164203644\n",
      "epoch: 4 step: 1485, loss is 0.17400223016738892\n",
      "epoch: 4 step: 1486, loss is 0.0018077429849654436\n",
      "epoch: 4 step: 1487, loss is 0.018504442647099495\n",
      "epoch: 4 step: 1488, loss is 0.1333448439836502\n",
      "epoch: 4 step: 1489, loss is 0.005051184445619583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 1490, loss is 0.0024688139092177153\n",
      "epoch: 4 step: 1491, loss is 0.008644983172416687\n",
      "epoch: 4 step: 1492, loss is 0.1111002117395401\n",
      "epoch: 4 step: 1493, loss is 0.01337373349815607\n",
      "epoch: 4 step: 1494, loss is 0.005018097348511219\n",
      "epoch: 4 step: 1495, loss is 0.0234274510294199\n",
      "epoch: 4 step: 1496, loss is 0.09441434592008591\n",
      "epoch: 4 step: 1497, loss is 0.008835302665829659\n",
      "epoch: 4 step: 1498, loss is 0.031485527753829956\n",
      "epoch: 4 step: 1499, loss is 0.004988670349121094\n",
      "epoch: 4 step: 1500, loss is 0.01950041577219963\n",
      "epoch: 4 step: 1501, loss is 0.04975701868534088\n",
      "epoch: 4 step: 1502, loss is 0.001275205402635038\n",
      "epoch: 4 step: 1503, loss is 0.0037397327832877636\n",
      "epoch: 4 step: 1504, loss is 0.02232014387845993\n",
      "epoch: 4 step: 1505, loss is 0.16748614609241486\n",
      "epoch: 4 step: 1506, loss is 0.015779925510287285\n",
      "epoch: 4 step: 1507, loss is 0.0028440554160624743\n",
      "epoch: 4 step: 1508, loss is 0.010851215571165085\n",
      "epoch: 4 step: 1509, loss is 0.07699103653430939\n",
      "epoch: 4 step: 1510, loss is 0.05166596919298172\n",
      "epoch: 4 step: 1511, loss is 0.02631138078868389\n",
      "epoch: 4 step: 1512, loss is 0.0023552693892270327\n",
      "epoch: 4 step: 1513, loss is 0.05233163386583328\n",
      "epoch: 4 step: 1514, loss is 0.012937921099364758\n",
      "epoch: 4 step: 1515, loss is 0.025992048904299736\n",
      "epoch: 4 step: 1516, loss is 0.012878523208200932\n",
      "epoch: 4 step: 1517, loss is 0.001673906808719039\n",
      "epoch: 4 step: 1518, loss is 0.032511156052351\n",
      "epoch: 4 step: 1519, loss is 0.013806561939418316\n",
      "epoch: 4 step: 1520, loss is 0.004354882054030895\n",
      "epoch: 4 step: 1521, loss is 0.0030194593127816916\n",
      "epoch: 4 step: 1522, loss is 0.03587125241756439\n",
      "epoch: 4 step: 1523, loss is 0.025170758366584778\n",
      "epoch: 4 step: 1524, loss is 0.013887672685086727\n",
      "epoch: 4 step: 1525, loss is 0.032976120710372925\n",
      "epoch: 4 step: 1526, loss is 0.004355693701654673\n",
      "epoch: 4 step: 1527, loss is 0.0095768291503191\n",
      "epoch: 4 step: 1528, loss is 0.012416624464094639\n",
      "epoch: 4 step: 1529, loss is 0.08284474164247513\n",
      "epoch: 4 step: 1530, loss is 0.015069696120917797\n",
      "epoch: 4 step: 1531, loss is 0.03516619652509689\n",
      "epoch: 4 step: 1532, loss is 0.007955788634717464\n",
      "epoch: 4 step: 1533, loss is 0.00803433358669281\n",
      "epoch: 4 step: 1534, loss is 0.050347574055194855\n",
      "epoch: 4 step: 1535, loss is 0.026279646903276443\n",
      "epoch: 4 step: 1536, loss is 0.009058809839189053\n",
      "epoch: 4 step: 1537, loss is 0.0012259786017239094\n",
      "epoch: 4 step: 1538, loss is 0.04775497317314148\n",
      "epoch: 4 step: 1539, loss is 0.018105914816260338\n",
      "epoch: 4 step: 1540, loss is 0.03917976841330528\n",
      "epoch: 4 step: 1541, loss is 0.03455185517668724\n",
      "epoch: 4 step: 1542, loss is 0.06799069046974182\n",
      "epoch: 4 step: 1543, loss is 0.0018105412600561976\n",
      "epoch: 4 step: 1544, loss is 0.07772248238325119\n",
      "epoch: 4 step: 1545, loss is 0.21383464336395264\n",
      "epoch: 4 step: 1546, loss is 0.10963525623083115\n",
      "epoch: 4 step: 1547, loss is 0.00017240381566807628\n",
      "epoch: 4 step: 1548, loss is 0.003716746112331748\n",
      "epoch: 4 step: 1549, loss is 0.002060776110738516\n",
      "epoch: 4 step: 1550, loss is 0.012970251962542534\n",
      "epoch: 4 step: 1551, loss is 0.013473094440996647\n",
      "epoch: 4 step: 1552, loss is 0.011812628246843815\n",
      "epoch: 4 step: 1553, loss is 0.06600700318813324\n",
      "epoch: 4 step: 1554, loss is 0.0014992201467975974\n",
      "epoch: 4 step: 1555, loss is 0.00037057630834169686\n",
      "epoch: 4 step: 1556, loss is 0.01408311352133751\n",
      "epoch: 4 step: 1557, loss is 0.04610960930585861\n",
      "epoch: 4 step: 1558, loss is 0.09744904935359955\n",
      "epoch: 4 step: 1559, loss is 0.0661306381225586\n",
      "epoch: 4 step: 1560, loss is 0.036606237292289734\n",
      "epoch: 4 step: 1561, loss is 0.004486729856580496\n",
      "epoch: 4 step: 1562, loss is 0.037956736981868744\n",
      "epoch: 4 step: 1563, loss is 0.0003634737222455442\n",
      "epoch: 4 step: 1564, loss is 0.003982005640864372\n",
      "epoch: 4 step: 1565, loss is 0.009009136818349361\n",
      "epoch: 4 step: 1566, loss is 0.0025710633490234613\n",
      "epoch: 4 step: 1567, loss is 0.1839369386434555\n",
      "epoch: 4 step: 1568, loss is 0.021650614216923714\n",
      "epoch: 4 step: 1569, loss is 0.011607401072978973\n",
      "epoch: 4 step: 1570, loss is 0.045498523861169815\n",
      "epoch: 4 step: 1571, loss is 0.06445685029029846\n",
      "epoch: 4 step: 1572, loss is 0.0027953500393778086\n",
      "epoch: 4 step: 1573, loss is 0.025691112503409386\n",
      "epoch: 4 step: 1574, loss is 0.006757733877748251\n",
      "epoch: 4 step: 1575, loss is 0.011646393686532974\n",
      "epoch: 4 step: 1576, loss is 0.0017339906189590693\n",
      "epoch: 4 step: 1577, loss is 0.00024507343186996877\n",
      "epoch: 4 step: 1578, loss is 0.004105181433260441\n",
      "epoch: 4 step: 1579, loss is 0.2632770240306854\n",
      "epoch: 4 step: 1580, loss is 0.015209505334496498\n",
      "epoch: 4 step: 1581, loss is 0.11748944222927094\n",
      "epoch: 4 step: 1582, loss is 0.0010206990409642458\n",
      "epoch: 4 step: 1583, loss is 0.09008025377988815\n",
      "epoch: 4 step: 1584, loss is 0.007879202254116535\n",
      "epoch: 4 step: 1585, loss is 0.01927732117474079\n",
      "epoch: 4 step: 1586, loss is 0.017166806384921074\n",
      "epoch: 4 step: 1587, loss is 0.009848082438111305\n",
      "epoch: 4 step: 1588, loss is 0.009102309122681618\n",
      "epoch: 4 step: 1589, loss is 0.03544525429606438\n",
      "epoch: 4 step: 1590, loss is 0.009465565904974937\n",
      "epoch: 4 step: 1591, loss is 0.042003944516181946\n",
      "epoch: 4 step: 1592, loss is 0.0026853762101382017\n",
      "epoch: 4 step: 1593, loss is 0.025729842483997345\n",
      "epoch: 4 step: 1594, loss is 0.0045632971450686455\n",
      "epoch: 4 step: 1595, loss is 0.02210596203804016\n",
      "epoch: 4 step: 1596, loss is 0.22888624668121338\n",
      "epoch: 4 step: 1597, loss is 0.041837453842163086\n",
      "epoch: 4 step: 1598, loss is 0.005831635091453791\n",
      "epoch: 4 step: 1599, loss is 0.019567154347896576\n",
      "epoch: 4 step: 1600, loss is 0.02193717285990715\n",
      "epoch: 4 step: 1601, loss is 0.05021985247731209\n",
      "epoch: 4 step: 1602, loss is 0.001037338050082326\n",
      "epoch: 4 step: 1603, loss is 0.009363958612084389\n",
      "epoch: 4 step: 1604, loss is 0.00037614713073708117\n",
      "epoch: 4 step: 1605, loss is 0.0004147557192482054\n",
      "epoch: 4 step: 1606, loss is 0.05426578223705292\n",
      "epoch: 4 step: 1607, loss is 0.0056610992178320885\n",
      "epoch: 4 step: 1608, loss is 0.019126303493976593\n",
      "epoch: 4 step: 1609, loss is 0.009706872515380383\n",
      "epoch: 4 step: 1610, loss is 0.001900575589388609\n",
      "epoch: 4 step: 1611, loss is 0.0035162074491381645\n",
      "epoch: 4 step: 1612, loss is 0.008456704206764698\n",
      "epoch: 4 step: 1613, loss is 0.051111843436956406\n",
      "epoch: 4 step: 1614, loss is 0.0013818913139402866\n",
      "epoch: 4 step: 1615, loss is 0.0010062847286462784\n",
      "epoch: 4 step: 1616, loss is 0.01343623548746109\n",
      "epoch: 4 step: 1617, loss is 0.03735530748963356\n",
      "epoch: 4 step: 1618, loss is 0.020760655403137207\n",
      "epoch: 4 step: 1619, loss is 0.003009788691997528\n",
      "epoch: 4 step: 1620, loss is 0.001601889729499817\n",
      "epoch: 4 step: 1621, loss is 0.07035373151302338\n",
      "epoch: 4 step: 1622, loss is 0.002863278379663825\n",
      "epoch: 4 step: 1623, loss is 0.05396934226155281\n",
      "epoch: 4 step: 1624, loss is 0.00025115589960478246\n",
      "epoch: 4 step: 1625, loss is 0.001267596147954464\n",
      "epoch: 4 step: 1626, loss is 0.001228863256983459\n",
      "epoch: 4 step: 1627, loss is 0.016499171033501625\n",
      "epoch: 4 step: 1628, loss is 7.443210779456422e-05\n",
      "epoch: 4 step: 1629, loss is 0.010472357273101807\n",
      "epoch: 4 step: 1630, loss is 0.001058721449226141\n",
      "epoch: 4 step: 1631, loss is 0.04736338183283806\n",
      "epoch: 4 step: 1632, loss is 0.010312912985682487\n",
      "epoch: 4 step: 1633, loss is 0.017802678048610687\n",
      "epoch: 4 step: 1634, loss is 0.003952179569751024\n",
      "epoch: 4 step: 1635, loss is 0.00010396471770945936\n",
      "epoch: 4 step: 1636, loss is 0.00922178104519844\n",
      "epoch: 4 step: 1637, loss is 0.21821992099285126\n",
      "epoch: 4 step: 1638, loss is 0.12802965939044952\n",
      "epoch: 4 step: 1639, loss is 0.06548338383436203\n",
      "epoch: 4 step: 1640, loss is 0.010078352876007557\n",
      "epoch: 4 step: 1641, loss is 0.22015386819839478\n",
      "epoch: 4 step: 1642, loss is 0.0033020498231053352\n",
      "epoch: 4 step: 1643, loss is 0.06883975863456726\n",
      "epoch: 4 step: 1644, loss is 0.0010774352122098207\n",
      "epoch: 4 step: 1645, loss is 0.00042596805724315345\n",
      "epoch: 4 step: 1646, loss is 0.0007580475066788495\n",
      "epoch: 4 step: 1647, loss is 0.0032876289915293455\n",
      "epoch: 4 step: 1648, loss is 0.08081177622079849\n",
      "epoch: 4 step: 1649, loss is 0.017409002408385277\n",
      "epoch: 4 step: 1650, loss is 0.00626888545230031\n",
      "epoch: 4 step: 1651, loss is 0.15246082842350006\n",
      "epoch: 4 step: 1652, loss is 0.019441457465291023\n",
      "epoch: 4 step: 1653, loss is 0.031595125794410706\n",
      "epoch: 4 step: 1654, loss is 0.08563978224992752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 1655, loss is 0.10233912616968155\n",
      "epoch: 4 step: 1656, loss is 0.006141034886240959\n",
      "epoch: 4 step: 1657, loss is 0.007019322365522385\n",
      "epoch: 4 step: 1658, loss is 0.40459439158439636\n",
      "epoch: 4 step: 1659, loss is 0.13023090362548828\n",
      "epoch: 4 step: 1660, loss is 0.0002022391272475943\n",
      "epoch: 4 step: 1661, loss is 0.010059669613838196\n",
      "epoch: 4 step: 1662, loss is 0.04928797483444214\n",
      "epoch: 4 step: 1663, loss is 0.10097446292638779\n",
      "epoch: 4 step: 1664, loss is 0.0013967151753604412\n",
      "epoch: 4 step: 1665, loss is 0.00834768358618021\n",
      "epoch: 4 step: 1666, loss is 0.007445805706083775\n",
      "epoch: 4 step: 1667, loss is 0.0037872707471251488\n",
      "epoch: 4 step: 1668, loss is 0.006874507758766413\n",
      "epoch: 4 step: 1669, loss is 0.016160612925887108\n",
      "epoch: 4 step: 1670, loss is 0.0007055365131236613\n",
      "epoch: 4 step: 1671, loss is 0.10701356828212738\n",
      "epoch: 4 step: 1672, loss is 0.032499268651008606\n",
      "epoch: 4 step: 1673, loss is 0.001718880725093186\n",
      "epoch: 4 step: 1674, loss is 0.010308332741260529\n",
      "epoch: 4 step: 1675, loss is 0.10111212730407715\n",
      "epoch: 4 step: 1676, loss is 0.020088614895939827\n",
      "epoch: 4 step: 1677, loss is 0.0011673534754663706\n",
      "epoch: 4 step: 1678, loss is 0.008792397566139698\n",
      "epoch: 4 step: 1679, loss is 0.10374864190816879\n",
      "epoch: 4 step: 1680, loss is 0.0030350664164870977\n",
      "epoch: 4 step: 1681, loss is 0.062079232186079025\n",
      "epoch: 4 step: 1682, loss is 0.0029499121010303497\n",
      "epoch: 4 step: 1683, loss is 0.0028578743804246187\n",
      "epoch: 4 step: 1684, loss is 0.14619633555412292\n",
      "epoch: 4 step: 1685, loss is 0.07383689284324646\n",
      "epoch: 4 step: 1686, loss is 0.006030398886650801\n",
      "epoch: 4 step: 1687, loss is 0.01015411876142025\n",
      "epoch: 4 step: 1688, loss is 0.028896812349557877\n",
      "epoch: 4 step: 1689, loss is 0.12468882650136948\n",
      "epoch: 4 step: 1690, loss is 0.14988858997821808\n",
      "epoch: 4 step: 1691, loss is 0.01285137515515089\n",
      "epoch: 4 step: 1692, loss is 0.004989092703908682\n",
      "epoch: 4 step: 1693, loss is 0.00028607502463273704\n",
      "epoch: 4 step: 1694, loss is 0.001986323855817318\n",
      "epoch: 4 step: 1695, loss is 0.06254387646913528\n",
      "epoch: 4 step: 1696, loss is 0.062304772436618805\n",
      "epoch: 4 step: 1697, loss is 0.13147856295108795\n",
      "epoch: 4 step: 1698, loss is 0.0034011523239314556\n",
      "epoch: 4 step: 1699, loss is 0.003825992811471224\n",
      "epoch: 4 step: 1700, loss is 0.11867531388998032\n",
      "epoch: 4 step: 1701, loss is 0.06500076502561569\n",
      "epoch: 4 step: 1702, loss is 0.01723938062787056\n",
      "epoch: 4 step: 1703, loss is 0.0029886120464652777\n",
      "epoch: 4 step: 1704, loss is 0.20383092761039734\n",
      "epoch: 4 step: 1705, loss is 0.030542733147740364\n",
      "epoch: 4 step: 1706, loss is 0.0014372484292834997\n",
      "epoch: 4 step: 1707, loss is 0.004680521786212921\n",
      "epoch: 4 step: 1708, loss is 0.016905877739191055\n",
      "epoch: 4 step: 1709, loss is 0.16214366257190704\n",
      "epoch: 4 step: 1710, loss is 0.022908179089426994\n",
      "epoch: 4 step: 1711, loss is 0.05262136459350586\n",
      "epoch: 4 step: 1712, loss is 0.04905176907777786\n",
      "epoch: 4 step: 1713, loss is 0.0007020680932328105\n",
      "epoch: 4 step: 1714, loss is 0.0163467638194561\n",
      "epoch: 4 step: 1715, loss is 0.12630680203437805\n",
      "epoch: 4 step: 1716, loss is 0.2023257613182068\n",
      "epoch: 4 step: 1717, loss is 0.31140658259391785\n",
      "epoch: 4 step: 1718, loss is 0.09564126282930374\n",
      "epoch: 4 step: 1719, loss is 0.11371734738349915\n",
      "epoch: 4 step: 1720, loss is 0.019864121451973915\n",
      "epoch: 4 step: 1721, loss is 0.2671963572502136\n",
      "epoch: 4 step: 1722, loss is 0.012343520298600197\n",
      "epoch: 4 step: 1723, loss is 0.02169102057814598\n",
      "epoch: 4 step: 1724, loss is 0.008790919557213783\n",
      "epoch: 4 step: 1725, loss is 0.007383920717984438\n",
      "epoch: 4 step: 1726, loss is 0.03496261686086655\n",
      "epoch: 4 step: 1727, loss is 0.1043485626578331\n",
      "epoch: 4 step: 1728, loss is 0.002271791687235236\n",
      "epoch: 4 step: 1729, loss is 0.015714239329099655\n",
      "epoch: 4 step: 1730, loss is 0.0018575203139334917\n",
      "epoch: 4 step: 1731, loss is 0.17549224197864532\n",
      "epoch: 4 step: 1732, loss is 0.1246517151594162\n",
      "epoch: 4 step: 1733, loss is 0.0050271740183234215\n",
      "epoch: 4 step: 1734, loss is 0.023790651932358742\n",
      "epoch: 4 step: 1735, loss is 0.0033421094994992018\n",
      "epoch: 4 step: 1736, loss is 0.029213106259703636\n",
      "epoch: 4 step: 1737, loss is 0.19510436058044434\n",
      "epoch: 4 step: 1738, loss is 0.002117456402629614\n",
      "epoch: 4 step: 1739, loss is 0.06221969798207283\n",
      "epoch: 4 step: 1740, loss is 0.2120620161294937\n",
      "epoch: 4 step: 1741, loss is 0.008230380713939667\n",
      "epoch: 4 step: 1742, loss is 0.07670028507709503\n",
      "epoch: 4 step: 1743, loss is 0.06574656069278717\n",
      "epoch: 4 step: 1744, loss is 0.27810603380203247\n",
      "epoch: 4 step: 1745, loss is 0.011204876936972141\n",
      "epoch: 4 step: 1746, loss is 0.0061235856264829636\n",
      "epoch: 4 step: 1747, loss is 0.01487560011446476\n",
      "epoch: 4 step: 1748, loss is 0.0183134526014328\n",
      "epoch: 4 step: 1749, loss is 0.011383073404431343\n",
      "epoch: 4 step: 1750, loss is 0.10125720500946045\n",
      "epoch: 4 step: 1751, loss is 0.01654462330043316\n",
      "epoch: 4 step: 1752, loss is 0.011154995299875736\n",
      "epoch: 4 step: 1753, loss is 0.05344163253903389\n",
      "epoch: 4 step: 1754, loss is 0.007736302446573973\n",
      "epoch: 4 step: 1755, loss is 0.021010739728808403\n",
      "epoch: 4 step: 1756, loss is 0.11422999948263168\n",
      "epoch: 4 step: 1757, loss is 0.011094072833657265\n",
      "epoch: 4 step: 1758, loss is 0.0015497107524424791\n",
      "epoch: 4 step: 1759, loss is 0.02037050575017929\n",
      "epoch: 4 step: 1760, loss is 0.014345452189445496\n",
      "epoch: 4 step: 1761, loss is 0.013370072469115257\n",
      "epoch: 4 step: 1762, loss is 0.04424801096320152\n",
      "epoch: 4 step: 1763, loss is 0.014315934851765633\n",
      "epoch: 4 step: 1764, loss is 0.01297681499272585\n",
      "epoch: 4 step: 1765, loss is 0.08494114875793457\n",
      "epoch: 4 step: 1766, loss is 0.00420519569888711\n",
      "epoch: 4 step: 1767, loss is 0.017948374152183533\n",
      "epoch: 4 step: 1768, loss is 0.11525560170412064\n",
      "epoch: 4 step: 1769, loss is 0.06443322449922562\n",
      "epoch: 4 step: 1770, loss is 0.014043199829757214\n",
      "epoch: 4 step: 1771, loss is 0.1895056962966919\n",
      "epoch: 4 step: 1772, loss is 0.02503054030239582\n",
      "epoch: 4 step: 1773, loss is 0.0017359820194542408\n",
      "epoch: 4 step: 1774, loss is 0.10340432077646255\n",
      "epoch: 4 step: 1775, loss is 0.01137426309287548\n",
      "epoch: 4 step: 1776, loss is 0.007652914151549339\n",
      "epoch: 4 step: 1777, loss is 0.16958335041999817\n",
      "epoch: 4 step: 1778, loss is 0.002265732269734144\n",
      "epoch: 4 step: 1779, loss is 0.002577368402853608\n",
      "epoch: 4 step: 1780, loss is 0.03555576875805855\n",
      "epoch: 4 step: 1781, loss is 0.08259474486112595\n",
      "epoch: 4 step: 1782, loss is 0.0011734815780073404\n",
      "epoch: 4 step: 1783, loss is 0.015043245628476143\n",
      "epoch: 4 step: 1784, loss is 0.0056283241137862206\n",
      "epoch: 4 step: 1785, loss is 0.03822443634271622\n",
      "epoch: 4 step: 1786, loss is 0.16287390887737274\n",
      "epoch: 4 step: 1787, loss is 0.15670570731163025\n",
      "epoch: 4 step: 1788, loss is 0.023713424801826477\n",
      "epoch: 4 step: 1789, loss is 0.1057341992855072\n",
      "epoch: 4 step: 1790, loss is 0.0014979214174672961\n",
      "epoch: 4 step: 1791, loss is 0.12313169986009598\n",
      "epoch: 4 step: 1792, loss is 0.1910174936056137\n",
      "epoch: 4 step: 1793, loss is 0.00800776295363903\n",
      "epoch: 4 step: 1794, loss is 0.03577568382024765\n",
      "epoch: 4 step: 1795, loss is 0.020657548680901527\n",
      "epoch: 4 step: 1796, loss is 0.02232779562473297\n",
      "epoch: 4 step: 1797, loss is 0.01084938645362854\n",
      "epoch: 4 step: 1798, loss is 0.022009117528796196\n",
      "epoch: 4 step: 1799, loss is 0.07062241435050964\n",
      "epoch: 4 step: 1800, loss is 0.008954167366027832\n",
      "epoch: 4 step: 1801, loss is 0.03249846398830414\n",
      "epoch: 4 step: 1802, loss is 0.008753621019423008\n",
      "epoch: 4 step: 1803, loss is 0.0032098882365971804\n",
      "epoch: 4 step: 1804, loss is 0.06279531866312027\n",
      "epoch: 4 step: 1805, loss is 0.019425170496106148\n",
      "epoch: 4 step: 1806, loss is 0.008395656943321228\n",
      "epoch: 4 step: 1807, loss is 0.002099992474541068\n",
      "epoch: 4 step: 1808, loss is 0.0009735421626828611\n",
      "epoch: 4 step: 1809, loss is 0.03635445237159729\n",
      "epoch: 4 step: 1810, loss is 0.011988379992544651\n",
      "epoch: 4 step: 1811, loss is 0.00713045010343194\n",
      "epoch: 4 step: 1812, loss is 0.04093556106090546\n",
      "epoch: 4 step: 1813, loss is 0.0225942712277174\n",
      "epoch: 4 step: 1814, loss is 0.00523766502737999\n",
      "epoch: 4 step: 1815, loss is 0.026025183498859406\n",
      "epoch: 4 step: 1816, loss is 0.0010240883566439152\n",
      "epoch: 4 step: 1817, loss is 0.007736686151474714\n",
      "epoch: 4 step: 1818, loss is 0.01870931312441826\n",
      "epoch: 4 step: 1819, loss is 0.0680246353149414\n",
      "epoch: 4 step: 1820, loss is 0.025317993015050888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 1821, loss is 0.0011936842929571867\n",
      "epoch: 4 step: 1822, loss is 0.0019068062538281083\n",
      "epoch: 4 step: 1823, loss is 0.08083698898553848\n",
      "epoch: 4 step: 1824, loss is 0.028030218556523323\n",
      "epoch: 4 step: 1825, loss is 0.1052200049161911\n",
      "epoch: 4 step: 1826, loss is 0.054442860186100006\n",
      "epoch: 4 step: 1827, loss is 0.007736418396234512\n",
      "epoch: 4 step: 1828, loss is 0.14957304298877716\n",
      "epoch: 4 step: 1829, loss is 0.20488688349723816\n",
      "epoch: 4 step: 1830, loss is 0.3286738395690918\n",
      "epoch: 4 step: 1831, loss is 0.00670781172811985\n",
      "epoch: 4 step: 1832, loss is 0.004126047249883413\n",
      "epoch: 4 step: 1833, loss is 0.002261551795527339\n",
      "epoch: 4 step: 1834, loss is 0.00021610716066788882\n",
      "epoch: 4 step: 1835, loss is 0.0017667969223111868\n",
      "epoch: 4 step: 1836, loss is 0.0016725941095501184\n",
      "epoch: 4 step: 1837, loss is 0.003634144552052021\n",
      "epoch: 4 step: 1838, loss is 0.03451204672455788\n",
      "epoch: 4 step: 1839, loss is 0.00853926595300436\n",
      "epoch: 4 step: 1840, loss is 0.00737197557464242\n",
      "epoch: 4 step: 1841, loss is 0.09053990244865417\n",
      "epoch: 4 step: 1842, loss is 0.022265708073973656\n",
      "epoch: 4 step: 1843, loss is 0.13647253811359406\n",
      "epoch: 4 step: 1844, loss is 0.020701883360743523\n",
      "epoch: 4 step: 1845, loss is 0.005901719909161329\n",
      "epoch: 4 step: 1846, loss is 0.02274247817695141\n",
      "epoch: 4 step: 1847, loss is 0.0045721144415438175\n",
      "epoch: 4 step: 1848, loss is 0.008671029470860958\n",
      "epoch: 4 step: 1849, loss is 0.09945517778396606\n",
      "epoch: 4 step: 1850, loss is 0.02949805185198784\n",
      "epoch: 4 step: 1851, loss is 0.4128998816013336\n",
      "epoch: 4 step: 1852, loss is 0.03346522897481918\n",
      "epoch: 4 step: 1853, loss is 0.07033330947160721\n",
      "epoch: 4 step: 1854, loss is 0.004004230722784996\n",
      "epoch: 4 step: 1855, loss is 0.008814261294901371\n",
      "epoch: 4 step: 1856, loss is 0.09895589202642441\n",
      "epoch: 4 step: 1857, loss is 0.1210673525929451\n",
      "epoch: 4 step: 1858, loss is 0.006597647909075022\n",
      "epoch: 4 step: 1859, loss is 0.13317470252513885\n",
      "epoch: 4 step: 1860, loss is 0.04228132218122482\n",
      "epoch: 4 step: 1861, loss is 0.003326598322018981\n",
      "epoch: 4 step: 1862, loss is 0.029734013602137566\n",
      "epoch: 4 step: 1863, loss is 0.049222711473703384\n",
      "epoch: 4 step: 1864, loss is 0.02521013654768467\n",
      "epoch: 4 step: 1865, loss is 0.07896851003170013\n",
      "epoch: 4 step: 1866, loss is 0.005929813254624605\n",
      "epoch: 4 step: 1867, loss is 0.04220255836844444\n",
      "epoch: 4 step: 1868, loss is 0.1429784595966339\n",
      "epoch: 4 step: 1869, loss is 0.09466823935508728\n",
      "epoch: 4 step: 1870, loss is 0.008237028494477272\n",
      "epoch: 4 step: 1871, loss is 0.0822947695851326\n",
      "epoch: 4 step: 1872, loss is 0.03163668140769005\n",
      "epoch: 4 step: 1873, loss is 0.0054210275411605835\n",
      "epoch: 4 step: 1874, loss is 0.0554896779358387\n",
      "epoch: 4 step: 1875, loss is 0.0014548025792464614\n",
      "epoch: 5 step: 1, loss is 0.003935282118618488\n",
      "epoch: 5 step: 2, loss is 0.013720119372010231\n",
      "epoch: 5 step: 3, loss is 0.019407745450735092\n",
      "epoch: 5 step: 4, loss is 0.0678391084074974\n",
      "epoch: 5 step: 5, loss is 0.06942647695541382\n",
      "epoch: 5 step: 6, loss is 0.02733868546783924\n",
      "epoch: 5 step: 7, loss is 0.029513850808143616\n",
      "epoch: 5 step: 8, loss is 0.05616176500916481\n",
      "epoch: 5 step: 9, loss is 0.0029765863437205553\n",
      "epoch: 5 step: 10, loss is 0.025810543447732925\n",
      "epoch: 5 step: 11, loss is 0.03089369460940361\n",
      "epoch: 5 step: 12, loss is 0.009257231839001179\n",
      "epoch: 5 step: 13, loss is 0.02663263864815235\n",
      "epoch: 5 step: 14, loss is 0.005990088917315006\n",
      "epoch: 5 step: 15, loss is 0.004955856595188379\n",
      "epoch: 5 step: 16, loss is 0.0011935244547203183\n",
      "epoch: 5 step: 17, loss is 0.008984632790088654\n",
      "epoch: 5 step: 18, loss is 0.013967394828796387\n",
      "epoch: 5 step: 19, loss is 0.03142227604985237\n",
      "epoch: 5 step: 20, loss is 0.025256579741835594\n",
      "epoch: 5 step: 21, loss is 0.01205999031662941\n",
      "epoch: 5 step: 22, loss is 0.006779611110687256\n",
      "epoch: 5 step: 23, loss is 0.0005302154459059238\n",
      "epoch: 5 step: 24, loss is 0.10506606847047806\n",
      "epoch: 5 step: 25, loss is 0.21744024753570557\n",
      "epoch: 5 step: 26, loss is 0.0041421507485210896\n",
      "epoch: 5 step: 27, loss is 0.00021067025954835117\n",
      "epoch: 5 step: 28, loss is 0.00325373443774879\n",
      "epoch: 5 step: 29, loss is 0.048361871391534805\n",
      "epoch: 5 step: 30, loss is 0.0076772854663431644\n",
      "epoch: 5 step: 31, loss is 0.020246082916855812\n",
      "epoch: 5 step: 32, loss is 0.01775813288986683\n",
      "epoch: 5 step: 33, loss is 0.01577434130012989\n",
      "epoch: 5 step: 34, loss is 0.0015513680409640074\n",
      "epoch: 5 step: 35, loss is 0.002232161583378911\n",
      "epoch: 5 step: 36, loss is 0.024739844724535942\n",
      "epoch: 5 step: 37, loss is 0.014425275847315788\n",
      "epoch: 5 step: 38, loss is 0.005176527891308069\n",
      "epoch: 5 step: 39, loss is 0.0063423956744372845\n",
      "epoch: 5 step: 40, loss is 0.002957521704956889\n",
      "epoch: 5 step: 41, loss is 0.0032368935644626617\n",
      "epoch: 5 step: 42, loss is 0.000671377289108932\n",
      "epoch: 5 step: 43, loss is 0.007003904320299625\n",
      "epoch: 5 step: 44, loss is 0.2199377715587616\n",
      "epoch: 5 step: 45, loss is 0.0662599429488182\n",
      "epoch: 5 step: 46, loss is 0.003175416961312294\n",
      "epoch: 5 step: 47, loss is 0.005900308955460787\n",
      "epoch: 5 step: 48, loss is 0.007253520656377077\n",
      "epoch: 5 step: 49, loss is 0.0025327939074486494\n",
      "epoch: 5 step: 50, loss is 0.014112275093793869\n",
      "epoch: 5 step: 51, loss is 0.009105037897825241\n",
      "epoch: 5 step: 52, loss is 0.008085206151008606\n",
      "epoch: 5 step: 53, loss is 0.002274095779284835\n",
      "epoch: 5 step: 54, loss is 0.004175927955657244\n",
      "epoch: 5 step: 55, loss is 0.04796286299824715\n",
      "epoch: 5 step: 56, loss is 0.19784174859523773\n",
      "epoch: 5 step: 57, loss is 0.012822462245821953\n",
      "epoch: 5 step: 58, loss is 0.00818057730793953\n",
      "epoch: 5 step: 59, loss is 0.004963246639817953\n",
      "epoch: 5 step: 60, loss is 0.0022663085255771875\n",
      "epoch: 5 step: 61, loss is 0.011418678797781467\n",
      "epoch: 5 step: 62, loss is 0.08846437931060791\n",
      "epoch: 5 step: 63, loss is 0.0014516869559884071\n",
      "epoch: 5 step: 64, loss is 0.03170213848352432\n",
      "epoch: 5 step: 65, loss is 0.09532010555267334\n",
      "epoch: 5 step: 66, loss is 0.003297575982287526\n",
      "epoch: 5 step: 67, loss is 0.00033880601404234767\n",
      "epoch: 5 step: 68, loss is 0.005456048529595137\n",
      "epoch: 5 step: 69, loss is 0.0016607854049652815\n",
      "epoch: 5 step: 70, loss is 0.06235130876302719\n",
      "epoch: 5 step: 71, loss is 0.019301021471619606\n",
      "epoch: 5 step: 72, loss is 0.00040043058106675744\n",
      "epoch: 5 step: 73, loss is 0.012684214860200882\n",
      "epoch: 5 step: 74, loss is 0.04746377095580101\n",
      "epoch: 5 step: 75, loss is 0.014036743901669979\n",
      "epoch: 5 step: 76, loss is 0.000974101887550205\n",
      "epoch: 5 step: 77, loss is 6.871992081869394e-05\n",
      "epoch: 5 step: 78, loss is 0.0005868296721018851\n",
      "epoch: 5 step: 79, loss is 0.01165788248181343\n",
      "epoch: 5 step: 80, loss is 0.0029862145893275738\n",
      "epoch: 5 step: 81, loss is 0.0001729630312183872\n",
      "epoch: 5 step: 82, loss is 0.014651277102530003\n",
      "epoch: 5 step: 83, loss is 0.007134584244340658\n",
      "epoch: 5 step: 84, loss is 0.0009820163249969482\n",
      "epoch: 5 step: 85, loss is 0.0024339936207979918\n",
      "epoch: 5 step: 86, loss is 0.11683189123868942\n",
      "epoch: 5 step: 87, loss is 0.0022704354487359524\n",
      "epoch: 5 step: 88, loss is 0.009199902415275574\n",
      "epoch: 5 step: 89, loss is 0.26624733209609985\n",
      "epoch: 5 step: 90, loss is 0.1145656630396843\n",
      "epoch: 5 step: 91, loss is 0.09506496787071228\n",
      "epoch: 5 step: 92, loss is 0.0004237173416186124\n",
      "epoch: 5 step: 93, loss is 0.0006816689856350422\n",
      "epoch: 5 step: 94, loss is 0.018708610907197\n",
      "epoch: 5 step: 95, loss is 0.0008026756113395095\n",
      "epoch: 5 step: 96, loss is 0.0038557536900043488\n",
      "epoch: 5 step: 97, loss is 0.001395610743202269\n",
      "epoch: 5 step: 98, loss is 0.0029332609847187996\n",
      "epoch: 5 step: 99, loss is 0.04832970350980759\n",
      "epoch: 5 step: 100, loss is 0.03904077410697937\n",
      "epoch: 5 step: 101, loss is 0.009865935891866684\n",
      "epoch: 5 step: 102, loss is 0.017662303522229195\n",
      "epoch: 5 step: 103, loss is 0.005292853806167841\n",
      "epoch: 5 step: 104, loss is 0.003145951312035322\n",
      "epoch: 5 step: 105, loss is 0.005628057289868593\n",
      "epoch: 5 step: 106, loss is 0.0013481166679412127\n",
      "epoch: 5 step: 107, loss is 0.0405552051961422\n",
      "epoch: 5 step: 108, loss is 0.04127994924783707\n",
      "epoch: 5 step: 109, loss is 0.0004597765509970486\n",
      "epoch: 5 step: 110, loss is 0.009671156294643879\n",
      "epoch: 5 step: 111, loss is 0.008261067792773247\n",
      "epoch: 5 step: 112, loss is 0.006970165763050318\n",
      "epoch: 5 step: 113, loss is 0.00014382840890903026\n",
      "epoch: 5 step: 114, loss is 0.004489287734031677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 115, loss is 0.06942588835954666\n",
      "epoch: 5 step: 116, loss is 0.030676014721393585\n",
      "epoch: 5 step: 117, loss is 0.04803512990474701\n",
      "epoch: 5 step: 118, loss is 0.0018074188847094774\n",
      "epoch: 5 step: 119, loss is 0.0031228819862008095\n",
      "epoch: 5 step: 120, loss is 0.005740536842495203\n",
      "epoch: 5 step: 121, loss is 0.037389930337667465\n",
      "epoch: 5 step: 122, loss is 0.08330059796571732\n",
      "epoch: 5 step: 123, loss is 0.056915394961833954\n",
      "epoch: 5 step: 124, loss is 0.00024224079970736057\n",
      "epoch: 5 step: 125, loss is 0.00426034489646554\n",
      "epoch: 5 step: 126, loss is 0.042729876935482025\n",
      "epoch: 5 step: 127, loss is 0.02214747667312622\n",
      "epoch: 5 step: 128, loss is 0.0749920904636383\n",
      "epoch: 5 step: 129, loss is 0.0008036558283492923\n",
      "epoch: 5 step: 130, loss is 0.002435521222651005\n",
      "epoch: 5 step: 131, loss is 0.018427668139338493\n",
      "epoch: 5 step: 132, loss is 0.021810993552207947\n",
      "epoch: 5 step: 133, loss is 0.00537747610360384\n",
      "epoch: 5 step: 134, loss is 0.018714116886258125\n",
      "epoch: 5 step: 135, loss is 0.03335872292518616\n",
      "epoch: 5 step: 136, loss is 0.0011565720196813345\n",
      "epoch: 5 step: 137, loss is 0.08618157356977463\n",
      "epoch: 5 step: 138, loss is 0.031245455145835876\n",
      "epoch: 5 step: 139, loss is 0.005105928052216768\n",
      "epoch: 5 step: 140, loss is 0.012548488564789295\n",
      "epoch: 5 step: 141, loss is 0.03466188907623291\n",
      "epoch: 5 step: 142, loss is 0.013310721144080162\n",
      "epoch: 5 step: 143, loss is 0.0007441641646437347\n",
      "epoch: 5 step: 144, loss is 0.01065407320857048\n",
      "epoch: 5 step: 145, loss is 0.06514410674571991\n",
      "epoch: 5 step: 146, loss is 0.043540749698877335\n",
      "epoch: 5 step: 147, loss is 0.0007522819796577096\n",
      "epoch: 5 step: 148, loss is 0.002882022876292467\n",
      "epoch: 5 step: 149, loss is 0.0012903210008516908\n",
      "epoch: 5 step: 150, loss is 0.028110068291425705\n",
      "epoch: 5 step: 151, loss is 0.001236491370946169\n",
      "epoch: 5 step: 152, loss is 0.0008343430235981941\n",
      "epoch: 5 step: 153, loss is 0.001873427303507924\n",
      "epoch: 5 step: 154, loss is 0.22308573126792908\n",
      "epoch: 5 step: 155, loss is 0.032688070088624954\n",
      "epoch: 5 step: 156, loss is 0.007241550832986832\n",
      "epoch: 5 step: 157, loss is 0.008622457273304462\n",
      "epoch: 5 step: 158, loss is 0.0018717554630711675\n",
      "epoch: 5 step: 159, loss is 0.03893173485994339\n",
      "epoch: 5 step: 160, loss is 0.001965753035619855\n",
      "epoch: 5 step: 161, loss is 0.00013288084301166236\n",
      "epoch: 5 step: 162, loss is 0.013390177860856056\n",
      "epoch: 5 step: 163, loss is 0.016317393630743027\n",
      "epoch: 5 step: 164, loss is 8.533519576303661e-05\n",
      "epoch: 5 step: 165, loss is 0.0012386452872306108\n",
      "epoch: 5 step: 166, loss is 0.01126768160611391\n",
      "epoch: 5 step: 167, loss is 0.1186414584517479\n",
      "epoch: 5 step: 168, loss is 0.0028309619519859552\n",
      "epoch: 5 step: 169, loss is 0.002025333233177662\n",
      "epoch: 5 step: 170, loss is 0.008839207701385021\n",
      "epoch: 5 step: 171, loss is 0.015573958866298199\n",
      "epoch: 5 step: 172, loss is 0.07846182584762573\n",
      "epoch: 5 step: 173, loss is 0.0195095706731081\n",
      "epoch: 5 step: 174, loss is 0.17788860201835632\n",
      "epoch: 5 step: 175, loss is 0.06894917786121368\n",
      "epoch: 5 step: 176, loss is 0.01873740740120411\n",
      "epoch: 5 step: 177, loss is 0.004705701023340225\n",
      "epoch: 5 step: 178, loss is 0.0009458687854930758\n",
      "epoch: 5 step: 179, loss is 0.016206050291657448\n",
      "epoch: 5 step: 180, loss is 0.0006249953876249492\n",
      "epoch: 5 step: 181, loss is 0.0011172718368470669\n",
      "epoch: 5 step: 182, loss is 0.39287427067756653\n",
      "epoch: 5 step: 183, loss is 0.2853624224662781\n",
      "epoch: 5 step: 184, loss is 0.0007854470168240368\n",
      "epoch: 5 step: 185, loss is 0.0010198462987318635\n",
      "epoch: 5 step: 186, loss is 0.04074738919734955\n",
      "epoch: 5 step: 187, loss is 0.004238905850797892\n",
      "epoch: 5 step: 188, loss is 0.0774482861161232\n",
      "epoch: 5 step: 189, loss is 0.026965441182255745\n",
      "epoch: 5 step: 190, loss is 0.0016751247458159924\n",
      "epoch: 5 step: 191, loss is 0.0022006207145750523\n",
      "epoch: 5 step: 192, loss is 0.005667433608323336\n",
      "epoch: 5 step: 193, loss is 0.006425835657864809\n",
      "epoch: 5 step: 194, loss is 0.0066076056100428104\n",
      "epoch: 5 step: 195, loss is 0.03631124645471573\n",
      "epoch: 5 step: 196, loss is 0.01977093145251274\n",
      "epoch: 5 step: 197, loss is 0.010761858895421028\n",
      "epoch: 5 step: 198, loss is 0.006828096695244312\n",
      "epoch: 5 step: 199, loss is 0.07004301995038986\n",
      "epoch: 5 step: 200, loss is 0.056119952350854874\n",
      "epoch: 5 step: 201, loss is 0.04343583062291145\n",
      "epoch: 5 step: 202, loss is 0.09739375114440918\n",
      "epoch: 5 step: 203, loss is 0.040327489376068115\n",
      "epoch: 5 step: 204, loss is 0.0054382216185331345\n",
      "epoch: 5 step: 205, loss is 0.07468532025814056\n",
      "epoch: 5 step: 206, loss is 0.0008468682644888759\n",
      "epoch: 5 step: 207, loss is 0.0008798382477834821\n",
      "epoch: 5 step: 208, loss is 0.0004901182255707681\n",
      "epoch: 5 step: 209, loss is 0.005105781834572554\n",
      "epoch: 5 step: 210, loss is 0.07255589216947556\n",
      "epoch: 5 step: 211, loss is 0.05737367272377014\n",
      "epoch: 5 step: 212, loss is 0.0031493869610130787\n",
      "epoch: 5 step: 213, loss is 0.1064903512597084\n",
      "epoch: 5 step: 214, loss is 0.0165350753813982\n",
      "epoch: 5 step: 215, loss is 0.11288245767354965\n",
      "epoch: 5 step: 216, loss is 0.006171367596834898\n",
      "epoch: 5 step: 217, loss is 0.0009911864763125777\n",
      "epoch: 5 step: 218, loss is 0.0032233758829534054\n",
      "epoch: 5 step: 219, loss is 0.009894178248941898\n",
      "epoch: 5 step: 220, loss is 0.014319920912384987\n",
      "epoch: 5 step: 221, loss is 0.0038506851997226477\n",
      "epoch: 5 step: 222, loss is 0.009609125554561615\n",
      "epoch: 5 step: 223, loss is 0.014355472289025784\n",
      "epoch: 5 step: 224, loss is 0.014068132266402245\n",
      "epoch: 5 step: 225, loss is 0.0026691644452512264\n",
      "epoch: 5 step: 226, loss is 0.0026172504294663668\n",
      "epoch: 5 step: 227, loss is 0.007571895606815815\n",
      "epoch: 5 step: 228, loss is 0.019560951739549637\n",
      "epoch: 5 step: 229, loss is 0.00022032781271263957\n",
      "epoch: 5 step: 230, loss is 0.06062163785099983\n",
      "epoch: 5 step: 231, loss is 0.07403577864170074\n",
      "epoch: 5 step: 232, loss is 0.07013783603906631\n",
      "epoch: 5 step: 233, loss is 0.0008774721645750105\n",
      "epoch: 5 step: 234, loss is 0.016991691663861275\n",
      "epoch: 5 step: 235, loss is 0.0008951285271905363\n",
      "epoch: 5 step: 236, loss is 0.059023819863796234\n",
      "epoch: 5 step: 237, loss is 0.011100868694484234\n",
      "epoch: 5 step: 238, loss is 0.007344831712543964\n",
      "epoch: 5 step: 239, loss is 0.08143234997987747\n",
      "epoch: 5 step: 240, loss is 0.01007290929555893\n",
      "epoch: 5 step: 241, loss is 0.0003701798268593848\n",
      "epoch: 5 step: 242, loss is 0.01074141077697277\n",
      "epoch: 5 step: 243, loss is 0.042017556726932526\n",
      "epoch: 5 step: 244, loss is 0.025052634999155998\n",
      "epoch: 5 step: 245, loss is 0.009574525989592075\n",
      "epoch: 5 step: 246, loss is 0.0001561875978950411\n",
      "epoch: 5 step: 247, loss is 0.028236154466867447\n",
      "epoch: 5 step: 248, loss is 0.001864114892669022\n",
      "epoch: 5 step: 249, loss is 0.03801799565553665\n",
      "epoch: 5 step: 250, loss is 0.05764208734035492\n",
      "epoch: 5 step: 251, loss is 0.03294214606285095\n",
      "epoch: 5 step: 252, loss is 0.0018355421489104629\n",
      "epoch: 5 step: 253, loss is 0.003323648124933243\n",
      "epoch: 5 step: 254, loss is 0.03669501468539238\n",
      "epoch: 5 step: 255, loss is 0.0013609732268378139\n",
      "epoch: 5 step: 256, loss is 0.04379255324602127\n",
      "epoch: 5 step: 257, loss is 0.011027553118765354\n",
      "epoch: 5 step: 258, loss is 0.0018169183749705553\n",
      "epoch: 5 step: 259, loss is 0.02477598376572132\n",
      "epoch: 5 step: 260, loss is 0.005793913267552853\n",
      "epoch: 5 step: 261, loss is 0.11832095682621002\n",
      "epoch: 5 step: 262, loss is 0.09995560348033905\n",
      "epoch: 5 step: 263, loss is 0.2031051069498062\n",
      "epoch: 5 step: 264, loss is 0.04965711012482643\n",
      "epoch: 5 step: 265, loss is 0.009351873770356178\n",
      "epoch: 5 step: 266, loss is 0.058808695524930954\n",
      "epoch: 5 step: 267, loss is 0.04716230928897858\n",
      "epoch: 5 step: 268, loss is 0.01722998358309269\n",
      "epoch: 5 step: 269, loss is 0.029200246557593346\n",
      "epoch: 5 step: 270, loss is 0.0029998449608683586\n",
      "epoch: 5 step: 271, loss is 0.004062707535922527\n",
      "epoch: 5 step: 272, loss is 0.03531640022993088\n",
      "epoch: 5 step: 273, loss is 0.06349740922451019\n",
      "epoch: 5 step: 274, loss is 0.06180296465754509\n",
      "epoch: 5 step: 275, loss is 0.0048850891180336475\n",
      "epoch: 5 step: 276, loss is 0.12226859480142593\n",
      "epoch: 5 step: 277, loss is 0.0025008476804941893\n",
      "epoch: 5 step: 278, loss is 0.0034709381870925426\n",
      "epoch: 5 step: 279, loss is 0.01449980866163969\n",
      "epoch: 5 step: 280, loss is 0.013167975470423698\n",
      "epoch: 5 step: 281, loss is 0.011627920903265476\n",
      "epoch: 5 step: 282, loss is 0.013876231387257576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 283, loss is 0.002899938030168414\n",
      "epoch: 5 step: 284, loss is 0.08711167424917221\n",
      "epoch: 5 step: 285, loss is 0.06668789684772491\n",
      "epoch: 5 step: 286, loss is 0.0010533285094425082\n",
      "epoch: 5 step: 287, loss is 0.0015365458093583584\n",
      "epoch: 5 step: 288, loss is 0.005146151874214411\n",
      "epoch: 5 step: 289, loss is 0.09872546046972275\n",
      "epoch: 5 step: 290, loss is 0.0725160613656044\n",
      "epoch: 5 step: 291, loss is 0.0006017955602146685\n",
      "epoch: 5 step: 292, loss is 0.009060429409146309\n",
      "epoch: 5 step: 293, loss is 0.058815211057662964\n",
      "epoch: 5 step: 294, loss is 0.01255087647587061\n",
      "epoch: 5 step: 295, loss is 0.03218601644039154\n",
      "epoch: 5 step: 296, loss is 0.000204627969651483\n",
      "epoch: 5 step: 297, loss is 0.02865716814994812\n",
      "epoch: 5 step: 298, loss is 0.0016956205945461988\n",
      "epoch: 5 step: 299, loss is 0.07713235169649124\n",
      "epoch: 5 step: 300, loss is 0.046756744384765625\n",
      "epoch: 5 step: 301, loss is 0.0003406437754165381\n",
      "epoch: 5 step: 302, loss is 0.010112984105944633\n",
      "epoch: 5 step: 303, loss is 0.00017654690600465983\n",
      "epoch: 5 step: 304, loss is 0.0019344617612659931\n",
      "epoch: 5 step: 305, loss is 0.017409665510058403\n",
      "epoch: 5 step: 306, loss is 0.0003554786671884358\n",
      "epoch: 5 step: 307, loss is 0.0018407015595585108\n",
      "epoch: 5 step: 308, loss is 0.013755989260971546\n",
      "epoch: 5 step: 309, loss is 0.016778096556663513\n",
      "epoch: 5 step: 310, loss is 0.0035264617763459682\n",
      "epoch: 5 step: 311, loss is 0.013683907687664032\n",
      "epoch: 5 step: 312, loss is 0.008146789856255054\n",
      "epoch: 5 step: 313, loss is 0.07415911555290222\n",
      "epoch: 5 step: 314, loss is 0.32471561431884766\n",
      "epoch: 5 step: 315, loss is 0.004500208422541618\n",
      "epoch: 5 step: 316, loss is 0.0007269344641827047\n",
      "epoch: 5 step: 317, loss is 0.0013543083332479\n",
      "epoch: 5 step: 318, loss is 0.0006159954937174916\n",
      "epoch: 5 step: 319, loss is 0.0042671989649534225\n",
      "epoch: 5 step: 320, loss is 0.19519788026809692\n",
      "epoch: 5 step: 321, loss is 0.029872242361307144\n",
      "epoch: 5 step: 322, loss is 0.00586412800475955\n",
      "epoch: 5 step: 323, loss is 0.08887264877557755\n",
      "epoch: 5 step: 324, loss is 0.0028862818144261837\n",
      "epoch: 5 step: 325, loss is 0.00014342555368784815\n",
      "epoch: 5 step: 326, loss is 0.006639254745095968\n",
      "epoch: 5 step: 327, loss is 0.07122386991977692\n",
      "epoch: 5 step: 328, loss is 0.09683887660503387\n",
      "epoch: 5 step: 329, loss is 0.0227784663438797\n",
      "epoch: 5 step: 330, loss is 0.01407616212964058\n",
      "epoch: 5 step: 331, loss is 0.012015709653496742\n",
      "epoch: 5 step: 332, loss is 0.003161703934893012\n",
      "epoch: 5 step: 333, loss is 0.004008278716355562\n",
      "epoch: 5 step: 334, loss is 0.0008171802037395537\n",
      "epoch: 5 step: 335, loss is 0.00020961755944881588\n",
      "epoch: 5 step: 336, loss is 0.1959303468465805\n",
      "epoch: 5 step: 337, loss is 0.005862811114639044\n",
      "epoch: 5 step: 338, loss is 0.005125441588461399\n",
      "epoch: 5 step: 339, loss is 0.00024047133047133684\n",
      "epoch: 5 step: 340, loss is 0.024729611352086067\n",
      "epoch: 5 step: 341, loss is 0.007723151706159115\n",
      "epoch: 5 step: 342, loss is 0.01863274537026882\n",
      "epoch: 5 step: 343, loss is 0.00011270337563473731\n",
      "epoch: 5 step: 344, loss is 0.00013443546777125448\n",
      "epoch: 5 step: 345, loss is 0.11602726578712463\n",
      "epoch: 5 step: 346, loss is 0.026079446077346802\n",
      "epoch: 5 step: 347, loss is 0.005121578928083181\n",
      "epoch: 5 step: 348, loss is 0.05601705238223076\n",
      "epoch: 5 step: 349, loss is 0.007368425838649273\n",
      "epoch: 5 step: 350, loss is 0.4204798936843872\n",
      "epoch: 5 step: 351, loss is 0.0005719210603274405\n",
      "epoch: 5 step: 352, loss is 0.002405214821919799\n",
      "epoch: 5 step: 353, loss is 0.0031459913589060307\n",
      "epoch: 5 step: 354, loss is 0.004807663150131702\n",
      "epoch: 5 step: 355, loss is 0.016983922570943832\n",
      "epoch: 5 step: 356, loss is 0.038032207638025284\n",
      "epoch: 5 step: 357, loss is 0.021748293191194534\n",
      "epoch: 5 step: 358, loss is 0.007605004124343395\n",
      "epoch: 5 step: 359, loss is 0.09669546037912369\n",
      "epoch: 5 step: 360, loss is 0.01880181021988392\n",
      "epoch: 5 step: 361, loss is 0.0009762316476553679\n",
      "epoch: 5 step: 362, loss is 0.0014113170327618718\n",
      "epoch: 5 step: 363, loss is 0.05638115853071213\n",
      "epoch: 5 step: 364, loss is 0.00094548927154392\n",
      "epoch: 5 step: 365, loss is 0.0004305906768422574\n",
      "epoch: 5 step: 366, loss is 0.0018920281436294317\n",
      "epoch: 5 step: 367, loss is 0.00017887339345179498\n",
      "epoch: 5 step: 368, loss is 0.0033059522975236177\n",
      "epoch: 5 step: 369, loss is 0.0016909235855564475\n",
      "epoch: 5 step: 370, loss is 0.0816199779510498\n",
      "epoch: 5 step: 371, loss is 0.0031268559396266937\n",
      "epoch: 5 step: 372, loss is 0.002774294000118971\n",
      "epoch: 5 step: 373, loss is 0.001125145354308188\n",
      "epoch: 5 step: 374, loss is 0.00859927199780941\n",
      "epoch: 5 step: 375, loss is 0.013935992494225502\n",
      "epoch: 5 step: 376, loss is 0.1108592078089714\n",
      "epoch: 5 step: 377, loss is 0.010696286335587502\n",
      "epoch: 5 step: 378, loss is 0.022118328139185905\n",
      "epoch: 5 step: 379, loss is 0.026613490656018257\n",
      "epoch: 5 step: 380, loss is 0.02126913145184517\n",
      "epoch: 5 step: 381, loss is 0.00023575460363645107\n",
      "epoch: 5 step: 382, loss is 0.029615003615617752\n",
      "epoch: 5 step: 383, loss is 0.09434409439563751\n",
      "epoch: 5 step: 384, loss is 0.005794553551822901\n",
      "epoch: 5 step: 385, loss is 0.0005576054099947214\n",
      "epoch: 5 step: 386, loss is 0.003105045761913061\n",
      "epoch: 5 step: 387, loss is 0.002972108544781804\n",
      "epoch: 5 step: 388, loss is 0.09407800436019897\n",
      "epoch: 5 step: 389, loss is 0.02771839313209057\n",
      "epoch: 5 step: 390, loss is 0.04327736049890518\n",
      "epoch: 5 step: 391, loss is 0.004108753055334091\n",
      "epoch: 5 step: 392, loss is 0.01188507117331028\n",
      "epoch: 5 step: 393, loss is 0.004602239932864904\n",
      "epoch: 5 step: 394, loss is 0.015612917020916939\n",
      "epoch: 5 step: 395, loss is 0.004470271524041891\n",
      "epoch: 5 step: 396, loss is 0.010399599559605122\n",
      "epoch: 5 step: 397, loss is 0.00031227522413246334\n",
      "epoch: 5 step: 398, loss is 0.01622062921524048\n",
      "epoch: 5 step: 399, loss is 0.012249091640114784\n",
      "epoch: 5 step: 400, loss is 0.00022804732725489885\n",
      "epoch: 5 step: 401, loss is 0.0015982569893822074\n",
      "epoch: 5 step: 402, loss is 0.005964393727481365\n",
      "epoch: 5 step: 403, loss is 0.0008061125408858061\n",
      "epoch: 5 step: 404, loss is 0.01670587807893753\n",
      "epoch: 5 step: 405, loss is 0.012920008040964603\n",
      "epoch: 5 step: 406, loss is 0.0013543644454330206\n",
      "epoch: 5 step: 407, loss is 0.0030941739678382874\n",
      "epoch: 5 step: 408, loss is 0.01493095327168703\n",
      "epoch: 5 step: 409, loss is 0.0015697266208007932\n",
      "epoch: 5 step: 410, loss is 0.003520738799124956\n",
      "epoch: 5 step: 411, loss is 0.000292820215690881\n",
      "epoch: 5 step: 412, loss is 0.06243610009551048\n",
      "epoch: 5 step: 413, loss is 0.010303230956196785\n",
      "epoch: 5 step: 414, loss is 0.13987457752227783\n",
      "epoch: 5 step: 415, loss is 0.051447946578264236\n",
      "epoch: 5 step: 416, loss is 0.003141326131299138\n",
      "epoch: 5 step: 417, loss is 0.0020731131080538034\n",
      "epoch: 5 step: 418, loss is 0.00022594447364099324\n",
      "epoch: 5 step: 419, loss is 0.05399434268474579\n",
      "epoch: 5 step: 420, loss is 0.024581249803304672\n",
      "epoch: 5 step: 421, loss is 0.028324520215392113\n",
      "epoch: 5 step: 422, loss is 0.010379872284829617\n",
      "epoch: 5 step: 423, loss is 0.003021258395165205\n",
      "epoch: 5 step: 424, loss is 0.02311040833592415\n",
      "epoch: 5 step: 425, loss is 0.005122875329107046\n",
      "epoch: 5 step: 426, loss is 0.0187350045889616\n",
      "epoch: 5 step: 427, loss is 0.0009125777869485319\n",
      "epoch: 5 step: 428, loss is 0.04826914891600609\n",
      "epoch: 5 step: 429, loss is 0.0029234786052256823\n",
      "epoch: 5 step: 430, loss is 0.02834256924688816\n",
      "epoch: 5 step: 431, loss is 0.0004126568092033267\n",
      "epoch: 5 step: 432, loss is 0.008346132934093475\n",
      "epoch: 5 step: 433, loss is 7.188891322584823e-05\n",
      "epoch: 5 step: 434, loss is 0.028379585593938828\n",
      "epoch: 5 step: 435, loss is 0.0003090912941843271\n",
      "epoch: 5 step: 436, loss is 0.11701025813817978\n",
      "epoch: 5 step: 437, loss is 0.017047006636857986\n",
      "epoch: 5 step: 438, loss is 0.19641150534152985\n",
      "epoch: 5 step: 439, loss is 0.0912163257598877\n",
      "epoch: 5 step: 440, loss is 0.05061713978648186\n",
      "epoch: 5 step: 441, loss is 0.010850717313587666\n",
      "epoch: 5 step: 442, loss is 0.0003045162884518504\n",
      "epoch: 5 step: 443, loss is 0.035830602049827576\n",
      "epoch: 5 step: 444, loss is 0.06945997476577759\n",
      "epoch: 5 step: 445, loss is 0.1143660694360733\n",
      "epoch: 5 step: 446, loss is 0.00040274698403663933\n",
      "epoch: 5 step: 447, loss is 0.019760549068450928\n",
      "epoch: 5 step: 448, loss is 0.01509647537022829\n",
      "epoch: 5 step: 449, loss is 0.00024200575717259198\n",
      "epoch: 5 step: 450, loss is 0.002093687653541565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 451, loss is 0.12891830503940582\n",
      "epoch: 5 step: 452, loss is 0.008872885257005692\n",
      "epoch: 5 step: 453, loss is 0.000783719471655786\n",
      "epoch: 5 step: 454, loss is 0.00037364286254160106\n",
      "epoch: 5 step: 455, loss is 0.0049362992867827415\n",
      "epoch: 5 step: 456, loss is 0.14675167202949524\n",
      "epoch: 5 step: 457, loss is 0.004240578040480614\n",
      "epoch: 5 step: 458, loss is 0.020993642508983612\n",
      "epoch: 5 step: 459, loss is 0.00011838020873256028\n",
      "epoch: 5 step: 460, loss is 0.0019593003671616316\n",
      "epoch: 5 step: 461, loss is 0.00797248724848032\n",
      "epoch: 5 step: 462, loss is 0.005336579866707325\n",
      "epoch: 5 step: 463, loss is 0.08398032188415527\n",
      "epoch: 5 step: 464, loss is 0.06852913647890091\n",
      "epoch: 5 step: 465, loss is 0.00016883360513020307\n",
      "epoch: 5 step: 466, loss is 0.008981282822787762\n",
      "epoch: 5 step: 467, loss is 0.023824255913496017\n",
      "epoch: 5 step: 468, loss is 0.005818525794893503\n",
      "epoch: 5 step: 469, loss is 0.13394810259342194\n",
      "epoch: 5 step: 470, loss is 0.11405858397483826\n",
      "epoch: 5 step: 471, loss is 0.06830286979675293\n",
      "epoch: 5 step: 472, loss is 0.007558493409305811\n",
      "epoch: 5 step: 473, loss is 0.03189254552125931\n",
      "epoch: 5 step: 474, loss is 0.0003431454242672771\n",
      "epoch: 5 step: 475, loss is 0.002014091005548835\n",
      "epoch: 5 step: 476, loss is 0.003863269928842783\n",
      "epoch: 5 step: 477, loss is 0.004413765389472246\n",
      "epoch: 5 step: 478, loss is 0.1146279126405716\n",
      "epoch: 5 step: 479, loss is 0.0029074829071760178\n",
      "epoch: 5 step: 480, loss is 0.16011667251586914\n",
      "epoch: 5 step: 481, loss is 0.16566266119480133\n",
      "epoch: 5 step: 482, loss is 0.052578795701265335\n",
      "epoch: 5 step: 483, loss is 0.0411243736743927\n",
      "epoch: 5 step: 484, loss is 0.024265313521027565\n",
      "epoch: 5 step: 485, loss is 0.023121600970625877\n",
      "epoch: 5 step: 486, loss is 0.3010999262332916\n",
      "epoch: 5 step: 487, loss is 0.028657114133238792\n",
      "epoch: 5 step: 488, loss is 0.0028109652921557426\n",
      "epoch: 5 step: 489, loss is 0.019926562905311584\n",
      "epoch: 5 step: 490, loss is 0.06210954487323761\n",
      "epoch: 5 step: 491, loss is 0.0023319765459746122\n",
      "epoch: 5 step: 492, loss is 0.0009526843205094337\n",
      "epoch: 5 step: 493, loss is 0.001254494534805417\n",
      "epoch: 5 step: 494, loss is 0.02842210792005062\n",
      "epoch: 5 step: 495, loss is 0.006776838097721338\n",
      "epoch: 5 step: 496, loss is 0.00024290004512295127\n",
      "epoch: 5 step: 497, loss is 0.0032649601344019175\n",
      "epoch: 5 step: 498, loss is 0.009245331399142742\n",
      "epoch: 5 step: 499, loss is 0.02324812486767769\n",
      "epoch: 5 step: 500, loss is 0.005852915346622467\n",
      "epoch: 5 step: 501, loss is 0.000816407788079232\n",
      "epoch: 5 step: 502, loss is 0.02987685799598694\n",
      "epoch: 5 step: 503, loss is 0.0021787197329103947\n",
      "epoch: 5 step: 504, loss is 0.02135944738984108\n",
      "epoch: 5 step: 505, loss is 0.0006665371474809945\n",
      "epoch: 5 step: 506, loss is 0.056170880794525146\n",
      "epoch: 5 step: 507, loss is 0.026036512106657028\n",
      "epoch: 5 step: 508, loss is 0.003249086206778884\n",
      "epoch: 5 step: 509, loss is 0.009896693751215935\n",
      "epoch: 5 step: 510, loss is 0.0019398486474528909\n",
      "epoch: 5 step: 511, loss is 0.009588128887116909\n",
      "epoch: 5 step: 512, loss is 0.0009588194661773741\n",
      "epoch: 5 step: 513, loss is 0.09451603889465332\n",
      "epoch: 5 step: 514, loss is 0.0006923103355802596\n",
      "epoch: 5 step: 515, loss is 0.09757792204618454\n",
      "epoch: 5 step: 516, loss is 0.001578312716446817\n",
      "epoch: 5 step: 517, loss is 0.0009496186976321042\n",
      "epoch: 5 step: 518, loss is 0.03668307512998581\n",
      "epoch: 5 step: 519, loss is 0.004860845860093832\n",
      "epoch: 5 step: 520, loss is 0.06224477291107178\n",
      "epoch: 5 step: 521, loss is 0.0023995544761419296\n",
      "epoch: 5 step: 522, loss is 0.03228698670864105\n",
      "epoch: 5 step: 523, loss is 0.18931172788143158\n",
      "epoch: 5 step: 524, loss is 0.03334139660000801\n",
      "epoch: 5 step: 525, loss is 0.012569330632686615\n",
      "epoch: 5 step: 526, loss is 0.0002152079250663519\n",
      "epoch: 5 step: 527, loss is 0.1184297651052475\n",
      "epoch: 5 step: 528, loss is 0.0007278850534930825\n",
      "epoch: 5 step: 529, loss is 0.0013650766341015697\n",
      "epoch: 5 step: 530, loss is 0.03770017623901367\n",
      "epoch: 5 step: 531, loss is 0.0040641347877681255\n",
      "epoch: 5 step: 532, loss is 0.0023831482976675034\n",
      "epoch: 5 step: 533, loss is 0.2714047133922577\n",
      "epoch: 5 step: 534, loss is 0.003839049953967333\n",
      "epoch: 5 step: 535, loss is 0.027605105191469193\n",
      "epoch: 5 step: 536, loss is 0.0463704951107502\n",
      "epoch: 5 step: 537, loss is 0.008241591043770313\n",
      "epoch: 5 step: 538, loss is 0.10140764713287354\n",
      "epoch: 5 step: 539, loss is 0.0495113767683506\n",
      "epoch: 5 step: 540, loss is 0.11657382547855377\n",
      "epoch: 5 step: 541, loss is 0.1433124542236328\n",
      "epoch: 5 step: 542, loss is 0.2932817041873932\n",
      "epoch: 5 step: 543, loss is 0.0008418834768235683\n",
      "epoch: 5 step: 544, loss is 0.012534303590655327\n",
      "epoch: 5 step: 545, loss is 0.17333239316940308\n",
      "epoch: 5 step: 546, loss is 0.005429355427622795\n",
      "epoch: 5 step: 547, loss is 0.021383056417107582\n",
      "epoch: 5 step: 548, loss is 0.16466809809207916\n",
      "epoch: 5 step: 549, loss is 0.0006736039649695158\n",
      "epoch: 5 step: 550, loss is 0.2418787032365799\n",
      "epoch: 5 step: 551, loss is 0.0003391704522073269\n",
      "epoch: 5 step: 552, loss is 0.013466980308294296\n",
      "epoch: 5 step: 553, loss is 0.01805899105966091\n",
      "epoch: 5 step: 554, loss is 0.005040389019995928\n",
      "epoch: 5 step: 555, loss is 0.018182944506406784\n",
      "epoch: 5 step: 556, loss is 0.06902962923049927\n",
      "epoch: 5 step: 557, loss is 0.0014261818723753095\n",
      "epoch: 5 step: 558, loss is 0.007229792419821024\n",
      "epoch: 5 step: 559, loss is 0.0004101691010873765\n",
      "epoch: 5 step: 560, loss is 0.006162827368825674\n",
      "epoch: 5 step: 561, loss is 8.283505303552374e-05\n",
      "epoch: 5 step: 562, loss is 0.007632752880454063\n",
      "epoch: 5 step: 563, loss is 0.005977301392704248\n",
      "epoch: 5 step: 564, loss is 0.0020713747944682837\n",
      "epoch: 5 step: 565, loss is 0.05043433979153633\n",
      "epoch: 5 step: 566, loss is 0.00404339237138629\n",
      "epoch: 5 step: 567, loss is 0.025296321138739586\n",
      "epoch: 5 step: 568, loss is 0.012051215395331383\n",
      "epoch: 5 step: 569, loss is 0.0007171636680141091\n",
      "epoch: 5 step: 570, loss is 0.033043183386325836\n",
      "epoch: 5 step: 571, loss is 0.08204488456249237\n",
      "epoch: 5 step: 572, loss is 0.008365497924387455\n",
      "epoch: 5 step: 573, loss is 0.0019533648155629635\n",
      "epoch: 5 step: 574, loss is 0.049260400235652924\n",
      "epoch: 5 step: 575, loss is 0.0074838087894022465\n",
      "epoch: 5 step: 576, loss is 0.0008806494879536331\n",
      "epoch: 5 step: 577, loss is 0.001993406331166625\n",
      "epoch: 5 step: 578, loss is 0.0009265665430575609\n",
      "epoch: 5 step: 579, loss is 0.13637953996658325\n",
      "epoch: 5 step: 580, loss is 0.0004835964064113796\n",
      "epoch: 5 step: 581, loss is 0.015913188457489014\n",
      "epoch: 5 step: 582, loss is 0.27456727623939514\n",
      "epoch: 5 step: 583, loss is 0.0008516124216839671\n",
      "epoch: 5 step: 584, loss is 0.016134485602378845\n",
      "epoch: 5 step: 585, loss is 0.0026254632975906134\n",
      "epoch: 5 step: 586, loss is 0.09364756941795349\n",
      "epoch: 5 step: 587, loss is 0.008984857238829136\n",
      "epoch: 5 step: 588, loss is 0.0065386369824409485\n",
      "epoch: 5 step: 589, loss is 0.017667416483163834\n",
      "epoch: 5 step: 590, loss is 0.004145082551985979\n",
      "epoch: 5 step: 591, loss is 0.0016857900191098452\n",
      "epoch: 5 step: 592, loss is 0.003923526033759117\n",
      "epoch: 5 step: 593, loss is 0.015664292499423027\n",
      "epoch: 5 step: 594, loss is 0.006106621585786343\n",
      "epoch: 5 step: 595, loss is 0.023002691566944122\n",
      "epoch: 5 step: 596, loss is 0.00045507552567869425\n",
      "epoch: 5 step: 597, loss is 0.0941174253821373\n",
      "epoch: 5 step: 598, loss is 0.014628584496676922\n",
      "epoch: 5 step: 599, loss is 0.0005779463099315763\n",
      "epoch: 5 step: 600, loss is 0.000945134146604687\n",
      "epoch: 5 step: 601, loss is 0.053383227437734604\n",
      "epoch: 5 step: 602, loss is 0.0002611854288261384\n",
      "epoch: 5 step: 603, loss is 0.0015847327886149287\n",
      "epoch: 5 step: 604, loss is 0.005891403649002314\n",
      "epoch: 5 step: 605, loss is 0.0026855228934437037\n",
      "epoch: 5 step: 606, loss is 0.0961519405245781\n",
      "epoch: 5 step: 607, loss is 0.013331098482012749\n",
      "epoch: 5 step: 608, loss is 0.06005886569619179\n",
      "epoch: 5 step: 609, loss is 0.09695973247289658\n",
      "epoch: 5 step: 610, loss is 0.0011320217745378613\n",
      "epoch: 5 step: 611, loss is 0.001955091953277588\n",
      "epoch: 5 step: 612, loss is 0.04057683050632477\n",
      "epoch: 5 step: 613, loss is 0.001276798197068274\n",
      "epoch: 5 step: 614, loss is 0.003553116461262107\n",
      "epoch: 5 step: 615, loss is 0.003951783291995525\n",
      "epoch: 5 step: 616, loss is 0.011498447507619858\n",
      "epoch: 5 step: 617, loss is 0.010262448340654373\n",
      "epoch: 5 step: 618, loss is 0.030457204207777977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 619, loss is 0.008670907467603683\n",
      "epoch: 5 step: 620, loss is 0.02231248840689659\n",
      "epoch: 5 step: 621, loss is 0.0158515777438879\n",
      "epoch: 5 step: 622, loss is 0.06210820749402046\n",
      "epoch: 5 step: 623, loss is 0.15801282227039337\n",
      "epoch: 5 step: 624, loss is 0.01926174759864807\n",
      "epoch: 5 step: 625, loss is 0.23322327435016632\n",
      "epoch: 5 step: 626, loss is 0.0065943170338869095\n",
      "epoch: 5 step: 627, loss is 0.02794639579951763\n",
      "epoch: 5 step: 628, loss is 0.0049029444344341755\n",
      "epoch: 5 step: 629, loss is 0.005200206767767668\n",
      "epoch: 5 step: 630, loss is 0.01912221498787403\n",
      "epoch: 5 step: 631, loss is 0.10373112559318542\n",
      "epoch: 5 step: 632, loss is 0.0013029094552621245\n",
      "epoch: 5 step: 633, loss is 0.0021695424802601337\n",
      "epoch: 5 step: 634, loss is 0.03611382842063904\n",
      "epoch: 5 step: 635, loss is 0.010338205844163895\n",
      "epoch: 5 step: 636, loss is 0.140081524848938\n",
      "epoch: 5 step: 637, loss is 0.014862214215099812\n",
      "epoch: 5 step: 638, loss is 0.003940424881875515\n",
      "epoch: 5 step: 639, loss is 0.01724599115550518\n",
      "epoch: 5 step: 640, loss is 0.034258775413036346\n",
      "epoch: 5 step: 641, loss is 0.0017563182627782226\n",
      "epoch: 5 step: 642, loss is 0.007993033155798912\n",
      "epoch: 5 step: 643, loss is 0.08090554177761078\n",
      "epoch: 5 step: 644, loss is 0.004476641770452261\n",
      "epoch: 5 step: 645, loss is 0.010607926174998283\n",
      "epoch: 5 step: 646, loss is 0.1841956377029419\n",
      "epoch: 5 step: 647, loss is 0.008001893758773804\n",
      "epoch: 5 step: 648, loss is 0.001781633822247386\n",
      "epoch: 5 step: 649, loss is 0.005630534142255783\n",
      "epoch: 5 step: 650, loss is 0.013603504747152328\n",
      "epoch: 5 step: 651, loss is 0.007590287830680609\n",
      "epoch: 5 step: 652, loss is 0.000837078841868788\n",
      "epoch: 5 step: 653, loss is 0.003211868228390813\n",
      "epoch: 5 step: 654, loss is 0.035590868443250656\n",
      "epoch: 5 step: 655, loss is 0.09609805047512054\n",
      "epoch: 5 step: 656, loss is 0.0008480061078444123\n",
      "epoch: 5 step: 657, loss is 0.002374697709456086\n",
      "epoch: 5 step: 658, loss is 0.014200042001903057\n",
      "epoch: 5 step: 659, loss is 0.03629804775118828\n",
      "epoch: 5 step: 660, loss is 0.0005089073092676699\n",
      "epoch: 5 step: 661, loss is 0.0007968564750626683\n",
      "epoch: 5 step: 662, loss is 0.00572708435356617\n",
      "epoch: 5 step: 663, loss is 0.053447991609573364\n",
      "epoch: 5 step: 664, loss is 0.0015429053455591202\n",
      "epoch: 5 step: 665, loss is 8.10223282314837e-05\n",
      "epoch: 5 step: 666, loss is 0.010755117982625961\n",
      "epoch: 5 step: 667, loss is 0.0075428299605846405\n",
      "epoch: 5 step: 668, loss is 0.0064099435694515705\n",
      "epoch: 5 step: 669, loss is 0.011059042997658253\n",
      "epoch: 5 step: 670, loss is 0.0008228709921240807\n",
      "epoch: 5 step: 671, loss is 0.0013416700530797243\n",
      "epoch: 5 step: 672, loss is 0.004638866055756807\n",
      "epoch: 5 step: 673, loss is 0.0028704972937703133\n",
      "epoch: 5 step: 674, loss is 0.1247062012553215\n",
      "epoch: 5 step: 675, loss is 0.0033243244979530573\n",
      "epoch: 5 step: 676, loss is 0.012924914248287678\n",
      "epoch: 5 step: 677, loss is 0.05039680376648903\n",
      "epoch: 5 step: 678, loss is 0.026957852765917778\n",
      "epoch: 5 step: 679, loss is 0.005850173532962799\n",
      "epoch: 5 step: 680, loss is 0.0009784894064068794\n",
      "epoch: 5 step: 681, loss is 0.012210485525429249\n",
      "epoch: 5 step: 682, loss is 0.009922455064952374\n",
      "epoch: 5 step: 683, loss is 0.0011634518159553409\n",
      "epoch: 5 step: 684, loss is 0.03519037738442421\n",
      "epoch: 5 step: 685, loss is 0.05567396432161331\n",
      "epoch: 5 step: 686, loss is 0.001504880259744823\n",
      "epoch: 5 step: 687, loss is 0.1497754454612732\n",
      "epoch: 5 step: 688, loss is 0.01535339467227459\n",
      "epoch: 5 step: 689, loss is 0.0140030262991786\n",
      "epoch: 5 step: 690, loss is 0.09401772171258926\n",
      "epoch: 5 step: 691, loss is 0.012768279761075974\n",
      "epoch: 5 step: 692, loss is 0.03377224877476692\n",
      "epoch: 5 step: 693, loss is 0.0015546760987490416\n",
      "epoch: 5 step: 694, loss is 0.00033777760108932853\n",
      "epoch: 5 step: 695, loss is 0.0007064459496177733\n",
      "epoch: 5 step: 696, loss is 0.0637037381529808\n",
      "epoch: 5 step: 697, loss is 0.003225667169317603\n",
      "epoch: 5 step: 698, loss is 0.22151924669742584\n",
      "epoch: 5 step: 699, loss is 0.005244059022516012\n",
      "epoch: 5 step: 700, loss is 0.02410147152841091\n",
      "epoch: 5 step: 701, loss is 0.010333403944969177\n",
      "epoch: 5 step: 702, loss is 0.13888278603553772\n",
      "epoch: 5 step: 703, loss is 0.0017084109131246805\n",
      "epoch: 5 step: 704, loss is 0.005654277745634317\n",
      "epoch: 5 step: 705, loss is 0.11265287548303604\n",
      "epoch: 5 step: 706, loss is 0.010940943844616413\n",
      "epoch: 5 step: 707, loss is 0.28081655502319336\n",
      "epoch: 5 step: 708, loss is 0.000917905243113637\n",
      "epoch: 5 step: 709, loss is 0.06782947480678558\n",
      "epoch: 5 step: 710, loss is 0.23950700461864471\n",
      "epoch: 5 step: 711, loss is 0.12868979573249817\n",
      "epoch: 5 step: 712, loss is 0.011108336970210075\n",
      "epoch: 5 step: 713, loss is 0.08688319474458694\n",
      "epoch: 5 step: 714, loss is 0.0017896408680826426\n",
      "epoch: 5 step: 715, loss is 0.003175055142492056\n",
      "epoch: 5 step: 716, loss is 0.03373979404568672\n",
      "epoch: 5 step: 717, loss is 0.04382586479187012\n",
      "epoch: 5 step: 718, loss is 0.12703180313110352\n",
      "epoch: 5 step: 719, loss is 0.01673409342765808\n",
      "epoch: 5 step: 720, loss is 0.009722244925796986\n",
      "epoch: 5 step: 721, loss is 0.209122434258461\n",
      "epoch: 5 step: 722, loss is 0.017764830961823463\n",
      "epoch: 5 step: 723, loss is 0.03733732923865318\n",
      "epoch: 5 step: 724, loss is 0.1409430354833603\n",
      "epoch: 5 step: 725, loss is 0.0009452340309508145\n",
      "epoch: 5 step: 726, loss is 0.0004106681444682181\n",
      "epoch: 5 step: 727, loss is 0.00039460259722545743\n",
      "epoch: 5 step: 728, loss is 0.04483673349022865\n",
      "epoch: 5 step: 729, loss is 0.041079163551330566\n",
      "epoch: 5 step: 730, loss is 0.014975840225815773\n",
      "epoch: 5 step: 731, loss is 0.04891315475106239\n",
      "epoch: 5 step: 732, loss is 0.010588584467768669\n",
      "epoch: 5 step: 733, loss is 0.16448089480400085\n",
      "epoch: 5 step: 734, loss is 0.0008109752088785172\n",
      "epoch: 5 step: 735, loss is 0.006163900718092918\n",
      "epoch: 5 step: 736, loss is 0.04339173808693886\n",
      "epoch: 5 step: 737, loss is 0.012358994223177433\n",
      "epoch: 5 step: 738, loss is 0.027754738926887512\n",
      "epoch: 5 step: 739, loss is 0.1304033100605011\n",
      "epoch: 5 step: 740, loss is 0.003859685268253088\n",
      "epoch: 5 step: 741, loss is 0.0008924826397560537\n",
      "epoch: 5 step: 742, loss is 0.010876251384615898\n",
      "epoch: 5 step: 743, loss is 0.2796839773654938\n",
      "epoch: 5 step: 744, loss is 0.048820167779922485\n",
      "epoch: 5 step: 745, loss is 0.006118255667388439\n",
      "epoch: 5 step: 746, loss is 0.004566649440675974\n",
      "epoch: 5 step: 747, loss is 0.22320683300495148\n",
      "epoch: 5 step: 748, loss is 0.0738399401307106\n",
      "epoch: 5 step: 749, loss is 0.01580897718667984\n",
      "epoch: 5 step: 750, loss is 0.001534061855636537\n",
      "epoch: 5 step: 751, loss is 0.019804425537586212\n",
      "epoch: 5 step: 752, loss is 0.11989981681108475\n",
      "epoch: 5 step: 753, loss is 0.039058905094861984\n",
      "epoch: 5 step: 754, loss is 0.008821618743240833\n",
      "epoch: 5 step: 755, loss is 0.019462499767541885\n",
      "epoch: 5 step: 756, loss is 0.0028984195087105036\n",
      "epoch: 5 step: 757, loss is 0.024549949914216995\n",
      "epoch: 5 step: 758, loss is 0.00529889902099967\n",
      "epoch: 5 step: 759, loss is 0.08404857665300369\n",
      "epoch: 5 step: 760, loss is 0.0004434948496054858\n",
      "epoch: 5 step: 761, loss is 0.054912637919187546\n",
      "epoch: 5 step: 762, loss is 0.006994755007326603\n",
      "epoch: 5 step: 763, loss is 0.0007831592811271548\n",
      "epoch: 5 step: 764, loss is 0.02935347706079483\n",
      "epoch: 5 step: 765, loss is 0.04476514086127281\n",
      "epoch: 5 step: 766, loss is 0.0020566002931445837\n",
      "epoch: 5 step: 767, loss is 0.000609390321187675\n",
      "epoch: 5 step: 768, loss is 0.1276477575302124\n",
      "epoch: 5 step: 769, loss is 0.016928400844335556\n",
      "epoch: 5 step: 770, loss is 0.018751759082078934\n",
      "epoch: 5 step: 771, loss is 0.006429048255085945\n",
      "epoch: 5 step: 772, loss is 0.0010994120966643095\n",
      "epoch: 5 step: 773, loss is 0.003964029718190432\n",
      "epoch: 5 step: 774, loss is 0.019728010520339012\n",
      "epoch: 5 step: 775, loss is 0.02112233079969883\n",
      "epoch: 5 step: 776, loss is 0.09154308587312698\n",
      "epoch: 5 step: 777, loss is 0.06062854081392288\n",
      "epoch: 5 step: 778, loss is 0.04878630116581917\n",
      "epoch: 5 step: 779, loss is 0.00966591201722622\n",
      "epoch: 5 step: 780, loss is 0.006182933691889048\n",
      "epoch: 5 step: 781, loss is 0.015807803720235825\n",
      "epoch: 5 step: 782, loss is 0.0037911043036729097\n",
      "epoch: 5 step: 783, loss is 0.07345917820930481\n",
      "epoch: 5 step: 784, loss is 0.020899193361401558\n",
      "epoch: 5 step: 785, loss is 0.03780993074178696\n",
      "epoch: 5 step: 786, loss is 0.11157136410474777\n",
      "epoch: 5 step: 787, loss is 0.004346457775682211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 788, loss is 0.007837073877453804\n",
      "epoch: 5 step: 789, loss is 0.09087207913398743\n",
      "epoch: 5 step: 790, loss is 0.007770446594804525\n",
      "epoch: 5 step: 791, loss is 0.03194408118724823\n",
      "epoch: 5 step: 792, loss is 0.004193945787847042\n",
      "epoch: 5 step: 793, loss is 0.03480374068021774\n",
      "epoch: 5 step: 794, loss is 0.001213607843965292\n",
      "epoch: 5 step: 795, loss is 0.015224414877593517\n",
      "epoch: 5 step: 796, loss is 0.04461431875824928\n",
      "epoch: 5 step: 797, loss is 0.010102380998432636\n",
      "epoch: 5 step: 798, loss is 0.004538063891232014\n",
      "epoch: 5 step: 799, loss is 0.005813180934637785\n",
      "epoch: 5 step: 800, loss is 0.03160683065652847\n",
      "epoch: 5 step: 801, loss is 0.21928295493125916\n",
      "epoch: 5 step: 802, loss is 0.0014683203771710396\n",
      "epoch: 5 step: 803, loss is 0.06835072487592697\n",
      "epoch: 5 step: 804, loss is 0.0005457430379465222\n",
      "epoch: 5 step: 805, loss is 0.005868277512490749\n",
      "epoch: 5 step: 806, loss is 0.011891414411365986\n",
      "epoch: 5 step: 807, loss is 0.00037365418393164873\n",
      "epoch: 5 step: 808, loss is 0.03575333580374718\n",
      "epoch: 5 step: 809, loss is 0.0006934495759196579\n",
      "epoch: 5 step: 810, loss is 0.021449830383062363\n",
      "epoch: 5 step: 811, loss is 0.06912167370319366\n",
      "epoch: 5 step: 812, loss is 0.002174686873331666\n",
      "epoch: 5 step: 813, loss is 0.001890074578113854\n",
      "epoch: 5 step: 814, loss is 0.00017650927475187927\n",
      "epoch: 5 step: 815, loss is 0.2432442307472229\n",
      "epoch: 5 step: 816, loss is 0.11712908744812012\n",
      "epoch: 5 step: 817, loss is 0.008334925398230553\n",
      "epoch: 5 step: 818, loss is 0.005749231204390526\n",
      "epoch: 5 step: 819, loss is 0.0006149723776616156\n",
      "epoch: 5 step: 820, loss is 0.001387782278470695\n",
      "epoch: 5 step: 821, loss is 0.005559682380408049\n",
      "epoch: 5 step: 822, loss is 0.010123804211616516\n",
      "epoch: 5 step: 823, loss is 0.10811153799295425\n",
      "epoch: 5 step: 824, loss is 0.005541588179767132\n",
      "epoch: 5 step: 825, loss is 0.01762568950653076\n",
      "epoch: 5 step: 826, loss is 0.0365351140499115\n",
      "epoch: 5 step: 827, loss is 0.005420793313533068\n",
      "epoch: 5 step: 828, loss is 0.013880440965294838\n",
      "epoch: 5 step: 829, loss is 0.025725413113832474\n",
      "epoch: 5 step: 830, loss is 0.006266728509217501\n",
      "epoch: 5 step: 831, loss is 9.058663999894634e-05\n",
      "epoch: 5 step: 832, loss is 0.08229325711727142\n",
      "epoch: 5 step: 833, loss is 0.0055149514228105545\n",
      "epoch: 5 step: 834, loss is 0.008283945731818676\n",
      "epoch: 5 step: 835, loss is 0.07930620014667511\n",
      "epoch: 5 step: 836, loss is 0.010098599828779697\n",
      "epoch: 5 step: 837, loss is 0.16851115226745605\n",
      "epoch: 5 step: 838, loss is 0.005396649241447449\n",
      "epoch: 5 step: 839, loss is 0.003986365161836147\n",
      "epoch: 5 step: 840, loss is 0.19865666329860687\n",
      "epoch: 5 step: 841, loss is 0.004818368703126907\n",
      "epoch: 5 step: 842, loss is 0.03905779868364334\n",
      "epoch: 5 step: 843, loss is 0.0025778638664633036\n",
      "epoch: 5 step: 844, loss is 0.004028425086289644\n",
      "epoch: 5 step: 845, loss is 0.06256872415542603\n",
      "epoch: 5 step: 846, loss is 0.1473415493965149\n",
      "epoch: 5 step: 847, loss is 0.0016662931302562356\n",
      "epoch: 5 step: 848, loss is 0.0031943831127136946\n",
      "epoch: 5 step: 849, loss is 0.00501983892172575\n",
      "epoch: 5 step: 850, loss is 0.014600804075598717\n",
      "epoch: 5 step: 851, loss is 0.1399163156747818\n",
      "epoch: 5 step: 852, loss is 0.0008954533259384334\n",
      "epoch: 5 step: 853, loss is 0.004871911369264126\n",
      "epoch: 5 step: 854, loss is 0.036670487374067307\n",
      "epoch: 5 step: 855, loss is 0.05144151672720909\n",
      "epoch: 5 step: 856, loss is 0.05736799165606499\n",
      "epoch: 5 step: 857, loss is 0.029163353145122528\n",
      "epoch: 5 step: 858, loss is 0.04736945033073425\n",
      "epoch: 5 step: 859, loss is 0.001316035632044077\n",
      "epoch: 5 step: 860, loss is 0.005719590932130814\n",
      "epoch: 5 step: 861, loss is 0.007639758288860321\n",
      "epoch: 5 step: 862, loss is 0.0013050895649939775\n",
      "epoch: 5 step: 863, loss is 0.012465444393455982\n",
      "epoch: 5 step: 864, loss is 0.009664949029684067\n",
      "epoch: 5 step: 865, loss is 0.0004907214897684753\n",
      "epoch: 5 step: 866, loss is 0.0013894739095121622\n",
      "epoch: 5 step: 867, loss is 0.005433698184788227\n",
      "epoch: 5 step: 868, loss is 0.0009738651569932699\n",
      "epoch: 5 step: 869, loss is 0.017017416656017303\n",
      "epoch: 5 step: 870, loss is 0.009362228214740753\n",
      "epoch: 5 step: 871, loss is 0.00234197243116796\n",
      "epoch: 5 step: 872, loss is 0.02085041254758835\n",
      "epoch: 5 step: 873, loss is 0.0004845190851483494\n",
      "epoch: 5 step: 874, loss is 0.021192289888858795\n",
      "epoch: 5 step: 875, loss is 0.02778543159365654\n",
      "epoch: 5 step: 876, loss is 0.004924386739730835\n",
      "epoch: 5 step: 877, loss is 0.04645103961229324\n",
      "epoch: 5 step: 878, loss is 0.002254983177408576\n",
      "epoch: 5 step: 879, loss is 0.014571093022823334\n",
      "epoch: 5 step: 880, loss is 0.0005129783530719578\n",
      "epoch: 5 step: 881, loss is 0.022025099024176598\n",
      "epoch: 5 step: 882, loss is 0.0011857374338433146\n",
      "epoch: 5 step: 883, loss is 0.0014650013763457537\n",
      "epoch: 5 step: 884, loss is 0.0003816763637587428\n",
      "epoch: 5 step: 885, loss is 0.00036434413050301373\n",
      "epoch: 5 step: 886, loss is 0.10586929321289062\n",
      "epoch: 5 step: 887, loss is 0.0020208756905049086\n",
      "epoch: 5 step: 888, loss is 0.001366081298328936\n",
      "epoch: 5 step: 889, loss is 0.12148090451955795\n",
      "epoch: 5 step: 890, loss is 0.00017940708494279534\n",
      "epoch: 5 step: 891, loss is 0.004864807706326246\n",
      "epoch: 5 step: 892, loss is 0.00024305017723236233\n",
      "epoch: 5 step: 893, loss is 0.15221813321113586\n",
      "epoch: 5 step: 894, loss is 0.10193945467472076\n",
      "epoch: 5 step: 895, loss is 0.04037540778517723\n",
      "epoch: 5 step: 896, loss is 0.002785252872854471\n",
      "epoch: 5 step: 897, loss is 0.001459296327084303\n",
      "epoch: 5 step: 898, loss is 0.028551148250699043\n",
      "epoch: 5 step: 899, loss is 0.09696832299232483\n",
      "epoch: 5 step: 900, loss is 0.012330297380685806\n",
      "epoch: 5 step: 901, loss is 0.006145197432488203\n",
      "epoch: 5 step: 902, loss is 0.17696340382099152\n",
      "epoch: 5 step: 903, loss is 0.005087819416075945\n",
      "epoch: 5 step: 904, loss is 0.07396816462278366\n",
      "epoch: 5 step: 905, loss is 0.07187828421592712\n",
      "epoch: 5 step: 906, loss is 0.043048180639743805\n",
      "epoch: 5 step: 907, loss is 0.0010172431357204914\n",
      "epoch: 5 step: 908, loss is 0.009533444419503212\n",
      "epoch: 5 step: 909, loss is 0.021520541980862617\n",
      "epoch: 5 step: 910, loss is 0.08311455696821213\n",
      "epoch: 5 step: 911, loss is 0.052108049392700195\n",
      "epoch: 5 step: 912, loss is 0.03453412652015686\n",
      "epoch: 5 step: 913, loss is 0.002350202761590481\n",
      "epoch: 5 step: 914, loss is 0.04697645083069801\n",
      "epoch: 5 step: 915, loss is 0.06415749341249466\n",
      "epoch: 5 step: 916, loss is 0.001729354728013277\n",
      "epoch: 5 step: 917, loss is 0.0017219609580934048\n",
      "epoch: 5 step: 918, loss is 0.01646302081644535\n",
      "epoch: 5 step: 919, loss is 0.04623742774128914\n",
      "epoch: 5 step: 920, loss is 0.0032794252038002014\n",
      "epoch: 5 step: 921, loss is 0.0005081584095023572\n",
      "epoch: 5 step: 922, loss is 0.12798596918582916\n",
      "epoch: 5 step: 923, loss is 0.31189948320388794\n",
      "epoch: 5 step: 924, loss is 0.0019100683275610209\n",
      "epoch: 5 step: 925, loss is 0.1067868024110794\n",
      "epoch: 5 step: 926, loss is 0.009920907206833363\n",
      "epoch: 5 step: 927, loss is 0.002821843372657895\n",
      "epoch: 5 step: 928, loss is 0.06862950325012207\n",
      "epoch: 5 step: 929, loss is 0.07157225906848907\n",
      "epoch: 5 step: 930, loss is 0.10962779074907303\n",
      "epoch: 5 step: 931, loss is 0.030098503455519676\n",
      "epoch: 5 step: 932, loss is 0.0018146401271224022\n",
      "epoch: 5 step: 933, loss is 0.012653036043047905\n",
      "epoch: 5 step: 934, loss is 0.0029441823717206717\n",
      "epoch: 5 step: 935, loss is 0.002827384974807501\n",
      "epoch: 5 step: 936, loss is 0.106839120388031\n",
      "epoch: 5 step: 937, loss is 0.03483005613088608\n",
      "epoch: 5 step: 938, loss is 0.020607246086001396\n",
      "epoch: 5 step: 939, loss is 0.03616667166352272\n",
      "epoch: 5 step: 940, loss is 0.03700685128569603\n",
      "epoch: 5 step: 941, loss is 0.12526701390743256\n",
      "epoch: 5 step: 942, loss is 0.0032996635418385267\n",
      "epoch: 5 step: 943, loss is 0.01020235288888216\n",
      "epoch: 5 step: 944, loss is 0.0028391326777637005\n",
      "epoch: 5 step: 945, loss is 0.008362950757145882\n",
      "epoch: 5 step: 946, loss is 0.0026274051051586866\n",
      "epoch: 5 step: 947, loss is 0.014333564788103104\n",
      "epoch: 5 step: 948, loss is 0.0007506699184887111\n",
      "epoch: 5 step: 949, loss is 0.0006517918664030731\n",
      "epoch: 5 step: 950, loss is 0.0005027762963436544\n",
      "epoch: 5 step: 951, loss is 0.007400557864457369\n",
      "epoch: 5 step: 952, loss is 0.013154401443898678\n",
      "epoch: 5 step: 953, loss is 0.11085479706525803\n",
      "epoch: 5 step: 954, loss is 0.0004389884998090565\n",
      "epoch: 5 step: 955, loss is 0.0014522898709401488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 956, loss is 0.08702664822340012\n",
      "epoch: 5 step: 957, loss is 0.0020135638769716024\n",
      "epoch: 5 step: 958, loss is 0.003624001983553171\n",
      "epoch: 5 step: 959, loss is 0.02388700470328331\n",
      "epoch: 5 step: 960, loss is 0.003633559215813875\n",
      "epoch: 5 step: 961, loss is 0.007607137784361839\n",
      "epoch: 5 step: 962, loss is 0.015502972528338432\n",
      "epoch: 5 step: 963, loss is 0.011045390740036964\n",
      "epoch: 5 step: 964, loss is 0.0006362028070725501\n",
      "epoch: 5 step: 965, loss is 0.06367915123701096\n",
      "epoch: 5 step: 966, loss is 0.06957307457923889\n",
      "epoch: 5 step: 967, loss is 0.036307260394096375\n",
      "epoch: 5 step: 968, loss is 0.0011216015554964542\n",
      "epoch: 5 step: 969, loss is 0.00803491473197937\n",
      "epoch: 5 step: 970, loss is 0.0012674223398789763\n",
      "epoch: 5 step: 971, loss is 0.006852737162262201\n",
      "epoch: 5 step: 972, loss is 0.0022289352491497993\n",
      "epoch: 5 step: 973, loss is 0.002224390394985676\n",
      "epoch: 5 step: 974, loss is 0.025613415986299515\n",
      "epoch: 5 step: 975, loss is 0.10216417163610458\n",
      "epoch: 5 step: 976, loss is 0.017147555947303772\n",
      "epoch: 5 step: 977, loss is 0.3279309570789337\n",
      "epoch: 5 step: 978, loss is 0.00896658468991518\n",
      "epoch: 5 step: 979, loss is 0.004446063656359911\n",
      "epoch: 5 step: 980, loss is 0.18300172686576843\n",
      "epoch: 5 step: 981, loss is 0.13646996021270752\n",
      "epoch: 5 step: 982, loss is 0.006572835147380829\n",
      "epoch: 5 step: 983, loss is 0.015730882063508034\n",
      "epoch: 5 step: 984, loss is 0.1056685671210289\n",
      "epoch: 5 step: 985, loss is 0.001717567560262978\n",
      "epoch: 5 step: 986, loss is 0.00520965363830328\n",
      "epoch: 5 step: 987, loss is 0.0024921277072280645\n",
      "epoch: 5 step: 988, loss is 0.006479788105934858\n",
      "epoch: 5 step: 989, loss is 0.02648262120783329\n",
      "epoch: 5 step: 990, loss is 0.0011187929194420576\n",
      "epoch: 5 step: 991, loss is 0.00038554021739400923\n",
      "epoch: 5 step: 992, loss is 0.009738391265273094\n",
      "epoch: 5 step: 993, loss is 0.018075784668326378\n",
      "epoch: 5 step: 994, loss is 0.03303177282214165\n",
      "epoch: 5 step: 995, loss is 0.006464365404099226\n",
      "epoch: 5 step: 996, loss is 0.01353857945650816\n",
      "epoch: 5 step: 997, loss is 0.23414388298988342\n",
      "epoch: 5 step: 998, loss is 0.0029960940591990948\n",
      "epoch: 5 step: 999, loss is 0.09432505816221237\n",
      "epoch: 5 step: 1000, loss is 0.003208660054951906\n",
      "epoch: 5 step: 1001, loss is 0.0003254148759879172\n",
      "epoch: 5 step: 1002, loss is 0.0008381934021599591\n",
      "epoch: 5 step: 1003, loss is 0.007107357494533062\n",
      "epoch: 5 step: 1004, loss is 0.0005103176808916032\n",
      "epoch: 5 step: 1005, loss is 0.0016229572938755155\n",
      "epoch: 5 step: 1006, loss is 0.1853393316268921\n",
      "epoch: 5 step: 1007, loss is 0.013375652022659779\n",
      "epoch: 5 step: 1008, loss is 0.0004235369269736111\n",
      "epoch: 5 step: 1009, loss is 0.0020310503896325827\n",
      "epoch: 5 step: 1010, loss is 0.00036896837991662323\n",
      "epoch: 5 step: 1011, loss is 0.06429950892925262\n",
      "epoch: 5 step: 1012, loss is 0.007273444440215826\n",
      "epoch: 5 step: 1013, loss is 0.006883426569402218\n",
      "epoch: 5 step: 1014, loss is 0.001663836883381009\n",
      "epoch: 5 step: 1015, loss is 0.003304725745692849\n",
      "epoch: 5 step: 1016, loss is 0.013144420459866524\n",
      "epoch: 5 step: 1017, loss is 0.07154520601034164\n",
      "epoch: 5 step: 1018, loss is 0.013690639287233353\n",
      "epoch: 5 step: 1019, loss is 0.017566842958331108\n",
      "epoch: 5 step: 1020, loss is 0.0018845637096092105\n",
      "epoch: 5 step: 1021, loss is 0.07457336038351059\n",
      "epoch: 5 step: 1022, loss is 0.06672389805316925\n",
      "epoch: 5 step: 1023, loss is 0.0015336170326918364\n",
      "epoch: 5 step: 1024, loss is 0.020090127363801003\n",
      "epoch: 5 step: 1025, loss is 0.004704617895185947\n",
      "epoch: 5 step: 1026, loss is 0.0018551056273281574\n",
      "epoch: 5 step: 1027, loss is 0.002836581552401185\n",
      "epoch: 5 step: 1028, loss is 0.05861857160925865\n",
      "epoch: 5 step: 1029, loss is 0.002311450196430087\n",
      "epoch: 5 step: 1030, loss is 0.07315216213464737\n",
      "epoch: 5 step: 1031, loss is 0.0011267795925959945\n",
      "epoch: 5 step: 1032, loss is 0.0036063487641513348\n",
      "epoch: 5 step: 1033, loss is 0.0028448603115975857\n",
      "epoch: 5 step: 1034, loss is 0.00032780133187770844\n",
      "epoch: 5 step: 1035, loss is 0.03148377686738968\n",
      "epoch: 5 step: 1036, loss is 0.00037266354775056243\n",
      "epoch: 5 step: 1037, loss is 0.018982455134391785\n",
      "epoch: 5 step: 1038, loss is 0.004660668782889843\n",
      "epoch: 5 step: 1039, loss is 0.0758131667971611\n",
      "epoch: 5 step: 1040, loss is 0.0012681371299549937\n",
      "epoch: 5 step: 1041, loss is 0.0037627271376550198\n",
      "epoch: 5 step: 1042, loss is 0.01542968861758709\n",
      "epoch: 5 step: 1043, loss is 0.0014850780135020614\n",
      "epoch: 5 step: 1044, loss is 0.0005336154717952013\n",
      "epoch: 5 step: 1045, loss is 0.003252586117014289\n",
      "epoch: 5 step: 1046, loss is 0.0021642677020281553\n",
      "epoch: 5 step: 1047, loss is 0.001151772914454341\n",
      "epoch: 5 step: 1048, loss is 0.06215512380003929\n",
      "epoch: 5 step: 1049, loss is 0.026070542633533478\n",
      "epoch: 5 step: 1050, loss is 0.0019550009164959192\n",
      "epoch: 5 step: 1051, loss is 0.07510222494602203\n",
      "epoch: 5 step: 1052, loss is 0.3213179111480713\n",
      "epoch: 5 step: 1053, loss is 0.001810348010621965\n",
      "epoch: 5 step: 1054, loss is 0.0011436606291681528\n",
      "epoch: 5 step: 1055, loss is 0.004694931209087372\n",
      "epoch: 5 step: 1056, loss is 0.004620156716555357\n",
      "epoch: 5 step: 1057, loss is 0.046895626932382584\n",
      "epoch: 5 step: 1058, loss is 0.02054266445338726\n",
      "epoch: 5 step: 1059, loss is 0.004219978116452694\n",
      "epoch: 5 step: 1060, loss is 0.03256918862462044\n",
      "epoch: 5 step: 1061, loss is 0.01959194988012314\n",
      "epoch: 5 step: 1062, loss is 0.008290247991681099\n",
      "epoch: 5 step: 1063, loss is 0.0020255809649825096\n",
      "epoch: 5 step: 1064, loss is 0.0012225399259477854\n",
      "epoch: 5 step: 1065, loss is 0.026943707838654518\n",
      "epoch: 5 step: 1066, loss is 0.0009636272443458438\n",
      "epoch: 5 step: 1067, loss is 0.011198840104043484\n",
      "epoch: 5 step: 1068, loss is 0.003508924040943384\n",
      "epoch: 5 step: 1069, loss is 0.19307594001293182\n",
      "epoch: 5 step: 1070, loss is 0.2059188038110733\n",
      "epoch: 5 step: 1071, loss is 0.0006139161996543407\n",
      "epoch: 5 step: 1072, loss is 0.00260886806063354\n",
      "epoch: 5 step: 1073, loss is 0.11122786998748779\n",
      "epoch: 5 step: 1074, loss is 0.0046501546166837215\n",
      "epoch: 5 step: 1075, loss is 0.012079393491148949\n",
      "epoch: 5 step: 1076, loss is 0.15326039493083954\n",
      "epoch: 5 step: 1077, loss is 0.01810617372393608\n",
      "epoch: 5 step: 1078, loss is 0.07502917945384979\n",
      "epoch: 5 step: 1079, loss is 0.0023556770756840706\n",
      "epoch: 5 step: 1080, loss is 0.011415564455091953\n",
      "epoch: 5 step: 1081, loss is 0.0006293013575486839\n",
      "epoch: 5 step: 1082, loss is 0.009122874587774277\n",
      "epoch: 5 step: 1083, loss is 0.0029164317529648542\n",
      "epoch: 5 step: 1084, loss is 0.01616276055574417\n",
      "epoch: 5 step: 1085, loss is 0.00034405593760311604\n",
      "epoch: 5 step: 1086, loss is 0.020309319719672203\n",
      "epoch: 5 step: 1087, loss is 0.012577644549310207\n",
      "epoch: 5 step: 1088, loss is 0.0013230780605226755\n",
      "epoch: 5 step: 1089, loss is 0.12428433448076248\n",
      "epoch: 5 step: 1090, loss is 0.0508938692510128\n",
      "epoch: 5 step: 1091, loss is 0.002985939383506775\n",
      "epoch: 5 step: 1092, loss is 0.0004854013095609844\n",
      "epoch: 5 step: 1093, loss is 0.0009584439685568213\n",
      "epoch: 5 step: 1094, loss is 0.0023901378735899925\n",
      "epoch: 5 step: 1095, loss is 0.005438214633613825\n",
      "epoch: 5 step: 1096, loss is 0.007688502781093121\n",
      "epoch: 5 step: 1097, loss is 0.03300072252750397\n",
      "epoch: 5 step: 1098, loss is 0.007364825811237097\n",
      "epoch: 5 step: 1099, loss is 0.001915246364660561\n",
      "epoch: 5 step: 1100, loss is 0.02242225967347622\n",
      "epoch: 5 step: 1101, loss is 0.0003777186502702534\n",
      "epoch: 5 step: 1102, loss is 0.009236401878297329\n",
      "epoch: 5 step: 1103, loss is 0.00015900892321951687\n",
      "epoch: 5 step: 1104, loss is 0.00289352354593575\n",
      "epoch: 5 step: 1105, loss is 0.002189370570704341\n",
      "epoch: 5 step: 1106, loss is 0.029324539005756378\n",
      "epoch: 5 step: 1107, loss is 0.0009579749894328415\n",
      "epoch: 5 step: 1108, loss is 0.1349301040172577\n",
      "epoch: 5 step: 1109, loss is 0.006887021940201521\n",
      "epoch: 5 step: 1110, loss is 0.004900970961898565\n",
      "epoch: 5 step: 1111, loss is 0.01995602808892727\n",
      "epoch: 5 step: 1112, loss is 0.07917820662260056\n",
      "epoch: 5 step: 1113, loss is 0.0004283828311599791\n",
      "epoch: 5 step: 1114, loss is 0.012022246606647968\n",
      "epoch: 5 step: 1115, loss is 0.018176399171352386\n",
      "epoch: 5 step: 1116, loss is 0.0042054308578372\n",
      "epoch: 5 step: 1117, loss is 0.0065618553198874\n",
      "epoch: 5 step: 1118, loss is 0.001649071229621768\n",
      "epoch: 5 step: 1119, loss is 0.006195271387696266\n",
      "epoch: 5 step: 1120, loss is 0.13279037177562714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 1121, loss is 0.12827828526496887\n",
      "epoch: 5 step: 1122, loss is 0.00883079320192337\n",
      "epoch: 5 step: 1123, loss is 0.0002553238591644913\n",
      "epoch: 5 step: 1124, loss is 0.004163836594671011\n",
      "epoch: 5 step: 1125, loss is 0.09434330463409424\n",
      "epoch: 5 step: 1126, loss is 0.0013321501901373267\n",
      "epoch: 5 step: 1127, loss is 0.00017830097931437194\n",
      "epoch: 5 step: 1128, loss is 0.1383735090494156\n",
      "epoch: 5 step: 1129, loss is 0.0005635810666717589\n",
      "epoch: 5 step: 1130, loss is 0.014338122680783272\n",
      "epoch: 5 step: 1131, loss is 0.0007638480747118592\n",
      "epoch: 5 step: 1132, loss is 0.009324325248599052\n",
      "epoch: 5 step: 1133, loss is 0.04820241779088974\n",
      "epoch: 5 step: 1134, loss is 0.0054915901273489\n",
      "epoch: 5 step: 1135, loss is 0.0007648704922758043\n",
      "epoch: 5 step: 1136, loss is 0.0011239154264330864\n",
      "epoch: 5 step: 1137, loss is 0.0029066691640764475\n",
      "epoch: 5 step: 1138, loss is 0.0011738414177671075\n",
      "epoch: 5 step: 1139, loss is 0.009281900711357594\n",
      "epoch: 5 step: 1140, loss is 0.05049433186650276\n",
      "epoch: 5 step: 1141, loss is 0.049371737986803055\n",
      "epoch: 5 step: 1142, loss is 0.02543604001402855\n",
      "epoch: 5 step: 1143, loss is 0.0666896402835846\n",
      "epoch: 5 step: 1144, loss is 0.07166580855846405\n",
      "epoch: 5 step: 1145, loss is 0.05586908012628555\n",
      "epoch: 5 step: 1146, loss is 0.0018605053192004561\n",
      "epoch: 5 step: 1147, loss is 0.002861541463062167\n",
      "epoch: 5 step: 1148, loss is 0.09974315017461777\n",
      "epoch: 5 step: 1149, loss is 0.26493021845817566\n",
      "epoch: 5 step: 1150, loss is 0.0031480896286666393\n",
      "epoch: 5 step: 1151, loss is 0.19256003201007843\n",
      "epoch: 5 step: 1152, loss is 0.002527025295421481\n",
      "epoch: 5 step: 1153, loss is 0.00138402811717242\n",
      "epoch: 5 step: 1154, loss is 0.0068641840480268\n",
      "epoch: 5 step: 1155, loss is 0.005345209501683712\n",
      "epoch: 5 step: 1156, loss is 0.0036996197886765003\n",
      "epoch: 5 step: 1157, loss is 0.0007298112032003701\n",
      "epoch: 5 step: 1158, loss is 0.03004474565386772\n",
      "epoch: 5 step: 1159, loss is 0.003971116151660681\n",
      "epoch: 5 step: 1160, loss is 0.0005691509577445686\n",
      "epoch: 5 step: 1161, loss is 0.014545943588018417\n",
      "epoch: 5 step: 1162, loss is 0.11583882570266724\n",
      "epoch: 5 step: 1163, loss is 0.04361322894692421\n",
      "epoch: 5 step: 1164, loss is 0.1151842251420021\n",
      "epoch: 5 step: 1165, loss is 8.477608935208991e-05\n",
      "epoch: 5 step: 1166, loss is 0.005561861675232649\n",
      "epoch: 5 step: 1167, loss is 0.17353039979934692\n",
      "epoch: 5 step: 1168, loss is 0.0023084033746272326\n",
      "epoch: 5 step: 1169, loss is 0.0012688528513535857\n",
      "epoch: 5 step: 1170, loss is 0.03607296198606491\n",
      "epoch: 5 step: 1171, loss is 0.074920654296875\n",
      "epoch: 5 step: 1172, loss is 0.0002377983182668686\n",
      "epoch: 5 step: 1173, loss is 0.10442152619361877\n",
      "epoch: 5 step: 1174, loss is 0.0012516429414972663\n",
      "epoch: 5 step: 1175, loss is 0.03489335998892784\n",
      "epoch: 5 step: 1176, loss is 0.0007344601326622069\n",
      "epoch: 5 step: 1177, loss is 0.11401410400867462\n",
      "epoch: 5 step: 1178, loss is 0.0026836588513106108\n",
      "epoch: 5 step: 1179, loss is 0.17921575903892517\n",
      "epoch: 5 step: 1180, loss is 0.0184151791036129\n",
      "epoch: 5 step: 1181, loss is 0.038071148097515106\n",
      "epoch: 5 step: 1182, loss is 0.04369989037513733\n",
      "epoch: 5 step: 1183, loss is 0.04749688878655434\n",
      "epoch: 5 step: 1184, loss is 0.04151462763547897\n",
      "epoch: 5 step: 1185, loss is 0.0003166384994983673\n",
      "epoch: 5 step: 1186, loss is 0.0007675773231312633\n",
      "epoch: 5 step: 1187, loss is 0.019216200336813927\n",
      "epoch: 5 step: 1188, loss is 0.0005582247395068407\n",
      "epoch: 5 step: 1189, loss is 0.037315547466278076\n",
      "epoch: 5 step: 1190, loss is 0.22305874526500702\n",
      "epoch: 5 step: 1191, loss is 0.03572462871670723\n",
      "epoch: 5 step: 1192, loss is 0.008887879550457\n",
      "epoch: 5 step: 1193, loss is 0.0017835297621786594\n",
      "epoch: 5 step: 1194, loss is 0.07511603087186813\n",
      "epoch: 5 step: 1195, loss is 0.08961737155914307\n",
      "epoch: 5 step: 1196, loss is 0.08394620567560196\n",
      "epoch: 5 step: 1197, loss is 0.0013665498699992895\n",
      "epoch: 5 step: 1198, loss is 0.4403417706489563\n",
      "epoch: 5 step: 1199, loss is 0.00030119720031507313\n",
      "epoch: 5 step: 1200, loss is 0.0001932334853336215\n",
      "epoch: 5 step: 1201, loss is 0.002937190467491746\n",
      "epoch: 5 step: 1202, loss is 0.0002527002361603081\n",
      "epoch: 5 step: 1203, loss is 0.03603091090917587\n",
      "epoch: 5 step: 1204, loss is 0.0011019054800271988\n",
      "epoch: 5 step: 1205, loss is 0.18588882684707642\n",
      "epoch: 5 step: 1206, loss is 0.05079711228609085\n",
      "epoch: 5 step: 1207, loss is 0.0025724151637405157\n",
      "epoch: 5 step: 1208, loss is 0.018241679295897484\n",
      "epoch: 5 step: 1209, loss is 0.013601560145616531\n",
      "epoch: 5 step: 1210, loss is 0.06925663352012634\n",
      "epoch: 5 step: 1211, loss is 0.057142604142427444\n",
      "epoch: 5 step: 1212, loss is 0.028590673580765724\n",
      "epoch: 5 step: 1213, loss is 0.07653297483921051\n",
      "epoch: 5 step: 1214, loss is 0.018232375383377075\n",
      "epoch: 5 step: 1215, loss is 0.0008356754551641643\n",
      "epoch: 5 step: 1216, loss is 0.05601118505001068\n",
      "epoch: 5 step: 1217, loss is 0.010495063848793507\n",
      "epoch: 5 step: 1218, loss is 0.16936460137367249\n",
      "epoch: 5 step: 1219, loss is 0.06933464854955673\n",
      "epoch: 5 step: 1220, loss is 0.0011266670189797878\n",
      "epoch: 5 step: 1221, loss is 0.05240754410624504\n",
      "epoch: 5 step: 1222, loss is 0.055722858756780624\n",
      "epoch: 5 step: 1223, loss is 0.0028452840633690357\n",
      "epoch: 5 step: 1224, loss is 0.0014044870622456074\n",
      "epoch: 5 step: 1225, loss is 0.011680765077471733\n",
      "epoch: 5 step: 1226, loss is 0.008809538558125496\n",
      "epoch: 5 step: 1227, loss is 0.01868181675672531\n",
      "epoch: 5 step: 1228, loss is 0.003916812129318714\n",
      "epoch: 5 step: 1229, loss is 0.0389322005212307\n",
      "epoch: 5 step: 1230, loss is 0.006465671584010124\n",
      "epoch: 5 step: 1231, loss is 0.005133582279086113\n",
      "epoch: 5 step: 1232, loss is 0.0025169202126562595\n",
      "epoch: 5 step: 1233, loss is 0.025150449946522713\n",
      "epoch: 5 step: 1234, loss is 0.007751955185085535\n",
      "epoch: 5 step: 1235, loss is 0.0009682050440460443\n",
      "epoch: 5 step: 1236, loss is 0.04465648531913757\n",
      "epoch: 5 step: 1237, loss is 0.006022200919687748\n",
      "epoch: 5 step: 1238, loss is 0.007965605705976486\n",
      "epoch: 5 step: 1239, loss is 0.003684287890791893\n",
      "epoch: 5 step: 1240, loss is 0.014549586921930313\n",
      "epoch: 5 step: 1241, loss is 0.022105688229203224\n",
      "epoch: 5 step: 1242, loss is 0.008512504398822784\n",
      "epoch: 5 step: 1243, loss is 0.0035175306256860495\n",
      "epoch: 5 step: 1244, loss is 0.021369917318224907\n",
      "epoch: 5 step: 1245, loss is 0.001180326216854155\n",
      "epoch: 5 step: 1246, loss is 0.0005043348064646125\n",
      "epoch: 5 step: 1247, loss is 0.03361416235566139\n",
      "epoch: 5 step: 1248, loss is 0.2214699536561966\n",
      "epoch: 5 step: 1249, loss is 0.024544818326830864\n",
      "epoch: 5 step: 1250, loss is 0.004401571117341518\n",
      "epoch: 5 step: 1251, loss is 0.0451786071062088\n",
      "epoch: 5 step: 1252, loss is 0.0041633122600615025\n",
      "epoch: 5 step: 1253, loss is 0.04729275777935982\n",
      "epoch: 5 step: 1254, loss is 0.02650383673608303\n",
      "epoch: 5 step: 1255, loss is 0.00531408004462719\n",
      "epoch: 5 step: 1256, loss is 0.0012466389453038573\n",
      "epoch: 5 step: 1257, loss is 0.0018496494740247726\n",
      "epoch: 5 step: 1258, loss is 0.027640962973237038\n",
      "epoch: 5 step: 1259, loss is 5.646855788654648e-05\n",
      "epoch: 5 step: 1260, loss is 0.09205013513565063\n",
      "epoch: 5 step: 1261, loss is 0.00030230305856093764\n",
      "epoch: 5 step: 1262, loss is 0.08042746782302856\n",
      "epoch: 5 step: 1263, loss is 0.033299755305051804\n",
      "epoch: 5 step: 1264, loss is 0.0020106344018131495\n",
      "epoch: 5 step: 1265, loss is 0.09297291934490204\n",
      "epoch: 5 step: 1266, loss is 0.004044991452246904\n",
      "epoch: 5 step: 1267, loss is 0.02478792890906334\n",
      "epoch: 5 step: 1268, loss is 0.0007129817386157811\n",
      "epoch: 5 step: 1269, loss is 0.0020404260139912367\n",
      "epoch: 5 step: 1270, loss is 0.04518360644578934\n",
      "epoch: 5 step: 1271, loss is 0.051681339740753174\n",
      "epoch: 5 step: 1272, loss is 0.011258257552981377\n",
      "epoch: 5 step: 1273, loss is 0.026958953589200974\n",
      "epoch: 5 step: 1274, loss is 0.0002642761683091521\n",
      "epoch: 5 step: 1275, loss is 0.05127200111746788\n",
      "epoch: 5 step: 1276, loss is 0.12273787707090378\n",
      "epoch: 5 step: 1277, loss is 0.06904914975166321\n",
      "epoch: 5 step: 1278, loss is 0.0005283954087644815\n",
      "epoch: 5 step: 1279, loss is 0.0045369588769972324\n",
      "epoch: 5 step: 1280, loss is 0.00734374113380909\n",
      "epoch: 5 step: 1281, loss is 0.02972608059644699\n",
      "epoch: 5 step: 1282, loss is 0.006420625373721123\n",
      "epoch: 5 step: 1283, loss is 0.01929478533565998\n",
      "epoch: 5 step: 1284, loss is 0.012047282420098782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 1285, loss is 0.0014044578419998288\n",
      "epoch: 5 step: 1286, loss is 0.001766315195709467\n",
      "epoch: 5 step: 1287, loss is 0.0021843756549060345\n",
      "epoch: 5 step: 1288, loss is 0.006905178539454937\n",
      "epoch: 5 step: 1289, loss is 0.0030589301604777575\n",
      "epoch: 5 step: 1290, loss is 0.0857723280787468\n",
      "epoch: 5 step: 1291, loss is 0.014599353075027466\n",
      "epoch: 5 step: 1292, loss is 0.010472428984940052\n",
      "epoch: 5 step: 1293, loss is 0.055514972656965256\n",
      "epoch: 5 step: 1294, loss is 0.030116138979792595\n",
      "epoch: 5 step: 1295, loss is 0.03361686319112778\n",
      "epoch: 5 step: 1296, loss is 0.001253517810255289\n",
      "epoch: 5 step: 1297, loss is 0.05048297345638275\n",
      "epoch: 5 step: 1298, loss is 0.002592116827145219\n",
      "epoch: 5 step: 1299, loss is 0.00020561377459671348\n",
      "epoch: 5 step: 1300, loss is 0.00022766104666516185\n",
      "epoch: 5 step: 1301, loss is 0.002587432274594903\n",
      "epoch: 5 step: 1302, loss is 0.0041294279508292675\n",
      "epoch: 5 step: 1303, loss is 0.00460706977173686\n",
      "epoch: 5 step: 1304, loss is 0.04523550719022751\n",
      "epoch: 5 step: 1305, loss is 0.0004736136179417372\n",
      "epoch: 5 step: 1306, loss is 0.00028213014593347907\n",
      "epoch: 5 step: 1307, loss is 0.004185471683740616\n",
      "epoch: 5 step: 1308, loss is 0.0005968478508293629\n",
      "epoch: 5 step: 1309, loss is 0.014061769470572472\n",
      "epoch: 5 step: 1310, loss is 0.09600810706615448\n",
      "epoch: 5 step: 1311, loss is 0.011282619088888168\n",
      "epoch: 5 step: 1312, loss is 0.0020563043653964996\n",
      "epoch: 5 step: 1313, loss is 0.0021066826302558184\n",
      "epoch: 5 step: 1314, loss is 0.06925351172685623\n",
      "epoch: 5 step: 1315, loss is 0.05617668852210045\n",
      "epoch: 5 step: 1316, loss is 0.0013034245930612087\n",
      "epoch: 5 step: 1317, loss is 0.006523573771119118\n",
      "epoch: 5 step: 1318, loss is 0.0006386545137502253\n",
      "epoch: 5 step: 1319, loss is 0.024467088282108307\n",
      "epoch: 5 step: 1320, loss is 0.0022756338585168123\n",
      "epoch: 5 step: 1321, loss is 0.007484491914510727\n",
      "epoch: 5 step: 1322, loss is 0.051681037992239\n",
      "epoch: 5 step: 1323, loss is 0.0032093580812215805\n",
      "epoch: 5 step: 1324, loss is 0.03351280093193054\n",
      "epoch: 5 step: 1325, loss is 0.005632875952869654\n",
      "epoch: 5 step: 1326, loss is 0.008049665950238705\n",
      "epoch: 5 step: 1327, loss is 0.01073012501001358\n",
      "epoch: 5 step: 1328, loss is 0.0009930311935022473\n",
      "epoch: 5 step: 1329, loss is 0.07322360575199127\n",
      "epoch: 5 step: 1330, loss is 0.01927209086716175\n",
      "epoch: 5 step: 1331, loss is 0.001057117828167975\n",
      "epoch: 5 step: 1332, loss is 0.2522115409374237\n",
      "epoch: 5 step: 1333, loss is 0.09570388495922089\n",
      "epoch: 5 step: 1334, loss is 9.48435117607005e-05\n",
      "epoch: 5 step: 1335, loss is 0.0014505134895443916\n",
      "epoch: 5 step: 1336, loss is 0.0013561409432440996\n",
      "epoch: 5 step: 1337, loss is 0.011596735566854477\n",
      "epoch: 5 step: 1338, loss is 0.011084877885878086\n",
      "epoch: 5 step: 1339, loss is 0.003265666076913476\n",
      "epoch: 5 step: 1340, loss is 0.10616422444581985\n",
      "epoch: 5 step: 1341, loss is 0.0028830808587372303\n",
      "epoch: 5 step: 1342, loss is 0.0798388123512268\n",
      "epoch: 5 step: 1343, loss is 0.0005512578063644469\n",
      "epoch: 5 step: 1344, loss is 0.04909709841012955\n",
      "epoch: 5 step: 1345, loss is 0.0012784821446985006\n",
      "epoch: 5 step: 1346, loss is 0.009828667156398296\n",
      "epoch: 5 step: 1347, loss is 0.06328333914279938\n",
      "epoch: 5 step: 1348, loss is 0.006953728385269642\n",
      "epoch: 5 step: 1349, loss is 0.03179585188627243\n",
      "epoch: 5 step: 1350, loss is 0.08988814800977707\n",
      "epoch: 5 step: 1351, loss is 0.02137160114943981\n",
      "epoch: 5 step: 1352, loss is 0.054253000766038895\n",
      "epoch: 5 step: 1353, loss is 0.0693901777267456\n",
      "epoch: 5 step: 1354, loss is 0.011005142703652382\n",
      "epoch: 5 step: 1355, loss is 0.0005039519746787846\n",
      "epoch: 5 step: 1356, loss is 0.04058407247066498\n",
      "epoch: 5 step: 1357, loss is 0.015379550866782665\n",
      "epoch: 5 step: 1358, loss is 0.0667378380894661\n",
      "epoch: 5 step: 1359, loss is 0.003011148888617754\n",
      "epoch: 5 step: 1360, loss is 0.007402264513075352\n",
      "epoch: 5 step: 1361, loss is 0.007326142396777868\n",
      "epoch: 5 step: 1362, loss is 0.004186384379863739\n",
      "epoch: 5 step: 1363, loss is 0.009321870282292366\n",
      "epoch: 5 step: 1364, loss is 0.12417370826005936\n",
      "epoch: 5 step: 1365, loss is 0.005373731721192598\n",
      "epoch: 5 step: 1366, loss is 0.06664071977138519\n",
      "epoch: 5 step: 1367, loss is 0.009603220969438553\n",
      "epoch: 5 step: 1368, loss is 0.11419081687927246\n",
      "epoch: 5 step: 1369, loss is 0.0008444716222584248\n",
      "epoch: 5 step: 1370, loss is 0.040932875126600266\n",
      "epoch: 5 step: 1371, loss is 0.015314549207687378\n",
      "epoch: 5 step: 1372, loss is 0.011225411668419838\n",
      "epoch: 5 step: 1373, loss is 0.09833963215351105\n",
      "epoch: 5 step: 1374, loss is 0.0049353395588696\n",
      "epoch: 5 step: 1375, loss is 0.01046078372746706\n",
      "epoch: 5 step: 1376, loss is 0.01863112300634384\n",
      "epoch: 5 step: 1377, loss is 0.012259962037205696\n",
      "epoch: 5 step: 1378, loss is 0.19357378780841827\n",
      "epoch: 5 step: 1379, loss is 0.011475163511931896\n",
      "epoch: 5 step: 1380, loss is 0.06949520111083984\n",
      "epoch: 5 step: 1381, loss is 0.0505211316049099\n",
      "epoch: 5 step: 1382, loss is 0.1038447916507721\n",
      "epoch: 5 step: 1383, loss is 0.004807068966329098\n",
      "epoch: 5 step: 1384, loss is 0.006783925462514162\n",
      "epoch: 5 step: 1385, loss is 0.02843206375837326\n",
      "epoch: 5 step: 1386, loss is 0.0004557615611702204\n",
      "epoch: 5 step: 1387, loss is 0.0011932548368349671\n",
      "epoch: 5 step: 1388, loss is 0.014119097031652927\n",
      "epoch: 5 step: 1389, loss is 0.018510054796934128\n",
      "epoch: 5 step: 1390, loss is 0.0007302745361812413\n",
      "epoch: 5 step: 1391, loss is 0.0033191756810992956\n",
      "epoch: 5 step: 1392, loss is 0.0022932386491447687\n",
      "epoch: 5 step: 1393, loss is 0.003388926386833191\n",
      "epoch: 5 step: 1394, loss is 0.09972184896469116\n",
      "epoch: 5 step: 1395, loss is 0.03937828168272972\n",
      "epoch: 5 step: 1396, loss is 0.016819151118397713\n",
      "epoch: 5 step: 1397, loss is 0.0012284853728488088\n",
      "epoch: 5 step: 1398, loss is 0.046601552516222\n",
      "epoch: 5 step: 1399, loss is 0.1855214536190033\n",
      "epoch: 5 step: 1400, loss is 0.002047969726845622\n",
      "epoch: 5 step: 1401, loss is 0.001090239267796278\n",
      "epoch: 5 step: 1402, loss is 0.004647298716008663\n",
      "epoch: 5 step: 1403, loss is 0.0022715108934789896\n",
      "epoch: 5 step: 1404, loss is 0.0032966190483421087\n",
      "epoch: 5 step: 1405, loss is 0.14795474708080292\n",
      "epoch: 5 step: 1406, loss is 0.011814530938863754\n",
      "epoch: 5 step: 1407, loss is 0.08846922963857651\n",
      "epoch: 5 step: 1408, loss is 0.005146252457052469\n",
      "epoch: 5 step: 1409, loss is 0.1278805285692215\n",
      "epoch: 5 step: 1410, loss is 0.0004790383973158896\n",
      "epoch: 5 step: 1411, loss is 0.007743321359157562\n",
      "epoch: 5 step: 1412, loss is 0.0011822313535958529\n",
      "epoch: 5 step: 1413, loss is 0.005310440436005592\n",
      "epoch: 5 step: 1414, loss is 0.0034835897386074066\n",
      "epoch: 5 step: 1415, loss is 0.002625387627631426\n",
      "epoch: 5 step: 1416, loss is 0.003373820334672928\n",
      "epoch: 5 step: 1417, loss is 0.00019111025903839618\n",
      "epoch: 5 step: 1418, loss is 0.08921468257904053\n",
      "epoch: 5 step: 1419, loss is 0.15436261892318726\n",
      "epoch: 5 step: 1420, loss is 0.17350420355796814\n",
      "epoch: 5 step: 1421, loss is 0.011426878161728382\n",
      "epoch: 5 step: 1422, loss is 0.13641370832920074\n",
      "epoch: 5 step: 1423, loss is 0.0016456970479339361\n",
      "epoch: 5 step: 1424, loss is 0.00043817138066515326\n",
      "epoch: 5 step: 1425, loss is 0.003141529858112335\n",
      "epoch: 5 step: 1426, loss is 0.0036469472106546164\n",
      "epoch: 5 step: 1427, loss is 0.001239659497514367\n",
      "epoch: 5 step: 1428, loss is 0.0005587927298620343\n",
      "epoch: 5 step: 1429, loss is 0.003923691343516111\n",
      "epoch: 5 step: 1430, loss is 0.04297195002436638\n",
      "epoch: 5 step: 1431, loss is 0.0720292255282402\n",
      "epoch: 5 step: 1432, loss is 0.0369846373796463\n",
      "epoch: 5 step: 1433, loss is 0.0015445679891854525\n",
      "epoch: 5 step: 1434, loss is 0.01778586022555828\n",
      "epoch: 5 step: 1435, loss is 0.0012358026579022408\n",
      "epoch: 5 step: 1436, loss is 0.0014694947749376297\n",
      "epoch: 5 step: 1437, loss is 0.040115565061569214\n",
      "epoch: 5 step: 1438, loss is 0.0007485883543267846\n",
      "epoch: 5 step: 1439, loss is 0.023007793352007866\n",
      "epoch: 5 step: 1440, loss is 0.03087317943572998\n",
      "epoch: 5 step: 1441, loss is 0.001515949028544128\n",
      "epoch: 5 step: 1442, loss is 0.028277205303311348\n",
      "epoch: 5 step: 1443, loss is 0.003969075158238411\n",
      "epoch: 5 step: 1444, loss is 0.0011478833621367812\n",
      "epoch: 5 step: 1445, loss is 0.040766410529613495\n",
      "epoch: 5 step: 1446, loss is 0.0011921387631446123\n",
      "epoch: 5 step: 1447, loss is 0.00812877994030714\n",
      "epoch: 5 step: 1448, loss is 0.0003128049720544368\n",
      "epoch: 5 step: 1449, loss is 0.0003765615983866155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 1450, loss is 0.010895644314587116\n",
      "epoch: 5 step: 1451, loss is 0.055750712752342224\n",
      "epoch: 5 step: 1452, loss is 0.08970224112272263\n",
      "epoch: 5 step: 1453, loss is 0.011121051385998726\n",
      "epoch: 5 step: 1454, loss is 0.0009770585456863046\n",
      "epoch: 5 step: 1455, loss is 0.16311754286289215\n",
      "epoch: 5 step: 1456, loss is 0.06658879667520523\n",
      "epoch: 5 step: 1457, loss is 0.04985486716032028\n",
      "epoch: 5 step: 1458, loss is 0.24538889527320862\n",
      "epoch: 5 step: 1459, loss is 0.0025241111870855093\n",
      "epoch: 5 step: 1460, loss is 0.025543227791786194\n",
      "epoch: 5 step: 1461, loss is 0.00902105588465929\n",
      "epoch: 5 step: 1462, loss is 0.00706531573086977\n",
      "epoch: 5 step: 1463, loss is 0.07544548064470291\n",
      "epoch: 5 step: 1464, loss is 0.06953010708093643\n",
      "epoch: 5 step: 1465, loss is 0.006965707987546921\n",
      "epoch: 5 step: 1466, loss is 0.05077407509088516\n",
      "epoch: 5 step: 1467, loss is 0.025227021425962448\n",
      "epoch: 5 step: 1468, loss is 0.02157704159617424\n",
      "epoch: 5 step: 1469, loss is 0.05671161413192749\n",
      "epoch: 5 step: 1470, loss is 0.011591065675020218\n",
      "epoch: 5 step: 1471, loss is 0.004264976363629103\n",
      "epoch: 5 step: 1472, loss is 0.0026484611444175243\n",
      "epoch: 5 step: 1473, loss is 0.001989508280530572\n",
      "epoch: 5 step: 1474, loss is 0.0026532907504588366\n",
      "epoch: 5 step: 1475, loss is 0.052763618528842926\n",
      "epoch: 5 step: 1476, loss is 0.0008552013314329088\n",
      "epoch: 5 step: 1477, loss is 0.06885718554258347\n",
      "epoch: 5 step: 1478, loss is 0.04208311438560486\n",
      "epoch: 5 step: 1479, loss is 0.09940262138843536\n",
      "epoch: 5 step: 1480, loss is 0.02165948413312435\n",
      "epoch: 5 step: 1481, loss is 0.03386745974421501\n",
      "epoch: 5 step: 1482, loss is 0.004689127206802368\n",
      "epoch: 5 step: 1483, loss is 0.01780545897781849\n",
      "epoch: 5 step: 1484, loss is 0.035513631999492645\n",
      "epoch: 5 step: 1485, loss is 0.004042040556669235\n",
      "epoch: 5 step: 1486, loss is 0.007156793028116226\n",
      "epoch: 5 step: 1487, loss is 0.018328560516238213\n",
      "epoch: 5 step: 1488, loss is 0.0003425053437240422\n",
      "epoch: 5 step: 1489, loss is 0.04187798500061035\n",
      "epoch: 5 step: 1490, loss is 0.001847183215431869\n",
      "epoch: 5 step: 1491, loss is 0.02752773091197014\n",
      "epoch: 5 step: 1492, loss is 0.0010201246477663517\n",
      "epoch: 5 step: 1493, loss is 0.027788950130343437\n",
      "epoch: 5 step: 1494, loss is 0.00193357327952981\n",
      "epoch: 5 step: 1495, loss is 0.003143936861306429\n",
      "epoch: 5 step: 1496, loss is 0.0745270624756813\n",
      "epoch: 5 step: 1497, loss is 0.014873473905026913\n",
      "epoch: 5 step: 1498, loss is 0.08988881856203079\n",
      "epoch: 5 step: 1499, loss is 0.04483131319284439\n",
      "epoch: 5 step: 1500, loss is 0.005781715735793114\n",
      "epoch: 5 step: 1501, loss is 0.216118723154068\n",
      "epoch: 5 step: 1502, loss is 0.043128304183483124\n",
      "epoch: 5 step: 1503, loss is 0.003667761804535985\n",
      "epoch: 5 step: 1504, loss is 0.004408822860568762\n",
      "epoch: 5 step: 1505, loss is 0.000834117061458528\n",
      "epoch: 5 step: 1506, loss is 0.010402128100395203\n",
      "epoch: 5 step: 1507, loss is 0.0018005671445280313\n",
      "epoch: 5 step: 1508, loss is 0.02023305557668209\n",
      "epoch: 5 step: 1509, loss is 0.03417807072401047\n",
      "epoch: 5 step: 1510, loss is 0.0759919136762619\n",
      "epoch: 5 step: 1511, loss is 0.001236486597917974\n",
      "epoch: 5 step: 1512, loss is 0.012931530363857746\n",
      "epoch: 5 step: 1513, loss is 0.014885454438626766\n",
      "epoch: 5 step: 1514, loss is 0.022726694121956825\n",
      "epoch: 5 step: 1515, loss is 0.002423594007268548\n",
      "epoch: 5 step: 1516, loss is 0.00046228908468037844\n",
      "epoch: 5 step: 1517, loss is 0.007329277228564024\n",
      "epoch: 5 step: 1518, loss is 0.005033641122281551\n",
      "epoch: 5 step: 1519, loss is 0.010453378781676292\n",
      "epoch: 5 step: 1520, loss is 0.1953597217798233\n",
      "epoch: 5 step: 1521, loss is 0.3373273015022278\n",
      "epoch: 5 step: 1522, loss is 0.030252451077103615\n",
      "epoch: 5 step: 1523, loss is 0.02220439910888672\n",
      "epoch: 5 step: 1524, loss is 0.0016524521633982658\n",
      "epoch: 5 step: 1525, loss is 0.00036265849485062063\n",
      "epoch: 5 step: 1526, loss is 0.00045085366582497954\n",
      "epoch: 5 step: 1527, loss is 0.00045387676800601184\n",
      "epoch: 5 step: 1528, loss is 0.014159366488456726\n",
      "epoch: 5 step: 1529, loss is 0.19924452900886536\n",
      "epoch: 5 step: 1530, loss is 0.003440314205363393\n",
      "epoch: 5 step: 1531, loss is 0.002850536024197936\n",
      "epoch: 5 step: 1532, loss is 0.015834996476769447\n",
      "epoch: 5 step: 1533, loss is 0.04622168093919754\n",
      "epoch: 5 step: 1534, loss is 0.001343278563581407\n",
      "epoch: 5 step: 1535, loss is 0.09934314340353012\n",
      "epoch: 5 step: 1536, loss is 0.10763414949178696\n",
      "epoch: 5 step: 1537, loss is 0.00795966386795044\n",
      "epoch: 5 step: 1538, loss is 0.008136913180351257\n",
      "epoch: 5 step: 1539, loss is 0.022529713809490204\n",
      "epoch: 5 step: 1540, loss is 0.0005274808499962091\n",
      "epoch: 5 step: 1541, loss is 0.00478363549336791\n",
      "epoch: 5 step: 1542, loss is 0.10265438258647919\n",
      "epoch: 5 step: 1543, loss is 0.13038884103298187\n",
      "epoch: 5 step: 1544, loss is 0.12968580424785614\n",
      "epoch: 5 step: 1545, loss is 0.04889723286032677\n",
      "epoch: 5 step: 1546, loss is 0.007587662432342768\n",
      "epoch: 5 step: 1547, loss is 0.013548223301768303\n",
      "epoch: 5 step: 1548, loss is 0.011364281177520752\n",
      "epoch: 5 step: 1549, loss is 0.04504604637622833\n",
      "epoch: 5 step: 1550, loss is 0.025302443653345108\n",
      "epoch: 5 step: 1551, loss is 0.031189139932394028\n",
      "epoch: 5 step: 1552, loss is 0.015760447829961777\n",
      "epoch: 5 step: 1553, loss is 0.025487227365374565\n",
      "epoch: 5 step: 1554, loss is 0.0319388173520565\n",
      "epoch: 5 step: 1555, loss is 0.20287875831127167\n",
      "epoch: 5 step: 1556, loss is 0.05156195908784866\n",
      "epoch: 5 step: 1557, loss is 0.019098632037639618\n",
      "epoch: 5 step: 1558, loss is 0.004900583066046238\n",
      "epoch: 5 step: 1559, loss is 0.05220213904976845\n",
      "epoch: 5 step: 1560, loss is 0.0031095934100449085\n",
      "epoch: 5 step: 1561, loss is 0.026186540722846985\n",
      "epoch: 5 step: 1562, loss is 0.024528663605451584\n",
      "epoch: 5 step: 1563, loss is 0.09300260245800018\n",
      "epoch: 5 step: 1564, loss is 0.02996661141514778\n",
      "epoch: 5 step: 1565, loss is 0.038815755397081375\n",
      "epoch: 5 step: 1566, loss is 0.03880239650607109\n",
      "epoch: 5 step: 1567, loss is 0.14291350543498993\n",
      "epoch: 5 step: 1568, loss is 0.0023317516315728426\n",
      "epoch: 5 step: 1569, loss is 0.011002535931766033\n",
      "epoch: 5 step: 1570, loss is 0.01822459138929844\n",
      "epoch: 5 step: 1571, loss is 0.02382390946149826\n",
      "epoch: 5 step: 1572, loss is 0.010210147127509117\n",
      "epoch: 5 step: 1573, loss is 0.0027774793561547995\n",
      "epoch: 5 step: 1574, loss is 0.046251941472291946\n",
      "epoch: 5 step: 1575, loss is 0.0016590466257184744\n",
      "epoch: 5 step: 1576, loss is 0.005874271038919687\n",
      "epoch: 5 step: 1577, loss is 0.04558315873146057\n",
      "epoch: 5 step: 1578, loss is 0.11051740497350693\n",
      "epoch: 5 step: 1579, loss is 0.010739874094724655\n",
      "epoch: 5 step: 1580, loss is 0.00251079467125237\n",
      "epoch: 5 step: 1581, loss is 0.06943849474191666\n",
      "epoch: 5 step: 1582, loss is 0.1540890485048294\n",
      "epoch: 5 step: 1583, loss is 0.006031946744769812\n",
      "epoch: 5 step: 1584, loss is 0.013776891864836216\n",
      "epoch: 5 step: 1585, loss is 0.03087438829243183\n",
      "epoch: 5 step: 1586, loss is 0.0090873371809721\n",
      "epoch: 5 step: 1587, loss is 0.019313709810376167\n",
      "epoch: 5 step: 1588, loss is 0.01311776414513588\n",
      "epoch: 5 step: 1589, loss is 0.18095344305038452\n",
      "epoch: 5 step: 1590, loss is 0.0036778158973902464\n",
      "epoch: 5 step: 1591, loss is 0.11782097816467285\n",
      "epoch: 5 step: 1592, loss is 0.024164721369743347\n",
      "epoch: 5 step: 1593, loss is 0.05670560523867607\n",
      "epoch: 5 step: 1594, loss is 0.009712799452245235\n",
      "epoch: 5 step: 1595, loss is 0.04134509712457657\n",
      "epoch: 5 step: 1596, loss is 0.004394802264869213\n",
      "epoch: 5 step: 1597, loss is 0.030260395258665085\n",
      "epoch: 5 step: 1598, loss is 0.016501931473612785\n",
      "epoch: 5 step: 1599, loss is 0.009979140013456345\n",
      "epoch: 5 step: 1600, loss is 0.021339822560548782\n",
      "epoch: 5 step: 1601, loss is 0.0688733235001564\n",
      "epoch: 5 step: 1602, loss is 0.09786292910575867\n",
      "epoch: 5 step: 1603, loss is 0.052555229514837265\n",
      "epoch: 5 step: 1604, loss is 0.000747410929761827\n",
      "epoch: 5 step: 1605, loss is 0.080997034907341\n",
      "epoch: 5 step: 1606, loss is 0.01590539887547493\n",
      "epoch: 5 step: 1607, loss is 0.021534906700253487\n",
      "epoch: 5 step: 1608, loss is 0.003095156978815794\n",
      "epoch: 5 step: 1609, loss is 0.0006309302989393473\n",
      "epoch: 5 step: 1610, loss is 0.011622564867138863\n",
      "epoch: 5 step: 1611, loss is 0.012127305381000042\n",
      "epoch: 5 step: 1612, loss is 0.007373508531600237\n",
      "epoch: 5 step: 1613, loss is 0.007542319130152464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 1614, loss is 0.026156099513173103\n",
      "epoch: 5 step: 1615, loss is 0.029398689046502113\n",
      "epoch: 5 step: 1616, loss is 0.0005086324526928365\n",
      "epoch: 5 step: 1617, loss is 0.00274938833899796\n",
      "epoch: 5 step: 1618, loss is 0.006655316799879074\n",
      "epoch: 5 step: 1619, loss is 0.02340824529528618\n",
      "epoch: 5 step: 1620, loss is 0.02673356793820858\n",
      "epoch: 5 step: 1621, loss is 0.0023240335285663605\n",
      "epoch: 5 step: 1622, loss is 0.0037092617712914944\n",
      "epoch: 5 step: 1623, loss is 0.05103900656104088\n",
      "epoch: 5 step: 1624, loss is 0.04891642555594444\n",
      "epoch: 5 step: 1625, loss is 0.004890458658337593\n",
      "epoch: 5 step: 1626, loss is 0.02592165768146515\n",
      "epoch: 5 step: 1627, loss is 0.15658940374851227\n",
      "epoch: 5 step: 1628, loss is 0.10434156656265259\n",
      "epoch: 5 step: 1629, loss is 0.2473917156457901\n",
      "epoch: 5 step: 1630, loss is 0.002484662691131234\n",
      "epoch: 5 step: 1631, loss is 0.0012735759373754263\n",
      "epoch: 5 step: 1632, loss is 0.0017118630930781364\n",
      "epoch: 5 step: 1633, loss is 0.0008677113801240921\n",
      "epoch: 5 step: 1634, loss is 0.005852109752595425\n",
      "epoch: 5 step: 1635, loss is 0.24448612332344055\n",
      "epoch: 5 step: 1636, loss is 0.0008209304651245475\n",
      "epoch: 5 step: 1637, loss is 0.016830407083034515\n",
      "epoch: 5 step: 1638, loss is 0.0016360440058633685\n",
      "epoch: 5 step: 1639, loss is 0.007516616024076939\n",
      "epoch: 5 step: 1640, loss is 0.0077008879743516445\n",
      "epoch: 5 step: 1641, loss is 0.05068880692124367\n",
      "epoch: 5 step: 1642, loss is 0.02015647292137146\n",
      "epoch: 5 step: 1643, loss is 0.0025013010017573833\n",
      "epoch: 5 step: 1644, loss is 0.001950822421349585\n",
      "epoch: 5 step: 1645, loss is 0.002048495691269636\n",
      "epoch: 5 step: 1646, loss is 0.08666915446519852\n",
      "epoch: 5 step: 1647, loss is 0.04674314707517624\n",
      "epoch: 5 step: 1648, loss is 0.04705650731921196\n",
      "epoch: 5 step: 1649, loss is 0.07425568997859955\n",
      "epoch: 5 step: 1650, loss is 0.013468632474541664\n",
      "epoch: 5 step: 1651, loss is 0.049106866121292114\n",
      "epoch: 5 step: 1652, loss is 0.16839292645454407\n",
      "epoch: 5 step: 1653, loss is 0.004396575503051281\n",
      "epoch: 5 step: 1654, loss is 0.01287104468792677\n",
      "epoch: 5 step: 1655, loss is 0.014666563831269741\n",
      "epoch: 5 step: 1656, loss is 0.02005806379020214\n",
      "epoch: 5 step: 1657, loss is 0.0036439241375774145\n",
      "epoch: 5 step: 1658, loss is 0.04825471341609955\n",
      "epoch: 5 step: 1659, loss is 0.0035522514954209328\n",
      "epoch: 5 step: 1660, loss is 0.0005668105441145599\n",
      "epoch: 5 step: 1661, loss is 0.002532243262976408\n",
      "epoch: 5 step: 1662, loss is 0.003101623384281993\n",
      "epoch: 5 step: 1663, loss is 0.06162991374731064\n",
      "epoch: 5 step: 1664, loss is 0.04644731059670448\n",
      "epoch: 5 step: 1665, loss is 0.13731008768081665\n",
      "epoch: 5 step: 1666, loss is 0.23510423302650452\n",
      "epoch: 5 step: 1667, loss is 0.0019788879435509443\n",
      "epoch: 5 step: 1668, loss is 0.08730287849903107\n",
      "epoch: 5 step: 1669, loss is 0.0012599697802215815\n",
      "epoch: 5 step: 1670, loss is 0.005253886803984642\n",
      "epoch: 5 step: 1671, loss is 0.008931302465498447\n",
      "epoch: 5 step: 1672, loss is 0.0005099570262245834\n",
      "epoch: 5 step: 1673, loss is 0.2731721103191376\n",
      "epoch: 5 step: 1674, loss is 0.012864342890679836\n",
      "epoch: 5 step: 1675, loss is 0.04179787635803223\n",
      "epoch: 5 step: 1676, loss is 0.06586771458387375\n",
      "epoch: 5 step: 1677, loss is 0.016860362142324448\n",
      "epoch: 5 step: 1678, loss is 0.008111045695841312\n",
      "epoch: 5 step: 1679, loss is 0.00921118538826704\n",
      "epoch: 5 step: 1680, loss is 0.11403220891952515\n",
      "epoch: 5 step: 1681, loss is 0.006034274585545063\n",
      "epoch: 5 step: 1682, loss is 0.016262434422969818\n",
      "epoch: 5 step: 1683, loss is 0.04068823531270027\n",
      "epoch: 5 step: 1684, loss is 0.10733422636985779\n",
      "epoch: 5 step: 1685, loss is 0.01039336621761322\n",
      "epoch: 5 step: 1686, loss is 0.005363259930163622\n",
      "epoch: 5 step: 1687, loss is 0.01653151959180832\n",
      "epoch: 5 step: 1688, loss is 0.009005697444081306\n",
      "epoch: 5 step: 1689, loss is 0.029795212671160698\n",
      "epoch: 5 step: 1690, loss is 0.18685390055179596\n",
      "epoch: 5 step: 1691, loss is 0.09800959378480911\n",
      "epoch: 5 step: 1692, loss is 0.0013073356822133064\n",
      "epoch: 5 step: 1693, loss is 0.0037437824066728354\n",
      "epoch: 5 step: 1694, loss is 0.001965541858226061\n",
      "epoch: 5 step: 1695, loss is 0.011960233561694622\n",
      "epoch: 5 step: 1696, loss is 0.005045577418059111\n",
      "epoch: 5 step: 1697, loss is 0.013790097087621689\n",
      "epoch: 5 step: 1698, loss is 0.0007639803225174546\n",
      "epoch: 5 step: 1699, loss is 0.0044991327449679375\n",
      "epoch: 5 step: 1700, loss is 0.007713773753494024\n",
      "epoch: 5 step: 1701, loss is 0.0011460818350315094\n",
      "epoch: 5 step: 1702, loss is 0.0010211491025984287\n",
      "epoch: 5 step: 1703, loss is 0.2913607358932495\n",
      "epoch: 5 step: 1704, loss is 0.0015601780032739043\n",
      "epoch: 5 step: 1705, loss is 0.10259771347045898\n",
      "epoch: 5 step: 1706, loss is 0.022930962964892387\n",
      "epoch: 5 step: 1707, loss is 0.11440513283014297\n",
      "epoch: 5 step: 1708, loss is 0.005284947343170643\n",
      "epoch: 5 step: 1709, loss is 0.04937569797039032\n",
      "epoch: 5 step: 1710, loss is 0.03865944594144821\n",
      "epoch: 5 step: 1711, loss is 0.04786783456802368\n",
      "epoch: 5 step: 1712, loss is 0.003072507679462433\n",
      "epoch: 5 step: 1713, loss is 0.0067646992392838\n",
      "epoch: 5 step: 1714, loss is 0.0013764757895842195\n",
      "epoch: 5 step: 1715, loss is 0.026252444833517075\n",
      "epoch: 5 step: 1716, loss is 0.1012381911277771\n",
      "epoch: 5 step: 1717, loss is 0.0059412019327282906\n",
      "epoch: 5 step: 1718, loss is 0.011241946369409561\n",
      "epoch: 5 step: 1719, loss is 0.10850583016872406\n",
      "epoch: 5 step: 1720, loss is 0.04392634332180023\n",
      "epoch: 5 step: 1721, loss is 0.14857305586338043\n",
      "epoch: 5 step: 1722, loss is 0.29335319995880127\n",
      "epoch: 5 step: 1723, loss is 0.0006529263337142766\n",
      "epoch: 5 step: 1724, loss is 0.044592536985874176\n",
      "epoch: 5 step: 1725, loss is 0.0003235865442547947\n",
      "epoch: 5 step: 1726, loss is 0.01863313838839531\n",
      "epoch: 5 step: 1727, loss is 0.008036513812839985\n",
      "epoch: 5 step: 1728, loss is 0.00459619564935565\n",
      "epoch: 5 step: 1729, loss is 0.0015781591646373272\n",
      "epoch: 5 step: 1730, loss is 0.004288343712687492\n",
      "epoch: 5 step: 1731, loss is 0.03252161666750908\n",
      "epoch: 5 step: 1732, loss is 0.001094234874472022\n",
      "epoch: 5 step: 1733, loss is 0.0036676847375929356\n",
      "epoch: 5 step: 1734, loss is 0.11047188192605972\n",
      "epoch: 5 step: 1735, loss is 0.004458859097212553\n",
      "epoch: 5 step: 1736, loss is 0.003102549584582448\n",
      "epoch: 5 step: 1737, loss is 0.002888808725401759\n",
      "epoch: 5 step: 1738, loss is 0.014653782360255718\n",
      "epoch: 5 step: 1739, loss is 0.0035006043035537004\n",
      "epoch: 5 step: 1740, loss is 0.04810976982116699\n",
      "epoch: 5 step: 1741, loss is 0.01896432787179947\n",
      "epoch: 5 step: 1742, loss is 0.12019886076450348\n",
      "epoch: 5 step: 1743, loss is 0.01794058084487915\n",
      "epoch: 5 step: 1744, loss is 0.0010522777447476983\n",
      "epoch: 5 step: 1745, loss is 0.0011250035604462028\n",
      "epoch: 5 step: 1746, loss is 0.03863980248570442\n",
      "epoch: 5 step: 1747, loss is 0.0784941241145134\n",
      "epoch: 5 step: 1748, loss is 0.0202102642506361\n",
      "epoch: 5 step: 1749, loss is 0.10872061550617218\n",
      "epoch: 5 step: 1750, loss is 0.001993028214201331\n",
      "epoch: 5 step: 1751, loss is 0.0034153293818235397\n",
      "epoch: 5 step: 1752, loss is 0.004177769646048546\n",
      "epoch: 5 step: 1753, loss is 0.020748604089021683\n",
      "epoch: 5 step: 1754, loss is 0.007850993424654007\n",
      "epoch: 5 step: 1755, loss is 0.026131149381399155\n",
      "epoch: 5 step: 1756, loss is 0.0129581643268466\n",
      "epoch: 5 step: 1757, loss is 0.0009839034173637629\n",
      "epoch: 5 step: 1758, loss is 0.02980462834239006\n",
      "epoch: 5 step: 1759, loss is 0.0013863800559192896\n",
      "epoch: 5 step: 1760, loss is 0.012459289282560349\n",
      "epoch: 5 step: 1761, loss is 0.0028450358659029007\n",
      "epoch: 5 step: 1762, loss is 0.00131549674551934\n",
      "epoch: 5 step: 1763, loss is 0.01466815359890461\n",
      "epoch: 5 step: 1764, loss is 0.01025126688182354\n",
      "epoch: 5 step: 1765, loss is 0.0010180958779528737\n",
      "epoch: 5 step: 1766, loss is 0.005219448357820511\n",
      "epoch: 5 step: 1767, loss is 0.002815568121150136\n",
      "epoch: 5 step: 1768, loss is 0.00036579996231012046\n",
      "epoch: 5 step: 1769, loss is 0.010072697885334492\n",
      "epoch: 5 step: 1770, loss is 0.004928131587803364\n",
      "epoch: 5 step: 1771, loss is 0.0026004733517766\n",
      "epoch: 5 step: 1772, loss is 0.023036906495690346\n",
      "epoch: 5 step: 1773, loss is 0.001494785537943244\n",
      "epoch: 5 step: 1774, loss is 0.0055124182254076\n",
      "epoch: 5 step: 1775, loss is 0.0024578224401921034\n",
      "epoch: 5 step: 1776, loss is 0.19878879189491272\n",
      "epoch: 5 step: 1777, loss is 0.0003203556116204709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 1778, loss is 0.06797462701797485\n",
      "epoch: 5 step: 1779, loss is 0.00909966416656971\n",
      "epoch: 5 step: 1780, loss is 0.027980968356132507\n",
      "epoch: 5 step: 1781, loss is 0.0006454946706071496\n",
      "epoch: 5 step: 1782, loss is 0.0029260399751365185\n",
      "epoch: 5 step: 1783, loss is 0.0028443308547139168\n",
      "epoch: 5 step: 1784, loss is 0.006867499556392431\n",
      "epoch: 5 step: 1785, loss is 0.0002648382796905935\n",
      "epoch: 5 step: 1786, loss is 0.0012379453983157873\n",
      "epoch: 5 step: 1787, loss is 0.0050921267829835415\n",
      "epoch: 5 step: 1788, loss is 0.0002264294889755547\n",
      "epoch: 5 step: 1789, loss is 0.003499858547002077\n",
      "epoch: 5 step: 1790, loss is 0.0030594412237405777\n",
      "epoch: 5 step: 1791, loss is 0.0029489975422620773\n",
      "epoch: 5 step: 1792, loss is 0.0031511839479207993\n",
      "epoch: 5 step: 1793, loss is 0.0008322988287545741\n",
      "epoch: 5 step: 1794, loss is 0.007809451315551996\n",
      "epoch: 5 step: 1795, loss is 0.09691933542490005\n",
      "epoch: 5 step: 1796, loss is 0.011674368754029274\n",
      "epoch: 5 step: 1797, loss is 0.000348641537129879\n",
      "epoch: 5 step: 1798, loss is 0.005736831575632095\n",
      "epoch: 5 step: 1799, loss is 0.0008296596352010965\n",
      "epoch: 5 step: 1800, loss is 0.004552012775093317\n",
      "epoch: 5 step: 1801, loss is 0.02605382166802883\n",
      "epoch: 5 step: 1802, loss is 0.0004929014830850065\n",
      "epoch: 5 step: 1803, loss is 0.01961934007704258\n",
      "epoch: 5 step: 1804, loss is 0.005950103048235178\n",
      "epoch: 5 step: 1805, loss is 0.2660975158214569\n",
      "epoch: 5 step: 1806, loss is 0.08208281546831131\n",
      "epoch: 5 step: 1807, loss is 0.002266477094963193\n",
      "epoch: 5 step: 1808, loss is 0.005012014880776405\n",
      "epoch: 5 step: 1809, loss is 0.0007790529634803534\n",
      "epoch: 5 step: 1810, loss is 0.04198089987039566\n",
      "epoch: 5 step: 1811, loss is 0.0002544802555348724\n",
      "epoch: 5 step: 1812, loss is 0.07189171761274338\n",
      "epoch: 5 step: 1813, loss is 0.08924022316932678\n",
      "epoch: 5 step: 1814, loss is 0.0010851785773411393\n",
      "epoch: 5 step: 1815, loss is 0.04045809432864189\n",
      "epoch: 5 step: 1816, loss is 0.0038017432671040297\n",
      "epoch: 5 step: 1817, loss is 0.0064332676120102406\n",
      "epoch: 5 step: 1818, loss is 0.08756760507822037\n",
      "epoch: 5 step: 1819, loss is 0.001466255635023117\n",
      "epoch: 5 step: 1820, loss is 0.03402101248502731\n",
      "epoch: 5 step: 1821, loss is 0.0009078404400497675\n",
      "epoch: 5 step: 1822, loss is 0.0017435353947803378\n",
      "epoch: 5 step: 1823, loss is 0.011987118050456047\n",
      "epoch: 5 step: 1824, loss is 0.026288872584700584\n",
      "epoch: 5 step: 1825, loss is 0.008836626075208187\n",
      "epoch: 5 step: 1826, loss is 0.0031487904489040375\n",
      "epoch: 5 step: 1827, loss is 0.004065726418048143\n",
      "epoch: 5 step: 1828, loss is 0.003048816230148077\n",
      "epoch: 5 step: 1829, loss is 0.0003883666358888149\n",
      "epoch: 5 step: 1830, loss is 0.0019201062386855483\n",
      "epoch: 5 step: 1831, loss is 0.017124563455581665\n",
      "epoch: 5 step: 1832, loss is 0.04246676340699196\n",
      "epoch: 5 step: 1833, loss is 0.0013420915929600596\n",
      "epoch: 5 step: 1834, loss is 0.022508762776851654\n",
      "epoch: 5 step: 1835, loss is 0.05118764564394951\n",
      "epoch: 5 step: 1836, loss is 0.0004255924723111093\n",
      "epoch: 5 step: 1837, loss is 0.006047811824828386\n",
      "epoch: 5 step: 1838, loss is 0.03177208453416824\n",
      "epoch: 5 step: 1839, loss is 0.010264676995575428\n",
      "epoch: 5 step: 1840, loss is 0.1379133015871048\n",
      "epoch: 5 step: 1841, loss is 0.13877369463443756\n",
      "epoch: 5 step: 1842, loss is 0.03526822850108147\n",
      "epoch: 5 step: 1843, loss is 0.0019310510251671076\n",
      "epoch: 5 step: 1844, loss is 0.00032882019877433777\n",
      "epoch: 5 step: 1845, loss is 0.0013521042419597507\n",
      "epoch: 5 step: 1846, loss is 0.011767066083848476\n",
      "epoch: 5 step: 1847, loss is 0.05040978267788887\n",
      "epoch: 5 step: 1848, loss is 0.0005450699827633798\n",
      "epoch: 5 step: 1849, loss is 0.0006817653775215149\n",
      "epoch: 5 step: 1850, loss is 0.0010303566232323647\n",
      "epoch: 5 step: 1851, loss is 0.00019114150200039148\n",
      "epoch: 5 step: 1852, loss is 0.2676716148853302\n",
      "epoch: 5 step: 1853, loss is 0.03495955094695091\n",
      "epoch: 5 step: 1854, loss is 0.002632803050801158\n",
      "epoch: 5 step: 1855, loss is 0.0004074459138792008\n",
      "epoch: 5 step: 1856, loss is 0.018166737630963326\n",
      "epoch: 5 step: 1857, loss is 0.004741677548736334\n",
      "epoch: 5 step: 1858, loss is 0.0009135709842666984\n",
      "epoch: 5 step: 1859, loss is 0.0878743976354599\n",
      "epoch: 5 step: 1860, loss is 0.00037999084452167153\n",
      "epoch: 5 step: 1861, loss is 0.00012989786046091467\n",
      "epoch: 5 step: 1862, loss is 0.013294108211994171\n",
      "epoch: 5 step: 1863, loss is 0.2526538372039795\n",
      "epoch: 5 step: 1864, loss is 0.006028346251696348\n",
      "epoch: 5 step: 1865, loss is 0.03113996982574463\n",
      "epoch: 5 step: 1866, loss is 0.0753057450056076\n",
      "epoch: 5 step: 1867, loss is 0.0015440764836966991\n",
      "epoch: 5 step: 1868, loss is 0.0023950424510985613\n",
      "epoch: 5 step: 1869, loss is 0.04140298441052437\n",
      "epoch: 5 step: 1870, loss is 0.0003976235166192055\n",
      "epoch: 5 step: 1871, loss is 0.0029041829984635115\n",
      "epoch: 5 step: 1872, loss is 0.02262464538216591\n",
      "epoch: 5 step: 1873, loss is 0.01736295409500599\n",
      "epoch: 5 step: 1874, loss is 0.000765920733101666\n",
      "epoch: 5 step: 1875, loss is 0.10106641054153442\n",
      "epoch: 6 step: 1, loss is 0.007696238346397877\n",
      "epoch: 6 step: 2, loss is 0.0017680221935734153\n",
      "epoch: 6 step: 3, loss is 0.002978103468194604\n",
      "epoch: 6 step: 4, loss is 0.03194545954465866\n",
      "epoch: 6 step: 5, loss is 0.0029353760182857513\n",
      "epoch: 6 step: 6, loss is 0.0474504679441452\n",
      "epoch: 6 step: 7, loss is 0.004273437894880772\n",
      "epoch: 6 step: 8, loss is 0.0018564470810815692\n",
      "epoch: 6 step: 9, loss is 0.14122045040130615\n",
      "epoch: 6 step: 10, loss is 0.0010523367673158646\n",
      "epoch: 6 step: 11, loss is 0.11626004427671432\n",
      "epoch: 6 step: 12, loss is 0.0064930091612041\n",
      "epoch: 6 step: 13, loss is 0.0017354643205180764\n",
      "epoch: 6 step: 14, loss is 0.0018788829911500216\n",
      "epoch: 6 step: 15, loss is 0.03005991131067276\n",
      "epoch: 6 step: 16, loss is 0.030979009345173836\n",
      "epoch: 6 step: 17, loss is 0.0731443390250206\n",
      "epoch: 6 step: 18, loss is 0.0024035710375756025\n",
      "epoch: 6 step: 19, loss is 0.000828381220344454\n",
      "epoch: 6 step: 20, loss is 0.02269163355231285\n",
      "epoch: 6 step: 21, loss is 0.0672178789973259\n",
      "epoch: 6 step: 22, loss is 0.07676329463720322\n",
      "epoch: 6 step: 23, loss is 0.02399679459631443\n",
      "epoch: 6 step: 24, loss is 0.0001653247745707631\n",
      "epoch: 6 step: 25, loss is 0.07330865412950516\n",
      "epoch: 6 step: 26, loss is 0.0016567133134230971\n",
      "epoch: 6 step: 27, loss is 0.0021845984738320112\n",
      "epoch: 6 step: 28, loss is 0.022616244852542877\n",
      "epoch: 6 step: 29, loss is 0.00015244391397573054\n",
      "epoch: 6 step: 30, loss is 0.0007305463077500463\n",
      "epoch: 6 step: 31, loss is 0.07724259048700333\n",
      "epoch: 6 step: 32, loss is 0.0023601592984050512\n",
      "epoch: 6 step: 33, loss is 0.005392616614699364\n",
      "epoch: 6 step: 34, loss is 0.012614531442523003\n",
      "epoch: 6 step: 35, loss is 0.008553024381399155\n",
      "epoch: 6 step: 36, loss is 0.08568049222230911\n",
      "epoch: 6 step: 37, loss is 0.01933969184756279\n",
      "epoch: 6 step: 38, loss is 0.009749555960297585\n",
      "epoch: 6 step: 39, loss is 0.022059479728341103\n",
      "epoch: 6 step: 40, loss is 0.11009995639324188\n",
      "epoch: 6 step: 41, loss is 0.0015865251189097762\n",
      "epoch: 6 step: 42, loss is 0.002219290006905794\n",
      "epoch: 6 step: 43, loss is 0.007862599566578865\n",
      "epoch: 6 step: 44, loss is 0.0016181464307010174\n",
      "epoch: 6 step: 45, loss is 0.003939720802009106\n",
      "epoch: 6 step: 46, loss is 0.04413317143917084\n",
      "epoch: 6 step: 47, loss is 0.1452115774154663\n",
      "epoch: 6 step: 48, loss is 0.038170959800481796\n",
      "epoch: 6 step: 49, loss is 0.05126846581697464\n",
      "epoch: 6 step: 50, loss is 0.007169848773628473\n",
      "epoch: 6 step: 51, loss is 0.007137912325561047\n",
      "epoch: 6 step: 52, loss is 0.0043438938446342945\n",
      "epoch: 6 step: 53, loss is 0.008373352698981762\n",
      "epoch: 6 step: 54, loss is 0.008254381828010082\n",
      "epoch: 6 step: 55, loss is 0.0012264016550034285\n",
      "epoch: 6 step: 56, loss is 0.0014236695133149624\n",
      "epoch: 6 step: 57, loss is 0.03160443529486656\n",
      "epoch: 6 step: 58, loss is 0.02768971212208271\n",
      "epoch: 6 step: 59, loss is 0.0011498251697048545\n",
      "epoch: 6 step: 60, loss is 0.01398070715367794\n",
      "epoch: 6 step: 61, loss is 0.1081032007932663\n",
      "epoch: 6 step: 62, loss is 0.00336809316650033\n",
      "epoch: 6 step: 63, loss is 0.001770313479937613\n",
      "epoch: 6 step: 64, loss is 0.01242734119296074\n",
      "epoch: 6 step: 65, loss is 0.0009507746435701847\n",
      "epoch: 6 step: 66, loss is 0.026080844923853874\n",
      "epoch: 6 step: 67, loss is 0.012170729227364063\n",
      "epoch: 6 step: 68, loss is 0.0026773959398269653\n",
      "epoch: 6 step: 69, loss is 0.010843226686120033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 70, loss is 0.019525395706295967\n",
      "epoch: 6 step: 71, loss is 0.00011266379442531615\n",
      "epoch: 6 step: 72, loss is 0.0021652134601026773\n",
      "epoch: 6 step: 73, loss is 0.010181402787566185\n",
      "epoch: 6 step: 74, loss is 0.02910340204834938\n",
      "epoch: 6 step: 75, loss is 0.05516638979315758\n",
      "epoch: 6 step: 76, loss is 0.012858372181653976\n",
      "epoch: 6 step: 77, loss is 0.0029309005476534367\n",
      "epoch: 6 step: 78, loss is 0.0004542700480669737\n",
      "epoch: 6 step: 79, loss is 0.05072614550590515\n",
      "epoch: 6 step: 80, loss is 0.0027332075405865908\n",
      "epoch: 6 step: 81, loss is 0.0004154928610660136\n",
      "epoch: 6 step: 82, loss is 0.0032307221554219723\n",
      "epoch: 6 step: 83, loss is 0.012408901005983353\n",
      "epoch: 6 step: 84, loss is 0.004885389003902674\n",
      "epoch: 6 step: 85, loss is 0.04617931693792343\n",
      "epoch: 6 step: 86, loss is 5.8148296375293285e-05\n",
      "epoch: 6 step: 87, loss is 0.03347183018922806\n",
      "epoch: 6 step: 88, loss is 0.008003166876733303\n",
      "epoch: 6 step: 89, loss is 0.0009729843586683273\n",
      "epoch: 6 step: 90, loss is 0.1215813085436821\n",
      "epoch: 6 step: 91, loss is 0.0018848165636882186\n",
      "epoch: 6 step: 92, loss is 0.0011158778797835112\n",
      "epoch: 6 step: 93, loss is 0.00013421924086287618\n",
      "epoch: 6 step: 94, loss is 0.2758801281452179\n",
      "epoch: 6 step: 95, loss is 0.001273424131795764\n",
      "epoch: 6 step: 96, loss is 0.006945389322936535\n",
      "epoch: 6 step: 97, loss is 0.016057351604104042\n",
      "epoch: 6 step: 98, loss is 0.0004476185713429004\n",
      "epoch: 6 step: 99, loss is 0.010960834100842476\n",
      "epoch: 6 step: 100, loss is 0.03330591320991516\n",
      "epoch: 6 step: 101, loss is 0.0029491472523659468\n",
      "epoch: 6 step: 102, loss is 0.0030549154616892338\n",
      "epoch: 6 step: 103, loss is 0.0023210770450532436\n",
      "epoch: 6 step: 104, loss is 0.005998834501951933\n",
      "epoch: 6 step: 105, loss is 0.0016680839471518993\n",
      "epoch: 6 step: 106, loss is 0.08507772535085678\n",
      "epoch: 6 step: 107, loss is 5.6979479268193245e-05\n",
      "epoch: 6 step: 108, loss is 0.030590003356337547\n",
      "epoch: 6 step: 109, loss is 0.0069377366453409195\n",
      "epoch: 6 step: 110, loss is 0.188648521900177\n",
      "epoch: 6 step: 111, loss is 0.024966105818748474\n",
      "epoch: 6 step: 112, loss is 0.004768040031194687\n",
      "epoch: 6 step: 113, loss is 0.0025489311665296555\n",
      "epoch: 6 step: 114, loss is 0.006526733282953501\n",
      "epoch: 6 step: 115, loss is 0.061269666999578476\n",
      "epoch: 6 step: 116, loss is 0.001794943818822503\n",
      "epoch: 6 step: 117, loss is 0.001753337332047522\n",
      "epoch: 6 step: 118, loss is 0.08338551968336105\n",
      "epoch: 6 step: 119, loss is 0.0180207509547472\n",
      "epoch: 6 step: 120, loss is 0.0051336041651666164\n",
      "epoch: 6 step: 121, loss is 0.009926010854542255\n",
      "epoch: 6 step: 122, loss is 0.00216860044747591\n",
      "epoch: 6 step: 123, loss is 0.0007431210833601654\n",
      "epoch: 6 step: 124, loss is 0.022954212501645088\n",
      "epoch: 6 step: 125, loss is 0.0008867689757607877\n",
      "epoch: 6 step: 126, loss is 0.02315477468073368\n",
      "epoch: 6 step: 127, loss is 0.018228910863399506\n",
      "epoch: 6 step: 128, loss is 0.016141252592206\n",
      "epoch: 6 step: 129, loss is 0.018522188067436218\n",
      "epoch: 6 step: 130, loss is 0.001119124353863299\n",
      "epoch: 6 step: 131, loss is 0.002689792076125741\n",
      "epoch: 6 step: 132, loss is 0.010474768467247486\n",
      "epoch: 6 step: 133, loss is 0.00013415655121207237\n",
      "epoch: 6 step: 134, loss is 0.0021020828280597925\n",
      "epoch: 6 step: 135, loss is 0.008523455820977688\n",
      "epoch: 6 step: 136, loss is 0.037205420434474945\n",
      "epoch: 6 step: 137, loss is 0.0011991678038612008\n",
      "epoch: 6 step: 138, loss is 0.000885572808329016\n",
      "epoch: 6 step: 139, loss is 0.0004702316364273429\n",
      "epoch: 6 step: 140, loss is 0.005278778728097677\n",
      "epoch: 6 step: 141, loss is 0.0004772095999214798\n",
      "epoch: 6 step: 142, loss is 0.008349215611815453\n",
      "epoch: 6 step: 143, loss is 0.0009677152265794575\n",
      "epoch: 6 step: 144, loss is 0.04818098992109299\n",
      "epoch: 6 step: 145, loss is 0.0010358121944591403\n",
      "epoch: 6 step: 146, loss is 0.005231046583503485\n",
      "epoch: 6 step: 147, loss is 0.001161503605544567\n",
      "epoch: 6 step: 148, loss is 0.08810680359601974\n",
      "epoch: 6 step: 149, loss is 0.00027811736799776554\n",
      "epoch: 6 step: 150, loss is 0.07637519389390945\n",
      "epoch: 6 step: 151, loss is 0.0005662351031787694\n",
      "epoch: 6 step: 152, loss is 0.004337489139288664\n",
      "epoch: 6 step: 153, loss is 0.004242338240146637\n",
      "epoch: 6 step: 154, loss is 0.0014552701031789184\n",
      "epoch: 6 step: 155, loss is 0.0002968659973703325\n",
      "epoch: 6 step: 156, loss is 0.009360435418784618\n",
      "epoch: 6 step: 157, loss is 0.0027841192204505205\n",
      "epoch: 6 step: 158, loss is 0.2404024600982666\n",
      "epoch: 6 step: 159, loss is 0.007722196169197559\n",
      "epoch: 6 step: 160, loss is 0.008970295079052448\n",
      "epoch: 6 step: 161, loss is 0.0018000481650233269\n",
      "epoch: 6 step: 162, loss is 0.00033028092002496123\n",
      "epoch: 6 step: 163, loss is 0.047802574932575226\n",
      "epoch: 6 step: 164, loss is 0.009184700436890125\n",
      "epoch: 6 step: 165, loss is 0.00031567225232720375\n",
      "epoch: 6 step: 166, loss is 0.0031495648436248302\n",
      "epoch: 6 step: 167, loss is 0.004399192985147238\n",
      "epoch: 6 step: 168, loss is 0.0032409969717264175\n",
      "epoch: 6 step: 169, loss is 0.0025061776395887136\n",
      "epoch: 6 step: 170, loss is 0.00018471448856871575\n",
      "epoch: 6 step: 171, loss is 0.0009968364611268044\n",
      "epoch: 6 step: 172, loss is 0.06466919928789139\n",
      "epoch: 6 step: 173, loss is 0.0030884898733347654\n",
      "epoch: 6 step: 174, loss is 0.002600103849545121\n",
      "epoch: 6 step: 175, loss is 0.00012029498611809686\n",
      "epoch: 6 step: 176, loss is 0.0003101303882431239\n",
      "epoch: 6 step: 177, loss is 0.0015709067229181528\n",
      "epoch: 6 step: 178, loss is 0.005171693861484528\n",
      "epoch: 6 step: 179, loss is 0.002928256755694747\n",
      "epoch: 6 step: 180, loss is 0.0022954787127673626\n",
      "epoch: 6 step: 181, loss is 0.0459476113319397\n",
      "epoch: 6 step: 182, loss is 0.0013492662692442536\n",
      "epoch: 6 step: 183, loss is 0.0008647512877359986\n",
      "epoch: 6 step: 184, loss is 0.0001375789288431406\n",
      "epoch: 6 step: 185, loss is 0.0021700276993215084\n",
      "epoch: 6 step: 186, loss is 0.030936850234866142\n",
      "epoch: 6 step: 187, loss is 0.0018792543560266495\n",
      "epoch: 6 step: 188, loss is 0.005506050772964954\n",
      "epoch: 6 step: 189, loss is 0.012457086704671383\n",
      "epoch: 6 step: 190, loss is 0.00300050457008183\n",
      "epoch: 6 step: 191, loss is 0.04108774662017822\n",
      "epoch: 6 step: 192, loss is 0.008775647729635239\n",
      "epoch: 6 step: 193, loss is 0.0006346610025502741\n",
      "epoch: 6 step: 194, loss is 0.14383862912654877\n",
      "epoch: 6 step: 195, loss is 0.009849742986261845\n",
      "epoch: 6 step: 196, loss is 0.0004065890971105546\n",
      "epoch: 6 step: 197, loss is 0.0294057447463274\n",
      "epoch: 6 step: 198, loss is 0.00026654163957573473\n",
      "epoch: 6 step: 199, loss is 0.08528312295675278\n",
      "epoch: 6 step: 200, loss is 8.184122270904481e-05\n",
      "epoch: 6 step: 201, loss is 0.0004340385494288057\n",
      "epoch: 6 step: 202, loss is 0.00998120941221714\n",
      "epoch: 6 step: 203, loss is 0.0001289304782403633\n",
      "epoch: 6 step: 204, loss is 0.00020116151426918805\n",
      "epoch: 6 step: 205, loss is 0.0006213479791767895\n",
      "epoch: 6 step: 206, loss is 0.002100758021697402\n",
      "epoch: 6 step: 207, loss is 0.03798728808760643\n",
      "epoch: 6 step: 208, loss is 0.00046073159319348633\n",
      "epoch: 6 step: 209, loss is 0.1415635645389557\n",
      "epoch: 6 step: 210, loss is 0.009919045493006706\n",
      "epoch: 6 step: 211, loss is 0.0008780487114563584\n",
      "epoch: 6 step: 212, loss is 0.002168018603697419\n",
      "epoch: 6 step: 213, loss is 0.006238138768821955\n",
      "epoch: 6 step: 214, loss is 0.012757782824337482\n",
      "epoch: 6 step: 215, loss is 0.043664686381816864\n",
      "epoch: 6 step: 216, loss is 0.005093345884233713\n",
      "epoch: 6 step: 217, loss is 0.00124864443205297\n",
      "epoch: 6 step: 218, loss is 0.0005582782323472202\n",
      "epoch: 6 step: 219, loss is 0.01106016244739294\n",
      "epoch: 6 step: 220, loss is 0.0006027203635312617\n",
      "epoch: 6 step: 221, loss is 0.002857066923752427\n",
      "epoch: 6 step: 222, loss is 0.0004302086017560214\n",
      "epoch: 6 step: 223, loss is 0.00402782391756773\n",
      "epoch: 6 step: 224, loss is 0.02836315520107746\n",
      "epoch: 6 step: 225, loss is 7.64241412980482e-05\n",
      "epoch: 6 step: 226, loss is 0.047677941620349884\n",
      "epoch: 6 step: 227, loss is 0.0217275433242321\n",
      "epoch: 6 step: 228, loss is 0.032242145389318466\n",
      "epoch: 6 step: 229, loss is 0.002982835751026869\n",
      "epoch: 6 step: 230, loss is 0.005794622469693422\n",
      "epoch: 6 step: 231, loss is 0.08118800073862076\n",
      "epoch: 6 step: 232, loss is 0.009743733331561089\n",
      "epoch: 6 step: 233, loss is 0.004206351935863495\n",
      "epoch: 6 step: 234, loss is 0.055424585938453674\n",
      "epoch: 6 step: 235, loss is 0.0053534721955657005\n",
      "epoch: 6 step: 236, loss is 0.017629878595471382\n",
      "epoch: 6 step: 237, loss is 0.0030439423862844706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 238, loss is 0.0049086809158325195\n",
      "epoch: 6 step: 239, loss is 0.2723802328109741\n",
      "epoch: 6 step: 240, loss is 0.155068501830101\n",
      "epoch: 6 step: 241, loss is 0.0006416569231078029\n",
      "epoch: 6 step: 242, loss is 0.001295679365284741\n",
      "epoch: 6 step: 243, loss is 0.05763809010386467\n",
      "epoch: 6 step: 244, loss is 0.014285266399383545\n",
      "epoch: 6 step: 245, loss is 0.0006427175831049681\n",
      "epoch: 6 step: 246, loss is 0.008163162507116795\n",
      "epoch: 6 step: 247, loss is 9.765770664671436e-05\n",
      "epoch: 6 step: 248, loss is 0.01675350032746792\n",
      "epoch: 6 step: 249, loss is 0.0005827090353704989\n",
      "epoch: 6 step: 250, loss is 0.015236921608448029\n",
      "epoch: 6 step: 251, loss is 0.004945519380271435\n",
      "epoch: 6 step: 252, loss is 0.0090027442201972\n",
      "epoch: 6 step: 253, loss is 0.02285575307905674\n",
      "epoch: 6 step: 254, loss is 0.01722452975809574\n",
      "epoch: 6 step: 255, loss is 0.0007455247105099261\n",
      "epoch: 6 step: 256, loss is 0.09460940212011337\n",
      "epoch: 6 step: 257, loss is 0.015057697892189026\n",
      "epoch: 6 step: 258, loss is 0.1137290745973587\n",
      "epoch: 6 step: 259, loss is 0.00015665727551095188\n",
      "epoch: 6 step: 260, loss is 0.20750777423381805\n",
      "epoch: 6 step: 261, loss is 0.3535054624080658\n",
      "epoch: 6 step: 262, loss is 0.00199648248963058\n",
      "epoch: 6 step: 263, loss is 0.01086369063705206\n",
      "epoch: 6 step: 264, loss is 0.04477883502840996\n",
      "epoch: 6 step: 265, loss is 0.00355639704503119\n",
      "epoch: 6 step: 266, loss is 0.004225794691592455\n",
      "epoch: 6 step: 267, loss is 0.00392145337536931\n",
      "epoch: 6 step: 268, loss is 0.04063541442155838\n",
      "epoch: 6 step: 269, loss is 0.003546192543581128\n",
      "epoch: 6 step: 270, loss is 0.030565619468688965\n",
      "epoch: 6 step: 271, loss is 0.000390923407394439\n",
      "epoch: 6 step: 272, loss is 0.009182571433484554\n",
      "epoch: 6 step: 273, loss is 0.0015459436690434813\n",
      "epoch: 6 step: 274, loss is 0.0002974543021991849\n",
      "epoch: 6 step: 275, loss is 0.07962969690561295\n",
      "epoch: 6 step: 276, loss is 0.0013016078155487776\n",
      "epoch: 6 step: 277, loss is 0.0020269949454814196\n",
      "epoch: 6 step: 278, loss is 0.03496919944882393\n",
      "epoch: 6 step: 279, loss is 0.00065593421459198\n",
      "epoch: 6 step: 280, loss is 0.0011628121137619019\n",
      "epoch: 6 step: 281, loss is 0.0038888012059032917\n",
      "epoch: 6 step: 282, loss is 0.008222612552344799\n",
      "epoch: 6 step: 283, loss is 0.007929889485239983\n",
      "epoch: 6 step: 284, loss is 0.08361854404211044\n",
      "epoch: 6 step: 285, loss is 0.006387814413756132\n",
      "epoch: 6 step: 286, loss is 0.0014354248996824026\n",
      "epoch: 6 step: 287, loss is 0.02953268215060234\n",
      "epoch: 6 step: 288, loss is 0.005441855639219284\n",
      "epoch: 6 step: 289, loss is 0.18850521743297577\n",
      "epoch: 6 step: 290, loss is 0.044379979372024536\n",
      "epoch: 6 step: 291, loss is 0.03273759037256241\n",
      "epoch: 6 step: 292, loss is 0.1180492639541626\n",
      "epoch: 6 step: 293, loss is 0.0011443328112363815\n",
      "epoch: 6 step: 294, loss is 0.0035096125211566687\n",
      "epoch: 6 step: 295, loss is 0.09486636519432068\n",
      "epoch: 6 step: 296, loss is 0.01771649532020092\n",
      "epoch: 6 step: 297, loss is 0.09407506138086319\n",
      "epoch: 6 step: 298, loss is 0.03023015521466732\n",
      "epoch: 6 step: 299, loss is 0.07568526268005371\n",
      "epoch: 6 step: 300, loss is 0.039208076894283295\n",
      "epoch: 6 step: 301, loss is 0.03658924624323845\n",
      "epoch: 6 step: 302, loss is 0.0002539315028116107\n",
      "epoch: 6 step: 303, loss is 0.0006868717027828097\n",
      "epoch: 6 step: 304, loss is 0.0037243138067424297\n",
      "epoch: 6 step: 305, loss is 0.187654510140419\n",
      "epoch: 6 step: 306, loss is 0.007870876230299473\n",
      "epoch: 6 step: 307, loss is 0.0015129505190998316\n",
      "epoch: 6 step: 308, loss is 0.0002067930472549051\n",
      "epoch: 6 step: 309, loss is 0.2019703984260559\n",
      "epoch: 6 step: 310, loss is 0.0009405954624526203\n",
      "epoch: 6 step: 311, loss is 0.023211263120174408\n",
      "epoch: 6 step: 312, loss is 0.024253226816654205\n",
      "epoch: 6 step: 313, loss is 0.008999594487249851\n",
      "epoch: 6 step: 314, loss is 0.006836442742496729\n",
      "epoch: 6 step: 315, loss is 0.0021187919192016125\n",
      "epoch: 6 step: 316, loss is 0.08627349138259888\n",
      "epoch: 6 step: 317, loss is 0.005860458128154278\n",
      "epoch: 6 step: 318, loss is 0.00453488202765584\n",
      "epoch: 6 step: 319, loss is 0.004814411513507366\n",
      "epoch: 6 step: 320, loss is 0.0006473649409599602\n",
      "epoch: 6 step: 321, loss is 0.009019339457154274\n",
      "epoch: 6 step: 322, loss is 0.006560716778039932\n",
      "epoch: 6 step: 323, loss is 0.0012712276075035334\n",
      "epoch: 6 step: 324, loss is 0.004319990053772926\n",
      "epoch: 6 step: 325, loss is 0.010618017986416817\n",
      "epoch: 6 step: 326, loss is 0.016695942729711533\n",
      "epoch: 6 step: 327, loss is 0.033303048461675644\n",
      "epoch: 6 step: 328, loss is 0.001388160395435989\n",
      "epoch: 6 step: 329, loss is 0.037590108811855316\n",
      "epoch: 6 step: 330, loss is 0.0018663128139451146\n",
      "epoch: 6 step: 331, loss is 0.000770841958001256\n",
      "epoch: 6 step: 332, loss is 0.08939386904239655\n",
      "epoch: 6 step: 333, loss is 0.000508874305523932\n",
      "epoch: 6 step: 334, loss is 0.011813322082161903\n",
      "epoch: 6 step: 335, loss is 0.03741399571299553\n",
      "epoch: 6 step: 336, loss is 0.07463125139474869\n",
      "epoch: 6 step: 337, loss is 0.002411892870441079\n",
      "epoch: 6 step: 338, loss is 0.003976352512836456\n",
      "epoch: 6 step: 339, loss is 0.0030415088403970003\n",
      "epoch: 6 step: 340, loss is 0.180307537317276\n",
      "epoch: 6 step: 341, loss is 0.023194149136543274\n",
      "epoch: 6 step: 342, loss is 0.0019492553547024727\n",
      "epoch: 6 step: 343, loss is 0.04744059965014458\n",
      "epoch: 6 step: 344, loss is 0.0018745233537629247\n",
      "epoch: 6 step: 345, loss is 0.0017015495104715228\n",
      "epoch: 6 step: 346, loss is 0.011507568880915642\n",
      "epoch: 6 step: 347, loss is 0.009976651519536972\n",
      "epoch: 6 step: 348, loss is 0.009783526882529259\n",
      "epoch: 6 step: 349, loss is 0.007427368313074112\n",
      "epoch: 6 step: 350, loss is 0.0032311398535966873\n",
      "epoch: 6 step: 351, loss is 0.0012394924415275455\n",
      "epoch: 6 step: 352, loss is 0.06858167052268982\n",
      "epoch: 6 step: 353, loss is 0.026985174044966698\n",
      "epoch: 6 step: 354, loss is 0.036160871386528015\n",
      "epoch: 6 step: 355, loss is 0.12080450356006622\n",
      "epoch: 6 step: 356, loss is 0.012292200699448586\n",
      "epoch: 6 step: 357, loss is 0.0013353620888665318\n",
      "epoch: 6 step: 358, loss is 0.01169852539896965\n",
      "epoch: 6 step: 359, loss is 0.06725849211215973\n",
      "epoch: 6 step: 360, loss is 0.002603390719741583\n",
      "epoch: 6 step: 361, loss is 0.013490318320691586\n",
      "epoch: 6 step: 362, loss is 0.0009679187787696719\n",
      "epoch: 6 step: 363, loss is 0.02451382204890251\n",
      "epoch: 6 step: 364, loss is 0.0028077363967895508\n",
      "epoch: 6 step: 365, loss is 0.0016354916151612997\n",
      "epoch: 6 step: 366, loss is 0.014966693706810474\n",
      "epoch: 6 step: 367, loss is 0.00041620503179728985\n",
      "epoch: 6 step: 368, loss is 0.01716133952140808\n",
      "epoch: 6 step: 369, loss is 0.0044411057606339455\n",
      "epoch: 6 step: 370, loss is 0.0005302876816131175\n",
      "epoch: 6 step: 371, loss is 0.0003360119881108403\n",
      "epoch: 6 step: 372, loss is 0.024139229208230972\n",
      "epoch: 6 step: 373, loss is 0.007188423536717892\n",
      "epoch: 6 step: 374, loss is 0.0003174517769366503\n",
      "epoch: 6 step: 375, loss is 0.0077034905552864075\n",
      "epoch: 6 step: 376, loss is 0.0005416846834123135\n",
      "epoch: 6 step: 377, loss is 0.013319631107151508\n",
      "epoch: 6 step: 378, loss is 0.022273825481534004\n",
      "epoch: 6 step: 379, loss is 0.002688153414055705\n",
      "epoch: 6 step: 380, loss is 9.588441753294319e-05\n",
      "epoch: 6 step: 381, loss is 0.0012606255477294326\n",
      "epoch: 6 step: 382, loss is 0.16287772357463837\n",
      "epoch: 6 step: 383, loss is 0.028602315112948418\n",
      "epoch: 6 step: 384, loss is 0.003736779559403658\n",
      "epoch: 6 step: 385, loss is 0.004383604507893324\n",
      "epoch: 6 step: 386, loss is 0.029194118455052376\n",
      "epoch: 6 step: 387, loss is 0.03127160668373108\n",
      "epoch: 6 step: 388, loss is 0.002134204376488924\n",
      "epoch: 6 step: 389, loss is 0.0013618557713925838\n",
      "epoch: 6 step: 390, loss is 0.0017152340151369572\n",
      "epoch: 6 step: 391, loss is 0.00014906069554854184\n",
      "epoch: 6 step: 392, loss is 0.008331931196153164\n",
      "epoch: 6 step: 393, loss is 0.1394428163766861\n",
      "epoch: 6 step: 394, loss is 0.007266559172421694\n",
      "epoch: 6 step: 395, loss is 0.03328822925686836\n",
      "epoch: 6 step: 396, loss is 0.05034217610955238\n",
      "epoch: 6 step: 397, loss is 0.0639810562133789\n",
      "epoch: 6 step: 398, loss is 0.010762723162770271\n",
      "epoch: 6 step: 399, loss is 0.029931437224149704\n",
      "epoch: 6 step: 400, loss is 5.847163629368879e-05\n",
      "epoch: 6 step: 401, loss is 0.0002934957155957818\n",
      "epoch: 6 step: 402, loss is 0.030748048797249794\n",
      "epoch: 6 step: 403, loss is 0.0037756171077489853\n",
      "epoch: 6 step: 404, loss is 0.0037507684901356697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 405, loss is 0.0002562635636422783\n",
      "epoch: 6 step: 406, loss is 0.00017740864132065326\n",
      "epoch: 6 step: 407, loss is 0.01790906861424446\n",
      "epoch: 6 step: 408, loss is 0.0009220807114616036\n",
      "epoch: 6 step: 409, loss is 0.026997273787856102\n",
      "epoch: 6 step: 410, loss is 0.0020875781774520874\n",
      "epoch: 6 step: 411, loss is 0.0017125235171988606\n",
      "epoch: 6 step: 412, loss is 0.0006926697096787393\n",
      "epoch: 6 step: 413, loss is 0.013243465684354305\n",
      "epoch: 6 step: 414, loss is 0.005232235416769981\n",
      "epoch: 6 step: 415, loss is 0.011296723037958145\n",
      "epoch: 6 step: 416, loss is 0.016915541142225266\n",
      "epoch: 6 step: 417, loss is 0.01919032633304596\n",
      "epoch: 6 step: 418, loss is 0.00416402705013752\n",
      "epoch: 6 step: 419, loss is 0.0005849710432812572\n",
      "epoch: 6 step: 420, loss is 0.09522438794374466\n",
      "epoch: 6 step: 421, loss is 0.004179889801889658\n",
      "epoch: 6 step: 422, loss is 0.0015675476752221584\n",
      "epoch: 6 step: 423, loss is 0.0046715326607227325\n",
      "epoch: 6 step: 424, loss is 0.007987665943801403\n",
      "epoch: 6 step: 425, loss is 0.006039154250174761\n",
      "epoch: 6 step: 426, loss is 0.0017401165096089244\n",
      "epoch: 6 step: 427, loss is 0.10220388323068619\n",
      "epoch: 6 step: 428, loss is 0.0004258760600350797\n",
      "epoch: 6 step: 429, loss is 0.0008776647155173123\n",
      "epoch: 6 step: 430, loss is 0.015818767249584198\n",
      "epoch: 6 step: 431, loss is 0.00010946767724817619\n",
      "epoch: 6 step: 432, loss is 0.08642272651195526\n",
      "epoch: 6 step: 433, loss is 0.007336826529353857\n",
      "epoch: 6 step: 434, loss is 0.010139165446162224\n",
      "epoch: 6 step: 435, loss is 0.008279280737042427\n",
      "epoch: 6 step: 436, loss is 0.09276960790157318\n",
      "epoch: 6 step: 437, loss is 0.031023889780044556\n",
      "epoch: 6 step: 438, loss is 0.004524379037320614\n",
      "epoch: 6 step: 439, loss is 0.002642384497448802\n",
      "epoch: 6 step: 440, loss is 0.00122892577201128\n",
      "epoch: 6 step: 441, loss is 0.008605853654444218\n",
      "epoch: 6 step: 442, loss is 0.02860885299742222\n",
      "epoch: 6 step: 443, loss is 0.02429729327559471\n",
      "epoch: 6 step: 444, loss is 0.040221430361270905\n",
      "epoch: 6 step: 445, loss is 0.005685055162757635\n",
      "epoch: 6 step: 446, loss is 0.0002642568142618984\n",
      "epoch: 6 step: 447, loss is 0.007100033573806286\n",
      "epoch: 6 step: 448, loss is 0.001863008481450379\n",
      "epoch: 6 step: 449, loss is 0.0006993691204115748\n",
      "epoch: 6 step: 450, loss is 0.10419314354658127\n",
      "epoch: 6 step: 451, loss is 0.03787655755877495\n",
      "epoch: 6 step: 452, loss is 0.007446364965289831\n",
      "epoch: 6 step: 453, loss is 0.017077388241887093\n",
      "epoch: 6 step: 454, loss is 0.014079065062105656\n",
      "epoch: 6 step: 455, loss is 0.00526037672534585\n",
      "epoch: 6 step: 456, loss is 0.002076566917821765\n",
      "epoch: 6 step: 457, loss is 0.0004106598498765379\n",
      "epoch: 6 step: 458, loss is 0.00017355250020045787\n",
      "epoch: 6 step: 459, loss is 0.1478378027677536\n",
      "epoch: 6 step: 460, loss is 0.08880750834941864\n",
      "epoch: 6 step: 461, loss is 0.06044715642929077\n",
      "epoch: 6 step: 462, loss is 0.0006456531118601561\n",
      "epoch: 6 step: 463, loss is 0.002105437219142914\n",
      "epoch: 6 step: 464, loss is 0.0006697119097225368\n",
      "epoch: 6 step: 465, loss is 0.062272943556308746\n",
      "epoch: 6 step: 466, loss is 0.0008252995903603733\n",
      "epoch: 6 step: 467, loss is 0.004488345701247454\n",
      "epoch: 6 step: 468, loss is 0.2310919612646103\n",
      "epoch: 6 step: 469, loss is 0.000371691829059273\n",
      "epoch: 6 step: 470, loss is 0.015888972207903862\n",
      "epoch: 6 step: 471, loss is 0.0011491967597976327\n",
      "epoch: 6 step: 472, loss is 0.03090074099600315\n",
      "epoch: 6 step: 473, loss is 0.00039147879579104483\n",
      "epoch: 6 step: 474, loss is 0.07106899470090866\n",
      "epoch: 6 step: 475, loss is 0.004018529783934355\n",
      "epoch: 6 step: 476, loss is 0.0005839405348524451\n",
      "epoch: 6 step: 477, loss is 0.011572849936783314\n",
      "epoch: 6 step: 478, loss is 0.05919880419969559\n",
      "epoch: 6 step: 479, loss is 0.0004704112361650914\n",
      "epoch: 6 step: 480, loss is 0.05213775113224983\n",
      "epoch: 6 step: 481, loss is 0.09415896236896515\n",
      "epoch: 6 step: 482, loss is 0.003931963350623846\n",
      "epoch: 6 step: 483, loss is 0.002132530091330409\n",
      "epoch: 6 step: 484, loss is 0.0014316356973722577\n",
      "epoch: 6 step: 485, loss is 0.012866281904280186\n",
      "epoch: 6 step: 486, loss is 0.005053657107055187\n",
      "epoch: 6 step: 487, loss is 0.0013515959726646543\n",
      "epoch: 6 step: 488, loss is 0.009969291277229786\n",
      "epoch: 6 step: 489, loss is 0.004239285364747047\n",
      "epoch: 6 step: 490, loss is 0.025413524359464645\n",
      "epoch: 6 step: 491, loss is 0.0008358183549717069\n",
      "epoch: 6 step: 492, loss is 0.040375471115112305\n",
      "epoch: 6 step: 493, loss is 0.08032659441232681\n",
      "epoch: 6 step: 494, loss is 0.0013999778311699629\n",
      "epoch: 6 step: 495, loss is 0.02244682051241398\n",
      "epoch: 6 step: 496, loss is 0.0006378978723660111\n",
      "epoch: 6 step: 497, loss is 0.0063727619126439095\n",
      "epoch: 6 step: 498, loss is 0.006087145768105984\n",
      "epoch: 6 step: 499, loss is 0.004465026315301657\n",
      "epoch: 6 step: 500, loss is 0.000389406515751034\n",
      "epoch: 6 step: 501, loss is 0.010172251611948013\n",
      "epoch: 6 step: 502, loss is 0.08801213651895523\n",
      "epoch: 6 step: 503, loss is 0.0006000633584335446\n",
      "epoch: 6 step: 504, loss is 0.004307478666305542\n",
      "epoch: 6 step: 505, loss is 0.00040037630242295563\n",
      "epoch: 6 step: 506, loss is 0.03553680703043938\n",
      "epoch: 6 step: 507, loss is 0.06208411976695061\n",
      "epoch: 6 step: 508, loss is 0.002252616686746478\n",
      "epoch: 6 step: 509, loss is 0.0009624776430428028\n",
      "epoch: 6 step: 510, loss is 0.023577047511935234\n",
      "epoch: 6 step: 511, loss is 0.0009102709591388702\n",
      "epoch: 6 step: 512, loss is 0.03634050115942955\n",
      "epoch: 6 step: 513, loss is 0.0005755344755016267\n",
      "epoch: 6 step: 514, loss is 0.0026012505404651165\n",
      "epoch: 6 step: 515, loss is 0.17033623158931732\n",
      "epoch: 6 step: 516, loss is 0.001786656677722931\n",
      "epoch: 6 step: 517, loss is 0.000888168578967452\n",
      "epoch: 6 step: 518, loss is 8.560210699215531e-05\n",
      "epoch: 6 step: 519, loss is 0.030651003122329712\n",
      "epoch: 6 step: 520, loss is 0.055166322737932205\n",
      "epoch: 6 step: 521, loss is 0.006639703642576933\n",
      "epoch: 6 step: 522, loss is 0.03314337879419327\n",
      "epoch: 6 step: 523, loss is 0.08534073084592819\n",
      "epoch: 6 step: 524, loss is 0.26140931248664856\n",
      "epoch: 6 step: 525, loss is 0.01035788469016552\n",
      "epoch: 6 step: 526, loss is 0.0027553855907171965\n",
      "epoch: 6 step: 527, loss is 0.015965387225151062\n",
      "epoch: 6 step: 528, loss is 0.010562516748905182\n",
      "epoch: 6 step: 529, loss is 0.00587134063243866\n",
      "epoch: 6 step: 530, loss is 0.0027723151724785566\n",
      "epoch: 6 step: 531, loss is 0.011432555504143238\n",
      "epoch: 6 step: 532, loss is 0.0002744970843195915\n",
      "epoch: 6 step: 533, loss is 0.009450011886656284\n",
      "epoch: 6 step: 534, loss is 0.15916508436203003\n",
      "epoch: 6 step: 535, loss is 0.012218428775668144\n",
      "epoch: 6 step: 536, loss is 0.01282102894037962\n",
      "epoch: 6 step: 537, loss is 0.005988183431327343\n",
      "epoch: 6 step: 538, loss is 0.03072676807641983\n",
      "epoch: 6 step: 539, loss is 0.08431186527013779\n",
      "epoch: 6 step: 540, loss is 0.003645310876891017\n",
      "epoch: 6 step: 541, loss is 0.00255944998934865\n",
      "epoch: 6 step: 542, loss is 0.046518608927726746\n",
      "epoch: 6 step: 543, loss is 0.030615253373980522\n",
      "epoch: 6 step: 544, loss is 0.0004119118966627866\n",
      "epoch: 6 step: 545, loss is 0.0011796809267252684\n",
      "epoch: 6 step: 546, loss is 0.004065420478582382\n",
      "epoch: 6 step: 547, loss is 0.04350201040506363\n",
      "epoch: 6 step: 548, loss is 0.00021063628082629293\n",
      "epoch: 6 step: 549, loss is 0.04358607158064842\n",
      "epoch: 6 step: 550, loss is 0.09064508229494095\n",
      "epoch: 6 step: 551, loss is 0.001077934866771102\n",
      "epoch: 6 step: 552, loss is 0.002616973826661706\n",
      "epoch: 6 step: 553, loss is 0.014027001336216927\n",
      "epoch: 6 step: 554, loss is 0.017124734818935394\n",
      "epoch: 6 step: 555, loss is 0.0001933479361468926\n",
      "epoch: 6 step: 556, loss is 0.10364940017461777\n",
      "epoch: 6 step: 557, loss is 0.0007488247356377542\n",
      "epoch: 6 step: 558, loss is 0.07201697677373886\n",
      "epoch: 6 step: 559, loss is 0.12588384747505188\n",
      "epoch: 6 step: 560, loss is 0.0004705530882347375\n",
      "epoch: 6 step: 561, loss is 0.07857629656791687\n",
      "epoch: 6 step: 562, loss is 0.028208188712596893\n",
      "epoch: 6 step: 563, loss is 0.0019528548000380397\n",
      "epoch: 6 step: 564, loss is 0.0006477835704572499\n",
      "epoch: 6 step: 565, loss is 0.007158647291362286\n",
      "epoch: 6 step: 566, loss is 0.00038171830237843096\n",
      "epoch: 6 step: 567, loss is 0.04716240614652634\n",
      "epoch: 6 step: 568, loss is 0.005536240059882402\n",
      "epoch: 6 step: 569, loss is 0.004466360434889793\n",
      "epoch: 6 step: 570, loss is 0.11039884388446808\n",
      "epoch: 6 step: 571, loss is 0.0012409285409376025\n",
      "epoch: 6 step: 572, loss is 0.1228090450167656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 573, loss is 0.03569608926773071\n",
      "epoch: 6 step: 574, loss is 0.0005115124513395131\n",
      "epoch: 6 step: 575, loss is 0.0023547224700450897\n",
      "epoch: 6 step: 576, loss is 0.011502517387270927\n",
      "epoch: 6 step: 577, loss is 0.003887757658958435\n",
      "epoch: 6 step: 578, loss is 0.04538571462035179\n",
      "epoch: 6 step: 579, loss is 0.0010998757788911462\n",
      "epoch: 6 step: 580, loss is 0.09307760745286942\n",
      "epoch: 6 step: 581, loss is 7.450882549164817e-05\n",
      "epoch: 6 step: 582, loss is 0.005623058881610632\n",
      "epoch: 6 step: 583, loss is 0.021196193993091583\n",
      "epoch: 6 step: 584, loss is 0.004518928937613964\n",
      "epoch: 6 step: 585, loss is 0.005601978860795498\n",
      "epoch: 6 step: 586, loss is 0.013140588067471981\n",
      "epoch: 6 step: 587, loss is 0.01685021072626114\n",
      "epoch: 6 step: 588, loss is 0.003980586305260658\n",
      "epoch: 6 step: 589, loss is 0.007991216145455837\n",
      "epoch: 6 step: 590, loss is 0.12426355481147766\n",
      "epoch: 6 step: 591, loss is 0.03993161395192146\n",
      "epoch: 6 step: 592, loss is 0.024470677599310875\n",
      "epoch: 6 step: 593, loss is 0.03180098533630371\n",
      "epoch: 6 step: 594, loss is 0.00268071168102324\n",
      "epoch: 6 step: 595, loss is 0.03779648244380951\n",
      "epoch: 6 step: 596, loss is 0.010036331601440907\n",
      "epoch: 6 step: 597, loss is 0.025644998997449875\n",
      "epoch: 6 step: 598, loss is 0.000339944614097476\n",
      "epoch: 6 step: 599, loss is 0.007362696807831526\n",
      "epoch: 6 step: 600, loss is 0.005350187886506319\n",
      "epoch: 6 step: 601, loss is 0.005756002385169268\n",
      "epoch: 6 step: 602, loss is 0.012922153808176517\n",
      "epoch: 6 step: 603, loss is 0.008698227815330029\n",
      "epoch: 6 step: 604, loss is 0.001362215611152351\n",
      "epoch: 6 step: 605, loss is 0.00025305902818217874\n",
      "epoch: 6 step: 606, loss is 0.01233354490250349\n",
      "epoch: 6 step: 607, loss is 0.0009748486336320639\n",
      "epoch: 6 step: 608, loss is 0.0006299780216068029\n",
      "epoch: 6 step: 609, loss is 0.00031557012698613107\n",
      "epoch: 6 step: 610, loss is 0.0006143450154922903\n",
      "epoch: 6 step: 611, loss is 0.00225960835814476\n",
      "epoch: 6 step: 612, loss is 6.705118721583858e-05\n",
      "epoch: 6 step: 613, loss is 0.11753629148006439\n",
      "epoch: 6 step: 614, loss is 0.015537428669631481\n",
      "epoch: 6 step: 615, loss is 0.00041514632175676525\n",
      "epoch: 6 step: 616, loss is 0.0005003520054742694\n",
      "epoch: 6 step: 617, loss is 0.0003878899442497641\n",
      "epoch: 6 step: 618, loss is 0.0010517005575820804\n",
      "epoch: 6 step: 619, loss is 0.023225992918014526\n",
      "epoch: 6 step: 620, loss is 0.008135342039167881\n",
      "epoch: 6 step: 621, loss is 0.008486028760671616\n",
      "epoch: 6 step: 622, loss is 0.0014287024969235063\n",
      "epoch: 6 step: 623, loss is 0.0037886968348175287\n",
      "epoch: 6 step: 624, loss is 0.010049095377326012\n",
      "epoch: 6 step: 625, loss is 0.001900519011542201\n",
      "epoch: 6 step: 626, loss is 0.07964671403169632\n",
      "epoch: 6 step: 627, loss is 0.002219089539721608\n",
      "epoch: 6 step: 628, loss is 0.0007399725145660341\n",
      "epoch: 6 step: 629, loss is 0.0009610799606889486\n",
      "epoch: 6 step: 630, loss is 0.00198580464348197\n",
      "epoch: 6 step: 631, loss is 0.0002831763122230768\n",
      "epoch: 6 step: 632, loss is 0.005569357890635729\n",
      "epoch: 6 step: 633, loss is 0.0009894411778077483\n",
      "epoch: 6 step: 634, loss is 0.026893889531493187\n",
      "epoch: 6 step: 635, loss is 0.019173895940184593\n",
      "epoch: 6 step: 636, loss is 0.06988731026649475\n",
      "epoch: 6 step: 637, loss is 0.0027819450479000807\n",
      "epoch: 6 step: 638, loss is 0.00015645430539734662\n",
      "epoch: 6 step: 639, loss is 0.0025233696214854717\n",
      "epoch: 6 step: 640, loss is 0.0035519732628017664\n",
      "epoch: 6 step: 641, loss is 0.0003265673294663429\n",
      "epoch: 6 step: 642, loss is 0.016263868659734726\n",
      "epoch: 6 step: 643, loss is 0.0018817271338775754\n",
      "epoch: 6 step: 644, loss is 0.002639155834913254\n",
      "epoch: 6 step: 645, loss is 0.013276968151330948\n",
      "epoch: 6 step: 646, loss is 0.0003290307358838618\n",
      "epoch: 6 step: 647, loss is 0.005121269728988409\n",
      "epoch: 6 step: 648, loss is 0.02988739311695099\n",
      "epoch: 6 step: 649, loss is 0.016269316896796227\n",
      "epoch: 6 step: 650, loss is 0.021510930731892586\n",
      "epoch: 6 step: 651, loss is 0.003308588871732354\n",
      "epoch: 6 step: 652, loss is 0.01481347531080246\n",
      "epoch: 6 step: 653, loss is 0.007461984176188707\n",
      "epoch: 6 step: 654, loss is 0.005595023278146982\n",
      "epoch: 6 step: 655, loss is 0.00017742023919709027\n",
      "epoch: 6 step: 656, loss is 0.0008144484600052238\n",
      "epoch: 6 step: 657, loss is 0.01425624918192625\n",
      "epoch: 6 step: 658, loss is 0.0009046912309713662\n",
      "epoch: 6 step: 659, loss is 0.00017161029973067343\n",
      "epoch: 6 step: 660, loss is 0.002132624387741089\n",
      "epoch: 6 step: 661, loss is 0.0005687950761057436\n",
      "epoch: 6 step: 662, loss is 6.347559974528849e-05\n",
      "epoch: 6 step: 663, loss is 0.03814227879047394\n",
      "epoch: 6 step: 664, loss is 0.0006481828750111163\n",
      "epoch: 6 step: 665, loss is 0.033639922738075256\n",
      "epoch: 6 step: 666, loss is 0.007284658960998058\n",
      "epoch: 6 step: 667, loss is 0.23857413232326508\n",
      "epoch: 6 step: 668, loss is 0.09809784591197968\n",
      "epoch: 6 step: 669, loss is 0.00023207912454381585\n",
      "epoch: 6 step: 670, loss is 0.00019658857490867376\n",
      "epoch: 6 step: 671, loss is 0.025077717378735542\n",
      "epoch: 6 step: 672, loss is 0.00017811240104492754\n",
      "epoch: 6 step: 673, loss is 0.0008122671861201525\n",
      "epoch: 6 step: 674, loss is 0.0008765580714680254\n",
      "epoch: 6 step: 675, loss is 0.01050103735178709\n",
      "epoch: 6 step: 676, loss is 0.014986202120780945\n",
      "epoch: 6 step: 677, loss is 0.022221840918064117\n",
      "epoch: 6 step: 678, loss is 0.0005388825084082782\n",
      "epoch: 6 step: 679, loss is 0.0034733701031655073\n",
      "epoch: 6 step: 680, loss is 0.03835242614150047\n",
      "epoch: 6 step: 681, loss is 0.0011412505991756916\n",
      "epoch: 6 step: 682, loss is 0.006521602626889944\n",
      "epoch: 6 step: 683, loss is 0.0001359035086352378\n",
      "epoch: 6 step: 684, loss is 0.0007552801398560405\n",
      "epoch: 6 step: 685, loss is 0.00492431316524744\n",
      "epoch: 6 step: 686, loss is 0.011325146071612835\n",
      "epoch: 6 step: 687, loss is 0.2517347037792206\n",
      "epoch: 6 step: 688, loss is 3.375159576535225e-05\n",
      "epoch: 6 step: 689, loss is 0.04159889742732048\n",
      "epoch: 6 step: 690, loss is 0.0214191023260355\n",
      "epoch: 6 step: 691, loss is 0.22365030646324158\n",
      "epoch: 6 step: 692, loss is 0.017068374902009964\n",
      "epoch: 6 step: 693, loss is 0.14701825380325317\n",
      "epoch: 6 step: 694, loss is 0.025554288178682327\n",
      "epoch: 6 step: 695, loss is 0.0005124314338900149\n",
      "epoch: 6 step: 696, loss is 0.00999366119503975\n",
      "epoch: 6 step: 697, loss is 0.0016600011149421334\n",
      "epoch: 6 step: 698, loss is 0.00040870849625207484\n",
      "epoch: 6 step: 699, loss is 0.0025114703457802534\n",
      "epoch: 6 step: 700, loss is 0.009427167475223541\n",
      "epoch: 6 step: 701, loss is 0.057933736592531204\n",
      "epoch: 6 step: 702, loss is 0.0010110146831721067\n",
      "epoch: 6 step: 703, loss is 0.16195222735404968\n",
      "epoch: 6 step: 704, loss is 0.030292460694909096\n",
      "epoch: 6 step: 705, loss is 0.052442312240600586\n",
      "epoch: 6 step: 706, loss is 0.06770264357328415\n",
      "epoch: 6 step: 707, loss is 0.002550640841946006\n",
      "epoch: 6 step: 708, loss is 0.0004386174550745636\n",
      "epoch: 6 step: 709, loss is 0.033182743936777115\n",
      "epoch: 6 step: 710, loss is 0.0010902725625783205\n",
      "epoch: 6 step: 711, loss is 0.024489587172865868\n",
      "epoch: 6 step: 712, loss is 0.0012335843639448285\n",
      "epoch: 6 step: 713, loss is 0.005885228514671326\n",
      "epoch: 6 step: 714, loss is 0.02262728475034237\n",
      "epoch: 6 step: 715, loss is 0.002261766931042075\n",
      "epoch: 6 step: 716, loss is 0.00019900855841115117\n",
      "epoch: 6 step: 717, loss is 0.0003177804755978286\n",
      "epoch: 6 step: 718, loss is 0.011512521654367447\n",
      "epoch: 6 step: 719, loss is 0.0006608349503949285\n",
      "epoch: 6 step: 720, loss is 0.0013060944620519876\n",
      "epoch: 6 step: 721, loss is 0.0021539595909416676\n",
      "epoch: 6 step: 722, loss is 0.046306267380714417\n",
      "epoch: 6 step: 723, loss is 0.021747874096035957\n",
      "epoch: 6 step: 724, loss is 0.020429765805602074\n",
      "epoch: 6 step: 725, loss is 0.026069125160574913\n",
      "epoch: 6 step: 726, loss is 0.007020704448223114\n",
      "epoch: 6 step: 727, loss is 0.0005050176987424493\n",
      "epoch: 6 step: 728, loss is 0.005167302675545216\n",
      "epoch: 6 step: 729, loss is 0.0009699483634904027\n",
      "epoch: 6 step: 730, loss is 0.01517899427562952\n",
      "epoch: 6 step: 731, loss is 0.0023685754276812077\n",
      "epoch: 6 step: 732, loss is 4.853381324210204e-05\n",
      "epoch: 6 step: 733, loss is 0.024384494870901108\n",
      "epoch: 6 step: 734, loss is 0.017860757187008858\n",
      "epoch: 6 step: 735, loss is 0.0011300980113446712\n",
      "epoch: 6 step: 736, loss is 0.0030644007492810488\n",
      "epoch: 6 step: 737, loss is 0.0018679527565836906\n",
      "epoch: 6 step: 738, loss is 0.19998091459274292\n",
      "epoch: 6 step: 739, loss is 0.007597834337502718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 740, loss is 0.04558480530977249\n",
      "epoch: 6 step: 741, loss is 0.0016661278204992414\n",
      "epoch: 6 step: 742, loss is 0.00613879319280386\n",
      "epoch: 6 step: 743, loss is 0.004144909791648388\n",
      "epoch: 6 step: 744, loss is 0.012485074810683727\n",
      "epoch: 6 step: 745, loss is 0.03731294721364975\n",
      "epoch: 6 step: 746, loss is 0.0155232734978199\n",
      "epoch: 6 step: 747, loss is 0.00014637902495451272\n",
      "epoch: 6 step: 748, loss is 0.0002302304928889498\n",
      "epoch: 6 step: 749, loss is 0.0004962002858519554\n",
      "epoch: 6 step: 750, loss is 0.00013309682253748178\n",
      "epoch: 6 step: 751, loss is 0.04234868660569191\n",
      "epoch: 6 step: 752, loss is 0.028249651193618774\n",
      "epoch: 6 step: 753, loss is 0.0007947092526592314\n",
      "epoch: 6 step: 754, loss is 0.013268895447254181\n",
      "epoch: 6 step: 755, loss is 0.11603672057390213\n",
      "epoch: 6 step: 756, loss is 0.161315456032753\n",
      "epoch: 6 step: 757, loss is 0.00999677088111639\n",
      "epoch: 6 step: 758, loss is 0.0013517622137442231\n",
      "epoch: 6 step: 759, loss is 0.0018318559741601348\n",
      "epoch: 6 step: 760, loss is 0.0015037149423733354\n",
      "epoch: 6 step: 761, loss is 0.00039961235597729683\n",
      "epoch: 6 step: 762, loss is 0.009614269249141216\n",
      "epoch: 6 step: 763, loss is 0.1264421045780182\n",
      "epoch: 6 step: 764, loss is 0.0005294292932376266\n",
      "epoch: 6 step: 765, loss is 9.71047265920788e-05\n",
      "epoch: 6 step: 766, loss is 0.006510194391012192\n",
      "epoch: 6 step: 767, loss is 0.02886672131717205\n",
      "epoch: 6 step: 768, loss is 0.009561620652675629\n",
      "epoch: 6 step: 769, loss is 0.006737781222909689\n",
      "epoch: 6 step: 770, loss is 0.007390831131488085\n",
      "epoch: 6 step: 771, loss is 0.012260018847882748\n",
      "epoch: 6 step: 772, loss is 0.0017176095861941576\n",
      "epoch: 6 step: 773, loss is 0.010956951417028904\n",
      "epoch: 6 step: 774, loss is 0.03879403695464134\n",
      "epoch: 6 step: 775, loss is 0.00042603316251188517\n",
      "epoch: 6 step: 776, loss is 0.01960425078868866\n",
      "epoch: 6 step: 777, loss is 0.1972423493862152\n",
      "epoch: 6 step: 778, loss is 0.016297252848744392\n",
      "epoch: 6 step: 779, loss is 0.005607249215245247\n",
      "epoch: 6 step: 780, loss is 0.010589652694761753\n",
      "epoch: 6 step: 781, loss is 0.24570967257022858\n",
      "epoch: 6 step: 782, loss is 0.3516974449157715\n",
      "epoch: 6 step: 783, loss is 0.012437992729246616\n",
      "epoch: 6 step: 784, loss is 0.00048018756206147373\n",
      "epoch: 6 step: 785, loss is 0.03435361757874489\n",
      "epoch: 6 step: 786, loss is 0.008199070580303669\n",
      "epoch: 6 step: 787, loss is 0.2003002166748047\n",
      "epoch: 6 step: 788, loss is 0.016610492020845413\n",
      "epoch: 6 step: 789, loss is 0.001373819075524807\n",
      "epoch: 6 step: 790, loss is 0.026397056877613068\n",
      "epoch: 6 step: 791, loss is 0.03197087347507477\n",
      "epoch: 6 step: 792, loss is 0.0016481481725350022\n",
      "epoch: 6 step: 793, loss is 0.0022727050818502903\n",
      "epoch: 6 step: 794, loss is 0.009363239631056786\n",
      "epoch: 6 step: 795, loss is 0.04373842850327492\n",
      "epoch: 6 step: 796, loss is 0.0032786212395876646\n",
      "epoch: 6 step: 797, loss is 0.02237153984606266\n",
      "epoch: 6 step: 798, loss is 0.0008867515134625137\n",
      "epoch: 6 step: 799, loss is 0.0016519095515832305\n",
      "epoch: 6 step: 800, loss is 0.1045733243227005\n",
      "epoch: 6 step: 801, loss is 0.1548921763896942\n",
      "epoch: 6 step: 802, loss is 0.003515292191877961\n",
      "epoch: 6 step: 803, loss is 0.02174386940896511\n",
      "epoch: 6 step: 804, loss is 0.1912168711423874\n",
      "epoch: 6 step: 805, loss is 0.030639540404081345\n",
      "epoch: 6 step: 806, loss is 0.12532059848308563\n",
      "epoch: 6 step: 807, loss is 0.0006725508137606084\n",
      "epoch: 6 step: 808, loss is 0.05432715639472008\n",
      "epoch: 6 step: 809, loss is 0.016938015818595886\n",
      "epoch: 6 step: 810, loss is 0.008009226992726326\n",
      "epoch: 6 step: 811, loss is 0.0060257981531322\n",
      "epoch: 6 step: 812, loss is 0.11355593800544739\n",
      "epoch: 6 step: 813, loss is 0.04749368503689766\n",
      "epoch: 6 step: 814, loss is 0.061037588864564896\n",
      "epoch: 6 step: 815, loss is 0.00824849121272564\n",
      "epoch: 6 step: 816, loss is 0.0007228398462757468\n",
      "epoch: 6 step: 817, loss is 0.0006338204839266837\n",
      "epoch: 6 step: 818, loss is 0.001319785020314157\n",
      "epoch: 6 step: 819, loss is 0.01818414404988289\n",
      "epoch: 6 step: 820, loss is 0.0003539954195730388\n",
      "epoch: 6 step: 821, loss is 0.004245087970048189\n",
      "epoch: 6 step: 822, loss is 0.023714080452919006\n",
      "epoch: 6 step: 823, loss is 0.005345009732991457\n",
      "epoch: 6 step: 824, loss is 0.001370605663396418\n",
      "epoch: 6 step: 825, loss is 0.006582547444850206\n",
      "epoch: 6 step: 826, loss is 0.010700082406401634\n",
      "epoch: 6 step: 827, loss is 0.01338911522179842\n",
      "epoch: 6 step: 828, loss is 0.010219287127256393\n",
      "epoch: 6 step: 829, loss is 0.00119695661123842\n",
      "epoch: 6 step: 830, loss is 0.01066184788942337\n",
      "epoch: 6 step: 831, loss is 0.008811975829303265\n",
      "epoch: 6 step: 832, loss is 0.0006424426683224738\n",
      "epoch: 6 step: 833, loss is 0.00922614336013794\n",
      "epoch: 6 step: 834, loss is 0.004566140938550234\n",
      "epoch: 6 step: 835, loss is 0.0007786659989506006\n",
      "epoch: 6 step: 836, loss is 0.015796318650245667\n",
      "epoch: 6 step: 837, loss is 0.04725726321339607\n",
      "epoch: 6 step: 838, loss is 0.0020956045482307673\n",
      "epoch: 6 step: 839, loss is 0.0018790882313624024\n",
      "epoch: 6 step: 840, loss is 0.1251252144575119\n",
      "epoch: 6 step: 841, loss is 0.007098648697137833\n",
      "epoch: 6 step: 842, loss is 0.03962893784046173\n",
      "epoch: 6 step: 843, loss is 0.0021326353307813406\n",
      "epoch: 6 step: 844, loss is 0.011608990840613842\n",
      "epoch: 6 step: 845, loss is 0.0024028406478464603\n",
      "epoch: 6 step: 846, loss is 0.04799175262451172\n",
      "epoch: 6 step: 847, loss is 0.0050615305081009865\n",
      "epoch: 6 step: 848, loss is 0.004019859712570906\n",
      "epoch: 6 step: 849, loss is 0.009774360805749893\n",
      "epoch: 6 step: 850, loss is 0.0011510016629472375\n",
      "epoch: 6 step: 851, loss is 0.023742182180285454\n",
      "epoch: 6 step: 852, loss is 0.022304991260170937\n",
      "epoch: 6 step: 853, loss is 0.0017023510299623013\n",
      "epoch: 6 step: 854, loss is 0.00024329789448529482\n",
      "epoch: 6 step: 855, loss is 0.3759349286556244\n",
      "epoch: 6 step: 856, loss is 0.004079006612300873\n",
      "epoch: 6 step: 857, loss is 0.027473794296383858\n",
      "epoch: 6 step: 858, loss is 0.00032114138593897223\n",
      "epoch: 6 step: 859, loss is 0.0004664323932956904\n",
      "epoch: 6 step: 860, loss is 0.0007031968561932445\n",
      "epoch: 6 step: 861, loss is 0.07959342002868652\n",
      "epoch: 6 step: 862, loss is 0.0015160420443862677\n",
      "epoch: 6 step: 863, loss is 0.0001751089293975383\n",
      "epoch: 6 step: 864, loss is 0.053180187940597534\n",
      "epoch: 6 step: 865, loss is 0.0009934100089594722\n",
      "epoch: 6 step: 866, loss is 0.15465892851352692\n",
      "epoch: 6 step: 867, loss is 0.0011628008214756846\n",
      "epoch: 6 step: 868, loss is 0.021195244044065475\n",
      "epoch: 6 step: 869, loss is 0.0008195578120648861\n",
      "epoch: 6 step: 870, loss is 0.0002973858208861202\n",
      "epoch: 6 step: 871, loss is 0.0015657155308872461\n",
      "epoch: 6 step: 872, loss is 0.0023224633187055588\n",
      "epoch: 6 step: 873, loss is 0.04307418316602707\n",
      "epoch: 6 step: 874, loss is 0.10014229267835617\n",
      "epoch: 6 step: 875, loss is 0.0967797189950943\n",
      "epoch: 6 step: 876, loss is 0.055242639034986496\n",
      "epoch: 6 step: 877, loss is 0.0050213998183608055\n",
      "epoch: 6 step: 878, loss is 0.2578641176223755\n",
      "epoch: 6 step: 879, loss is 0.0026814781595021486\n",
      "epoch: 6 step: 880, loss is 0.0005199357401579618\n",
      "epoch: 6 step: 881, loss is 0.06638845056295395\n",
      "epoch: 6 step: 882, loss is 0.0016620270907878876\n",
      "epoch: 6 step: 883, loss is 0.0008463498088531196\n",
      "epoch: 6 step: 884, loss is 0.021196963265538216\n",
      "epoch: 6 step: 885, loss is 0.00493527390062809\n",
      "epoch: 6 step: 886, loss is 0.023530831560492516\n",
      "epoch: 6 step: 887, loss is 0.025796491652727127\n",
      "epoch: 6 step: 888, loss is 0.05376943200826645\n",
      "epoch: 6 step: 889, loss is 0.01690938137471676\n",
      "epoch: 6 step: 890, loss is 0.0009287555585615337\n",
      "epoch: 6 step: 891, loss is 0.0004259742854628712\n",
      "epoch: 6 step: 892, loss is 0.00032131056650541723\n",
      "epoch: 6 step: 893, loss is 0.016060682013630867\n",
      "epoch: 6 step: 894, loss is 0.029669657349586487\n",
      "epoch: 6 step: 895, loss is 0.0053324708715081215\n",
      "epoch: 6 step: 896, loss is 0.0018533689435571432\n",
      "epoch: 6 step: 897, loss is 0.05400487780570984\n",
      "epoch: 6 step: 898, loss is 0.013020826503634453\n",
      "epoch: 6 step: 899, loss is 0.014243017882108688\n",
      "epoch: 6 step: 900, loss is 0.0046941922046244144\n",
      "epoch: 6 step: 901, loss is 0.0006428042543120682\n",
      "epoch: 6 step: 902, loss is 0.016914017498493195\n",
      "epoch: 6 step: 903, loss is 0.005805442109704018\n",
      "epoch: 6 step: 904, loss is 0.01752251759171486\n",
      "epoch: 6 step: 905, loss is 0.12640301883220673\n",
      "epoch: 6 step: 906, loss is 0.04491206258535385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 907, loss is 0.0007008928223513067\n",
      "epoch: 6 step: 908, loss is 0.0013370244996622205\n",
      "epoch: 6 step: 909, loss is 8.125520980684087e-05\n",
      "epoch: 6 step: 910, loss is 0.0036818203516304493\n",
      "epoch: 6 step: 911, loss is 0.0005074547952972353\n",
      "epoch: 6 step: 912, loss is 0.16238881647586823\n",
      "epoch: 6 step: 913, loss is 0.004399342928081751\n",
      "epoch: 6 step: 914, loss is 0.0003973717102780938\n",
      "epoch: 6 step: 915, loss is 0.0008580635185353458\n",
      "epoch: 6 step: 916, loss is 0.005967624485492706\n",
      "epoch: 6 step: 917, loss is 0.008458808064460754\n",
      "epoch: 6 step: 918, loss is 0.00018332987383473665\n",
      "epoch: 6 step: 919, loss is 0.00047380547039210796\n",
      "epoch: 6 step: 920, loss is 0.12447606772184372\n",
      "epoch: 6 step: 921, loss is 0.004817761946469545\n",
      "epoch: 6 step: 922, loss is 0.03161948174238205\n",
      "epoch: 6 step: 923, loss is 0.0014745609369128942\n",
      "epoch: 6 step: 924, loss is 0.019205793738365173\n",
      "epoch: 6 step: 925, loss is 0.039860986173152924\n",
      "epoch: 6 step: 926, loss is 0.09881201386451721\n",
      "epoch: 6 step: 927, loss is 0.06700590252876282\n",
      "epoch: 6 step: 928, loss is 0.025578850880265236\n",
      "epoch: 6 step: 929, loss is 0.152876079082489\n",
      "epoch: 6 step: 930, loss is 0.06572970002889633\n",
      "epoch: 6 step: 931, loss is 0.0098725575953722\n",
      "epoch: 6 step: 932, loss is 0.015959357842803\n",
      "epoch: 6 step: 933, loss is 0.004450881853699684\n",
      "epoch: 6 step: 934, loss is 0.002133951522409916\n",
      "epoch: 6 step: 935, loss is 0.0007619065581820905\n",
      "epoch: 6 step: 936, loss is 0.006797525566071272\n",
      "epoch: 6 step: 937, loss is 0.02087034285068512\n",
      "epoch: 6 step: 938, loss is 0.0003755345242097974\n",
      "epoch: 6 step: 939, loss is 0.0022343616001307964\n",
      "epoch: 6 step: 940, loss is 0.0005254792631603777\n",
      "epoch: 6 step: 941, loss is 0.32455959916114807\n",
      "epoch: 6 step: 942, loss is 0.001278437441214919\n",
      "epoch: 6 step: 943, loss is 0.0011748666875064373\n",
      "epoch: 6 step: 944, loss is 0.05032527446746826\n",
      "epoch: 6 step: 945, loss is 0.008445553481578827\n",
      "epoch: 6 step: 946, loss is 0.004808621946722269\n",
      "epoch: 6 step: 947, loss is 0.0036245437804609537\n",
      "epoch: 6 step: 948, loss is 0.0017464302945882082\n",
      "epoch: 6 step: 949, loss is 0.004609346855431795\n",
      "epoch: 6 step: 950, loss is 0.0013431310653686523\n",
      "epoch: 6 step: 951, loss is 0.0010770522058010101\n",
      "epoch: 6 step: 952, loss is 0.0004922603839077055\n",
      "epoch: 6 step: 953, loss is 0.09892680495977402\n",
      "epoch: 6 step: 954, loss is 0.002839233260601759\n",
      "epoch: 6 step: 955, loss is 0.02719305455684662\n",
      "epoch: 6 step: 956, loss is 0.22164517641067505\n",
      "epoch: 6 step: 957, loss is 0.0005814456380903721\n",
      "epoch: 6 step: 958, loss is 0.009182462468743324\n",
      "epoch: 6 step: 959, loss is 0.0022238874807953835\n",
      "epoch: 6 step: 960, loss is 0.018525579944252968\n",
      "epoch: 6 step: 961, loss is 0.004608201328665018\n",
      "epoch: 6 step: 962, loss is 0.003658934496343136\n",
      "epoch: 6 step: 963, loss is 0.0003536503645591438\n",
      "epoch: 6 step: 964, loss is 0.0004969319561496377\n",
      "epoch: 6 step: 965, loss is 0.0009821477578952909\n",
      "epoch: 6 step: 966, loss is 0.0038483459502458572\n",
      "epoch: 6 step: 967, loss is 0.004110017791390419\n",
      "epoch: 6 step: 968, loss is 0.07254187762737274\n",
      "epoch: 6 step: 969, loss is 0.07496356964111328\n",
      "epoch: 6 step: 970, loss is 0.00042276160093024373\n",
      "epoch: 6 step: 971, loss is 0.0019994168542325497\n",
      "epoch: 6 step: 972, loss is 0.0069710551761090755\n",
      "epoch: 6 step: 973, loss is 0.04405996575951576\n",
      "epoch: 6 step: 974, loss is 0.040894515812397\n",
      "epoch: 6 step: 975, loss is 0.016093939542770386\n",
      "epoch: 6 step: 976, loss is 0.05450347438454628\n",
      "epoch: 6 step: 977, loss is 0.015754589810967445\n",
      "epoch: 6 step: 978, loss is 0.0017482576658949256\n",
      "epoch: 6 step: 979, loss is 0.0006285561830736697\n",
      "epoch: 6 step: 980, loss is 0.13149049878120422\n",
      "epoch: 6 step: 981, loss is 0.0025769195053726435\n",
      "epoch: 6 step: 982, loss is 0.005388855468481779\n",
      "epoch: 6 step: 983, loss is 0.03531636670231819\n",
      "epoch: 6 step: 984, loss is 0.0012394337682053447\n",
      "epoch: 6 step: 985, loss is 0.0044767302460968494\n",
      "epoch: 6 step: 986, loss is 0.0009179377229884267\n",
      "epoch: 6 step: 987, loss is 0.004698348697274923\n",
      "epoch: 6 step: 988, loss is 0.00010794495756272227\n",
      "epoch: 6 step: 989, loss is 0.004923588130623102\n",
      "epoch: 6 step: 990, loss is 0.002868244657292962\n",
      "epoch: 6 step: 991, loss is 0.007932689972221851\n",
      "epoch: 6 step: 992, loss is 0.00025363272288814187\n",
      "epoch: 6 step: 993, loss is 0.0005279732285998762\n",
      "epoch: 6 step: 994, loss is 0.0409013070166111\n",
      "epoch: 6 step: 995, loss is 0.0002579224237706512\n",
      "epoch: 6 step: 996, loss is 0.025196311995387077\n",
      "epoch: 6 step: 997, loss is 0.0019660380203276873\n",
      "epoch: 6 step: 998, loss is 0.1449752002954483\n",
      "epoch: 6 step: 999, loss is 0.0005783818196505308\n",
      "epoch: 6 step: 1000, loss is 0.1207965612411499\n",
      "epoch: 6 step: 1001, loss is 0.0007406178046949208\n",
      "epoch: 6 step: 1002, loss is 0.17014098167419434\n",
      "epoch: 6 step: 1003, loss is 0.0006083725020289421\n",
      "epoch: 6 step: 1004, loss is 0.0029672870878130198\n",
      "epoch: 6 step: 1005, loss is 0.10578733682632446\n",
      "epoch: 6 step: 1006, loss is 0.00943772867321968\n",
      "epoch: 6 step: 1007, loss is 0.0007538655772805214\n",
      "epoch: 6 step: 1008, loss is 0.006565185729414225\n",
      "epoch: 6 step: 1009, loss is 0.01691991463303566\n",
      "epoch: 6 step: 1010, loss is 0.001733053708449006\n",
      "epoch: 6 step: 1011, loss is 0.023077119141817093\n",
      "epoch: 6 step: 1012, loss is 0.002709029708057642\n",
      "epoch: 6 step: 1013, loss is 0.007792426273226738\n",
      "epoch: 6 step: 1014, loss is 0.02098235860466957\n",
      "epoch: 6 step: 1015, loss is 0.02587210014462471\n",
      "epoch: 6 step: 1016, loss is 0.0032291975803673267\n",
      "epoch: 6 step: 1017, loss is 0.0009015496470965445\n",
      "epoch: 6 step: 1018, loss is 0.007377330679446459\n",
      "epoch: 6 step: 1019, loss is 0.03501134365797043\n",
      "epoch: 6 step: 1020, loss is 0.0010531604057177901\n",
      "epoch: 6 step: 1021, loss is 0.05804041773080826\n",
      "epoch: 6 step: 1022, loss is 0.02616630122065544\n",
      "epoch: 6 step: 1023, loss is 0.0011906593572348356\n",
      "epoch: 6 step: 1024, loss is 0.014758100733160973\n",
      "epoch: 6 step: 1025, loss is 0.007145098876208067\n",
      "epoch: 6 step: 1026, loss is 0.027010899037122726\n",
      "epoch: 6 step: 1027, loss is 0.015525146387517452\n",
      "epoch: 6 step: 1028, loss is 0.02430519089102745\n",
      "epoch: 6 step: 1029, loss is 0.0011524059809744358\n",
      "epoch: 6 step: 1030, loss is 0.002003302564844489\n",
      "epoch: 6 step: 1031, loss is 0.00424695061519742\n",
      "epoch: 6 step: 1032, loss is 0.002463795244693756\n",
      "epoch: 6 step: 1033, loss is 0.0013327592751011252\n",
      "epoch: 6 step: 1034, loss is 0.05625380575656891\n",
      "epoch: 6 step: 1035, loss is 0.002284969901666045\n",
      "epoch: 6 step: 1036, loss is 0.0004197309317532927\n",
      "epoch: 6 step: 1037, loss is 0.02563314698636532\n",
      "epoch: 6 step: 1038, loss is 0.001654728432185948\n",
      "epoch: 6 step: 1039, loss is 0.001780752558261156\n",
      "epoch: 6 step: 1040, loss is 0.0002991830406244844\n",
      "epoch: 6 step: 1041, loss is 0.005003474652767181\n",
      "epoch: 6 step: 1042, loss is 0.005482307635247707\n",
      "epoch: 6 step: 1043, loss is 0.034387391060590744\n",
      "epoch: 6 step: 1044, loss is 0.07151563465595245\n",
      "epoch: 6 step: 1045, loss is 0.01494548749178648\n",
      "epoch: 6 step: 1046, loss is 0.00076405139407143\n",
      "epoch: 6 step: 1047, loss is 0.0005046534934081137\n",
      "epoch: 6 step: 1048, loss is 0.26279687881469727\n",
      "epoch: 6 step: 1049, loss is 0.005201099440455437\n",
      "epoch: 6 step: 1050, loss is 0.12045758962631226\n",
      "epoch: 6 step: 1051, loss is 0.010595827363431454\n",
      "epoch: 6 step: 1052, loss is 0.010928374715149403\n",
      "epoch: 6 step: 1053, loss is 0.0023744190111756325\n",
      "epoch: 6 step: 1054, loss is 0.0009820431005209684\n",
      "epoch: 6 step: 1055, loss is 0.004497071728110313\n",
      "epoch: 6 step: 1056, loss is 0.006415548734366894\n",
      "epoch: 6 step: 1057, loss is 0.0003418049600441009\n",
      "epoch: 6 step: 1058, loss is 0.07110771536827087\n",
      "epoch: 6 step: 1059, loss is 0.052275773137807846\n",
      "epoch: 6 step: 1060, loss is 0.3962157666683197\n",
      "epoch: 6 step: 1061, loss is 0.012468553148210049\n",
      "epoch: 6 step: 1062, loss is 0.0009799288818612695\n",
      "epoch: 6 step: 1063, loss is 0.08704008162021637\n",
      "epoch: 6 step: 1064, loss is 0.02465081587433815\n",
      "epoch: 6 step: 1065, loss is 0.13269634544849396\n",
      "epoch: 6 step: 1066, loss is 0.011386627331376076\n",
      "epoch: 6 step: 1067, loss is 0.0009985738433897495\n",
      "epoch: 6 step: 1068, loss is 0.00035637724795378745\n",
      "epoch: 6 step: 1069, loss is 0.001717491657473147\n",
      "epoch: 6 step: 1070, loss is 0.02636347897350788\n",
      "epoch: 6 step: 1071, loss is 0.0008606709307059646\n",
      "epoch: 6 step: 1072, loss is 0.009380463510751724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 1073, loss is 0.005150820594280958\n",
      "epoch: 6 step: 1074, loss is 0.18623347580432892\n",
      "epoch: 6 step: 1075, loss is 0.025121526792645454\n",
      "epoch: 6 step: 1076, loss is 0.06539997458457947\n",
      "epoch: 6 step: 1077, loss is 0.012249593622982502\n",
      "epoch: 6 step: 1078, loss is 0.0211758054792881\n",
      "epoch: 6 step: 1079, loss is 0.0077022891491651535\n",
      "epoch: 6 step: 1080, loss is 0.028856424614787102\n",
      "epoch: 6 step: 1081, loss is 0.002427168656140566\n",
      "epoch: 6 step: 1082, loss is 0.012727698311209679\n",
      "epoch: 6 step: 1083, loss is 0.07114304602146149\n",
      "epoch: 6 step: 1084, loss is 0.012669059447944164\n",
      "epoch: 6 step: 1085, loss is 0.05476084351539612\n",
      "epoch: 6 step: 1086, loss is 0.0011678715236485004\n",
      "epoch: 6 step: 1087, loss is 0.0052740913815796375\n",
      "epoch: 6 step: 1088, loss is 0.006525097414851189\n",
      "epoch: 6 step: 1089, loss is 0.05146780237555504\n",
      "epoch: 6 step: 1090, loss is 0.1209578588604927\n",
      "epoch: 6 step: 1091, loss is 0.00012160927872173488\n",
      "epoch: 6 step: 1092, loss is 0.004081770312041044\n",
      "epoch: 6 step: 1093, loss is 0.018271034583449364\n",
      "epoch: 6 step: 1094, loss is 0.0003017114067915827\n",
      "epoch: 6 step: 1095, loss is 0.011600845493376255\n",
      "epoch: 6 step: 1096, loss is 0.0034942487254738808\n",
      "epoch: 6 step: 1097, loss is 0.00020174759265501052\n",
      "epoch: 6 step: 1098, loss is 0.06628329306840897\n",
      "epoch: 6 step: 1099, loss is 0.0006071548559702933\n",
      "epoch: 6 step: 1100, loss is 0.0012059079017490149\n",
      "epoch: 6 step: 1101, loss is 0.008891072124242783\n",
      "epoch: 6 step: 1102, loss is 0.07823709398508072\n",
      "epoch: 6 step: 1103, loss is 0.0044576567597687244\n",
      "epoch: 6 step: 1104, loss is 0.015106407925486565\n",
      "epoch: 6 step: 1105, loss is 0.0007361857569776475\n",
      "epoch: 6 step: 1106, loss is 0.000969697255641222\n",
      "epoch: 6 step: 1107, loss is 0.019455017521977425\n",
      "epoch: 6 step: 1108, loss is 0.03113461658358574\n",
      "epoch: 6 step: 1109, loss is 0.0010275699896737933\n",
      "epoch: 6 step: 1110, loss is 0.002337921177968383\n",
      "epoch: 6 step: 1111, loss is 0.28071317076683044\n",
      "epoch: 6 step: 1112, loss is 0.01989576779305935\n",
      "epoch: 6 step: 1113, loss is 0.21279655396938324\n",
      "epoch: 6 step: 1114, loss is 0.06655985116958618\n",
      "epoch: 6 step: 1115, loss is 0.008618848398327827\n",
      "epoch: 6 step: 1116, loss is 0.0043173376470804214\n",
      "epoch: 6 step: 1117, loss is 0.0008548233890905976\n",
      "epoch: 6 step: 1118, loss is 0.057859163731336594\n",
      "epoch: 6 step: 1119, loss is 0.003274794900789857\n",
      "epoch: 6 step: 1120, loss is 0.003629243467003107\n",
      "epoch: 6 step: 1121, loss is 0.0029737455770373344\n",
      "epoch: 6 step: 1122, loss is 0.01785321533679962\n",
      "epoch: 6 step: 1123, loss is 0.0010484801605343819\n",
      "epoch: 6 step: 1124, loss is 0.012051120400428772\n",
      "epoch: 6 step: 1125, loss is 0.017378801479935646\n",
      "epoch: 6 step: 1126, loss is 0.32884687185287476\n",
      "epoch: 6 step: 1127, loss is 0.11372236907482147\n",
      "epoch: 6 step: 1128, loss is 0.03651699423789978\n",
      "epoch: 6 step: 1129, loss is 0.025797542184591293\n",
      "epoch: 6 step: 1130, loss is 0.0021903356537222862\n",
      "epoch: 6 step: 1131, loss is 0.011854609474539757\n",
      "epoch: 6 step: 1132, loss is 0.0031967475078999996\n",
      "epoch: 6 step: 1133, loss is 0.010470983572304249\n",
      "epoch: 6 step: 1134, loss is 0.05609939992427826\n",
      "epoch: 6 step: 1135, loss is 0.02301844395697117\n",
      "epoch: 6 step: 1136, loss is 0.006250446196645498\n",
      "epoch: 6 step: 1137, loss is 0.0015472476370632648\n",
      "epoch: 6 step: 1138, loss is 0.03576278313994408\n",
      "epoch: 6 step: 1139, loss is 0.000763983465731144\n",
      "epoch: 6 step: 1140, loss is 0.011840978637337685\n",
      "epoch: 6 step: 1141, loss is 0.02644267864525318\n",
      "epoch: 6 step: 1142, loss is 0.0024929773062467575\n",
      "epoch: 6 step: 1143, loss is 9.979285096051171e-05\n",
      "epoch: 6 step: 1144, loss is 0.00037542692734859884\n",
      "epoch: 6 step: 1145, loss is 0.0033711676951497793\n",
      "epoch: 6 step: 1146, loss is 0.02949095517396927\n",
      "epoch: 6 step: 1147, loss is 0.031114818528294563\n",
      "epoch: 6 step: 1148, loss is 0.0014748673420399427\n",
      "epoch: 6 step: 1149, loss is 0.0856519564986229\n",
      "epoch: 6 step: 1150, loss is 0.010973840951919556\n",
      "epoch: 6 step: 1151, loss is 0.017832083627581596\n",
      "epoch: 6 step: 1152, loss is 0.00047648968757130206\n",
      "epoch: 6 step: 1153, loss is 0.0022881103213876486\n",
      "epoch: 6 step: 1154, loss is 0.004335016943514347\n",
      "epoch: 6 step: 1155, loss is 0.0786917433142662\n",
      "epoch: 6 step: 1156, loss is 0.0016701671993359923\n",
      "epoch: 6 step: 1157, loss is 0.01504066027700901\n",
      "epoch: 6 step: 1158, loss is 0.01670345850288868\n",
      "epoch: 6 step: 1159, loss is 0.015563521534204483\n",
      "epoch: 6 step: 1160, loss is 0.008024576120078564\n",
      "epoch: 6 step: 1161, loss is 0.0024969009682536125\n",
      "epoch: 6 step: 1162, loss is 0.05264288932085037\n",
      "epoch: 6 step: 1163, loss is 0.013620564714074135\n",
      "epoch: 6 step: 1164, loss is 0.0009037845302373171\n",
      "epoch: 6 step: 1165, loss is 0.11066259443759918\n",
      "epoch: 6 step: 1166, loss is 0.0009348922176286578\n",
      "epoch: 6 step: 1167, loss is 0.006938175298273563\n",
      "epoch: 6 step: 1168, loss is 0.0004854268627241254\n",
      "epoch: 6 step: 1169, loss is 0.0008266671793535352\n",
      "epoch: 6 step: 1170, loss is 0.005163181573152542\n",
      "epoch: 6 step: 1171, loss is 0.20429188013076782\n",
      "epoch: 6 step: 1172, loss is 0.00166244525462389\n",
      "epoch: 6 step: 1173, loss is 0.0021717860363423824\n",
      "epoch: 6 step: 1174, loss is 0.0031777089461684227\n",
      "epoch: 6 step: 1175, loss is 0.00017529084288980812\n",
      "epoch: 6 step: 1176, loss is 0.0017552420031279325\n",
      "epoch: 6 step: 1177, loss is 0.07702010124921799\n",
      "epoch: 6 step: 1178, loss is 0.0019537091720849276\n",
      "epoch: 6 step: 1179, loss is 0.022647883743047714\n",
      "epoch: 6 step: 1180, loss is 0.007327842526137829\n",
      "epoch: 6 step: 1181, loss is 0.035464126616716385\n",
      "epoch: 6 step: 1182, loss is 0.004347804002463818\n",
      "epoch: 6 step: 1183, loss is 0.01365195494145155\n",
      "epoch: 6 step: 1184, loss is 0.15582093596458435\n",
      "epoch: 6 step: 1185, loss is 0.00029503789846785367\n",
      "epoch: 6 step: 1186, loss is 0.001124541973695159\n",
      "epoch: 6 step: 1187, loss is 0.012329292483627796\n",
      "epoch: 6 step: 1188, loss is 0.09724757075309753\n",
      "epoch: 6 step: 1189, loss is 0.0010853022104129195\n",
      "epoch: 6 step: 1190, loss is 0.0006353062344714999\n",
      "epoch: 6 step: 1191, loss is 0.02418789267539978\n",
      "epoch: 6 step: 1192, loss is 0.005128325428813696\n",
      "epoch: 6 step: 1193, loss is 0.06888159364461899\n",
      "epoch: 6 step: 1194, loss is 0.023304447531700134\n",
      "epoch: 6 step: 1195, loss is 0.031868141144514084\n",
      "epoch: 6 step: 1196, loss is 0.006619554478675127\n",
      "epoch: 6 step: 1197, loss is 0.0018221607897430658\n",
      "epoch: 6 step: 1198, loss is 0.0019022730411961675\n",
      "epoch: 6 step: 1199, loss is 0.08489890396595001\n",
      "epoch: 6 step: 1200, loss is 0.004258325789123774\n",
      "epoch: 6 step: 1201, loss is 0.008556767366826534\n",
      "epoch: 6 step: 1202, loss is 0.0013270566705614328\n",
      "epoch: 6 step: 1203, loss is 0.03195774182677269\n",
      "epoch: 6 step: 1204, loss is 0.03934610262513161\n",
      "epoch: 6 step: 1205, loss is 0.0012501932214945555\n",
      "epoch: 6 step: 1206, loss is 0.028512829914689064\n",
      "epoch: 6 step: 1207, loss is 0.0002616419515106827\n",
      "epoch: 6 step: 1208, loss is 0.004045675043016672\n",
      "epoch: 6 step: 1209, loss is 0.0005978393601253629\n",
      "epoch: 6 step: 1210, loss is 0.00040220984374172986\n",
      "epoch: 6 step: 1211, loss is 0.016192160546779633\n",
      "epoch: 6 step: 1212, loss is 0.0006513023399747908\n",
      "epoch: 6 step: 1213, loss is 0.005054074805229902\n",
      "epoch: 6 step: 1214, loss is 0.022348443046212196\n",
      "epoch: 6 step: 1215, loss is 4.328657450969331e-05\n",
      "epoch: 6 step: 1216, loss is 0.0017134321387857199\n",
      "epoch: 6 step: 1217, loss is 0.006699949037283659\n",
      "epoch: 6 step: 1218, loss is 0.00044454875751398504\n",
      "epoch: 6 step: 1219, loss is 0.00044503825483843684\n",
      "epoch: 6 step: 1220, loss is 2.9544145945692435e-05\n",
      "epoch: 6 step: 1221, loss is 0.0015618912875652313\n",
      "epoch: 6 step: 1222, loss is 0.010817894712090492\n",
      "epoch: 6 step: 1223, loss is 0.005368692334741354\n",
      "epoch: 6 step: 1224, loss is 5.8769484894583e-05\n",
      "epoch: 6 step: 1225, loss is 0.0002094808587571606\n",
      "epoch: 6 step: 1226, loss is 0.024126941338181496\n",
      "epoch: 6 step: 1227, loss is 0.0005298943724483252\n",
      "epoch: 6 step: 1228, loss is 0.0025521148927509785\n",
      "epoch: 6 step: 1229, loss is 0.11796198040246964\n",
      "epoch: 6 step: 1230, loss is 0.05039595440030098\n",
      "epoch: 6 step: 1231, loss is 0.00044584713759832084\n",
      "epoch: 6 step: 1232, loss is 0.00284658488817513\n",
      "epoch: 6 step: 1233, loss is 0.0004495579341892153\n",
      "epoch: 6 step: 1234, loss is 0.0008991369395516813\n",
      "epoch: 6 step: 1235, loss is 0.01159981545060873\n",
      "epoch: 6 step: 1236, loss is 0.03943309187889099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 1237, loss is 0.13820523023605347\n",
      "epoch: 6 step: 1238, loss is 0.016365984454751015\n",
      "epoch: 6 step: 1239, loss is 0.0549473874270916\n",
      "epoch: 6 step: 1240, loss is 0.06019768491387367\n",
      "epoch: 6 step: 1241, loss is 0.0006818644469603896\n",
      "epoch: 6 step: 1242, loss is 0.10105974972248077\n",
      "epoch: 6 step: 1243, loss is 0.010927289724349976\n",
      "epoch: 6 step: 1244, loss is 0.0002523204602766782\n",
      "epoch: 6 step: 1245, loss is 0.0034903520718216896\n",
      "epoch: 6 step: 1246, loss is 0.00016465566295664757\n",
      "epoch: 6 step: 1247, loss is 0.15148790180683136\n",
      "epoch: 6 step: 1248, loss is 0.00028010766254737973\n",
      "epoch: 6 step: 1249, loss is 3.861334698740393e-05\n",
      "epoch: 6 step: 1250, loss is 0.00038144629797898233\n",
      "epoch: 6 step: 1251, loss is 0.00030317113851197064\n",
      "epoch: 6 step: 1252, loss is 0.0002311004209332168\n",
      "epoch: 6 step: 1253, loss is 0.0010093554155901074\n",
      "epoch: 6 step: 1254, loss is 0.09069060534238815\n",
      "epoch: 6 step: 1255, loss is 0.00242310157045722\n",
      "epoch: 6 step: 1256, loss is 0.0010607902659103274\n",
      "epoch: 6 step: 1257, loss is 0.10552303493022919\n",
      "epoch: 6 step: 1258, loss is 0.07594767212867737\n",
      "epoch: 6 step: 1259, loss is 0.001973269507288933\n",
      "epoch: 6 step: 1260, loss is 0.0229134950786829\n",
      "epoch: 6 step: 1261, loss is 0.002847218420356512\n",
      "epoch: 6 step: 1262, loss is 0.027104491367936134\n",
      "epoch: 6 step: 1263, loss is 0.005874551832675934\n",
      "epoch: 6 step: 1264, loss is 0.020816152915358543\n",
      "epoch: 6 step: 1265, loss is 0.0031509546097368\n",
      "epoch: 6 step: 1266, loss is 0.03466625511646271\n",
      "epoch: 6 step: 1267, loss is 0.2338119000196457\n",
      "epoch: 6 step: 1268, loss is 0.001092843827791512\n",
      "epoch: 6 step: 1269, loss is 0.015243290923535824\n",
      "epoch: 6 step: 1270, loss is 0.0016089262207970023\n",
      "epoch: 6 step: 1271, loss is 0.058923229575157166\n",
      "epoch: 6 step: 1272, loss is 0.0625654011964798\n",
      "epoch: 6 step: 1273, loss is 0.006501550320535898\n",
      "epoch: 6 step: 1274, loss is 0.004006542265415192\n",
      "epoch: 6 step: 1275, loss is 0.041932202875614166\n",
      "epoch: 6 step: 1276, loss is 0.016242941841483116\n",
      "epoch: 6 step: 1277, loss is 0.0034446122590452433\n",
      "epoch: 6 step: 1278, loss is 0.0035337249282747507\n",
      "epoch: 6 step: 1279, loss is 0.059460606426000595\n",
      "epoch: 6 step: 1280, loss is 0.02995859459042549\n",
      "epoch: 6 step: 1281, loss is 0.021993296220898628\n",
      "epoch: 6 step: 1282, loss is 0.012202078476548195\n",
      "epoch: 6 step: 1283, loss is 0.14365193247795105\n",
      "epoch: 6 step: 1284, loss is 0.14171656966209412\n",
      "epoch: 6 step: 1285, loss is 0.0001065571341314353\n",
      "epoch: 6 step: 1286, loss is 0.0051004416309297085\n",
      "epoch: 6 step: 1287, loss is 0.0018601624760776758\n",
      "epoch: 6 step: 1288, loss is 0.0025643669068813324\n",
      "epoch: 6 step: 1289, loss is 0.01818522997200489\n",
      "epoch: 6 step: 1290, loss is 0.00569853326305747\n",
      "epoch: 6 step: 1291, loss is 0.0005320871714502573\n",
      "epoch: 6 step: 1292, loss is 0.07353191077709198\n",
      "epoch: 6 step: 1293, loss is 0.03644391521811485\n",
      "epoch: 6 step: 1294, loss is 0.0164964497089386\n",
      "epoch: 6 step: 1295, loss is 0.11227439343929291\n",
      "epoch: 6 step: 1296, loss is 0.00374421663582325\n",
      "epoch: 6 step: 1297, loss is 0.035055261105298996\n",
      "epoch: 6 step: 1298, loss is 0.016055604442954063\n",
      "epoch: 6 step: 1299, loss is 0.002903816057369113\n",
      "epoch: 6 step: 1300, loss is 0.0020665405318140984\n",
      "epoch: 6 step: 1301, loss is 0.003145703347399831\n",
      "epoch: 6 step: 1302, loss is 0.02401277795433998\n",
      "epoch: 6 step: 1303, loss is 0.0002537699765525758\n",
      "epoch: 6 step: 1304, loss is 0.059173036366701126\n",
      "epoch: 6 step: 1305, loss is 0.05617174133658409\n",
      "epoch: 6 step: 1306, loss is 0.0004899305640719831\n",
      "epoch: 6 step: 1307, loss is 0.06747458875179291\n",
      "epoch: 6 step: 1308, loss is 0.21666480600833893\n",
      "epoch: 6 step: 1309, loss is 0.0022785926703363657\n",
      "epoch: 6 step: 1310, loss is 0.032438091933727264\n",
      "epoch: 6 step: 1311, loss is 0.014689329080283642\n",
      "epoch: 6 step: 1312, loss is 0.026912571862339973\n",
      "epoch: 6 step: 1313, loss is 0.001946810050867498\n",
      "epoch: 6 step: 1314, loss is 0.0038361400365829468\n",
      "epoch: 6 step: 1315, loss is 0.005110520403832197\n",
      "epoch: 6 step: 1316, loss is 0.14045961201190948\n",
      "epoch: 6 step: 1317, loss is 0.09175360947847366\n",
      "epoch: 6 step: 1318, loss is 0.003906593192368746\n",
      "epoch: 6 step: 1319, loss is 0.003010297892615199\n",
      "epoch: 6 step: 1320, loss is 0.23645682632923126\n",
      "epoch: 6 step: 1321, loss is 0.12783557176589966\n",
      "epoch: 6 step: 1322, loss is 0.017856506630778313\n",
      "epoch: 6 step: 1323, loss is 0.07932168245315552\n",
      "epoch: 6 step: 1324, loss is 0.056292615830898285\n",
      "epoch: 6 step: 1325, loss is 0.1044546514749527\n",
      "epoch: 6 step: 1326, loss is 0.011761524714529514\n",
      "epoch: 6 step: 1327, loss is 0.00014455147902481258\n",
      "epoch: 6 step: 1328, loss is 0.013592468574643135\n",
      "epoch: 6 step: 1329, loss is 0.0008573195082135499\n",
      "epoch: 6 step: 1330, loss is 0.06738021969795227\n",
      "epoch: 6 step: 1331, loss is 0.011510275304317474\n",
      "epoch: 6 step: 1332, loss is 0.15682706236839294\n",
      "epoch: 6 step: 1333, loss is 0.10329412668943405\n",
      "epoch: 6 step: 1334, loss is 0.20104175806045532\n",
      "epoch: 6 step: 1335, loss is 0.014031412079930305\n",
      "epoch: 6 step: 1336, loss is 0.000895994424354285\n",
      "epoch: 6 step: 1337, loss is 0.046350106596946716\n",
      "epoch: 6 step: 1338, loss is 0.008116168901324272\n",
      "epoch: 6 step: 1339, loss is 0.001391781959682703\n",
      "epoch: 6 step: 1340, loss is 0.0006046611233614385\n",
      "epoch: 6 step: 1341, loss is 0.007936934009194374\n",
      "epoch: 6 step: 1342, loss is 0.00119691900908947\n",
      "epoch: 6 step: 1343, loss is 0.002158564515411854\n",
      "epoch: 6 step: 1344, loss is 0.005212472751736641\n",
      "epoch: 6 step: 1345, loss is 0.008462234400212765\n",
      "epoch: 6 step: 1346, loss is 0.02955901436507702\n",
      "epoch: 6 step: 1347, loss is 0.03291963040828705\n",
      "epoch: 6 step: 1348, loss is 0.002478233305737376\n",
      "epoch: 6 step: 1349, loss is 0.07899384200572968\n",
      "epoch: 6 step: 1350, loss is 0.10582154244184494\n",
      "epoch: 6 step: 1351, loss is 0.0013836194993928075\n",
      "epoch: 6 step: 1352, loss is 0.001436027348972857\n",
      "epoch: 6 step: 1353, loss is 0.006028875708580017\n",
      "epoch: 6 step: 1354, loss is 0.005656816530972719\n",
      "epoch: 6 step: 1355, loss is 0.1488647311925888\n",
      "epoch: 6 step: 1356, loss is 0.00838422030210495\n",
      "epoch: 6 step: 1357, loss is 0.0014136286918073893\n",
      "epoch: 6 step: 1358, loss is 0.003263062098994851\n",
      "epoch: 6 step: 1359, loss is 0.1799309402704239\n",
      "epoch: 6 step: 1360, loss is 0.0394856333732605\n",
      "epoch: 6 step: 1361, loss is 0.0010261933784931898\n",
      "epoch: 6 step: 1362, loss is 0.06834539771080017\n",
      "epoch: 6 step: 1363, loss is 0.004251984879374504\n",
      "epoch: 6 step: 1364, loss is 0.003891591215506196\n",
      "epoch: 6 step: 1365, loss is 0.0060318848118186\n",
      "epoch: 6 step: 1366, loss is 0.04738955944776535\n",
      "epoch: 6 step: 1367, loss is 0.004740274045616388\n",
      "epoch: 6 step: 1368, loss is 0.01506703719496727\n",
      "epoch: 6 step: 1369, loss is 0.0014688821975141764\n",
      "epoch: 6 step: 1370, loss is 0.005958442110568285\n",
      "epoch: 6 step: 1371, loss is 0.011738463304936886\n",
      "epoch: 6 step: 1372, loss is 0.0075454432517290115\n",
      "epoch: 6 step: 1373, loss is 0.002074539428576827\n",
      "epoch: 6 step: 1374, loss is 0.003963999915868044\n",
      "epoch: 6 step: 1375, loss is 0.0005679523455910385\n",
      "epoch: 6 step: 1376, loss is 0.03238559141755104\n",
      "epoch: 6 step: 1377, loss is 0.0018944403855130076\n",
      "epoch: 6 step: 1378, loss is 0.0072694458067417145\n",
      "epoch: 6 step: 1379, loss is 0.3284795582294464\n",
      "epoch: 6 step: 1380, loss is 0.0620889887213707\n",
      "epoch: 6 step: 1381, loss is 0.00426638126373291\n",
      "epoch: 6 step: 1382, loss is 0.0057610818184912205\n",
      "epoch: 6 step: 1383, loss is 0.18183749914169312\n",
      "epoch: 6 step: 1384, loss is 0.000524149218108505\n",
      "epoch: 6 step: 1385, loss is 0.013682299293577671\n",
      "epoch: 6 step: 1386, loss is 0.0006560953333973885\n",
      "epoch: 6 step: 1387, loss is 0.0014749275287613273\n",
      "epoch: 6 step: 1388, loss is 0.09755434840917587\n",
      "epoch: 6 step: 1389, loss is 0.005707652773708105\n",
      "epoch: 6 step: 1390, loss is 0.002473441418260336\n",
      "epoch: 6 step: 1391, loss is 0.006949016358703375\n",
      "epoch: 6 step: 1392, loss is 0.0027133708354085684\n",
      "epoch: 6 step: 1393, loss is 0.007994341664016247\n",
      "epoch: 6 step: 1394, loss is 0.007607741747051477\n",
      "epoch: 6 step: 1395, loss is 0.006237583700567484\n",
      "epoch: 6 step: 1396, loss is 0.04223848879337311\n",
      "epoch: 6 step: 1397, loss is 0.0009562555351294577\n",
      "epoch: 6 step: 1398, loss is 0.003991227131336927\n",
      "epoch: 6 step: 1399, loss is 0.010085741057991982\n",
      "epoch: 6 step: 1400, loss is 0.006634334567934275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 1401, loss is 0.20031385123729706\n",
      "epoch: 6 step: 1402, loss is 0.030387550592422485\n",
      "epoch: 6 step: 1403, loss is 0.11982303857803345\n",
      "epoch: 6 step: 1404, loss is 0.005609550047665834\n",
      "epoch: 6 step: 1405, loss is 0.07249971479177475\n",
      "epoch: 6 step: 1406, loss is 0.030383305624127388\n",
      "epoch: 6 step: 1407, loss is 0.01447602454572916\n",
      "epoch: 6 step: 1408, loss is 0.00031628384022042155\n",
      "epoch: 6 step: 1409, loss is 0.11754005402326584\n",
      "epoch: 6 step: 1410, loss is 0.012718386948108673\n",
      "epoch: 6 step: 1411, loss is 0.2626333236694336\n",
      "epoch: 6 step: 1412, loss is 0.007265771739184856\n",
      "epoch: 6 step: 1413, loss is 0.008681007660925388\n",
      "epoch: 6 step: 1414, loss is 0.0030106764752417803\n",
      "epoch: 6 step: 1415, loss is 0.0002580179425422102\n",
      "epoch: 6 step: 1416, loss is 0.1260717213153839\n",
      "epoch: 6 step: 1417, loss is 0.0007376009016297758\n",
      "epoch: 6 step: 1418, loss is 0.04656672850251198\n",
      "epoch: 6 step: 1419, loss is 0.006583850365132093\n",
      "epoch: 6 step: 1420, loss is 0.017936931923031807\n",
      "epoch: 6 step: 1421, loss is 0.00027088922797702253\n",
      "epoch: 6 step: 1422, loss is 0.020107010379433632\n",
      "epoch: 6 step: 1423, loss is 0.02440156601369381\n",
      "epoch: 6 step: 1424, loss is 0.015771405771374702\n",
      "epoch: 6 step: 1425, loss is 0.10432118922472\n",
      "epoch: 6 step: 1426, loss is 0.004330761730670929\n",
      "epoch: 6 step: 1427, loss is 0.00591669324785471\n",
      "epoch: 6 step: 1428, loss is 0.0011482827831059694\n",
      "epoch: 6 step: 1429, loss is 0.0009338901727460325\n",
      "epoch: 6 step: 1430, loss is 0.004886527080088854\n",
      "epoch: 6 step: 1431, loss is 0.0005373149760998785\n",
      "epoch: 6 step: 1432, loss is 0.0057548596523702145\n",
      "epoch: 6 step: 1433, loss is 0.11702165007591248\n",
      "epoch: 6 step: 1434, loss is 0.007773919031023979\n",
      "epoch: 6 step: 1435, loss is 0.006389876827597618\n",
      "epoch: 6 step: 1436, loss is 0.0006521660834550858\n",
      "epoch: 6 step: 1437, loss is 0.03528895601630211\n",
      "epoch: 6 step: 1438, loss is 0.023342300206422806\n",
      "epoch: 6 step: 1439, loss is 0.018679602071642876\n",
      "epoch: 6 step: 1440, loss is 0.21092510223388672\n",
      "epoch: 6 step: 1441, loss is 0.0003352927742525935\n",
      "epoch: 6 step: 1442, loss is 0.013486170209944248\n",
      "epoch: 6 step: 1443, loss is 0.02557954750955105\n",
      "epoch: 6 step: 1444, loss is 0.04099029302597046\n",
      "epoch: 6 step: 1445, loss is 0.003328869817778468\n",
      "epoch: 6 step: 1446, loss is 0.0009905628394335508\n",
      "epoch: 6 step: 1447, loss is 0.00030249779229052365\n",
      "epoch: 6 step: 1448, loss is 0.0007273110677488148\n",
      "epoch: 6 step: 1449, loss is 0.01374509371817112\n",
      "epoch: 6 step: 1450, loss is 0.002123889746144414\n",
      "epoch: 6 step: 1451, loss is 0.0017337618628516793\n",
      "epoch: 6 step: 1452, loss is 0.0014465766726061702\n",
      "epoch: 6 step: 1453, loss is 0.09799227863550186\n",
      "epoch: 6 step: 1454, loss is 0.04592202976346016\n",
      "epoch: 6 step: 1455, loss is 0.0007969604339450598\n",
      "epoch: 6 step: 1456, loss is 0.0005058274255134165\n",
      "epoch: 6 step: 1457, loss is 0.1861587017774582\n",
      "epoch: 6 step: 1458, loss is 0.007485010661184788\n",
      "epoch: 6 step: 1459, loss is 0.0003923318290617317\n",
      "epoch: 6 step: 1460, loss is 0.00012067370698787272\n",
      "epoch: 6 step: 1461, loss is 0.04291568696498871\n",
      "epoch: 6 step: 1462, loss is 0.008713277988135815\n",
      "epoch: 6 step: 1463, loss is 0.008497054688632488\n",
      "epoch: 6 step: 1464, loss is 0.002617927733808756\n",
      "epoch: 6 step: 1465, loss is 0.0051472061313688755\n",
      "epoch: 6 step: 1466, loss is 0.09163796156644821\n",
      "epoch: 6 step: 1467, loss is 0.0754583328962326\n",
      "epoch: 6 step: 1468, loss is 0.03246847912669182\n",
      "epoch: 6 step: 1469, loss is 0.03361763805150986\n",
      "epoch: 6 step: 1470, loss is 0.019950944930315018\n",
      "epoch: 6 step: 1471, loss is 0.06047026067972183\n",
      "epoch: 6 step: 1472, loss is 0.016999084502458572\n",
      "epoch: 6 step: 1473, loss is 0.00347086600959301\n",
      "epoch: 6 step: 1474, loss is 0.0027313779573887587\n",
      "epoch: 6 step: 1475, loss is 0.1379488706588745\n",
      "epoch: 6 step: 1476, loss is 0.1547178477048874\n",
      "epoch: 6 step: 1477, loss is 0.0002425654383841902\n",
      "epoch: 6 step: 1478, loss is 0.23771534860134125\n",
      "epoch: 6 step: 1479, loss is 0.4069080650806427\n",
      "epoch: 6 step: 1480, loss is 0.0006631265860050917\n",
      "epoch: 6 step: 1481, loss is 0.001793994684703648\n",
      "epoch: 6 step: 1482, loss is 0.006621204316616058\n",
      "epoch: 6 step: 1483, loss is 0.0022816690616309643\n",
      "epoch: 6 step: 1484, loss is 0.001676385523751378\n",
      "epoch: 6 step: 1485, loss is 0.003174315905198455\n",
      "epoch: 6 step: 1486, loss is 0.00958615355193615\n",
      "epoch: 6 step: 1487, loss is 0.006811322644352913\n",
      "epoch: 6 step: 1488, loss is 0.04314735159277916\n",
      "epoch: 6 step: 1489, loss is 0.0017902744002640247\n",
      "epoch: 6 step: 1490, loss is 0.07623547315597534\n",
      "epoch: 6 step: 1491, loss is 0.1384647637605667\n",
      "epoch: 6 step: 1492, loss is 0.012971552088856697\n",
      "epoch: 6 step: 1493, loss is 0.012716855853796005\n",
      "epoch: 6 step: 1494, loss is 0.0004193662607576698\n",
      "epoch: 6 step: 1495, loss is 0.08666881173849106\n",
      "epoch: 6 step: 1496, loss is 0.015582027845084667\n",
      "epoch: 6 step: 1497, loss is 0.1203547865152359\n",
      "epoch: 6 step: 1498, loss is 0.10904879122972488\n",
      "epoch: 6 step: 1499, loss is 0.1432577222585678\n",
      "epoch: 6 step: 1500, loss is 0.0006016665138304234\n",
      "epoch: 6 step: 1501, loss is 0.0019001353066414595\n",
      "epoch: 6 step: 1502, loss is 0.010625583119690418\n",
      "epoch: 6 step: 1503, loss is 0.003862304612994194\n",
      "epoch: 6 step: 1504, loss is 0.011172152124345303\n",
      "epoch: 6 step: 1505, loss is 0.14809691905975342\n",
      "epoch: 6 step: 1506, loss is 0.0023402124643325806\n",
      "epoch: 6 step: 1507, loss is 0.01915900409221649\n",
      "epoch: 6 step: 1508, loss is 0.0048802560195326805\n",
      "epoch: 6 step: 1509, loss is 0.041896577924489975\n",
      "epoch: 6 step: 1510, loss is 0.07781236618757248\n",
      "epoch: 6 step: 1511, loss is 0.08723247051239014\n",
      "epoch: 6 step: 1512, loss is 0.04120215028524399\n",
      "epoch: 6 step: 1513, loss is 0.011111395433545113\n",
      "epoch: 6 step: 1514, loss is 0.0022185437846928835\n",
      "epoch: 6 step: 1515, loss is 0.008251303806900978\n",
      "epoch: 6 step: 1516, loss is 0.06162058189511299\n",
      "epoch: 6 step: 1517, loss is 0.3021758794784546\n",
      "epoch: 6 step: 1518, loss is 0.01735907606780529\n",
      "epoch: 6 step: 1519, loss is 0.010406039655208588\n",
      "epoch: 6 step: 1520, loss is 0.007772170007228851\n",
      "epoch: 6 step: 1521, loss is 0.025401344522833824\n",
      "epoch: 6 step: 1522, loss is 0.003915566951036453\n",
      "epoch: 6 step: 1523, loss is 0.012625178322196007\n",
      "epoch: 6 step: 1524, loss is 0.013294973410665989\n",
      "epoch: 6 step: 1525, loss is 0.007430740166455507\n",
      "epoch: 6 step: 1526, loss is 0.003038262715563178\n",
      "epoch: 6 step: 1527, loss is 0.009991232305765152\n",
      "epoch: 6 step: 1528, loss is 0.002820975612848997\n",
      "epoch: 6 step: 1529, loss is 0.028194637969136238\n",
      "epoch: 6 step: 1530, loss is 0.001217536279000342\n",
      "epoch: 6 step: 1531, loss is 0.09643341600894928\n",
      "epoch: 6 step: 1532, loss is 0.36317822337150574\n",
      "epoch: 6 step: 1533, loss is 0.0042941332794725895\n",
      "epoch: 6 step: 1534, loss is 0.0081557622179389\n",
      "epoch: 6 step: 1535, loss is 0.027387818321585655\n",
      "epoch: 6 step: 1536, loss is 0.008212260901927948\n",
      "epoch: 6 step: 1537, loss is 0.012940465472638607\n",
      "epoch: 6 step: 1538, loss is 0.11238456517457962\n",
      "epoch: 6 step: 1539, loss is 0.0006479984731413424\n",
      "epoch: 6 step: 1540, loss is 0.0782446563243866\n",
      "epoch: 6 step: 1541, loss is 0.11561708897352219\n",
      "epoch: 6 step: 1542, loss is 0.1493125706911087\n",
      "epoch: 6 step: 1543, loss is 0.002625593915581703\n",
      "epoch: 6 step: 1544, loss is 0.01073176134377718\n",
      "epoch: 6 step: 1545, loss is 0.007688031531870365\n",
      "epoch: 6 step: 1546, loss is 0.002324833068996668\n",
      "epoch: 6 step: 1547, loss is 0.020584046840667725\n",
      "epoch: 6 step: 1548, loss is 0.01991056650876999\n",
      "epoch: 6 step: 1549, loss is 0.014876123517751694\n",
      "epoch: 6 step: 1550, loss is 0.005472260992974043\n",
      "epoch: 6 step: 1551, loss is 0.046013765037059784\n",
      "epoch: 6 step: 1552, loss is 0.022366177290678024\n",
      "epoch: 6 step: 1553, loss is 0.0035937982611358166\n",
      "epoch: 6 step: 1554, loss is 0.10273541510105133\n",
      "epoch: 6 step: 1555, loss is 0.0009653683518990874\n",
      "epoch: 6 step: 1556, loss is 0.046449292451143265\n",
      "epoch: 6 step: 1557, loss is 0.015384742990136147\n",
      "epoch: 6 step: 1558, loss is 0.1422739326953888\n",
      "epoch: 6 step: 1559, loss is 0.0029247538186609745\n",
      "epoch: 6 step: 1560, loss is 0.06355705857276917\n",
      "epoch: 6 step: 1561, loss is 0.0008769272826611996\n",
      "epoch: 6 step: 1562, loss is 0.007772365119308233\n",
      "epoch: 6 step: 1563, loss is 0.013207818381488323\n",
      "epoch: 6 step: 1564, loss is 0.0007688783225603402\n",
      "epoch: 6 step: 1565, loss is 0.07085848599672318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 1566, loss is 0.006372158881276846\n",
      "epoch: 6 step: 1567, loss is 0.03457724675536156\n",
      "epoch: 6 step: 1568, loss is 0.11538293957710266\n",
      "epoch: 6 step: 1569, loss is 0.00044551194878295064\n",
      "epoch: 6 step: 1570, loss is 0.14164109528064728\n",
      "epoch: 6 step: 1571, loss is 0.01634668931365013\n",
      "epoch: 6 step: 1572, loss is 0.01704879105091095\n",
      "epoch: 6 step: 1573, loss is 0.002643690211698413\n",
      "epoch: 6 step: 1574, loss is 0.000801311747636646\n",
      "epoch: 6 step: 1575, loss is 0.001441874774172902\n",
      "epoch: 6 step: 1576, loss is 0.0213417187333107\n",
      "epoch: 6 step: 1577, loss is 0.003653159597888589\n",
      "epoch: 6 step: 1578, loss is 0.047663696110248566\n",
      "epoch: 6 step: 1579, loss is 0.0008246779907494783\n",
      "epoch: 6 step: 1580, loss is 0.039546675980091095\n",
      "epoch: 6 step: 1581, loss is 0.008864878676831722\n",
      "epoch: 6 step: 1582, loss is 0.01684415340423584\n",
      "epoch: 6 step: 1583, loss is 0.031142422929406166\n",
      "epoch: 6 step: 1584, loss is 0.0006697073695249856\n",
      "epoch: 6 step: 1585, loss is 0.0014770124107599258\n",
      "epoch: 6 step: 1586, loss is 0.004455811344087124\n",
      "epoch: 6 step: 1587, loss is 0.00225479225628078\n",
      "epoch: 6 step: 1588, loss is 0.024862341582775116\n",
      "epoch: 6 step: 1589, loss is 0.15825709700584412\n",
      "epoch: 6 step: 1590, loss is 0.008419639430940151\n",
      "epoch: 6 step: 1591, loss is 0.004733451176434755\n",
      "epoch: 6 step: 1592, loss is 0.006142820231616497\n",
      "epoch: 6 step: 1593, loss is 0.004772076848894358\n",
      "epoch: 6 step: 1594, loss is 0.018514566123485565\n",
      "epoch: 6 step: 1595, loss is 0.02586781047284603\n",
      "epoch: 6 step: 1596, loss is 0.14639413356781006\n",
      "epoch: 6 step: 1597, loss is 0.008080580271780491\n",
      "epoch: 6 step: 1598, loss is 0.00033441619598306715\n",
      "epoch: 6 step: 1599, loss is 0.0006922803586348891\n",
      "epoch: 6 step: 1600, loss is 0.003673142520710826\n",
      "epoch: 6 step: 1601, loss is 0.004798513371497393\n",
      "epoch: 6 step: 1602, loss is 0.0016171563183888793\n",
      "epoch: 6 step: 1603, loss is 0.0070198904722929\n",
      "epoch: 6 step: 1604, loss is 0.008172922767698765\n",
      "epoch: 6 step: 1605, loss is 0.0073850457556545734\n",
      "epoch: 6 step: 1606, loss is 0.004997143056243658\n",
      "epoch: 6 step: 1607, loss is 0.004250649828463793\n",
      "epoch: 6 step: 1608, loss is 0.19266028702259064\n",
      "epoch: 6 step: 1609, loss is 0.020225808024406433\n",
      "epoch: 6 step: 1610, loss is 0.0012668673880398273\n",
      "epoch: 6 step: 1611, loss is 0.006331798620522022\n",
      "epoch: 6 step: 1612, loss is 0.008988327346742153\n",
      "epoch: 6 step: 1613, loss is 0.020341292023658752\n",
      "epoch: 6 step: 1614, loss is 0.02778719551861286\n",
      "epoch: 6 step: 1615, loss is 0.005558383651077747\n",
      "epoch: 6 step: 1616, loss is 0.046032197773456573\n",
      "epoch: 6 step: 1617, loss is 0.03065936639904976\n",
      "epoch: 6 step: 1618, loss is 0.0004696110263466835\n",
      "epoch: 6 step: 1619, loss is 0.0002807428245432675\n",
      "epoch: 6 step: 1620, loss is 0.015520527958869934\n",
      "epoch: 6 step: 1621, loss is 0.004462371580302715\n",
      "epoch: 6 step: 1622, loss is 0.012642163783311844\n",
      "epoch: 6 step: 1623, loss is 0.10006008297204971\n",
      "epoch: 6 step: 1624, loss is 0.022163979709148407\n",
      "epoch: 6 step: 1625, loss is 0.008668058551847935\n",
      "epoch: 6 step: 1626, loss is 0.008857771754264832\n",
      "epoch: 6 step: 1627, loss is 0.006660594139248133\n",
      "epoch: 6 step: 1628, loss is 0.005798010621219873\n",
      "epoch: 6 step: 1629, loss is 0.10957808792591095\n",
      "epoch: 6 step: 1630, loss is 0.01482508983463049\n",
      "epoch: 6 step: 1631, loss is 0.0018834657967090607\n",
      "epoch: 6 step: 1632, loss is 0.04105376824736595\n",
      "epoch: 6 step: 1633, loss is 0.00012173331197118387\n",
      "epoch: 6 step: 1634, loss is 0.002434756839647889\n",
      "epoch: 6 step: 1635, loss is 0.008478660136461258\n",
      "epoch: 6 step: 1636, loss is 0.0005722556379623711\n",
      "epoch: 6 step: 1637, loss is 0.002051540184766054\n",
      "epoch: 6 step: 1638, loss is 0.02452602982521057\n",
      "epoch: 6 step: 1639, loss is 0.007538307458162308\n",
      "epoch: 6 step: 1640, loss is 0.04666247218847275\n",
      "epoch: 6 step: 1641, loss is 0.002610043855383992\n",
      "epoch: 6 step: 1642, loss is 0.003736605169251561\n",
      "epoch: 6 step: 1643, loss is 0.014330809935927391\n",
      "epoch: 6 step: 1644, loss is 0.09856884181499481\n",
      "epoch: 6 step: 1645, loss is 0.0004321012238506228\n",
      "epoch: 6 step: 1646, loss is 0.004313807934522629\n",
      "epoch: 6 step: 1647, loss is 0.0052443682216107845\n",
      "epoch: 6 step: 1648, loss is 0.0008805569959804416\n",
      "epoch: 6 step: 1649, loss is 0.04764438420534134\n",
      "epoch: 6 step: 1650, loss is 0.0013446517987176776\n",
      "epoch: 6 step: 1651, loss is 0.0012934764381498098\n",
      "epoch: 6 step: 1652, loss is 0.0008150138892233372\n",
      "epoch: 6 step: 1653, loss is 0.12455575913190842\n",
      "epoch: 6 step: 1654, loss is 0.04845614731311798\n",
      "epoch: 6 step: 1655, loss is 0.007039656862616539\n",
      "epoch: 6 step: 1656, loss is 0.10933143645524979\n",
      "epoch: 6 step: 1657, loss is 0.20602048933506012\n",
      "epoch: 6 step: 1658, loss is 0.000778710120357573\n",
      "epoch: 6 step: 1659, loss is 0.014023569412529469\n",
      "epoch: 6 step: 1660, loss is 0.03772958368062973\n",
      "epoch: 6 step: 1661, loss is 0.0029432326555252075\n",
      "epoch: 6 step: 1662, loss is 0.10641758143901825\n",
      "epoch: 6 step: 1663, loss is 0.00340382382273674\n",
      "epoch: 6 step: 1664, loss is 0.001023029675707221\n",
      "epoch: 6 step: 1665, loss is 0.0004973621689714491\n",
      "epoch: 6 step: 1666, loss is 0.012364547699689865\n",
      "epoch: 6 step: 1667, loss is 0.02360418252646923\n",
      "epoch: 6 step: 1668, loss is 0.011094348505139351\n",
      "epoch: 6 step: 1669, loss is 0.0043767825700342655\n",
      "epoch: 6 step: 1670, loss is 0.056291911751031876\n",
      "epoch: 6 step: 1671, loss is 0.0012986351503059268\n",
      "epoch: 6 step: 1672, loss is 0.16208142042160034\n",
      "epoch: 6 step: 1673, loss is 0.011577552184462547\n",
      "epoch: 6 step: 1674, loss is 0.08413885533809662\n",
      "epoch: 6 step: 1675, loss is 0.002504850272089243\n",
      "epoch: 6 step: 1676, loss is 0.07015062123537064\n",
      "epoch: 6 step: 1677, loss is 0.030195778235793114\n",
      "epoch: 6 step: 1678, loss is 0.064192995429039\n",
      "epoch: 6 step: 1679, loss is 0.18725153803825378\n",
      "epoch: 6 step: 1680, loss is 0.11819595098495483\n",
      "epoch: 6 step: 1681, loss is 0.06960845738649368\n",
      "epoch: 6 step: 1682, loss is 0.04677587375044823\n",
      "epoch: 6 step: 1683, loss is 0.0023427470587193966\n",
      "epoch: 6 step: 1684, loss is 0.10602393001317978\n",
      "epoch: 6 step: 1685, loss is 0.00048691537813283503\n",
      "epoch: 6 step: 1686, loss is 0.02904016710817814\n",
      "epoch: 6 step: 1687, loss is 0.005826793145388365\n",
      "epoch: 6 step: 1688, loss is 0.03369424119591713\n",
      "epoch: 6 step: 1689, loss is 0.015478529036045074\n",
      "epoch: 6 step: 1690, loss is 0.006807317957282066\n",
      "epoch: 6 step: 1691, loss is 0.003879409981891513\n",
      "epoch: 6 step: 1692, loss is 0.07480678707361221\n",
      "epoch: 6 step: 1693, loss is 0.016243882477283478\n",
      "epoch: 6 step: 1694, loss is 0.011722225695848465\n",
      "epoch: 6 step: 1695, loss is 0.0007421108894050121\n",
      "epoch: 6 step: 1696, loss is 0.010448918677866459\n",
      "epoch: 6 step: 1697, loss is 0.12896588444709778\n",
      "epoch: 6 step: 1698, loss is 0.012929985299706459\n",
      "epoch: 6 step: 1699, loss is 0.004328740760684013\n",
      "epoch: 6 step: 1700, loss is 7.805861969245598e-05\n",
      "epoch: 6 step: 1701, loss is 0.0006697505014017224\n",
      "epoch: 6 step: 1702, loss is 0.0008031946490518749\n",
      "epoch: 6 step: 1703, loss is 0.0004412211710587144\n",
      "epoch: 6 step: 1704, loss is 0.03495659679174423\n",
      "epoch: 6 step: 1705, loss is 0.003982134163379669\n",
      "epoch: 6 step: 1706, loss is 0.0028766351751983166\n",
      "epoch: 6 step: 1707, loss is 0.11487951874732971\n",
      "epoch: 6 step: 1708, loss is 0.004352484364062548\n",
      "epoch: 6 step: 1709, loss is 0.0012414894299581647\n",
      "epoch: 6 step: 1710, loss is 0.002495233668014407\n",
      "epoch: 6 step: 1711, loss is 0.00910267885774374\n",
      "epoch: 6 step: 1712, loss is 0.043678030371665955\n",
      "epoch: 6 step: 1713, loss is 0.01636093482375145\n",
      "epoch: 6 step: 1714, loss is 0.03966551646590233\n",
      "epoch: 6 step: 1715, loss is 0.051120202988386154\n",
      "epoch: 6 step: 1716, loss is 0.026096247136592865\n",
      "epoch: 6 step: 1717, loss is 0.000675106537528336\n",
      "epoch: 6 step: 1718, loss is 0.0008263604249805212\n",
      "epoch: 6 step: 1719, loss is 0.0037106203380972147\n",
      "epoch: 6 step: 1720, loss is 0.04271502420306206\n",
      "epoch: 6 step: 1721, loss is 0.031982336193323135\n",
      "epoch: 6 step: 1722, loss is 0.015878815203905106\n",
      "epoch: 6 step: 1723, loss is 0.008364280685782433\n",
      "epoch: 6 step: 1724, loss is 0.004777595866471529\n",
      "epoch: 6 step: 1725, loss is 0.009308005683124065\n",
      "epoch: 6 step: 1726, loss is 0.09556961804628372\n",
      "epoch: 6 step: 1727, loss is 0.0011101489653810859\n",
      "epoch: 6 step: 1728, loss is 0.018834426999092102\n",
      "epoch: 6 step: 1729, loss is 0.009150016121566296\n",
      "epoch: 6 step: 1730, loss is 0.0839390978217125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 1731, loss is 0.047110266983509064\n",
      "epoch: 6 step: 1732, loss is 0.08404800295829773\n",
      "epoch: 6 step: 1733, loss is 0.10143083333969116\n",
      "epoch: 6 step: 1734, loss is 0.000733247899916023\n",
      "epoch: 6 step: 1735, loss is 0.0007921800133772194\n",
      "epoch: 6 step: 1736, loss is 0.0001349689846392721\n",
      "epoch: 6 step: 1737, loss is 0.00887230597436428\n",
      "epoch: 6 step: 1738, loss is 0.001955973682925105\n",
      "epoch: 6 step: 1739, loss is 0.001702473615296185\n",
      "epoch: 6 step: 1740, loss is 0.0008000591187737882\n",
      "epoch: 6 step: 1741, loss is 0.007030270993709564\n",
      "epoch: 6 step: 1742, loss is 0.004304682835936546\n",
      "epoch: 6 step: 1743, loss is 0.043444979935884476\n",
      "epoch: 6 step: 1744, loss is 0.03234868124127388\n",
      "epoch: 6 step: 1745, loss is 0.0077179488725960255\n",
      "epoch: 6 step: 1746, loss is 0.016823777928948402\n",
      "epoch: 6 step: 1747, loss is 0.03029954805970192\n",
      "epoch: 6 step: 1748, loss is 0.000884679495356977\n",
      "epoch: 6 step: 1749, loss is 0.00441927183419466\n",
      "epoch: 6 step: 1750, loss is 0.013441442511975765\n",
      "epoch: 6 step: 1751, loss is 0.02282468043267727\n",
      "epoch: 6 step: 1752, loss is 0.00818522647023201\n",
      "epoch: 6 step: 1753, loss is 0.01783856563270092\n",
      "epoch: 6 step: 1754, loss is 0.019564274698495865\n",
      "epoch: 6 step: 1755, loss is 0.0018204047810286283\n",
      "epoch: 6 step: 1756, loss is 0.0015101294266059995\n",
      "epoch: 6 step: 1757, loss is 0.054483626037836075\n",
      "epoch: 6 step: 1758, loss is 0.005186410620808601\n",
      "epoch: 6 step: 1759, loss is 0.028689732775092125\n",
      "epoch: 6 step: 1760, loss is 0.05112357810139656\n",
      "epoch: 6 step: 1761, loss is 0.17362025380134583\n",
      "epoch: 6 step: 1762, loss is 0.02062756009399891\n",
      "epoch: 6 step: 1763, loss is 0.001945305848494172\n",
      "epoch: 6 step: 1764, loss is 0.0016607886645942926\n",
      "epoch: 6 step: 1765, loss is 0.12301415205001831\n",
      "epoch: 6 step: 1766, loss is 0.10226872563362122\n",
      "epoch: 6 step: 1767, loss is 0.0012983274646103382\n",
      "epoch: 6 step: 1768, loss is 0.009175789542496204\n",
      "epoch: 6 step: 1769, loss is 0.00019639315723907202\n",
      "epoch: 6 step: 1770, loss is 0.005241743288934231\n",
      "epoch: 6 step: 1771, loss is 0.02364487200975418\n",
      "epoch: 6 step: 1772, loss is 0.35227343440055847\n",
      "epoch: 6 step: 1773, loss is 0.0017144063021987677\n",
      "epoch: 6 step: 1774, loss is 0.025300122797489166\n",
      "epoch: 6 step: 1775, loss is 0.006082320120185614\n",
      "epoch: 6 step: 1776, loss is 0.0001541618985356763\n",
      "epoch: 6 step: 1777, loss is 0.017312200739979744\n",
      "epoch: 6 step: 1778, loss is 0.04354337602853775\n",
      "epoch: 6 step: 1779, loss is 0.03245355561375618\n",
      "epoch: 6 step: 1780, loss is 0.0008308523683808744\n",
      "epoch: 6 step: 1781, loss is 0.014394928701221943\n",
      "epoch: 6 step: 1782, loss is 0.024382460862398148\n",
      "epoch: 6 step: 1783, loss is 0.008601192384958267\n",
      "epoch: 6 step: 1784, loss is 0.0008173655369319022\n",
      "epoch: 6 step: 1785, loss is 0.10231468081474304\n",
      "epoch: 6 step: 1786, loss is 0.0017844904214143753\n",
      "epoch: 6 step: 1787, loss is 0.003639989299699664\n",
      "epoch: 6 step: 1788, loss is 0.0035586406011134386\n",
      "epoch: 6 step: 1789, loss is 0.028149450197815895\n",
      "epoch: 6 step: 1790, loss is 0.11023963987827301\n",
      "epoch: 6 step: 1791, loss is 0.001336947432719171\n",
      "epoch: 6 step: 1792, loss is 9.70568653428927e-05\n",
      "epoch: 6 step: 1793, loss is 0.197462260723114\n",
      "epoch: 6 step: 1794, loss is 0.002376084914430976\n",
      "epoch: 6 step: 1795, loss is 0.0002785823598969728\n",
      "epoch: 6 step: 1796, loss is 0.0153803750872612\n",
      "epoch: 6 step: 1797, loss is 0.001566032413393259\n",
      "epoch: 6 step: 1798, loss is 0.0517878457903862\n",
      "epoch: 6 step: 1799, loss is 0.00027841792325489223\n",
      "epoch: 6 step: 1800, loss is 0.12112980335950851\n",
      "epoch: 6 step: 1801, loss is 0.010862525552511215\n",
      "epoch: 6 step: 1802, loss is 0.0005845064297318459\n",
      "epoch: 6 step: 1803, loss is 0.004295438528060913\n",
      "epoch: 6 step: 1804, loss is 0.0014169614296406507\n",
      "epoch: 6 step: 1805, loss is 0.0005699943285435438\n",
      "epoch: 6 step: 1806, loss is 0.020592346787452698\n",
      "epoch: 6 step: 1807, loss is 0.010451667942106724\n",
      "epoch: 6 step: 1808, loss is 0.0022172946482896805\n",
      "epoch: 6 step: 1809, loss is 0.09994511306285858\n",
      "epoch: 6 step: 1810, loss is 0.03266577050089836\n",
      "epoch: 6 step: 1811, loss is 0.0030521186999976635\n",
      "epoch: 6 step: 1812, loss is 0.013128614984452724\n",
      "epoch: 6 step: 1813, loss is 0.0011217201827093959\n",
      "epoch: 6 step: 1814, loss is 0.014698694460093975\n",
      "epoch: 6 step: 1815, loss is 0.001357754343189299\n",
      "epoch: 6 step: 1816, loss is 0.03947487846016884\n",
      "epoch: 6 step: 1817, loss is 0.0006626406102441251\n",
      "epoch: 6 step: 1818, loss is 0.08015976846218109\n",
      "epoch: 6 step: 1819, loss is 0.002400848548859358\n",
      "epoch: 6 step: 1820, loss is 0.011766333132982254\n",
      "epoch: 6 step: 1821, loss is 0.00032526624272577465\n",
      "epoch: 6 step: 1822, loss is 0.012651271186769009\n",
      "epoch: 6 step: 1823, loss is 0.009266776964068413\n",
      "epoch: 6 step: 1824, loss is 0.013681161217391491\n",
      "epoch: 6 step: 1825, loss is 0.013481819070875645\n",
      "epoch: 6 step: 1826, loss is 0.01878221519291401\n",
      "epoch: 6 step: 1827, loss is 0.012574024498462677\n",
      "epoch: 6 step: 1828, loss is 0.048360537737607956\n",
      "epoch: 6 step: 1829, loss is 0.0025023967027664185\n",
      "epoch: 6 step: 1830, loss is 0.17483791708946228\n",
      "epoch: 6 step: 1831, loss is 0.016144247725605965\n",
      "epoch: 6 step: 1832, loss is 0.030211569741368294\n",
      "epoch: 6 step: 1833, loss is 2.4800858227536082e-05\n",
      "epoch: 6 step: 1834, loss is 0.001049176906235516\n",
      "epoch: 6 step: 1835, loss is 0.0026105463039129972\n",
      "epoch: 6 step: 1836, loss is 0.006016659550368786\n",
      "epoch: 6 step: 1837, loss is 0.00053234031656757\n",
      "epoch: 6 step: 1838, loss is 0.015498299151659012\n",
      "epoch: 6 step: 1839, loss is 0.02820564992725849\n",
      "epoch: 6 step: 1840, loss is 0.00985929649323225\n",
      "epoch: 6 step: 1841, loss is 0.006680919323116541\n",
      "epoch: 6 step: 1842, loss is 0.049056991934776306\n",
      "epoch: 6 step: 1843, loss is 0.010154481045901775\n",
      "epoch: 6 step: 1844, loss is 0.0004628953174687922\n",
      "epoch: 6 step: 1845, loss is 0.00012963059998583049\n",
      "epoch: 6 step: 1846, loss is 4.0096947486745194e-05\n",
      "epoch: 6 step: 1847, loss is 0.0031694089993834496\n",
      "epoch: 6 step: 1848, loss is 0.00021271634614095092\n",
      "epoch: 6 step: 1849, loss is 0.0030224821530282497\n",
      "epoch: 6 step: 1850, loss is 0.012349870055913925\n",
      "epoch: 6 step: 1851, loss is 0.00542898615822196\n",
      "epoch: 6 step: 1852, loss is 0.021757187321782112\n",
      "epoch: 6 step: 1853, loss is 0.001931667560711503\n",
      "epoch: 6 step: 1854, loss is 0.006706864107400179\n",
      "epoch: 6 step: 1855, loss is 0.0043030912056565285\n",
      "epoch: 6 step: 1856, loss is 0.002638806588947773\n",
      "epoch: 6 step: 1857, loss is 0.00637488067150116\n",
      "epoch: 6 step: 1858, loss is 0.0015308019937947392\n",
      "epoch: 6 step: 1859, loss is 0.003657725639641285\n",
      "epoch: 6 step: 1860, loss is 0.0039038793183863163\n",
      "epoch: 6 step: 1861, loss is 0.0005539358244277537\n",
      "epoch: 6 step: 1862, loss is 0.0022339732386171818\n",
      "epoch: 6 step: 1863, loss is 0.005972348619252443\n",
      "epoch: 6 step: 1864, loss is 0.006975709460675716\n",
      "epoch: 6 step: 1865, loss is 0.00328277749940753\n",
      "epoch: 6 step: 1866, loss is 0.00040713808266445994\n",
      "epoch: 6 step: 1867, loss is 0.07209962606430054\n",
      "epoch: 6 step: 1868, loss is 0.14816945791244507\n",
      "epoch: 6 step: 1869, loss is 0.00011845649714814499\n",
      "epoch: 6 step: 1870, loss is 0.00978960283100605\n",
      "epoch: 6 step: 1871, loss is 0.2180056869983673\n",
      "epoch: 6 step: 1872, loss is 0.06944716721773148\n",
      "epoch: 6 step: 1873, loss is 0.0007381078903563321\n",
      "epoch: 6 step: 1874, loss is 0.036128826439380646\n",
      "epoch: 6 step: 1875, loss is 0.24134241044521332\n",
      "epoch: 7 step: 1, loss is 0.06336607784032822\n",
      "epoch: 7 step: 2, loss is 0.006128682754933834\n",
      "epoch: 7 step: 3, loss is 0.030113104730844498\n",
      "epoch: 7 step: 4, loss is 0.04688010737299919\n",
      "epoch: 7 step: 5, loss is 0.07486201077699661\n",
      "epoch: 7 step: 6, loss is 0.002197752008214593\n",
      "epoch: 7 step: 7, loss is 0.11046043038368225\n",
      "epoch: 7 step: 8, loss is 0.0016339882276952267\n",
      "epoch: 7 step: 9, loss is 0.0017770613776519895\n",
      "epoch: 7 step: 10, loss is 0.04409372806549072\n",
      "epoch: 7 step: 11, loss is 0.0021409839391708374\n",
      "epoch: 7 step: 12, loss is 0.0009770635515451431\n",
      "epoch: 7 step: 13, loss is 0.006783515214920044\n",
      "epoch: 7 step: 14, loss is 0.03679794445633888\n",
      "epoch: 7 step: 15, loss is 0.0005746664828620851\n",
      "epoch: 7 step: 16, loss is 0.00047317740973085165\n",
      "epoch: 7 step: 17, loss is 0.004519042558968067\n",
      "epoch: 7 step: 18, loss is 0.0012597094755619764\n",
      "epoch: 7 step: 19, loss is 0.0006807768368162215\n",
      "epoch: 7 step: 20, loss is 0.0051133823581039906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 21, loss is 0.003464154666289687\n",
      "epoch: 7 step: 22, loss is 0.0027091014198958874\n",
      "epoch: 7 step: 23, loss is 0.006730870343744755\n",
      "epoch: 7 step: 24, loss is 0.002156489063054323\n",
      "epoch: 7 step: 25, loss is 0.0013742861337959766\n",
      "epoch: 7 step: 26, loss is 0.004784286487847567\n",
      "epoch: 7 step: 27, loss is 0.07635214924812317\n",
      "epoch: 7 step: 28, loss is 0.00022168349823914468\n",
      "epoch: 7 step: 29, loss is 0.0007750488584861159\n",
      "epoch: 7 step: 30, loss is 0.0032027503475546837\n",
      "epoch: 7 step: 31, loss is 0.2011139988899231\n",
      "epoch: 7 step: 32, loss is 0.04714620113372803\n",
      "epoch: 7 step: 33, loss is 0.0004469846317078918\n",
      "epoch: 7 step: 34, loss is 0.004830662161111832\n",
      "epoch: 7 step: 35, loss is 0.003028508275747299\n",
      "epoch: 7 step: 36, loss is 0.1818787008523941\n",
      "epoch: 7 step: 37, loss is 0.019729599356651306\n",
      "epoch: 7 step: 38, loss is 0.0007754528196528554\n",
      "epoch: 7 step: 39, loss is 0.0010051174322143197\n",
      "epoch: 7 step: 40, loss is 0.05740489810705185\n",
      "epoch: 7 step: 41, loss is 0.0006969644455239177\n",
      "epoch: 7 step: 42, loss is 0.0012926770141348243\n",
      "epoch: 7 step: 43, loss is 0.04064960405230522\n",
      "epoch: 7 step: 44, loss is 0.0013703418662771583\n",
      "epoch: 7 step: 45, loss is 0.0028085403610020876\n",
      "epoch: 7 step: 46, loss is 0.00034667830914258957\n",
      "epoch: 7 step: 47, loss is 0.00018214632291346788\n",
      "epoch: 7 step: 48, loss is 0.030138753354549408\n",
      "epoch: 7 step: 49, loss is 0.07435624301433563\n",
      "epoch: 7 step: 50, loss is 0.01351501327008009\n",
      "epoch: 7 step: 51, loss is 0.04504621401429176\n",
      "epoch: 7 step: 52, loss is 0.00437716580927372\n",
      "epoch: 7 step: 53, loss is 7.640395779162645e-05\n",
      "epoch: 7 step: 54, loss is 0.0034122515935450792\n",
      "epoch: 7 step: 55, loss is 0.12661206722259521\n",
      "epoch: 7 step: 56, loss is 0.06765567511320114\n",
      "epoch: 7 step: 57, loss is 0.033375222235918045\n",
      "epoch: 7 step: 58, loss is 0.0040248967707157135\n",
      "epoch: 7 step: 59, loss is 0.001211268361657858\n",
      "epoch: 7 step: 60, loss is 0.03156223148107529\n",
      "epoch: 7 step: 61, loss is 0.005974216386675835\n",
      "epoch: 7 step: 62, loss is 0.0015468597412109375\n",
      "epoch: 7 step: 63, loss is 0.012543084099888802\n",
      "epoch: 7 step: 64, loss is 0.000374109746189788\n",
      "epoch: 7 step: 65, loss is 0.022792920470237732\n",
      "epoch: 7 step: 66, loss is 0.008775969035923481\n",
      "epoch: 7 step: 67, loss is 0.0023791727144271135\n",
      "epoch: 7 step: 68, loss is 0.007246681023389101\n",
      "epoch: 7 step: 69, loss is 0.023631541058421135\n",
      "epoch: 7 step: 70, loss is 0.0011303095379844308\n",
      "epoch: 7 step: 71, loss is 0.009943364188075066\n",
      "epoch: 7 step: 72, loss is 0.003119097789749503\n",
      "epoch: 7 step: 73, loss is 0.014944221824407578\n",
      "epoch: 7 step: 74, loss is 0.0036321538500487804\n",
      "epoch: 7 step: 75, loss is 0.0037978405598551035\n",
      "epoch: 7 step: 76, loss is 0.0016223139828070998\n",
      "epoch: 7 step: 77, loss is 0.006359769497066736\n",
      "epoch: 7 step: 78, loss is 0.005627952516078949\n",
      "epoch: 7 step: 79, loss is 0.013804753310978413\n",
      "epoch: 7 step: 80, loss is 0.0015026300679892302\n",
      "epoch: 7 step: 81, loss is 0.006617537699639797\n",
      "epoch: 7 step: 82, loss is 0.002310141222551465\n",
      "epoch: 7 step: 83, loss is 0.0019778034184128046\n",
      "epoch: 7 step: 84, loss is 0.0005738495383411646\n",
      "epoch: 7 step: 85, loss is 0.007305275648832321\n",
      "epoch: 7 step: 86, loss is 0.006288762670010328\n",
      "epoch: 7 step: 87, loss is 0.018115399405360222\n",
      "epoch: 7 step: 88, loss is 0.0014999114209786057\n",
      "epoch: 7 step: 89, loss is 0.005031032022088766\n",
      "epoch: 7 step: 90, loss is 0.05662906542420387\n",
      "epoch: 7 step: 91, loss is 0.00040124368388205767\n",
      "epoch: 7 step: 92, loss is 0.0096723148599267\n",
      "epoch: 7 step: 93, loss is 0.004073162563145161\n",
      "epoch: 7 step: 94, loss is 0.040666401386260986\n",
      "epoch: 7 step: 95, loss is 0.018544137477874756\n",
      "epoch: 7 step: 96, loss is 0.02563624083995819\n",
      "epoch: 7 step: 97, loss is 0.007778612431138754\n",
      "epoch: 7 step: 98, loss is 0.006278141401708126\n",
      "epoch: 7 step: 99, loss is 0.034169476479291916\n",
      "epoch: 7 step: 100, loss is 0.04796067625284195\n",
      "epoch: 7 step: 101, loss is 0.002384648658335209\n",
      "epoch: 7 step: 102, loss is 0.0018016815884038806\n",
      "epoch: 7 step: 103, loss is 0.0003634316090028733\n",
      "epoch: 7 step: 104, loss is 0.052324555814266205\n",
      "epoch: 7 step: 105, loss is 0.0007493215380236506\n",
      "epoch: 7 step: 106, loss is 0.0011153448140248656\n",
      "epoch: 7 step: 107, loss is 0.0003098380984738469\n",
      "epoch: 7 step: 108, loss is 0.0036621943581849337\n",
      "epoch: 7 step: 109, loss is 0.00036578794242814183\n",
      "epoch: 7 step: 110, loss is 0.00016722566215321422\n",
      "epoch: 7 step: 111, loss is 0.0010580943198874593\n",
      "epoch: 7 step: 112, loss is 0.001241395017132163\n",
      "epoch: 7 step: 113, loss is 0.0003828365879599005\n",
      "epoch: 7 step: 114, loss is 0.08865007013082504\n",
      "epoch: 7 step: 115, loss is 0.20148296654224396\n",
      "epoch: 7 step: 116, loss is 0.010333609767258167\n",
      "epoch: 7 step: 117, loss is 0.04572777450084686\n",
      "epoch: 7 step: 118, loss is 0.0496155247092247\n",
      "epoch: 7 step: 119, loss is 0.010553375817835331\n",
      "epoch: 7 step: 120, loss is 0.002020883606746793\n",
      "epoch: 7 step: 121, loss is 0.016615327447652817\n",
      "epoch: 7 step: 122, loss is 0.001731076859869063\n",
      "epoch: 7 step: 123, loss is 0.0007197102531790733\n",
      "epoch: 7 step: 124, loss is 0.0018891305662691593\n",
      "epoch: 7 step: 125, loss is 0.0014397738268598914\n",
      "epoch: 7 step: 126, loss is 0.0006836103857494891\n",
      "epoch: 7 step: 127, loss is 0.008210624568164349\n",
      "epoch: 7 step: 128, loss is 0.04451984912157059\n",
      "epoch: 7 step: 129, loss is 0.0018961216555908322\n",
      "epoch: 7 step: 130, loss is 0.0037254132330417633\n",
      "epoch: 7 step: 131, loss is 0.14018341898918152\n",
      "epoch: 7 step: 132, loss is 0.0037040237803012133\n",
      "epoch: 7 step: 133, loss is 0.08639399707317352\n",
      "epoch: 7 step: 134, loss is 0.0021894166711717844\n",
      "epoch: 7 step: 135, loss is 0.021314438432455063\n",
      "epoch: 7 step: 136, loss is 0.017982706427574158\n",
      "epoch: 7 step: 137, loss is 0.015874873846769333\n",
      "epoch: 7 step: 138, loss is 0.003881909651681781\n",
      "epoch: 7 step: 139, loss is 0.015289130620658398\n",
      "epoch: 7 step: 140, loss is 0.09591590613126755\n",
      "epoch: 7 step: 141, loss is 0.17923769354820251\n",
      "epoch: 7 step: 142, loss is 0.0010743214515969157\n",
      "epoch: 7 step: 143, loss is 0.02151726372539997\n",
      "epoch: 7 step: 144, loss is 0.0027850919868797064\n",
      "epoch: 7 step: 145, loss is 0.0035152314230799675\n",
      "epoch: 7 step: 146, loss is 0.0007990443264134228\n",
      "epoch: 7 step: 147, loss is 0.09161343425512314\n",
      "epoch: 7 step: 148, loss is 0.08082833886146545\n",
      "epoch: 7 step: 149, loss is 0.0014716384466737509\n",
      "epoch: 7 step: 150, loss is 0.002016979968175292\n",
      "epoch: 7 step: 151, loss is 0.00421736529096961\n",
      "epoch: 7 step: 152, loss is 0.0014260264579206705\n",
      "epoch: 7 step: 153, loss is 0.07766574621200562\n",
      "epoch: 7 step: 154, loss is 0.0009371159831061959\n",
      "epoch: 7 step: 155, loss is 0.005949343089014292\n",
      "epoch: 7 step: 156, loss is 0.0014075819635763764\n",
      "epoch: 7 step: 157, loss is 0.006974251940846443\n",
      "epoch: 7 step: 158, loss is 0.0008509946055710316\n",
      "epoch: 7 step: 159, loss is 0.409060537815094\n",
      "epoch: 7 step: 160, loss is 0.004855625331401825\n",
      "epoch: 7 step: 161, loss is 0.001904014265164733\n",
      "epoch: 7 step: 162, loss is 0.046578217297792435\n",
      "epoch: 7 step: 163, loss is 0.09347908198833466\n",
      "epoch: 7 step: 164, loss is 0.0025256744120270014\n",
      "epoch: 7 step: 165, loss is 0.0027050036005675793\n",
      "epoch: 7 step: 166, loss is 0.05713415518403053\n",
      "epoch: 7 step: 167, loss is 0.0024338793009519577\n",
      "epoch: 7 step: 168, loss is 0.0004342365136835724\n",
      "epoch: 7 step: 169, loss is 0.0043286168947815895\n",
      "epoch: 7 step: 170, loss is 0.0486118383705616\n",
      "epoch: 7 step: 171, loss is 0.015381970442831516\n",
      "epoch: 7 step: 172, loss is 0.019191812723875046\n",
      "epoch: 7 step: 173, loss is 0.00016380086890421808\n",
      "epoch: 7 step: 174, loss is 0.03307327628135681\n",
      "epoch: 7 step: 175, loss is 0.000552851299289614\n",
      "epoch: 7 step: 176, loss is 0.00022896916198078543\n",
      "epoch: 7 step: 177, loss is 0.013948294334113598\n",
      "epoch: 7 step: 178, loss is 0.007356937509030104\n",
      "epoch: 7 step: 179, loss is 0.03172456845641136\n",
      "epoch: 7 step: 180, loss is 0.00610431469976902\n",
      "epoch: 7 step: 181, loss is 0.000482209085021168\n",
      "epoch: 7 step: 182, loss is 0.11225057393312454\n",
      "epoch: 7 step: 183, loss is 0.034093957394361496\n",
      "epoch: 7 step: 184, loss is 0.0036033899523317814\n",
      "epoch: 7 step: 185, loss is 0.006076579913496971\n",
      "epoch: 7 step: 186, loss is 0.0015902334125712514\n",
      "epoch: 7 step: 187, loss is 0.0028659719973802567\n",
      "epoch: 7 step: 188, loss is 0.010348004288971424\n",
      "epoch: 7 step: 189, loss is 0.03843560814857483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 190, loss is 0.004096479620784521\n",
      "epoch: 7 step: 191, loss is 0.013342661783099174\n",
      "epoch: 7 step: 192, loss is 0.014230284839868546\n",
      "epoch: 7 step: 193, loss is 0.0010814626002684236\n",
      "epoch: 7 step: 194, loss is 0.0002480784896761179\n",
      "epoch: 7 step: 195, loss is 0.0003355625958647579\n",
      "epoch: 7 step: 196, loss is 0.00010713166557252407\n",
      "epoch: 7 step: 197, loss is 0.03260215371847153\n",
      "epoch: 7 step: 198, loss is 0.006076513789594173\n",
      "epoch: 7 step: 199, loss is 0.0003203312517143786\n",
      "epoch: 7 step: 200, loss is 0.0002859939122572541\n",
      "epoch: 7 step: 201, loss is 0.000910756818484515\n",
      "epoch: 7 step: 202, loss is 0.0015094351256266236\n",
      "epoch: 7 step: 203, loss is 0.018058184534311295\n",
      "epoch: 7 step: 204, loss is 0.0035325929056853056\n",
      "epoch: 7 step: 205, loss is 0.005388560704886913\n",
      "epoch: 7 step: 206, loss is 0.002113133203238249\n",
      "epoch: 7 step: 207, loss is 0.0023939332459121943\n",
      "epoch: 7 step: 208, loss is 0.00029086708673276007\n",
      "epoch: 7 step: 209, loss is 0.000983229372650385\n",
      "epoch: 7 step: 210, loss is 0.02223997935652733\n",
      "epoch: 7 step: 211, loss is 0.004175301641225815\n",
      "epoch: 7 step: 212, loss is 0.00024411598860751837\n",
      "epoch: 7 step: 213, loss is 0.00018594093853607774\n",
      "epoch: 7 step: 214, loss is 0.005355373490601778\n",
      "epoch: 7 step: 215, loss is 0.0026574109215289354\n",
      "epoch: 7 step: 216, loss is 0.1927405148744583\n",
      "epoch: 7 step: 217, loss is 0.00013109981955494732\n",
      "epoch: 7 step: 218, loss is 9.423440496902913e-05\n",
      "epoch: 7 step: 219, loss is 0.0010260610142722726\n",
      "epoch: 7 step: 220, loss is 0.03576104715466499\n",
      "epoch: 7 step: 221, loss is 0.00029830046696588397\n",
      "epoch: 7 step: 222, loss is 0.007419288624078035\n",
      "epoch: 7 step: 223, loss is 0.0024819616228342056\n",
      "epoch: 7 step: 224, loss is 0.0028732374776154757\n",
      "epoch: 7 step: 225, loss is 0.07359366863965988\n",
      "epoch: 7 step: 226, loss is 0.013701428659260273\n",
      "epoch: 7 step: 227, loss is 0.06330601125955582\n",
      "epoch: 7 step: 228, loss is 0.017569461837410927\n",
      "epoch: 7 step: 229, loss is 0.0015141416806727648\n",
      "epoch: 7 step: 230, loss is 0.0022471374832093716\n",
      "epoch: 7 step: 231, loss is 0.00046223317622207105\n",
      "epoch: 7 step: 232, loss is 0.00018582912161946297\n",
      "epoch: 7 step: 233, loss is 0.004088802728801966\n",
      "epoch: 7 step: 234, loss is 0.0036918839905411005\n",
      "epoch: 7 step: 235, loss is 0.000698174349963665\n",
      "epoch: 7 step: 236, loss is 0.0029701530002057552\n",
      "epoch: 7 step: 237, loss is 0.004632513038814068\n",
      "epoch: 7 step: 238, loss is 0.0014559426344931126\n",
      "epoch: 7 step: 239, loss is 0.0006719076191075146\n",
      "epoch: 7 step: 240, loss is 0.003499392420053482\n",
      "epoch: 7 step: 241, loss is 0.019416479393839836\n",
      "epoch: 7 step: 242, loss is 0.2522658407688141\n",
      "epoch: 7 step: 243, loss is 0.0005737652536481619\n",
      "epoch: 7 step: 244, loss is 0.024613860994577408\n",
      "epoch: 7 step: 245, loss is 0.06856157630681992\n",
      "epoch: 7 step: 246, loss is 0.0007149314042180777\n",
      "epoch: 7 step: 247, loss is 0.08846402168273926\n",
      "epoch: 7 step: 248, loss is 0.0026698161382228136\n",
      "epoch: 7 step: 249, loss is 0.019722094759345055\n",
      "epoch: 7 step: 250, loss is 0.003565171966329217\n",
      "epoch: 7 step: 251, loss is 0.013588429428637028\n",
      "epoch: 7 step: 252, loss is 0.005193004850298166\n",
      "epoch: 7 step: 253, loss is 0.0032012038864195347\n",
      "epoch: 7 step: 254, loss is 0.034455716609954834\n",
      "epoch: 7 step: 255, loss is 0.04677393659949303\n",
      "epoch: 7 step: 256, loss is 0.004237032495439053\n",
      "epoch: 7 step: 257, loss is 0.008360996842384338\n",
      "epoch: 7 step: 258, loss is 0.003307534847408533\n",
      "epoch: 7 step: 259, loss is 0.004072139505296946\n",
      "epoch: 7 step: 260, loss is 0.008409661240875721\n",
      "epoch: 7 step: 261, loss is 0.03334176912903786\n",
      "epoch: 7 step: 262, loss is 0.0003768444003071636\n",
      "epoch: 7 step: 263, loss is 0.11376633495092392\n",
      "epoch: 7 step: 264, loss is 0.0009668131242506206\n",
      "epoch: 7 step: 265, loss is 0.0002797003835439682\n",
      "epoch: 7 step: 266, loss is 0.007870608009397984\n",
      "epoch: 7 step: 267, loss is 0.009080614894628525\n",
      "epoch: 7 step: 268, loss is 0.0018444762099534273\n",
      "epoch: 7 step: 269, loss is 0.008564179763197899\n",
      "epoch: 7 step: 270, loss is 0.08716042339801788\n",
      "epoch: 7 step: 271, loss is 0.0017000423977151513\n",
      "epoch: 7 step: 272, loss is 0.05616408959031105\n",
      "epoch: 7 step: 273, loss is 0.002581217559054494\n",
      "epoch: 7 step: 274, loss is 0.0003537986776791513\n",
      "epoch: 7 step: 275, loss is 0.006461609620600939\n",
      "epoch: 7 step: 276, loss is 0.0018284550169482827\n",
      "epoch: 7 step: 277, loss is 0.004248320125043392\n",
      "epoch: 7 step: 278, loss is 0.0019263484282419086\n",
      "epoch: 7 step: 279, loss is 0.004182653501629829\n",
      "epoch: 7 step: 280, loss is 0.0008226155187003314\n",
      "epoch: 7 step: 281, loss is 0.004236284177750349\n",
      "epoch: 7 step: 282, loss is 0.004723967984318733\n",
      "epoch: 7 step: 283, loss is 0.008313065394759178\n",
      "epoch: 7 step: 284, loss is 0.012794595211744308\n",
      "epoch: 7 step: 285, loss is 0.0004181590920779854\n",
      "epoch: 7 step: 286, loss is 0.08041128516197205\n",
      "epoch: 7 step: 287, loss is 0.0024396609514951706\n",
      "epoch: 7 step: 288, loss is 0.0008946711895987391\n",
      "epoch: 7 step: 289, loss is 0.03570820763707161\n",
      "epoch: 7 step: 290, loss is 0.0015390521148219705\n",
      "epoch: 7 step: 291, loss is 0.0011668603401631117\n",
      "epoch: 7 step: 292, loss is 0.00037827182677574456\n",
      "epoch: 7 step: 293, loss is 0.0005360674113035202\n",
      "epoch: 7 step: 294, loss is 0.008709916844964027\n",
      "epoch: 7 step: 295, loss is 0.005521050188690424\n",
      "epoch: 7 step: 296, loss is 0.02805248461663723\n",
      "epoch: 7 step: 297, loss is 0.000827165087684989\n",
      "epoch: 7 step: 298, loss is 0.005980916786938906\n",
      "epoch: 7 step: 299, loss is 0.0011053233174607158\n",
      "epoch: 7 step: 300, loss is 0.0003416161925997585\n",
      "epoch: 7 step: 301, loss is 0.0013616590294986963\n",
      "epoch: 7 step: 302, loss is 0.0038395680021494627\n",
      "epoch: 7 step: 303, loss is 6.310148455668241e-05\n",
      "epoch: 7 step: 304, loss is 0.054919689893722534\n",
      "epoch: 7 step: 305, loss is 0.0005108878249302506\n",
      "epoch: 7 step: 306, loss is 0.0003451007651165128\n",
      "epoch: 7 step: 307, loss is 0.001334775355644524\n",
      "epoch: 7 step: 308, loss is 0.013243908062577248\n",
      "epoch: 7 step: 309, loss is 0.00014307355741038918\n",
      "epoch: 7 step: 310, loss is 0.000491181155666709\n",
      "epoch: 7 step: 311, loss is 0.0012375271180644631\n",
      "epoch: 7 step: 312, loss is 0.0007208283641375601\n",
      "epoch: 7 step: 313, loss is 0.006690951995551586\n",
      "epoch: 7 step: 314, loss is 0.10623454302549362\n",
      "epoch: 7 step: 315, loss is 0.004141378216445446\n",
      "epoch: 7 step: 316, loss is 0.034401725977659225\n",
      "epoch: 7 step: 317, loss is 0.034244269132614136\n",
      "epoch: 7 step: 318, loss is 0.013152058236300945\n",
      "epoch: 7 step: 319, loss is 0.011739940382540226\n",
      "epoch: 7 step: 320, loss is 0.0011695214780047536\n",
      "epoch: 7 step: 321, loss is 0.0005013535264879465\n",
      "epoch: 7 step: 322, loss is 0.025470776483416557\n",
      "epoch: 7 step: 323, loss is 0.003180768107995391\n",
      "epoch: 7 step: 324, loss is 0.0003935084387194365\n",
      "epoch: 7 step: 325, loss is 0.05633070692420006\n",
      "epoch: 7 step: 326, loss is 0.003232338698580861\n",
      "epoch: 7 step: 327, loss is 0.004157437477260828\n",
      "epoch: 7 step: 328, loss is 0.0002374787291046232\n",
      "epoch: 7 step: 329, loss is 6.23777523287572e-05\n",
      "epoch: 7 step: 330, loss is 0.0017871413147076964\n",
      "epoch: 7 step: 331, loss is 0.00047643709694966674\n",
      "epoch: 7 step: 332, loss is 0.0072011747397482395\n",
      "epoch: 7 step: 333, loss is 0.0010635649086907506\n",
      "epoch: 7 step: 334, loss is 0.016465403139591217\n",
      "epoch: 7 step: 335, loss is 0.001031234278343618\n",
      "epoch: 7 step: 336, loss is 0.0003651286824606359\n",
      "epoch: 7 step: 337, loss is 0.0020777867175638676\n",
      "epoch: 7 step: 338, loss is 0.001004665857180953\n",
      "epoch: 7 step: 339, loss is 0.007039617281407118\n",
      "epoch: 7 step: 340, loss is 0.001164548797532916\n",
      "epoch: 7 step: 341, loss is 0.001507126959040761\n",
      "epoch: 7 step: 342, loss is 0.06867729127407074\n",
      "epoch: 7 step: 343, loss is 0.0003908040525857359\n",
      "epoch: 7 step: 344, loss is 9.467001655139029e-05\n",
      "epoch: 7 step: 345, loss is 0.00013259252591524273\n",
      "epoch: 7 step: 346, loss is 0.01019302662461996\n",
      "epoch: 7 step: 347, loss is 0.0005796222831122577\n",
      "epoch: 7 step: 348, loss is 0.005664748139679432\n",
      "epoch: 7 step: 349, loss is 0.02195362001657486\n",
      "epoch: 7 step: 350, loss is 0.0019183621043339372\n",
      "epoch: 7 step: 351, loss is 5.3011022828286514e-05\n",
      "epoch: 7 step: 352, loss is 0.02949390560388565\n",
      "epoch: 7 step: 353, loss is 0.0011324493680149317\n",
      "epoch: 7 step: 354, loss is 0.04608951508998871\n",
      "epoch: 7 step: 355, loss is 0.00035770691465586424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 356, loss is 0.0020948899909853935\n",
      "epoch: 7 step: 357, loss is 0.04397192969918251\n",
      "epoch: 7 step: 358, loss is 0.034194719046354294\n",
      "epoch: 7 step: 359, loss is 0.0051353201270103455\n",
      "epoch: 7 step: 360, loss is 2.9065717171761207e-05\n",
      "epoch: 7 step: 361, loss is 0.005078487563878298\n",
      "epoch: 7 step: 362, loss is 0.0001687198382569477\n",
      "epoch: 7 step: 363, loss is 0.0006904819747433066\n",
      "epoch: 7 step: 364, loss is 0.0063762664794921875\n",
      "epoch: 7 step: 365, loss is 0.0026841757353395224\n",
      "epoch: 7 step: 366, loss is 0.0003614824381656945\n",
      "epoch: 7 step: 367, loss is 0.012135575525462627\n",
      "epoch: 7 step: 368, loss is 0.00354200741276145\n",
      "epoch: 7 step: 369, loss is 0.0010643318528309464\n",
      "epoch: 7 step: 370, loss is 0.12241379916667938\n",
      "epoch: 7 step: 371, loss is 0.005324934143573046\n",
      "epoch: 7 step: 372, loss is 0.0011136159300804138\n",
      "epoch: 7 step: 373, loss is 0.0027051230426877737\n",
      "epoch: 7 step: 374, loss is 0.02388645149767399\n",
      "epoch: 7 step: 375, loss is 0.02593313157558441\n",
      "epoch: 7 step: 376, loss is 0.0019318885169923306\n",
      "epoch: 7 step: 377, loss is 0.015139101073145866\n",
      "epoch: 7 step: 378, loss is 0.02120923437178135\n",
      "epoch: 7 step: 379, loss is 0.0064378525130450726\n",
      "epoch: 7 step: 380, loss is 0.00022506553796119988\n",
      "epoch: 7 step: 381, loss is 0.00023016412160359323\n",
      "epoch: 7 step: 382, loss is 0.003925638273358345\n",
      "epoch: 7 step: 383, loss is 0.0005360738141462207\n",
      "epoch: 7 step: 384, loss is 0.0424073301255703\n",
      "epoch: 7 step: 385, loss is 0.011672266758978367\n",
      "epoch: 7 step: 386, loss is 0.001083142007701099\n",
      "epoch: 7 step: 387, loss is 0.038915228098630905\n",
      "epoch: 7 step: 388, loss is 0.0007885874365456402\n",
      "epoch: 7 step: 389, loss is 0.0004448348190635443\n",
      "epoch: 7 step: 390, loss is 0.0010308531345799565\n",
      "epoch: 7 step: 391, loss is 0.003613698063418269\n",
      "epoch: 7 step: 392, loss is 0.006790952757000923\n",
      "epoch: 7 step: 393, loss is 0.0034903327468782663\n",
      "epoch: 7 step: 394, loss is 0.0021927212364971638\n",
      "epoch: 7 step: 395, loss is 0.05868195742368698\n",
      "epoch: 7 step: 396, loss is 0.004728116560727358\n",
      "epoch: 7 step: 397, loss is 0.07140081375837326\n",
      "epoch: 7 step: 398, loss is 0.0024227623362094164\n",
      "epoch: 7 step: 399, loss is 0.0006366163142956793\n",
      "epoch: 7 step: 400, loss is 0.01497054286301136\n",
      "epoch: 7 step: 401, loss is 0.0016781961312517524\n",
      "epoch: 7 step: 402, loss is 0.017138680443167686\n",
      "epoch: 7 step: 403, loss is 0.0026100422255694866\n",
      "epoch: 7 step: 404, loss is 0.0012573982821777463\n",
      "epoch: 7 step: 405, loss is 0.06562189012765884\n",
      "epoch: 7 step: 406, loss is 0.008732207119464874\n",
      "epoch: 7 step: 407, loss is 0.0003001800796482712\n",
      "epoch: 7 step: 408, loss is 0.00027788386796601117\n",
      "epoch: 7 step: 409, loss is 0.00017534714424982667\n",
      "epoch: 7 step: 410, loss is 0.0030419507529586554\n",
      "epoch: 7 step: 411, loss is 0.013267233967781067\n",
      "epoch: 7 step: 412, loss is 8.485381840728223e-05\n",
      "epoch: 7 step: 413, loss is 0.02227768674492836\n",
      "epoch: 7 step: 414, loss is 0.0006167173269204795\n",
      "epoch: 7 step: 415, loss is 0.0028195222839713097\n",
      "epoch: 7 step: 416, loss is 0.0003094045678153634\n",
      "epoch: 7 step: 417, loss is 0.0008808467537164688\n",
      "epoch: 7 step: 418, loss is 2.937610770459287e-05\n",
      "epoch: 7 step: 419, loss is 0.0008793124579824507\n",
      "epoch: 7 step: 420, loss is 0.015479528345167637\n",
      "epoch: 7 step: 421, loss is 9.410535130882636e-05\n",
      "epoch: 7 step: 422, loss is 8.547640754841268e-05\n",
      "epoch: 7 step: 423, loss is 0.0060240658931434155\n",
      "epoch: 7 step: 424, loss is 0.01087238546460867\n",
      "epoch: 7 step: 425, loss is 0.0009284617844969034\n",
      "epoch: 7 step: 426, loss is 5.0606289732968435e-05\n",
      "epoch: 7 step: 427, loss is 0.0002824250841513276\n",
      "epoch: 7 step: 428, loss is 0.004900061525404453\n",
      "epoch: 7 step: 429, loss is 0.0006142667261883616\n",
      "epoch: 7 step: 430, loss is 0.001711377059109509\n",
      "epoch: 7 step: 431, loss is 0.006428606808185577\n",
      "epoch: 7 step: 432, loss is 0.07588540762662888\n",
      "epoch: 7 step: 433, loss is 0.0002446272992528975\n",
      "epoch: 7 step: 434, loss is 0.0028091862332075834\n",
      "epoch: 7 step: 435, loss is 0.00046983701759018004\n",
      "epoch: 7 step: 436, loss is 0.0015577892772853374\n",
      "epoch: 7 step: 437, loss is 0.03941541165113449\n",
      "epoch: 7 step: 438, loss is 0.0013278701808303595\n",
      "epoch: 7 step: 439, loss is 0.017375625669956207\n",
      "epoch: 7 step: 440, loss is 0.10975703597068787\n",
      "epoch: 7 step: 441, loss is 0.0005370414000935853\n",
      "epoch: 7 step: 442, loss is 0.002000967739149928\n",
      "epoch: 7 step: 443, loss is 0.015290569514036179\n",
      "epoch: 7 step: 444, loss is 0.06121869757771492\n",
      "epoch: 7 step: 445, loss is 0.001016629277728498\n",
      "epoch: 7 step: 446, loss is 0.007048476487398148\n",
      "epoch: 7 step: 447, loss is 0.022653967142105103\n",
      "epoch: 7 step: 448, loss is 0.0054572224617004395\n",
      "epoch: 7 step: 449, loss is 0.1170063242316246\n",
      "epoch: 7 step: 450, loss is 0.015431673265993595\n",
      "epoch: 7 step: 451, loss is 0.004361929837614298\n",
      "epoch: 7 step: 452, loss is 0.016879918053746223\n",
      "epoch: 7 step: 453, loss is 0.00013048872642684728\n",
      "epoch: 7 step: 454, loss is 0.0008431369788013399\n",
      "epoch: 7 step: 455, loss is 4.805270509677939e-05\n",
      "epoch: 7 step: 456, loss is 0.05127914622426033\n",
      "epoch: 7 step: 457, loss is 0.00555637339130044\n",
      "epoch: 7 step: 458, loss is 0.001724395202472806\n",
      "epoch: 7 step: 459, loss is 0.007348696701228619\n",
      "epoch: 7 step: 460, loss is 0.00010244698933092877\n",
      "epoch: 7 step: 461, loss is 0.003870642278343439\n",
      "epoch: 7 step: 462, loss is 0.016082841902971268\n",
      "epoch: 7 step: 463, loss is 0.002402869053184986\n",
      "epoch: 7 step: 464, loss is 0.0003692134632728994\n",
      "epoch: 7 step: 465, loss is 0.008028087206184864\n",
      "epoch: 7 step: 466, loss is 0.028478719294071198\n",
      "epoch: 7 step: 467, loss is 0.20699147880077362\n",
      "epoch: 7 step: 468, loss is 0.001275312271900475\n",
      "epoch: 7 step: 469, loss is 0.00245248363353312\n",
      "epoch: 7 step: 470, loss is 0.00013835958088748157\n",
      "epoch: 7 step: 471, loss is 0.0002547334006521851\n",
      "epoch: 7 step: 472, loss is 0.0019002660410478711\n",
      "epoch: 7 step: 473, loss is 0.000937479198910296\n",
      "epoch: 7 step: 474, loss is 0.04182139411568642\n",
      "epoch: 7 step: 475, loss is 0.0013525736285373569\n",
      "epoch: 7 step: 476, loss is 0.019956551492214203\n",
      "epoch: 7 step: 477, loss is 0.0024124160408973694\n",
      "epoch: 7 step: 478, loss is 0.0005087978206574917\n",
      "epoch: 7 step: 479, loss is 0.005730411969125271\n",
      "epoch: 7 step: 480, loss is 0.002242038259282708\n",
      "epoch: 7 step: 481, loss is 0.0024840617552399635\n",
      "epoch: 7 step: 482, loss is 0.00028372544329613447\n",
      "epoch: 7 step: 483, loss is 0.0033041080459952354\n",
      "epoch: 7 step: 484, loss is 0.18353304266929626\n",
      "epoch: 7 step: 485, loss is 0.008053642697632313\n",
      "epoch: 7 step: 486, loss is 2.7029129341826774e-05\n",
      "epoch: 7 step: 487, loss is 0.08808865398168564\n",
      "epoch: 7 step: 488, loss is 0.003861938137561083\n",
      "epoch: 7 step: 489, loss is 0.0026223829481750727\n",
      "epoch: 7 step: 490, loss is 0.00043373482185415924\n",
      "epoch: 7 step: 491, loss is 0.00023613753728568554\n",
      "epoch: 7 step: 492, loss is 0.002660017693415284\n",
      "epoch: 7 step: 493, loss is 0.028774389997124672\n",
      "epoch: 7 step: 494, loss is 0.0047860401682555676\n",
      "epoch: 7 step: 495, loss is 0.0021363410633057356\n",
      "epoch: 7 step: 496, loss is 0.002684523817151785\n",
      "epoch: 7 step: 497, loss is 0.0005425795097835362\n",
      "epoch: 7 step: 498, loss is 0.004485550802201033\n",
      "epoch: 7 step: 499, loss is 0.0013274237280711532\n",
      "epoch: 7 step: 500, loss is 0.012697619386017323\n",
      "epoch: 7 step: 501, loss is 0.0008523610886186361\n",
      "epoch: 7 step: 502, loss is 0.001822147169150412\n",
      "epoch: 7 step: 503, loss is 0.01103449147194624\n",
      "epoch: 7 step: 504, loss is 0.0006659969803877175\n",
      "epoch: 7 step: 505, loss is 0.0005190903320908546\n",
      "epoch: 7 step: 506, loss is 0.011161808855831623\n",
      "epoch: 7 step: 507, loss is 0.07139794528484344\n",
      "epoch: 7 step: 508, loss is 0.005744641646742821\n",
      "epoch: 7 step: 509, loss is 0.005308877676725388\n",
      "epoch: 7 step: 510, loss is 0.002751014195382595\n",
      "epoch: 7 step: 511, loss is 3.7642064853571355e-05\n",
      "epoch: 7 step: 512, loss is 0.004060971550643444\n",
      "epoch: 7 step: 513, loss is 0.009970354847609997\n",
      "epoch: 7 step: 514, loss is 0.0007359214941971004\n",
      "epoch: 7 step: 515, loss is 0.0014787729596719146\n",
      "epoch: 7 step: 516, loss is 0.0029082258697599173\n",
      "epoch: 7 step: 517, loss is 0.008066480979323387\n",
      "epoch: 7 step: 518, loss is 0.08592084050178528\n",
      "epoch: 7 step: 519, loss is 9.08306974451989e-05\n",
      "epoch: 7 step: 520, loss is 0.0008737208554521203\n",
      "epoch: 7 step: 521, loss is 0.07953961938619614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 522, loss is 0.02285204455256462\n",
      "epoch: 7 step: 523, loss is 0.03993282467126846\n",
      "epoch: 7 step: 524, loss is 0.0009052703389897943\n",
      "epoch: 7 step: 525, loss is 0.0017144057201221585\n",
      "epoch: 7 step: 526, loss is 0.0822976678609848\n",
      "epoch: 7 step: 527, loss is 0.04238606616854668\n",
      "epoch: 7 step: 528, loss is 0.0006115860305726528\n",
      "epoch: 7 step: 529, loss is 0.0014597272966057062\n",
      "epoch: 7 step: 530, loss is 0.0013909428380429745\n",
      "epoch: 7 step: 531, loss is 0.001993316225707531\n",
      "epoch: 7 step: 532, loss is 0.0014662232715636492\n",
      "epoch: 7 step: 533, loss is 0.0011267461813986301\n",
      "epoch: 7 step: 534, loss is 0.003149556927382946\n",
      "epoch: 7 step: 535, loss is 0.14284932613372803\n",
      "epoch: 7 step: 536, loss is 0.00713729253038764\n",
      "epoch: 7 step: 537, loss is 0.0074060880579054356\n",
      "epoch: 7 step: 538, loss is 0.002249392680823803\n",
      "epoch: 7 step: 539, loss is 0.0003666133270598948\n",
      "epoch: 7 step: 540, loss is 0.003852257737889886\n",
      "epoch: 7 step: 541, loss is 0.024740563705563545\n",
      "epoch: 7 step: 542, loss is 0.0001633655047044158\n",
      "epoch: 7 step: 543, loss is 0.00023471158056054264\n",
      "epoch: 7 step: 544, loss is 0.005370388738811016\n",
      "epoch: 7 step: 545, loss is 0.0006431837100535631\n",
      "epoch: 7 step: 546, loss is 0.0006688887369818985\n",
      "epoch: 7 step: 547, loss is 0.0004372084513306618\n",
      "epoch: 7 step: 548, loss is 0.022809436544775963\n",
      "epoch: 7 step: 549, loss is 0.004335278179496527\n",
      "epoch: 7 step: 550, loss is 0.0028251323383301497\n",
      "epoch: 7 step: 551, loss is 0.0013242035638540983\n",
      "epoch: 7 step: 552, loss is 0.00023160874843597412\n",
      "epoch: 7 step: 553, loss is 0.20755243301391602\n",
      "epoch: 7 step: 554, loss is 0.00040322827408090234\n",
      "epoch: 7 step: 555, loss is 0.0005752106662839651\n",
      "epoch: 7 step: 556, loss is 0.0031255485955625772\n",
      "epoch: 7 step: 557, loss is 0.0023942324332892895\n",
      "epoch: 7 step: 558, loss is 0.0003451452648732811\n",
      "epoch: 7 step: 559, loss is 0.00015871146752033383\n",
      "epoch: 7 step: 560, loss is 0.011581659317016602\n",
      "epoch: 7 step: 561, loss is 0.0014441341627389193\n",
      "epoch: 7 step: 562, loss is 0.0026435591280460358\n",
      "epoch: 7 step: 563, loss is 0.0002977348631247878\n",
      "epoch: 7 step: 564, loss is 0.03603889420628548\n",
      "epoch: 7 step: 565, loss is 0.002448844024911523\n",
      "epoch: 7 step: 566, loss is 0.0008023632108233869\n",
      "epoch: 7 step: 567, loss is 0.009964979253709316\n",
      "epoch: 7 step: 568, loss is 0.0001306269405176863\n",
      "epoch: 7 step: 569, loss is 0.0018058994319289923\n",
      "epoch: 7 step: 570, loss is 0.06844983994960785\n",
      "epoch: 7 step: 571, loss is 0.0001847757084760815\n",
      "epoch: 7 step: 572, loss is 0.015771877020597458\n",
      "epoch: 7 step: 573, loss is 0.4298517405986786\n",
      "epoch: 7 step: 574, loss is 0.08904202282428741\n",
      "epoch: 7 step: 575, loss is 0.0005494941142387688\n",
      "epoch: 7 step: 576, loss is 0.03844849020242691\n",
      "epoch: 7 step: 577, loss is 0.0011438409565016627\n",
      "epoch: 7 step: 578, loss is 0.004944744985550642\n",
      "epoch: 7 step: 579, loss is 0.0032136645168066025\n",
      "epoch: 7 step: 580, loss is 0.0006684723775833845\n",
      "epoch: 7 step: 581, loss is 0.00021144829224795103\n",
      "epoch: 7 step: 582, loss is 0.0014058402739465237\n",
      "epoch: 7 step: 583, loss is 0.011434672400355339\n",
      "epoch: 7 step: 584, loss is 0.00047065329272300005\n",
      "epoch: 7 step: 585, loss is 0.014282628893852234\n",
      "epoch: 7 step: 586, loss is 0.010576951317489147\n",
      "epoch: 7 step: 587, loss is 0.001081508700735867\n",
      "epoch: 7 step: 588, loss is 0.004873465280979872\n",
      "epoch: 7 step: 589, loss is 0.0030102620366960764\n",
      "epoch: 7 step: 590, loss is 0.03924787789583206\n",
      "epoch: 7 step: 591, loss is 0.04871617257595062\n",
      "epoch: 7 step: 592, loss is 0.09501174837350845\n",
      "epoch: 7 step: 593, loss is 0.0005509150796569884\n",
      "epoch: 7 step: 594, loss is 0.0002770119463093579\n",
      "epoch: 7 step: 595, loss is 0.10554955154657364\n",
      "epoch: 7 step: 596, loss is 0.010312119498848915\n",
      "epoch: 7 step: 597, loss is 0.0012993118725717068\n",
      "epoch: 7 step: 598, loss is 0.005409222096204758\n",
      "epoch: 7 step: 599, loss is 0.000131810869788751\n",
      "epoch: 7 step: 600, loss is 0.0047930944710969925\n",
      "epoch: 7 step: 601, loss is 0.002225159667432308\n",
      "epoch: 7 step: 602, loss is 0.0929885283112526\n",
      "epoch: 7 step: 603, loss is 0.0019812448881566525\n",
      "epoch: 7 step: 604, loss is 0.007811433169990778\n",
      "epoch: 7 step: 605, loss is 0.008429937064647675\n",
      "epoch: 7 step: 606, loss is 0.0020955854561179876\n",
      "epoch: 7 step: 607, loss is 0.0004782190371770412\n",
      "epoch: 7 step: 608, loss is 0.004664299543946981\n",
      "epoch: 7 step: 609, loss is 0.029305513948202133\n",
      "epoch: 7 step: 610, loss is 0.013527308590710163\n",
      "epoch: 7 step: 611, loss is 0.0010951776057481766\n",
      "epoch: 7 step: 612, loss is 0.0021683929953724146\n",
      "epoch: 7 step: 613, loss is 0.09605048596858978\n",
      "epoch: 7 step: 614, loss is 0.015955233946442604\n",
      "epoch: 7 step: 615, loss is 0.021193519234657288\n",
      "epoch: 7 step: 616, loss is 0.008968180976808071\n",
      "epoch: 7 step: 617, loss is 0.0005467281443998218\n",
      "epoch: 7 step: 618, loss is 0.0010888134129345417\n",
      "epoch: 7 step: 619, loss is 0.0006496997084468603\n",
      "epoch: 7 step: 620, loss is 0.00223844638094306\n",
      "epoch: 7 step: 621, loss is 0.00043017740244977176\n",
      "epoch: 7 step: 622, loss is 0.00022291773348115385\n",
      "epoch: 7 step: 623, loss is 0.0004429808468557894\n",
      "epoch: 7 step: 624, loss is 0.0015983275370672345\n",
      "epoch: 7 step: 625, loss is 0.0008869045414030552\n",
      "epoch: 7 step: 626, loss is 0.0003169463889207691\n",
      "epoch: 7 step: 627, loss is 0.005579359829425812\n",
      "epoch: 7 step: 628, loss is 0.00017292232951149344\n",
      "epoch: 7 step: 629, loss is 0.0012380547123029828\n",
      "epoch: 7 step: 630, loss is 0.0007623942219652236\n",
      "epoch: 7 step: 631, loss is 0.03699434921145439\n",
      "epoch: 7 step: 632, loss is 0.005912482272833586\n",
      "epoch: 7 step: 633, loss is 0.00020364458032418042\n",
      "epoch: 7 step: 634, loss is 0.00039904803270474076\n",
      "epoch: 7 step: 635, loss is 0.0011119275586679578\n",
      "epoch: 7 step: 636, loss is 0.00014786419342271984\n",
      "epoch: 7 step: 637, loss is 0.003314357716590166\n",
      "epoch: 7 step: 638, loss is 0.016836445778608322\n",
      "epoch: 7 step: 639, loss is 0.0015366614097729325\n",
      "epoch: 7 step: 640, loss is 0.009691189043223858\n",
      "epoch: 7 step: 641, loss is 0.0008361783693544567\n",
      "epoch: 7 step: 642, loss is 0.01728125289082527\n",
      "epoch: 7 step: 643, loss is 0.0004489960556384176\n",
      "epoch: 7 step: 644, loss is 0.0043731918558478355\n",
      "epoch: 7 step: 645, loss is 0.0001337756693828851\n",
      "epoch: 7 step: 646, loss is 0.00020412668527569622\n",
      "epoch: 7 step: 647, loss is 0.07091114670038223\n",
      "epoch: 7 step: 648, loss is 0.0011118052061647177\n",
      "epoch: 7 step: 649, loss is 0.09160040318965912\n",
      "epoch: 7 step: 650, loss is 0.0006542450864799321\n",
      "epoch: 7 step: 651, loss is 0.00016682842397131026\n",
      "epoch: 7 step: 652, loss is 0.0029123497661203146\n",
      "epoch: 7 step: 653, loss is 0.0020690045785158873\n",
      "epoch: 7 step: 654, loss is 0.0006613636505790055\n",
      "epoch: 7 step: 655, loss is 7.374725100817159e-05\n",
      "epoch: 7 step: 656, loss is 0.006394552532583475\n",
      "epoch: 7 step: 657, loss is 0.041603464633226395\n",
      "epoch: 7 step: 658, loss is 0.0008210063097067177\n",
      "epoch: 7 step: 659, loss is 0.00827703159302473\n",
      "epoch: 7 step: 660, loss is 0.0003547369851730764\n",
      "epoch: 7 step: 661, loss is 0.0008593035745434463\n",
      "epoch: 7 step: 662, loss is 0.00013186239812057465\n",
      "epoch: 7 step: 663, loss is 0.004141912795603275\n",
      "epoch: 7 step: 664, loss is 0.00037207145942375064\n",
      "epoch: 7 step: 665, loss is 0.0003330921463202685\n",
      "epoch: 7 step: 666, loss is 0.0017091790214180946\n",
      "epoch: 7 step: 667, loss is 0.0026560609694570303\n",
      "epoch: 7 step: 668, loss is 0.00016718727420084178\n",
      "epoch: 7 step: 669, loss is 0.0009227792616002262\n",
      "epoch: 7 step: 670, loss is 0.0012858144473284483\n",
      "epoch: 7 step: 671, loss is 0.05641692131757736\n",
      "epoch: 7 step: 672, loss is 0.0007141925161704421\n",
      "epoch: 7 step: 673, loss is 0.022423142567276955\n",
      "epoch: 7 step: 674, loss is 0.004249125719070435\n",
      "epoch: 7 step: 675, loss is 0.00029625973547808826\n",
      "epoch: 7 step: 676, loss is 0.0009972060797736049\n",
      "epoch: 7 step: 677, loss is 0.0224720798432827\n",
      "epoch: 7 step: 678, loss is 0.012449423782527447\n",
      "epoch: 7 step: 679, loss is 0.0005936921807006001\n",
      "epoch: 7 step: 680, loss is 0.016094906255602837\n",
      "epoch: 7 step: 681, loss is 5.8964422350982204e-05\n",
      "epoch: 7 step: 682, loss is 0.033857762813568115\n",
      "epoch: 7 step: 683, loss is 0.0001809514797059819\n",
      "epoch: 7 step: 684, loss is 0.010605388320982456\n",
      "epoch: 7 step: 685, loss is 0.001765412394888699\n",
      "epoch: 7 step: 686, loss is 0.00423613702878356\n",
      "epoch: 7 step: 687, loss is 0.017144568264484406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 688, loss is 0.028994914144277573\n",
      "epoch: 7 step: 689, loss is 0.00552245182916522\n",
      "epoch: 7 step: 690, loss is 0.0026472441386431456\n",
      "epoch: 7 step: 691, loss is 0.0347701832652092\n",
      "epoch: 7 step: 692, loss is 0.0006124780629761517\n",
      "epoch: 7 step: 693, loss is 0.11236952245235443\n",
      "epoch: 7 step: 694, loss is 0.004858998581767082\n",
      "epoch: 7 step: 695, loss is 0.0014036056818440557\n",
      "epoch: 7 step: 696, loss is 0.0055023967288434505\n",
      "epoch: 7 step: 697, loss is 0.0021722542587667704\n",
      "epoch: 7 step: 698, loss is 0.003607336664572358\n",
      "epoch: 7 step: 699, loss is 0.010676889680325985\n",
      "epoch: 7 step: 700, loss is 0.041506096720695496\n",
      "epoch: 7 step: 701, loss is 0.0020268799271434546\n",
      "epoch: 7 step: 702, loss is 0.032493848353624344\n",
      "epoch: 7 step: 703, loss is 0.0007658965769223869\n",
      "epoch: 7 step: 704, loss is 0.00015177050954662263\n",
      "epoch: 7 step: 705, loss is 0.002960072597488761\n",
      "epoch: 7 step: 706, loss is 0.04008128494024277\n",
      "epoch: 7 step: 707, loss is 0.03899044543504715\n",
      "epoch: 7 step: 708, loss is 6.549416139023378e-05\n",
      "epoch: 7 step: 709, loss is 0.0013816305436193943\n",
      "epoch: 7 step: 710, loss is 0.06582655757665634\n",
      "epoch: 7 step: 711, loss is 6.9067558797542e-05\n",
      "epoch: 7 step: 712, loss is 0.0007843404891900718\n",
      "epoch: 7 step: 713, loss is 0.01297339890152216\n",
      "epoch: 7 step: 714, loss is 0.1289069801568985\n",
      "epoch: 7 step: 715, loss is 8.118303230730817e-05\n",
      "epoch: 7 step: 716, loss is 0.0006294643972069025\n",
      "epoch: 7 step: 717, loss is 0.00012293022882658988\n",
      "epoch: 7 step: 718, loss is 0.005431423429399729\n",
      "epoch: 7 step: 719, loss is 0.002011745236814022\n",
      "epoch: 7 step: 720, loss is 0.0015067083295434713\n",
      "epoch: 7 step: 721, loss is 0.0019445196958258748\n",
      "epoch: 7 step: 722, loss is 0.004419538192451\n",
      "epoch: 7 step: 723, loss is 0.005041494965553284\n",
      "epoch: 7 step: 724, loss is 0.13901068270206451\n",
      "epoch: 7 step: 725, loss is 0.019319452345371246\n",
      "epoch: 7 step: 726, loss is 0.0013599181547760963\n",
      "epoch: 7 step: 727, loss is 0.009036455303430557\n",
      "epoch: 7 step: 728, loss is 0.002974396338686347\n",
      "epoch: 7 step: 729, loss is 0.0003186048998031765\n",
      "epoch: 7 step: 730, loss is 0.000836899213027209\n",
      "epoch: 7 step: 731, loss is 0.00029626532341353595\n",
      "epoch: 7 step: 732, loss is 0.0019540851935744286\n",
      "epoch: 7 step: 733, loss is 0.019693482667207718\n",
      "epoch: 7 step: 734, loss is 0.0011036177165806293\n",
      "epoch: 7 step: 735, loss is 7.72851417423226e-05\n",
      "epoch: 7 step: 736, loss is 0.006822199560701847\n",
      "epoch: 7 step: 737, loss is 0.00018409689073450863\n",
      "epoch: 7 step: 738, loss is 3.57319186150562e-05\n",
      "epoch: 7 step: 739, loss is 8.115650416584685e-05\n",
      "epoch: 7 step: 740, loss is 8.262800292868633e-06\n",
      "epoch: 7 step: 741, loss is 0.013414079323410988\n",
      "epoch: 7 step: 742, loss is 0.0033693858422338963\n",
      "epoch: 7 step: 743, loss is 0.00053895398741588\n",
      "epoch: 7 step: 744, loss is 0.00100787915289402\n",
      "epoch: 7 step: 745, loss is 2.130508073605597e-05\n",
      "epoch: 7 step: 746, loss is 0.5330127477645874\n",
      "epoch: 7 step: 747, loss is 0.003695922903716564\n",
      "epoch: 7 step: 748, loss is 0.03498618304729462\n",
      "epoch: 7 step: 749, loss is 0.0028172607999294996\n",
      "epoch: 7 step: 750, loss is 0.0033310733269900084\n",
      "epoch: 7 step: 751, loss is 0.003017057664692402\n",
      "epoch: 7 step: 752, loss is 0.00016934386803768575\n",
      "epoch: 7 step: 753, loss is 0.0007009668624959886\n",
      "epoch: 7 step: 754, loss is 0.11513860523700714\n",
      "epoch: 7 step: 755, loss is 2.0691300960606895e-05\n",
      "epoch: 7 step: 756, loss is 0.013117529451847076\n",
      "epoch: 7 step: 757, loss is 0.0022510988637804985\n",
      "epoch: 7 step: 758, loss is 0.0026770324911922216\n",
      "epoch: 7 step: 759, loss is 0.0008238849695771933\n",
      "epoch: 7 step: 760, loss is 0.00019223237177357078\n",
      "epoch: 7 step: 761, loss is 0.00044951189192943275\n",
      "epoch: 7 step: 762, loss is 0.0055759865790605545\n",
      "epoch: 7 step: 763, loss is 0.0006602213252335787\n",
      "epoch: 7 step: 764, loss is 0.003094765357673168\n",
      "epoch: 7 step: 765, loss is 0.00021256813488435\n",
      "epoch: 7 step: 766, loss is 0.004042971879243851\n",
      "epoch: 7 step: 767, loss is 0.055567625910043716\n",
      "epoch: 7 step: 768, loss is 0.035411037504673004\n",
      "epoch: 7 step: 769, loss is 0.0006704045226797462\n",
      "epoch: 7 step: 770, loss is 0.0036326877307146788\n",
      "epoch: 7 step: 771, loss is 0.09024369716644287\n",
      "epoch: 7 step: 772, loss is 0.0020682162139564753\n",
      "epoch: 7 step: 773, loss is 0.04364689812064171\n",
      "epoch: 7 step: 774, loss is 0.0162504892796278\n",
      "epoch: 7 step: 775, loss is 0.013009041547775269\n",
      "epoch: 7 step: 776, loss is 0.010709271766245365\n",
      "epoch: 7 step: 777, loss is 0.0009820768609642982\n",
      "epoch: 7 step: 778, loss is 0.09055458754301071\n",
      "epoch: 7 step: 779, loss is 0.013586691580712795\n",
      "epoch: 7 step: 780, loss is 0.30038461089134216\n",
      "epoch: 7 step: 781, loss is 0.0011285002110525966\n",
      "epoch: 7 step: 782, loss is 0.028214475139975548\n",
      "epoch: 7 step: 783, loss is 0.006842872593551874\n",
      "epoch: 7 step: 784, loss is 0.08582444489002228\n",
      "epoch: 7 step: 785, loss is 0.0119102094322443\n",
      "epoch: 7 step: 786, loss is 0.016648709774017334\n",
      "epoch: 7 step: 787, loss is 0.00655027013272047\n",
      "epoch: 7 step: 788, loss is 0.00175558275077492\n",
      "epoch: 7 step: 789, loss is 0.09673185646533966\n",
      "epoch: 7 step: 790, loss is 0.013528809882700443\n",
      "epoch: 7 step: 791, loss is 0.05185256898403168\n",
      "epoch: 7 step: 792, loss is 0.006791442632675171\n",
      "epoch: 7 step: 793, loss is 0.0037630354054272175\n",
      "epoch: 7 step: 794, loss is 0.015859391540288925\n",
      "epoch: 7 step: 795, loss is 0.07834551483392715\n",
      "epoch: 7 step: 796, loss is 0.0040191044099628925\n",
      "epoch: 7 step: 797, loss is 0.003706221701577306\n",
      "epoch: 7 step: 798, loss is 0.0032277596183121204\n",
      "epoch: 7 step: 799, loss is 0.002752048894762993\n",
      "epoch: 7 step: 800, loss is 0.00029728823574259877\n",
      "epoch: 7 step: 801, loss is 0.0008418390643782914\n",
      "epoch: 7 step: 802, loss is 0.04234130680561066\n",
      "epoch: 7 step: 803, loss is 0.006018581334501505\n",
      "epoch: 7 step: 804, loss is 0.07890328019857407\n",
      "epoch: 7 step: 805, loss is 0.07476907968521118\n",
      "epoch: 7 step: 806, loss is 0.04414445906877518\n",
      "epoch: 7 step: 807, loss is 0.02254313975572586\n",
      "epoch: 7 step: 808, loss is 0.0027926890179514885\n",
      "epoch: 7 step: 809, loss is 0.0008445702260360122\n",
      "epoch: 7 step: 810, loss is 0.02718265727162361\n",
      "epoch: 7 step: 811, loss is 0.0011247521033510566\n",
      "epoch: 7 step: 812, loss is 0.0009903875179588795\n",
      "epoch: 7 step: 813, loss is 9.755920473253354e-05\n",
      "epoch: 7 step: 814, loss is 0.029702704399824142\n",
      "epoch: 7 step: 815, loss is 0.05052410066127777\n",
      "epoch: 7 step: 816, loss is 0.05816201493144035\n",
      "epoch: 7 step: 817, loss is 0.0020691431127488613\n",
      "epoch: 7 step: 818, loss is 0.002214068081229925\n",
      "epoch: 7 step: 819, loss is 0.0023391819559037685\n",
      "epoch: 7 step: 820, loss is 0.00012277366477064788\n",
      "epoch: 7 step: 821, loss is 0.0008129369234666228\n",
      "epoch: 7 step: 822, loss is 0.00032335284049622715\n",
      "epoch: 7 step: 823, loss is 0.00104140629991889\n",
      "epoch: 7 step: 824, loss is 0.0018223158549517393\n",
      "epoch: 7 step: 825, loss is 0.005069389473646879\n",
      "epoch: 7 step: 826, loss is 0.06660322844982147\n",
      "epoch: 7 step: 827, loss is 0.011781632900238037\n",
      "epoch: 7 step: 828, loss is 0.00016435120778623968\n",
      "epoch: 7 step: 829, loss is 0.05039641633629799\n",
      "epoch: 7 step: 830, loss is 0.0004242499708198011\n",
      "epoch: 7 step: 831, loss is 0.06475865840911865\n",
      "epoch: 7 step: 832, loss is 0.05990377813577652\n",
      "epoch: 7 step: 833, loss is 0.061360880732536316\n",
      "epoch: 7 step: 834, loss is 0.03272748738527298\n",
      "epoch: 7 step: 835, loss is 0.0007234166259877384\n",
      "epoch: 7 step: 836, loss is 0.022541992366313934\n",
      "epoch: 7 step: 837, loss is 0.0027000694535672665\n",
      "epoch: 7 step: 838, loss is 0.0030187093652784824\n",
      "epoch: 7 step: 839, loss is 0.032765865325927734\n",
      "epoch: 7 step: 840, loss is 0.03894945979118347\n",
      "epoch: 7 step: 841, loss is 0.002264414681121707\n",
      "epoch: 7 step: 842, loss is 0.0022123525850474834\n",
      "epoch: 7 step: 843, loss is 0.011306733824312687\n",
      "epoch: 7 step: 844, loss is 0.15427151322364807\n",
      "epoch: 7 step: 845, loss is 0.02408014051616192\n",
      "epoch: 7 step: 846, loss is 0.0010562562383711338\n",
      "epoch: 7 step: 847, loss is 0.008497781120240688\n",
      "epoch: 7 step: 848, loss is 0.022275272756814957\n",
      "epoch: 7 step: 849, loss is 0.0026471996679902077\n",
      "epoch: 7 step: 850, loss is 0.1771683692932129\n",
      "epoch: 7 step: 851, loss is 0.0023708133958280087\n",
      "epoch: 7 step: 852, loss is 0.06642599403858185\n",
      "epoch: 7 step: 853, loss is 0.000533638522028923\n",
      "epoch: 7 step: 854, loss is 0.0014254230773076415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 855, loss is 0.028035541996359825\n",
      "epoch: 7 step: 856, loss is 0.025985276326537132\n",
      "epoch: 7 step: 857, loss is 0.00031025995849631727\n",
      "epoch: 7 step: 858, loss is 0.00033123642788268626\n",
      "epoch: 7 step: 859, loss is 0.18504507839679718\n",
      "epoch: 7 step: 860, loss is 0.001638720277696848\n",
      "epoch: 7 step: 861, loss is 0.0012565493816509843\n",
      "epoch: 7 step: 862, loss is 0.004083389416337013\n",
      "epoch: 7 step: 863, loss is 0.0037912374828010798\n",
      "epoch: 7 step: 864, loss is 0.3112296164035797\n",
      "epoch: 7 step: 865, loss is 0.0023645719047635794\n",
      "epoch: 7 step: 866, loss is 0.022419683635234833\n",
      "epoch: 7 step: 867, loss is 0.002845664508640766\n",
      "epoch: 7 step: 868, loss is 0.01035682950168848\n",
      "epoch: 7 step: 869, loss is 0.0017969170585274696\n",
      "epoch: 7 step: 870, loss is 0.015387576073408127\n",
      "epoch: 7 step: 871, loss is 0.0006380211561918259\n",
      "epoch: 7 step: 872, loss is 0.13442322611808777\n",
      "epoch: 7 step: 873, loss is 0.0004063997766934335\n",
      "epoch: 7 step: 874, loss is 0.0003576621529646218\n",
      "epoch: 7 step: 875, loss is 0.007965633645653725\n",
      "epoch: 7 step: 876, loss is 0.023385951295495033\n",
      "epoch: 7 step: 877, loss is 0.17862477898597717\n",
      "epoch: 7 step: 878, loss is 5.1773753511952236e-05\n",
      "epoch: 7 step: 879, loss is 0.00020698722801171243\n",
      "epoch: 7 step: 880, loss is 0.0018361273687332869\n",
      "epoch: 7 step: 881, loss is 0.005502854008227587\n",
      "epoch: 7 step: 882, loss is 0.03750067949295044\n",
      "epoch: 7 step: 883, loss is 0.0008315211161971092\n",
      "epoch: 7 step: 884, loss is 0.013290882110595703\n",
      "epoch: 7 step: 885, loss is 0.030712498351931572\n",
      "epoch: 7 step: 886, loss is 0.004723649937659502\n",
      "epoch: 7 step: 887, loss is 0.00497183995321393\n",
      "epoch: 7 step: 888, loss is 0.002168183447793126\n",
      "epoch: 7 step: 889, loss is 0.0075414446182549\n",
      "epoch: 7 step: 890, loss is 0.021750707179307938\n",
      "epoch: 7 step: 891, loss is 0.004856633488088846\n",
      "epoch: 7 step: 892, loss is 0.12719868123531342\n",
      "epoch: 7 step: 893, loss is 0.0010108909336850047\n",
      "epoch: 7 step: 894, loss is 0.020377250388264656\n",
      "epoch: 7 step: 895, loss is 0.015695523470640182\n",
      "epoch: 7 step: 896, loss is 0.00034074208815582097\n",
      "epoch: 7 step: 897, loss is 0.01202594954520464\n",
      "epoch: 7 step: 898, loss is 0.039469923824071884\n",
      "epoch: 7 step: 899, loss is 0.0037984701339155436\n",
      "epoch: 7 step: 900, loss is 0.0005609662621282041\n",
      "epoch: 7 step: 901, loss is 0.00017843129171524197\n",
      "epoch: 7 step: 902, loss is 0.0002128059568349272\n",
      "epoch: 7 step: 903, loss is 0.0009313655318692327\n",
      "epoch: 7 step: 904, loss is 0.046828147023916245\n",
      "epoch: 7 step: 905, loss is 0.0006121452897787094\n",
      "epoch: 7 step: 906, loss is 0.00026996718952432275\n",
      "epoch: 7 step: 907, loss is 0.003617234528064728\n",
      "epoch: 7 step: 908, loss is 0.037252750247716904\n",
      "epoch: 7 step: 909, loss is 0.00018980508320964873\n",
      "epoch: 7 step: 910, loss is 0.13417847454547882\n",
      "epoch: 7 step: 911, loss is 0.017547108232975006\n",
      "epoch: 7 step: 912, loss is 0.0005352922016754746\n",
      "epoch: 7 step: 913, loss is 0.001869466039352119\n",
      "epoch: 7 step: 914, loss is 5.9855763538507745e-05\n",
      "epoch: 7 step: 915, loss is 0.014639023691415787\n",
      "epoch: 7 step: 916, loss is 0.005591080989688635\n",
      "epoch: 7 step: 917, loss is 0.004653703887015581\n",
      "epoch: 7 step: 918, loss is 0.01574394851922989\n",
      "epoch: 7 step: 919, loss is 0.07745695114135742\n",
      "epoch: 7 step: 920, loss is 0.008817587979137897\n",
      "epoch: 7 step: 921, loss is 0.011242762207984924\n",
      "epoch: 7 step: 922, loss is 0.006232925225049257\n",
      "epoch: 7 step: 923, loss is 0.04679416865110397\n",
      "epoch: 7 step: 924, loss is 0.004583554342389107\n",
      "epoch: 7 step: 925, loss is 0.14905838668346405\n",
      "epoch: 7 step: 926, loss is 0.0752321183681488\n",
      "epoch: 7 step: 927, loss is 0.00035933186882175505\n",
      "epoch: 7 step: 928, loss is 0.07349568605422974\n",
      "epoch: 7 step: 929, loss is 0.023966079577803612\n",
      "epoch: 7 step: 930, loss is 0.0007053661393001676\n",
      "epoch: 7 step: 931, loss is 0.013509951531887054\n",
      "epoch: 7 step: 932, loss is 0.0010363553883507848\n",
      "epoch: 7 step: 933, loss is 0.0002880821703001857\n",
      "epoch: 7 step: 934, loss is 0.13260243833065033\n",
      "epoch: 7 step: 935, loss is 0.06361831724643707\n",
      "epoch: 7 step: 936, loss is 0.0001192169074784033\n",
      "epoch: 7 step: 937, loss is 0.09522489458322525\n",
      "epoch: 7 step: 938, loss is 0.000204575844691135\n",
      "epoch: 7 step: 939, loss is 0.0010584582341834903\n",
      "epoch: 7 step: 940, loss is 0.0614725798368454\n",
      "epoch: 7 step: 941, loss is 0.0009073662804439664\n",
      "epoch: 7 step: 942, loss is 0.07194724678993225\n",
      "epoch: 7 step: 943, loss is 0.00027239188784733415\n",
      "epoch: 7 step: 944, loss is 0.13339462876319885\n",
      "epoch: 7 step: 945, loss is 0.0005075192893855274\n",
      "epoch: 7 step: 946, loss is 2.1970467059873044e-05\n",
      "epoch: 7 step: 947, loss is 0.00044330439413897693\n",
      "epoch: 7 step: 948, loss is 0.10786981135606766\n",
      "epoch: 7 step: 949, loss is 0.11766891926527023\n",
      "epoch: 7 step: 950, loss is 0.0006841644062660635\n",
      "epoch: 7 step: 951, loss is 6.452383240684867e-05\n",
      "epoch: 7 step: 952, loss is 0.00645424472168088\n",
      "epoch: 7 step: 953, loss is 0.004544693045318127\n",
      "epoch: 7 step: 954, loss is 0.0010362660977989435\n",
      "epoch: 7 step: 955, loss is 0.18580520153045654\n",
      "epoch: 7 step: 956, loss is 0.0008298887405544519\n",
      "epoch: 7 step: 957, loss is 5.802700252388604e-05\n",
      "epoch: 7 step: 958, loss is 0.016500258818268776\n",
      "epoch: 7 step: 959, loss is 0.0018015645910054445\n",
      "epoch: 7 step: 960, loss is 0.0019134817412123084\n",
      "epoch: 7 step: 961, loss is 0.010645642876625061\n",
      "epoch: 7 step: 962, loss is 0.002749287523329258\n",
      "epoch: 7 step: 963, loss is 0.0013048973632976413\n",
      "epoch: 7 step: 964, loss is 0.028082236647605896\n",
      "epoch: 7 step: 965, loss is 0.026471074670553207\n",
      "epoch: 7 step: 966, loss is 0.002042843494564295\n",
      "epoch: 7 step: 967, loss is 0.08665288984775543\n",
      "epoch: 7 step: 968, loss is 0.0004918541526421905\n",
      "epoch: 7 step: 969, loss is 0.00033836960210464895\n",
      "epoch: 7 step: 970, loss is 0.005167803727090359\n",
      "epoch: 7 step: 971, loss is 0.0003650627622846514\n",
      "epoch: 7 step: 972, loss is 0.0032520948443561792\n",
      "epoch: 7 step: 973, loss is 0.1267000138759613\n",
      "epoch: 7 step: 974, loss is 0.07207096368074417\n",
      "epoch: 7 step: 975, loss is 0.034127142280340195\n",
      "epoch: 7 step: 976, loss is 0.00024827441666275263\n",
      "epoch: 7 step: 977, loss is 0.010142737999558449\n",
      "epoch: 7 step: 978, loss is 0.03708507493138313\n",
      "epoch: 7 step: 979, loss is 0.038590624928474426\n",
      "epoch: 7 step: 980, loss is 0.19964757561683655\n",
      "epoch: 7 step: 981, loss is 0.00010864550858968869\n",
      "epoch: 7 step: 982, loss is 0.0009627147228457034\n",
      "epoch: 7 step: 983, loss is 0.00072111701592803\n",
      "epoch: 7 step: 984, loss is 0.050276655703783035\n",
      "epoch: 7 step: 985, loss is 0.1764976531267166\n",
      "epoch: 7 step: 986, loss is 0.004702240694314241\n",
      "epoch: 7 step: 987, loss is 0.03511860966682434\n",
      "epoch: 7 step: 988, loss is 0.1188749149441719\n",
      "epoch: 7 step: 989, loss is 0.0005331271677277982\n",
      "epoch: 7 step: 990, loss is 0.00946054793894291\n",
      "epoch: 7 step: 991, loss is 0.0777428075671196\n",
      "epoch: 7 step: 992, loss is 0.00017854679026640952\n",
      "epoch: 7 step: 993, loss is 0.07320356369018555\n",
      "epoch: 7 step: 994, loss is 0.011722082272171974\n",
      "epoch: 7 step: 995, loss is 0.0008130046771839261\n",
      "epoch: 7 step: 996, loss is 4.5328539272304624e-05\n",
      "epoch: 7 step: 997, loss is 0.0005922118434682488\n",
      "epoch: 7 step: 998, loss is 0.005244371015578508\n",
      "epoch: 7 step: 999, loss is 0.0015966987702995539\n",
      "epoch: 7 step: 1000, loss is 8.499229443259537e-05\n",
      "epoch: 7 step: 1001, loss is 0.11175443232059479\n",
      "epoch: 7 step: 1002, loss is 0.013056174851953983\n",
      "epoch: 7 step: 1003, loss is 0.0387578122317791\n",
      "epoch: 7 step: 1004, loss is 0.0019309375202283263\n",
      "epoch: 7 step: 1005, loss is 0.0441955104470253\n",
      "epoch: 7 step: 1006, loss is 0.0004037736216560006\n",
      "epoch: 7 step: 1007, loss is 0.020928168669342995\n",
      "epoch: 7 step: 1008, loss is 0.018786832690238953\n",
      "epoch: 7 step: 1009, loss is 0.07678689062595367\n",
      "epoch: 7 step: 1010, loss is 0.008062382228672504\n",
      "epoch: 7 step: 1011, loss is 0.013259271159768105\n",
      "epoch: 7 step: 1012, loss is 0.038906268775463104\n",
      "epoch: 7 step: 1013, loss is 0.002778866561129689\n",
      "epoch: 7 step: 1014, loss is 0.0031819138675928116\n",
      "epoch: 7 step: 1015, loss is 0.024353986606001854\n",
      "epoch: 7 step: 1016, loss is 0.003760085441172123\n",
      "epoch: 7 step: 1017, loss is 0.0002957329270429909\n",
      "epoch: 7 step: 1018, loss is 0.004093398340046406\n",
      "epoch: 7 step: 1019, loss is 0.00024510902585461736\n",
      "epoch: 7 step: 1020, loss is 0.0003540916077326983\n",
      "epoch: 7 step: 1021, loss is 0.000376260606572032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 1022, loss is 0.00013169484736863524\n",
      "epoch: 7 step: 1023, loss is 0.002500630682334304\n",
      "epoch: 7 step: 1024, loss is 0.0041323439218103886\n",
      "epoch: 7 step: 1025, loss is 0.00012731253809761256\n",
      "epoch: 7 step: 1026, loss is 0.005068558268249035\n",
      "epoch: 7 step: 1027, loss is 0.0028432796243578196\n",
      "epoch: 7 step: 1028, loss is 0.01928562857210636\n",
      "epoch: 7 step: 1029, loss is 0.0012296703644096851\n",
      "epoch: 7 step: 1030, loss is 0.009675164707005024\n",
      "epoch: 7 step: 1031, loss is 0.016296613961458206\n",
      "epoch: 7 step: 1032, loss is 0.002153362613171339\n",
      "epoch: 7 step: 1033, loss is 0.00335126044228673\n",
      "epoch: 7 step: 1034, loss is 0.03922148793935776\n",
      "epoch: 7 step: 1035, loss is 0.007814794778823853\n",
      "epoch: 7 step: 1036, loss is 0.006252453662455082\n",
      "epoch: 7 step: 1037, loss is 0.004809490405023098\n",
      "epoch: 7 step: 1038, loss is 0.1868066042661667\n",
      "epoch: 7 step: 1039, loss is 0.00012342107947915792\n",
      "epoch: 7 step: 1040, loss is 0.0021533744875341654\n",
      "epoch: 7 step: 1041, loss is 0.006770528387278318\n",
      "epoch: 7 step: 1042, loss is 0.0034780907444655895\n",
      "epoch: 7 step: 1043, loss is 0.0032330742105841637\n",
      "epoch: 7 step: 1044, loss is 0.0002415181224932894\n",
      "epoch: 7 step: 1045, loss is 0.00429797125980258\n",
      "epoch: 7 step: 1046, loss is 0.000478186528198421\n",
      "epoch: 7 step: 1047, loss is 0.008097736164927483\n",
      "epoch: 7 step: 1048, loss is 0.06688171625137329\n",
      "epoch: 7 step: 1049, loss is 0.03438153490424156\n",
      "epoch: 7 step: 1050, loss is 0.026543015614151955\n",
      "epoch: 7 step: 1051, loss is 0.0012018796987831593\n",
      "epoch: 7 step: 1052, loss is 0.0772177204489708\n",
      "epoch: 7 step: 1053, loss is 0.001179892336949706\n",
      "epoch: 7 step: 1054, loss is 0.14446592330932617\n",
      "epoch: 7 step: 1055, loss is 0.0008908085292205215\n",
      "epoch: 7 step: 1056, loss is 0.011805030517280102\n",
      "epoch: 7 step: 1057, loss is 0.0004651521740015596\n",
      "epoch: 7 step: 1058, loss is 0.0003399886772967875\n",
      "epoch: 7 step: 1059, loss is 0.00034715619403868914\n",
      "epoch: 7 step: 1060, loss is 0.0017372204456478357\n",
      "epoch: 7 step: 1061, loss is 0.0018069518264383078\n",
      "epoch: 7 step: 1062, loss is 0.0012886449694633484\n",
      "epoch: 7 step: 1063, loss is 0.08823152631521225\n",
      "epoch: 7 step: 1064, loss is 0.004071464296430349\n",
      "epoch: 7 step: 1065, loss is 0.003910234663635492\n",
      "epoch: 7 step: 1066, loss is 0.031117092818021774\n",
      "epoch: 7 step: 1067, loss is 0.06703165173530579\n",
      "epoch: 7 step: 1068, loss is 0.009632930159568787\n",
      "epoch: 7 step: 1069, loss is 0.005806473083794117\n",
      "epoch: 7 step: 1070, loss is 0.0002239061868749559\n",
      "epoch: 7 step: 1071, loss is 0.00023603667796123773\n",
      "epoch: 7 step: 1072, loss is 0.001318446360528469\n",
      "epoch: 7 step: 1073, loss is 0.006689092610031366\n",
      "epoch: 7 step: 1074, loss is 0.237530916929245\n",
      "epoch: 7 step: 1075, loss is 0.07909110188484192\n",
      "epoch: 7 step: 1076, loss is 0.008124863728880882\n",
      "epoch: 7 step: 1077, loss is 0.0013807039940729737\n",
      "epoch: 7 step: 1078, loss is 0.007917186245322227\n",
      "epoch: 7 step: 1079, loss is 0.1218942254781723\n",
      "epoch: 7 step: 1080, loss is 0.0028155760373920202\n",
      "epoch: 7 step: 1081, loss is 0.019096406176686287\n",
      "epoch: 7 step: 1082, loss is 0.0014051190810278058\n",
      "epoch: 7 step: 1083, loss is 0.161159485578537\n",
      "epoch: 7 step: 1084, loss is 0.00011890186578966677\n",
      "epoch: 7 step: 1085, loss is 0.03881567716598511\n",
      "epoch: 7 step: 1086, loss is 0.0004462436481844634\n",
      "epoch: 7 step: 1087, loss is 5.990082718199119e-05\n",
      "epoch: 7 step: 1088, loss is 0.003975986037403345\n",
      "epoch: 7 step: 1089, loss is 0.05423598363995552\n",
      "epoch: 7 step: 1090, loss is 0.0005182221066206694\n",
      "epoch: 7 step: 1091, loss is 0.05026450753211975\n",
      "epoch: 7 step: 1092, loss is 0.003101214300841093\n",
      "epoch: 7 step: 1093, loss is 0.07571938633918762\n",
      "epoch: 7 step: 1094, loss is 0.07559007406234741\n",
      "epoch: 7 step: 1095, loss is 0.001664897776208818\n",
      "epoch: 7 step: 1096, loss is 0.14909552037715912\n",
      "epoch: 7 step: 1097, loss is 0.032861363142728806\n",
      "epoch: 7 step: 1098, loss is 0.11010660231113434\n",
      "epoch: 7 step: 1099, loss is 0.029556384310126305\n",
      "epoch: 7 step: 1100, loss is 0.0014491323381662369\n",
      "epoch: 7 step: 1101, loss is 0.013104259967803955\n",
      "epoch: 7 step: 1102, loss is 0.04498958960175514\n",
      "epoch: 7 step: 1103, loss is 0.0014269599923864007\n",
      "epoch: 7 step: 1104, loss is 0.022354714572429657\n",
      "epoch: 7 step: 1105, loss is 0.019464101642370224\n",
      "epoch: 7 step: 1106, loss is 0.11539175361394882\n",
      "epoch: 7 step: 1107, loss is 0.16695408523082733\n",
      "epoch: 7 step: 1108, loss is 0.05147330462932587\n",
      "epoch: 7 step: 1109, loss is 0.0034247864969074726\n",
      "epoch: 7 step: 1110, loss is 0.06263750046491623\n",
      "epoch: 7 step: 1111, loss is 0.019971363246440887\n",
      "epoch: 7 step: 1112, loss is 0.05843494087457657\n",
      "epoch: 7 step: 1113, loss is 0.017473623156547546\n",
      "epoch: 7 step: 1114, loss is 0.11954154074192047\n",
      "epoch: 7 step: 1115, loss is 0.035016488283872604\n",
      "epoch: 7 step: 1116, loss is 0.024792971089482307\n",
      "epoch: 7 step: 1117, loss is 0.0022665022406727076\n",
      "epoch: 7 step: 1118, loss is 0.11964006721973419\n",
      "epoch: 7 step: 1119, loss is 0.002573455683887005\n",
      "epoch: 7 step: 1120, loss is 0.08515948802232742\n",
      "epoch: 7 step: 1121, loss is 0.2059958428144455\n",
      "epoch: 7 step: 1122, loss is 0.03810570016503334\n",
      "epoch: 7 step: 1123, loss is 0.005997984204441309\n",
      "epoch: 7 step: 1124, loss is 0.004029633942991495\n",
      "epoch: 7 step: 1125, loss is 0.002805730327963829\n",
      "epoch: 7 step: 1126, loss is 0.014011367224156857\n",
      "epoch: 7 step: 1127, loss is 0.06875645369291306\n",
      "epoch: 7 step: 1128, loss is 0.01186397671699524\n",
      "epoch: 7 step: 1129, loss is 0.00029649457428604364\n",
      "epoch: 7 step: 1130, loss is 0.027711374685168266\n",
      "epoch: 7 step: 1131, loss is 0.09850087761878967\n",
      "epoch: 7 step: 1132, loss is 0.0047654747031629086\n",
      "epoch: 7 step: 1133, loss is 0.0002204030897701159\n",
      "epoch: 7 step: 1134, loss is 0.00190573965664953\n",
      "epoch: 7 step: 1135, loss is 0.008048689924180508\n",
      "epoch: 7 step: 1136, loss is 0.046936944127082825\n",
      "epoch: 7 step: 1137, loss is 0.013226635754108429\n",
      "epoch: 7 step: 1138, loss is 0.04042409360408783\n",
      "epoch: 7 step: 1139, loss is 0.0016445298679172993\n",
      "epoch: 7 step: 1140, loss is 0.09916075319051743\n",
      "epoch: 7 step: 1141, loss is 0.05539625138044357\n",
      "epoch: 7 step: 1142, loss is 0.0009928332874551415\n",
      "epoch: 7 step: 1143, loss is 0.0794181153178215\n",
      "epoch: 7 step: 1144, loss is 0.015536759980022907\n",
      "epoch: 7 step: 1145, loss is 0.17884349822998047\n",
      "epoch: 7 step: 1146, loss is 0.005809153895825148\n",
      "epoch: 7 step: 1147, loss is 0.008587009273469448\n",
      "epoch: 7 step: 1148, loss is 0.0006888555362820625\n",
      "epoch: 7 step: 1149, loss is 0.0008204102050513029\n",
      "epoch: 7 step: 1150, loss is 0.023786520585417747\n",
      "epoch: 7 step: 1151, loss is 0.000541659421287477\n",
      "epoch: 7 step: 1152, loss is 0.1248696818947792\n",
      "epoch: 7 step: 1153, loss is 0.030022135004401207\n",
      "epoch: 7 step: 1154, loss is 0.03548553213477135\n",
      "epoch: 7 step: 1155, loss is 0.0007696698303334415\n",
      "epoch: 7 step: 1156, loss is 0.02479822374880314\n",
      "epoch: 7 step: 1157, loss is 0.0034506898373365402\n",
      "epoch: 7 step: 1158, loss is 0.0015560585306957364\n",
      "epoch: 7 step: 1159, loss is 0.002371140755712986\n",
      "epoch: 7 step: 1160, loss is 0.012048647738993168\n",
      "epoch: 7 step: 1161, loss is 0.010103096254169941\n",
      "epoch: 7 step: 1162, loss is 0.034651827067136765\n",
      "epoch: 7 step: 1163, loss is 0.006793352775275707\n",
      "epoch: 7 step: 1164, loss is 0.003911891952157021\n",
      "epoch: 7 step: 1165, loss is 0.012914528138935566\n",
      "epoch: 7 step: 1166, loss is 0.0027767291758209467\n",
      "epoch: 7 step: 1167, loss is 0.010291662998497486\n",
      "epoch: 7 step: 1168, loss is 0.0014048963785171509\n",
      "epoch: 7 step: 1169, loss is 0.0007517443737015128\n",
      "epoch: 7 step: 1170, loss is 0.001231893664225936\n",
      "epoch: 7 step: 1171, loss is 0.03895255923271179\n",
      "epoch: 7 step: 1172, loss is 0.23627588152885437\n",
      "epoch: 7 step: 1173, loss is 0.0023041318636387587\n",
      "epoch: 7 step: 1174, loss is 0.1266694962978363\n",
      "epoch: 7 step: 1175, loss is 0.02243630215525627\n",
      "epoch: 7 step: 1176, loss is 0.01123043429106474\n",
      "epoch: 7 step: 1177, loss is 0.00020371185382828116\n",
      "epoch: 7 step: 1178, loss is 0.028738373890519142\n",
      "epoch: 7 step: 1179, loss is 0.3619891405105591\n",
      "epoch: 7 step: 1180, loss is 0.007409245241433382\n",
      "epoch: 7 step: 1181, loss is 0.021722467616200447\n",
      "epoch: 7 step: 1182, loss is 0.0053944350220263\n",
      "epoch: 7 step: 1183, loss is 0.0040395730175077915\n",
      "epoch: 7 step: 1184, loss is 0.0243084579706192\n",
      "epoch: 7 step: 1185, loss is 0.02135486528277397\n",
      "epoch: 7 step: 1186, loss is 0.007727533113211393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 1187, loss is 0.0013476655585691333\n",
      "epoch: 7 step: 1188, loss is 0.002290789969265461\n",
      "epoch: 7 step: 1189, loss is 0.0026404797099530697\n",
      "epoch: 7 step: 1190, loss is 0.003379133529961109\n",
      "epoch: 7 step: 1191, loss is 0.025335557758808136\n",
      "epoch: 7 step: 1192, loss is 0.002764487639069557\n",
      "epoch: 7 step: 1193, loss is 0.008027557283639908\n",
      "epoch: 7 step: 1194, loss is 0.005948665086179972\n",
      "epoch: 7 step: 1195, loss is 0.006893972400575876\n",
      "epoch: 7 step: 1196, loss is 0.028405848890542984\n",
      "epoch: 7 step: 1197, loss is 0.003986221272498369\n",
      "epoch: 7 step: 1198, loss is 0.0073439814150333405\n",
      "epoch: 7 step: 1199, loss is 0.0005864662234671414\n",
      "epoch: 7 step: 1200, loss is 0.0014020734233781695\n",
      "epoch: 7 step: 1201, loss is 0.01635933481156826\n",
      "epoch: 7 step: 1202, loss is 0.0005517960526049137\n",
      "epoch: 7 step: 1203, loss is 0.0072165317833423615\n",
      "epoch: 7 step: 1204, loss is 0.21868596971035004\n",
      "epoch: 7 step: 1205, loss is 0.0008551867795176804\n",
      "epoch: 7 step: 1206, loss is 0.024369599297642708\n",
      "epoch: 7 step: 1207, loss is 0.023211337625980377\n",
      "epoch: 7 step: 1208, loss is 0.01809673197567463\n",
      "epoch: 7 step: 1209, loss is 0.005541000049561262\n",
      "epoch: 7 step: 1210, loss is 0.0008549221674911678\n",
      "epoch: 7 step: 1211, loss is 0.0019787615165114403\n",
      "epoch: 7 step: 1212, loss is 0.003472026204690337\n",
      "epoch: 7 step: 1213, loss is 0.0015511679230257869\n",
      "epoch: 7 step: 1214, loss is 0.055709004402160645\n",
      "epoch: 7 step: 1215, loss is 0.006535266060382128\n",
      "epoch: 7 step: 1216, loss is 0.013019530102610588\n",
      "epoch: 7 step: 1217, loss is 0.0008778776973485947\n",
      "epoch: 7 step: 1218, loss is 0.0017699315212666988\n",
      "epoch: 7 step: 1219, loss is 0.0006881303270347416\n",
      "epoch: 7 step: 1220, loss is 0.009693730622529984\n",
      "epoch: 7 step: 1221, loss is 0.011586509644985199\n",
      "epoch: 7 step: 1222, loss is 0.002020591637119651\n",
      "epoch: 7 step: 1223, loss is 0.01757982186973095\n",
      "epoch: 7 step: 1224, loss is 0.004266796167939901\n",
      "epoch: 7 step: 1225, loss is 0.008613096550107002\n",
      "epoch: 7 step: 1226, loss is 0.05356000363826752\n",
      "epoch: 7 step: 1227, loss is 0.010481569916009903\n",
      "epoch: 7 step: 1228, loss is 0.022046083584427834\n",
      "epoch: 7 step: 1229, loss is 0.0002981759316753596\n",
      "epoch: 7 step: 1230, loss is 0.2042621374130249\n",
      "epoch: 7 step: 1231, loss is 0.014942539855837822\n",
      "epoch: 7 step: 1232, loss is 0.0016643668059259653\n",
      "epoch: 7 step: 1233, loss is 0.0075577241368591785\n",
      "epoch: 7 step: 1234, loss is 0.0018783968407660723\n",
      "epoch: 7 step: 1235, loss is 0.10410693287849426\n",
      "epoch: 7 step: 1236, loss is 0.014008894562721252\n",
      "epoch: 7 step: 1237, loss is 0.0009718172950670123\n",
      "epoch: 7 step: 1238, loss is 0.037925563752651215\n",
      "epoch: 7 step: 1239, loss is 0.20678071677684784\n",
      "epoch: 7 step: 1240, loss is 0.0016915996093302965\n",
      "epoch: 7 step: 1241, loss is 0.004203167278319597\n",
      "epoch: 7 step: 1242, loss is 0.00033462318242527544\n",
      "epoch: 7 step: 1243, loss is 0.0004495640750974417\n",
      "epoch: 7 step: 1244, loss is 0.0005227252840995789\n",
      "epoch: 7 step: 1245, loss is 0.00984279066324234\n",
      "epoch: 7 step: 1246, loss is 0.0004492267034947872\n",
      "epoch: 7 step: 1247, loss is 0.0165227223187685\n",
      "epoch: 7 step: 1248, loss is 0.0007826777873560786\n",
      "epoch: 7 step: 1249, loss is 0.0288902185857296\n",
      "epoch: 7 step: 1250, loss is 0.10036922991275787\n",
      "epoch: 7 step: 1251, loss is 0.00196843221783638\n",
      "epoch: 7 step: 1252, loss is 0.11449826508760452\n",
      "epoch: 7 step: 1253, loss is 0.0012137790909036994\n",
      "epoch: 7 step: 1254, loss is 0.00048324899398721755\n",
      "epoch: 7 step: 1255, loss is 0.07597433775663376\n",
      "epoch: 7 step: 1256, loss is 0.01057968195527792\n",
      "epoch: 7 step: 1257, loss is 0.0006057419814169407\n",
      "epoch: 7 step: 1258, loss is 0.047280214726924896\n",
      "epoch: 7 step: 1259, loss is 0.010533413849771023\n",
      "epoch: 7 step: 1260, loss is 0.01430212240666151\n",
      "epoch: 7 step: 1261, loss is 0.008634951896965504\n",
      "epoch: 7 step: 1262, loss is 0.0502808503806591\n",
      "epoch: 7 step: 1263, loss is 0.07576269656419754\n",
      "epoch: 7 step: 1264, loss is 0.0008214331464841962\n",
      "epoch: 7 step: 1265, loss is 0.017526399344205856\n",
      "epoch: 7 step: 1266, loss is 0.002801997121423483\n",
      "epoch: 7 step: 1267, loss is 0.009431642480194569\n",
      "epoch: 7 step: 1268, loss is 0.00018156948499381542\n",
      "epoch: 7 step: 1269, loss is 0.0005016638315282762\n",
      "epoch: 7 step: 1270, loss is 0.04539879038929939\n",
      "epoch: 7 step: 1271, loss is 0.06282162666320801\n",
      "epoch: 7 step: 1272, loss is 0.001684438670054078\n",
      "epoch: 7 step: 1273, loss is 0.0023924149572849274\n",
      "epoch: 7 step: 1274, loss is 0.0004661260754801333\n",
      "epoch: 7 step: 1275, loss is 0.001669715391471982\n",
      "epoch: 7 step: 1276, loss is 0.012460949830710888\n",
      "epoch: 7 step: 1277, loss is 0.003418745705857873\n",
      "epoch: 7 step: 1278, loss is 0.19595929980278015\n",
      "epoch: 7 step: 1279, loss is 0.01965297944843769\n",
      "epoch: 7 step: 1280, loss is 0.1258750706911087\n",
      "epoch: 7 step: 1281, loss is 0.006101520266383886\n",
      "epoch: 7 step: 1282, loss is 0.020655080676078796\n",
      "epoch: 7 step: 1283, loss is 0.0011739530600607395\n",
      "epoch: 7 step: 1284, loss is 0.0017801361391320825\n",
      "epoch: 7 step: 1285, loss is 0.0009757356019690633\n",
      "epoch: 7 step: 1286, loss is 0.012430336326360703\n",
      "epoch: 7 step: 1287, loss is 0.0006991457776166499\n",
      "epoch: 7 step: 1288, loss is 0.0013258355902507901\n",
      "epoch: 7 step: 1289, loss is 0.02816447801887989\n",
      "epoch: 7 step: 1290, loss is 0.01452045887708664\n",
      "epoch: 7 step: 1291, loss is 0.002806250238791108\n",
      "epoch: 7 step: 1292, loss is 0.0013873922871425748\n",
      "epoch: 7 step: 1293, loss is 0.0022116133477538824\n",
      "epoch: 7 step: 1294, loss is 0.04307428002357483\n",
      "epoch: 7 step: 1295, loss is 0.19467446208000183\n",
      "epoch: 7 step: 1296, loss is 0.0028426703065633774\n",
      "epoch: 7 step: 1297, loss is 0.0002904614375438541\n",
      "epoch: 7 step: 1298, loss is 0.005498891696333885\n",
      "epoch: 7 step: 1299, loss is 0.03764284402132034\n",
      "epoch: 7 step: 1300, loss is 0.09950244426727295\n",
      "epoch: 7 step: 1301, loss is 0.10307633876800537\n",
      "epoch: 7 step: 1302, loss is 0.0028341044671833515\n",
      "epoch: 7 step: 1303, loss is 0.011655240319669247\n",
      "epoch: 7 step: 1304, loss is 0.08503396064043045\n",
      "epoch: 7 step: 1305, loss is 0.0019555867183953524\n",
      "epoch: 7 step: 1306, loss is 0.03874070569872856\n",
      "epoch: 7 step: 1307, loss is 0.001306616817601025\n",
      "epoch: 7 step: 1308, loss is 0.005722605623304844\n",
      "epoch: 7 step: 1309, loss is 0.005958149675279856\n",
      "epoch: 7 step: 1310, loss is 0.015191505663096905\n",
      "epoch: 7 step: 1311, loss is 0.03660432994365692\n",
      "epoch: 7 step: 1312, loss is 0.07657469063997269\n",
      "epoch: 7 step: 1313, loss is 0.0014995563542470336\n",
      "epoch: 7 step: 1314, loss is 0.002418343210592866\n",
      "epoch: 7 step: 1315, loss is 0.0016837574075907469\n",
      "epoch: 7 step: 1316, loss is 0.021618297323584557\n",
      "epoch: 7 step: 1317, loss is 0.015995780006051064\n",
      "epoch: 7 step: 1318, loss is 0.008213610388338566\n",
      "epoch: 7 step: 1319, loss is 0.0005746862152591348\n",
      "epoch: 7 step: 1320, loss is 0.0010898757027462125\n",
      "epoch: 7 step: 1321, loss is 0.02578596957027912\n",
      "epoch: 7 step: 1322, loss is 0.017162080854177475\n",
      "epoch: 7 step: 1323, loss is 0.040188979357481\n",
      "epoch: 7 step: 1324, loss is 0.011054267175495625\n",
      "epoch: 7 step: 1325, loss is 0.003058486385270953\n",
      "epoch: 7 step: 1326, loss is 0.001925168908201158\n",
      "epoch: 7 step: 1327, loss is 0.00158270716201514\n",
      "epoch: 7 step: 1328, loss is 0.000677159579936415\n",
      "epoch: 7 step: 1329, loss is 0.07033303380012512\n",
      "epoch: 7 step: 1330, loss is 0.0008649381343275309\n",
      "epoch: 7 step: 1331, loss is 0.0008992388029582798\n",
      "epoch: 7 step: 1332, loss is 0.0013354283291846514\n",
      "epoch: 7 step: 1333, loss is 0.058530542999506\n",
      "epoch: 7 step: 1334, loss is 0.0009166442323476076\n",
      "epoch: 7 step: 1335, loss is 0.002283691428601742\n",
      "epoch: 7 step: 1336, loss is 0.0009766754228621721\n",
      "epoch: 7 step: 1337, loss is 0.010949851013720036\n",
      "epoch: 7 step: 1338, loss is 0.00590474670752883\n",
      "epoch: 7 step: 1339, loss is 0.00019892580166924745\n",
      "epoch: 7 step: 1340, loss is 0.0009774244390428066\n",
      "epoch: 7 step: 1341, loss is 0.06610319763422012\n",
      "epoch: 7 step: 1342, loss is 0.00767271313816309\n",
      "epoch: 7 step: 1343, loss is 0.004007677081972361\n",
      "epoch: 7 step: 1344, loss is 0.08310601115226746\n",
      "epoch: 7 step: 1345, loss is 0.002810613950714469\n",
      "epoch: 7 step: 1346, loss is 0.0009680232033133507\n",
      "epoch: 7 step: 1347, loss is 0.002093101851642132\n",
      "epoch: 7 step: 1348, loss is 0.024440614506602287\n",
      "epoch: 7 step: 1349, loss is 0.0006662538507953286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 1350, loss is 0.006333526223897934\n",
      "epoch: 7 step: 1351, loss is 0.0014009670121595263\n",
      "epoch: 7 step: 1352, loss is 0.001475650118663907\n",
      "epoch: 7 step: 1353, loss is 0.05713988468050957\n",
      "epoch: 7 step: 1354, loss is 0.010629603639245033\n",
      "epoch: 7 step: 1355, loss is 0.0021345268469303846\n",
      "epoch: 7 step: 1356, loss is 0.07917594909667969\n",
      "epoch: 7 step: 1357, loss is 0.027994906529784203\n",
      "epoch: 7 step: 1358, loss is 0.029596293345093727\n",
      "epoch: 7 step: 1359, loss is 0.012464920058846474\n",
      "epoch: 7 step: 1360, loss is 0.001674348721280694\n",
      "epoch: 7 step: 1361, loss is 0.0021338241640478373\n",
      "epoch: 7 step: 1362, loss is 0.0002200788294430822\n",
      "epoch: 7 step: 1363, loss is 4.581876055453904e-05\n",
      "epoch: 7 step: 1364, loss is 0.00036048455513082445\n",
      "epoch: 7 step: 1365, loss is 0.0005081476992927492\n",
      "epoch: 7 step: 1366, loss is 0.004466692917048931\n",
      "epoch: 7 step: 1367, loss is 0.02195858582854271\n",
      "epoch: 7 step: 1368, loss is 0.006527663674205542\n",
      "epoch: 7 step: 1369, loss is 0.0006361729465425014\n",
      "epoch: 7 step: 1370, loss is 0.00017405780090484768\n",
      "epoch: 7 step: 1371, loss is 0.00030347483698278666\n",
      "epoch: 7 step: 1372, loss is 0.04784347116947174\n",
      "epoch: 7 step: 1373, loss is 0.03718136250972748\n",
      "epoch: 7 step: 1374, loss is 0.0007840336766093969\n",
      "epoch: 7 step: 1375, loss is 0.0026886228006333113\n",
      "epoch: 7 step: 1376, loss is 0.0014363708905875683\n",
      "epoch: 7 step: 1377, loss is 0.015209052711725235\n",
      "epoch: 7 step: 1378, loss is 0.00014089667820371687\n",
      "epoch: 7 step: 1379, loss is 0.00041462425724603236\n",
      "epoch: 7 step: 1380, loss is 0.0002503860741853714\n",
      "epoch: 7 step: 1381, loss is 0.005019783042371273\n",
      "epoch: 7 step: 1382, loss is 0.000182404211955145\n",
      "epoch: 7 step: 1383, loss is 0.002148018917068839\n",
      "epoch: 7 step: 1384, loss is 0.0014912467449903488\n",
      "epoch: 7 step: 1385, loss is 0.011206526309251785\n",
      "epoch: 7 step: 1386, loss is 0.009836109355092049\n",
      "epoch: 7 step: 1387, loss is 0.012054088525474072\n",
      "epoch: 7 step: 1388, loss is 0.007972274906933308\n",
      "epoch: 7 step: 1389, loss is 0.00012155163858551532\n",
      "epoch: 7 step: 1390, loss is 0.005804385058581829\n",
      "epoch: 7 step: 1391, loss is 0.01940922439098358\n",
      "epoch: 7 step: 1392, loss is 0.0015909957000985742\n",
      "epoch: 7 step: 1393, loss is 0.0013261993881314993\n",
      "epoch: 7 step: 1394, loss is 0.0018328625010326505\n",
      "epoch: 7 step: 1395, loss is 0.015828192234039307\n",
      "epoch: 7 step: 1396, loss is 0.004596029408276081\n",
      "epoch: 7 step: 1397, loss is 0.0008426508284173906\n",
      "epoch: 7 step: 1398, loss is 0.028326205909252167\n",
      "epoch: 7 step: 1399, loss is 0.0003598968905862421\n",
      "epoch: 7 step: 1400, loss is 0.0013281787978485227\n",
      "epoch: 7 step: 1401, loss is 0.002982360776513815\n",
      "epoch: 7 step: 1402, loss is 0.0002729471889324486\n",
      "epoch: 7 step: 1403, loss is 0.0003169229603372514\n",
      "epoch: 7 step: 1404, loss is 0.005374509375542402\n",
      "epoch: 7 step: 1405, loss is 0.003690637880936265\n",
      "epoch: 7 step: 1406, loss is 0.0005564881139434874\n",
      "epoch: 7 step: 1407, loss is 0.03191377595067024\n",
      "epoch: 7 step: 1408, loss is 0.0003641811781562865\n",
      "epoch: 7 step: 1409, loss is 0.0008415492484346032\n",
      "epoch: 7 step: 1410, loss is 0.009694792330265045\n",
      "epoch: 7 step: 1411, loss is 0.0002741364296525717\n",
      "epoch: 7 step: 1412, loss is 0.014501119032502174\n",
      "epoch: 7 step: 1413, loss is 0.0021267470438033342\n",
      "epoch: 7 step: 1414, loss is 0.005744664929807186\n",
      "epoch: 7 step: 1415, loss is 0.002432557987049222\n",
      "epoch: 7 step: 1416, loss is 0.04694275185465813\n",
      "epoch: 7 step: 1417, loss is 0.00023131593479774892\n",
      "epoch: 7 step: 1418, loss is 0.00027338118525221944\n",
      "epoch: 7 step: 1419, loss is 0.20640583336353302\n",
      "epoch: 7 step: 1420, loss is 0.0006888123461976647\n",
      "epoch: 7 step: 1421, loss is 0.0010978372301906347\n",
      "epoch: 7 step: 1422, loss is 0.012141376733779907\n",
      "epoch: 7 step: 1423, loss is 0.001339729642495513\n",
      "epoch: 7 step: 1424, loss is 0.004289455711841583\n",
      "epoch: 7 step: 1425, loss is 8.530891500413418e-05\n",
      "epoch: 7 step: 1426, loss is 0.037249915301799774\n",
      "epoch: 7 step: 1427, loss is 0.03154170140624046\n",
      "epoch: 7 step: 1428, loss is 0.00022164531401358545\n",
      "epoch: 7 step: 1429, loss is 0.054625239223241806\n",
      "epoch: 7 step: 1430, loss is 0.000734498375095427\n",
      "epoch: 7 step: 1431, loss is 0.00016433687414973974\n",
      "epoch: 7 step: 1432, loss is 0.018947336822748184\n",
      "epoch: 7 step: 1433, loss is 0.0375002883374691\n",
      "epoch: 7 step: 1434, loss is 0.00026435553445480764\n",
      "epoch: 7 step: 1435, loss is 0.14649605751037598\n",
      "epoch: 7 step: 1436, loss is 0.0043101138435304165\n",
      "epoch: 7 step: 1437, loss is 0.001187636167742312\n",
      "epoch: 7 step: 1438, loss is 0.00045999453868716955\n",
      "epoch: 7 step: 1439, loss is 0.016671093180775642\n",
      "epoch: 7 step: 1440, loss is 0.005054604262113571\n",
      "epoch: 7 step: 1441, loss is 0.0015605094376951456\n",
      "epoch: 7 step: 1442, loss is 0.0002218665467808023\n",
      "epoch: 7 step: 1443, loss is 0.0035172216594219208\n",
      "epoch: 7 step: 1444, loss is 0.04876381531357765\n",
      "epoch: 7 step: 1445, loss is 0.004441656172275543\n",
      "epoch: 7 step: 1446, loss is 0.022749144583940506\n",
      "epoch: 7 step: 1447, loss is 0.0007576429634355009\n",
      "epoch: 7 step: 1448, loss is 0.011336204595863819\n",
      "epoch: 7 step: 1449, loss is 0.05550292506814003\n",
      "epoch: 7 step: 1450, loss is 0.003962830174714327\n",
      "epoch: 7 step: 1451, loss is 6.37880148133263e-05\n",
      "epoch: 7 step: 1452, loss is 0.09268849343061447\n",
      "epoch: 7 step: 1453, loss is 0.06249865144491196\n",
      "epoch: 7 step: 1454, loss is 0.017243482172489166\n",
      "epoch: 7 step: 1455, loss is 0.0208391472697258\n",
      "epoch: 7 step: 1456, loss is 0.009164530783891678\n",
      "epoch: 7 step: 1457, loss is 0.001974624115973711\n",
      "epoch: 7 step: 1458, loss is 0.0017576742684468627\n",
      "epoch: 7 step: 1459, loss is 0.024717282503843307\n",
      "epoch: 7 step: 1460, loss is 0.024039696902036667\n",
      "epoch: 7 step: 1461, loss is 0.041105717420578\n",
      "epoch: 7 step: 1462, loss is 0.022517122328281403\n",
      "epoch: 7 step: 1463, loss is 0.14935211837291718\n",
      "epoch: 7 step: 1464, loss is 0.00012214912567287683\n",
      "epoch: 7 step: 1465, loss is 0.0028009600937366486\n",
      "epoch: 7 step: 1466, loss is 0.0006784386350773275\n",
      "epoch: 7 step: 1467, loss is 0.05326703563332558\n",
      "epoch: 7 step: 1468, loss is 0.11214298009872437\n",
      "epoch: 7 step: 1469, loss is 0.01740362122654915\n",
      "epoch: 7 step: 1470, loss is 0.0012725826818495989\n",
      "epoch: 7 step: 1471, loss is 0.00035179249243810773\n",
      "epoch: 7 step: 1472, loss is 0.0009292536415159702\n",
      "epoch: 7 step: 1473, loss is 0.06545186787843704\n",
      "epoch: 7 step: 1474, loss is 0.00532531226053834\n",
      "epoch: 7 step: 1475, loss is 0.0021415918599814177\n",
      "epoch: 7 step: 1476, loss is 0.014784551225602627\n",
      "epoch: 7 step: 1477, loss is 0.31523585319519043\n",
      "epoch: 7 step: 1478, loss is 0.01847803220152855\n",
      "epoch: 7 step: 1479, loss is 0.04285735636949539\n",
      "epoch: 7 step: 1480, loss is 0.0017438913928344846\n",
      "epoch: 7 step: 1481, loss is 0.0005214660777710378\n",
      "epoch: 7 step: 1482, loss is 0.00012261237134225667\n",
      "epoch: 7 step: 1483, loss is 0.00943418312817812\n",
      "epoch: 7 step: 1484, loss is 0.0003420127322897315\n",
      "epoch: 7 step: 1485, loss is 0.0020982453133910894\n",
      "epoch: 7 step: 1486, loss is 0.001881117234006524\n",
      "epoch: 7 step: 1487, loss is 0.0031126823741942644\n",
      "epoch: 7 step: 1488, loss is 0.00030160730239003897\n",
      "epoch: 7 step: 1489, loss is 0.00037803337909281254\n",
      "epoch: 7 step: 1490, loss is 0.001450213952921331\n",
      "epoch: 7 step: 1491, loss is 0.000905424531083554\n",
      "epoch: 7 step: 1492, loss is 0.003205874003469944\n",
      "epoch: 7 step: 1493, loss is 0.005328316241502762\n",
      "epoch: 7 step: 1494, loss is 0.09402281045913696\n",
      "epoch: 7 step: 1495, loss is 0.0038922573439776897\n",
      "epoch: 7 step: 1496, loss is 0.004241611808538437\n",
      "epoch: 7 step: 1497, loss is 0.13840028643608093\n",
      "epoch: 7 step: 1498, loss is 0.004266906064003706\n",
      "epoch: 7 step: 1499, loss is 0.0005529681802727282\n",
      "epoch: 7 step: 1500, loss is 0.0003319630050100386\n",
      "epoch: 7 step: 1501, loss is 0.003527131164446473\n",
      "epoch: 7 step: 1502, loss is 0.0003773340431507677\n",
      "epoch: 7 step: 1503, loss is 0.002781527815386653\n",
      "epoch: 7 step: 1504, loss is 0.014056325890123844\n",
      "epoch: 7 step: 1505, loss is 0.017530739307403564\n",
      "epoch: 7 step: 1506, loss is 0.05718744173645973\n",
      "epoch: 7 step: 1507, loss is 0.002453946741297841\n",
      "epoch: 7 step: 1508, loss is 0.0007421075133606791\n",
      "epoch: 7 step: 1509, loss is 0.0611482709646225\n",
      "epoch: 7 step: 1510, loss is 0.016643093898892403\n",
      "epoch: 7 step: 1511, loss is 0.0007297375705093145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 1512, loss is 0.03873530775308609\n",
      "epoch: 7 step: 1513, loss is 0.0003771250485442579\n",
      "epoch: 7 step: 1514, loss is 0.0012466056505218148\n",
      "epoch: 7 step: 1515, loss is 0.01032106950879097\n",
      "epoch: 7 step: 1516, loss is 0.0003743159759324044\n",
      "epoch: 7 step: 1517, loss is 0.004007027950137854\n",
      "epoch: 7 step: 1518, loss is 0.00011851221643155441\n",
      "epoch: 7 step: 1519, loss is 0.010796436108648777\n",
      "epoch: 7 step: 1520, loss is 0.0002747650723904371\n",
      "epoch: 7 step: 1521, loss is 0.0005308336694724858\n",
      "epoch: 7 step: 1522, loss is 0.0027566987555474043\n",
      "epoch: 7 step: 1523, loss is 0.0006888571660965681\n",
      "epoch: 7 step: 1524, loss is 0.004876422695815563\n",
      "epoch: 7 step: 1525, loss is 0.001430485281161964\n",
      "epoch: 7 step: 1526, loss is 0.0005812898743897676\n",
      "epoch: 7 step: 1527, loss is 0.0028326152823865414\n",
      "epoch: 7 step: 1528, loss is 0.03124220110476017\n",
      "epoch: 7 step: 1529, loss is 0.0012848759070038795\n",
      "epoch: 7 step: 1530, loss is 0.0008913673227652907\n",
      "epoch: 7 step: 1531, loss is 0.034633196890354156\n",
      "epoch: 7 step: 1532, loss is 2.6020976292784326e-05\n",
      "epoch: 7 step: 1533, loss is 0.0010516892652958632\n",
      "epoch: 7 step: 1534, loss is 0.2671409249305725\n",
      "epoch: 7 step: 1535, loss is 0.007156011648476124\n",
      "epoch: 7 step: 1536, loss is 0.005526616238057613\n",
      "epoch: 7 step: 1537, loss is 0.0001419827458448708\n",
      "epoch: 7 step: 1538, loss is 0.026020105928182602\n",
      "epoch: 7 step: 1539, loss is 0.08381196856498718\n",
      "epoch: 7 step: 1540, loss is 0.004354232456535101\n",
      "epoch: 7 step: 1541, loss is 5.3049647249281406e-05\n",
      "epoch: 7 step: 1542, loss is 0.10966044664382935\n",
      "epoch: 7 step: 1543, loss is 0.03419221192598343\n",
      "epoch: 7 step: 1544, loss is 0.0008120976272039115\n",
      "epoch: 7 step: 1545, loss is 0.0008490172913298011\n",
      "epoch: 7 step: 1546, loss is 0.0018585063517093658\n",
      "epoch: 7 step: 1547, loss is 0.030798237770795822\n",
      "epoch: 7 step: 1548, loss is 0.0011490446049720049\n",
      "epoch: 7 step: 1549, loss is 0.12916570901870728\n",
      "epoch: 7 step: 1550, loss is 0.0004451349377632141\n",
      "epoch: 7 step: 1551, loss is 0.1896001398563385\n",
      "epoch: 7 step: 1552, loss is 0.0051802052184939384\n",
      "epoch: 7 step: 1553, loss is 0.027277782559394836\n",
      "epoch: 7 step: 1554, loss is 0.0003930919338017702\n",
      "epoch: 7 step: 1555, loss is 0.003493121126666665\n",
      "epoch: 7 step: 1556, loss is 0.017520181834697723\n",
      "epoch: 7 step: 1557, loss is 0.006254720967262983\n",
      "epoch: 7 step: 1558, loss is 0.024521024897694588\n",
      "epoch: 7 step: 1559, loss is 0.00017018472135532647\n",
      "epoch: 7 step: 1560, loss is 0.0002394366019871086\n",
      "epoch: 7 step: 1561, loss is 0.008511580526828766\n",
      "epoch: 7 step: 1562, loss is 0.00045276241144165397\n",
      "epoch: 7 step: 1563, loss is 0.00010686243331292644\n",
      "epoch: 7 step: 1564, loss is 0.00021187023958191276\n",
      "epoch: 7 step: 1565, loss is 0.0021809458266943693\n",
      "epoch: 7 step: 1566, loss is 0.025541171431541443\n",
      "epoch: 7 step: 1567, loss is 8.727269596420228e-05\n",
      "epoch: 7 step: 1568, loss is 0.01721385307610035\n",
      "epoch: 7 step: 1569, loss is 0.01600281521677971\n",
      "epoch: 7 step: 1570, loss is 2.7163030608789995e-05\n",
      "epoch: 7 step: 1571, loss is 0.0002633010735735297\n",
      "epoch: 7 step: 1572, loss is 0.0017613127129152417\n",
      "epoch: 7 step: 1573, loss is 0.0851428434252739\n",
      "epoch: 7 step: 1574, loss is 0.0003431375080253929\n",
      "epoch: 7 step: 1575, loss is 0.0066442424431443214\n",
      "epoch: 7 step: 1576, loss is 0.023399589583277702\n",
      "epoch: 7 step: 1577, loss is 0.000785205396823585\n",
      "epoch: 7 step: 1578, loss is 0.05455358698964119\n",
      "epoch: 7 step: 1579, loss is 0.009768993593752384\n",
      "epoch: 7 step: 1580, loss is 0.003723571542650461\n",
      "epoch: 7 step: 1581, loss is 0.00015057067503221333\n",
      "epoch: 7 step: 1582, loss is 0.06416108459234238\n",
      "epoch: 7 step: 1583, loss is 0.0006713876500725746\n",
      "epoch: 7 step: 1584, loss is 0.16633905470371246\n",
      "epoch: 7 step: 1585, loss is 0.002329921815544367\n",
      "epoch: 7 step: 1586, loss is 0.0007506527472287416\n",
      "epoch: 7 step: 1587, loss is 0.00038685003528371453\n",
      "epoch: 7 step: 1588, loss is 0.0018759069498628378\n",
      "epoch: 7 step: 1589, loss is 0.00017734944412950426\n",
      "epoch: 7 step: 1590, loss is 0.004220444709062576\n",
      "epoch: 7 step: 1591, loss is 0.0015574936987832189\n",
      "epoch: 7 step: 1592, loss is 0.004305415786802769\n",
      "epoch: 7 step: 1593, loss is 0.0003729101736098528\n",
      "epoch: 7 step: 1594, loss is 7.317645940929651e-05\n",
      "epoch: 7 step: 1595, loss is 0.00015138393791858107\n",
      "epoch: 7 step: 1596, loss is 0.09494622051715851\n",
      "epoch: 7 step: 1597, loss is 0.00025543587980791926\n",
      "epoch: 7 step: 1598, loss is 0.07832923531532288\n",
      "epoch: 7 step: 1599, loss is 0.0005888216546736658\n",
      "epoch: 7 step: 1600, loss is 0.026784276589751244\n",
      "epoch: 7 step: 1601, loss is 0.05970899015665054\n",
      "epoch: 7 step: 1602, loss is 0.015167711302638054\n",
      "epoch: 7 step: 1603, loss is 0.03827674314379692\n",
      "epoch: 7 step: 1604, loss is 0.024696670472621918\n",
      "epoch: 7 step: 1605, loss is 0.0005528220208361745\n",
      "epoch: 7 step: 1606, loss is 0.006807725876569748\n",
      "epoch: 7 step: 1607, loss is 0.10869190096855164\n",
      "epoch: 7 step: 1608, loss is 0.059156384319067\n",
      "epoch: 7 step: 1609, loss is 0.0033829088788479567\n",
      "epoch: 7 step: 1610, loss is 0.004539534915238619\n",
      "epoch: 7 step: 1611, loss is 0.011875413358211517\n",
      "epoch: 7 step: 1612, loss is 0.21951976418495178\n",
      "epoch: 7 step: 1613, loss is 0.05334339290857315\n",
      "epoch: 7 step: 1614, loss is 0.008981144987046719\n",
      "epoch: 7 step: 1615, loss is 0.00231600902043283\n",
      "epoch: 7 step: 1616, loss is 0.06713221967220306\n",
      "epoch: 7 step: 1617, loss is 0.0019113776506856084\n",
      "epoch: 7 step: 1618, loss is 0.038747578859329224\n",
      "epoch: 7 step: 1619, loss is 0.0004927563713863492\n",
      "epoch: 7 step: 1620, loss is 0.0008989976486191154\n",
      "epoch: 7 step: 1621, loss is 0.0016380025772377849\n",
      "epoch: 7 step: 1622, loss is 0.03548258915543556\n",
      "epoch: 7 step: 1623, loss is 0.00010476513853063807\n",
      "epoch: 7 step: 1624, loss is 0.0035544601269066334\n",
      "epoch: 7 step: 1625, loss is 0.0001792002731235698\n",
      "epoch: 7 step: 1626, loss is 0.00019068650726694614\n",
      "epoch: 7 step: 1627, loss is 0.0011011626338586211\n",
      "epoch: 7 step: 1628, loss is 0.0008952303323894739\n",
      "epoch: 7 step: 1629, loss is 0.3354851007461548\n",
      "epoch: 7 step: 1630, loss is 0.010183907113969326\n",
      "epoch: 7 step: 1631, loss is 0.0011809595162048936\n",
      "epoch: 7 step: 1632, loss is 0.0008796278852969408\n",
      "epoch: 7 step: 1633, loss is 0.13178306818008423\n",
      "epoch: 7 step: 1634, loss is 0.0065192473120987415\n",
      "epoch: 7 step: 1635, loss is 0.0005990858771838248\n",
      "epoch: 7 step: 1636, loss is 0.00011165165051352233\n",
      "epoch: 7 step: 1637, loss is 0.009791495278477669\n",
      "epoch: 7 step: 1638, loss is 0.13170459866523743\n",
      "epoch: 7 step: 1639, loss is 1.1492900739540346e-05\n",
      "epoch: 7 step: 1640, loss is 0.010316022671759129\n",
      "epoch: 7 step: 1641, loss is 0.008624752052128315\n",
      "epoch: 7 step: 1642, loss is 0.005567583721131086\n",
      "epoch: 7 step: 1643, loss is 0.0027619143947958946\n",
      "epoch: 7 step: 1644, loss is 0.006238937843590975\n",
      "epoch: 7 step: 1645, loss is 0.0063826292753219604\n",
      "epoch: 7 step: 1646, loss is 0.0019072277937084436\n",
      "epoch: 7 step: 1647, loss is 0.003074126783758402\n",
      "epoch: 7 step: 1648, loss is 0.004832027480006218\n",
      "epoch: 7 step: 1649, loss is 0.00768778333440423\n",
      "epoch: 7 step: 1650, loss is 0.00018060924776364118\n",
      "epoch: 7 step: 1651, loss is 0.0002096855896525085\n",
      "epoch: 7 step: 1652, loss is 0.0030973549000918865\n",
      "epoch: 7 step: 1653, loss is 0.0436229445040226\n",
      "epoch: 7 step: 1654, loss is 0.005518070422112942\n",
      "epoch: 7 step: 1655, loss is 0.1317766159772873\n",
      "epoch: 7 step: 1656, loss is 0.0026681937742978334\n",
      "epoch: 7 step: 1657, loss is 0.0006656454643234611\n",
      "epoch: 7 step: 1658, loss is 0.0036498496774584055\n",
      "epoch: 7 step: 1659, loss is 6.407337787095457e-05\n",
      "epoch: 7 step: 1660, loss is 0.0014501751866191626\n",
      "epoch: 7 step: 1661, loss is 0.1224745362997055\n",
      "epoch: 7 step: 1662, loss is 0.00020176431280560791\n",
      "epoch: 7 step: 1663, loss is 0.15287290513515472\n",
      "epoch: 7 step: 1664, loss is 0.01693825237452984\n",
      "epoch: 7 step: 1665, loss is 0.10380467772483826\n",
      "epoch: 7 step: 1666, loss is 0.0010005213553085923\n",
      "epoch: 7 step: 1667, loss is 0.0006285982090048492\n",
      "epoch: 7 step: 1668, loss is 0.00025142531376332045\n",
      "epoch: 7 step: 1669, loss is 0.18785518407821655\n",
      "epoch: 7 step: 1670, loss is 0.10161500424146652\n",
      "epoch: 7 step: 1671, loss is 0.006662077270448208\n",
      "epoch: 7 step: 1672, loss is 0.0063058664090931416\n",
      "epoch: 7 step: 1673, loss is 0.006868460215628147\n",
      "epoch: 7 step: 1674, loss is 0.001221305807121098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 1675, loss is 0.0001983680995181203\n",
      "epoch: 7 step: 1676, loss is 0.03509494662284851\n",
      "epoch: 7 step: 1677, loss is 0.017091097310185432\n",
      "epoch: 7 step: 1678, loss is 0.0004946920089423656\n",
      "epoch: 7 step: 1679, loss is 0.012296199798583984\n",
      "epoch: 7 step: 1680, loss is 9.819828846957535e-05\n",
      "epoch: 7 step: 1681, loss is 0.00024064996978268027\n",
      "epoch: 7 step: 1682, loss is 0.0004352896648924798\n",
      "epoch: 7 step: 1683, loss is 0.08114343136548996\n",
      "epoch: 7 step: 1684, loss is 0.0004930435679852962\n",
      "epoch: 7 step: 1685, loss is 0.0012303515104576945\n",
      "epoch: 7 step: 1686, loss is 0.02916433848440647\n",
      "epoch: 7 step: 1687, loss is 0.030505282804369926\n",
      "epoch: 7 step: 1688, loss is 0.0025018819142132998\n",
      "epoch: 7 step: 1689, loss is 0.00375219970010221\n",
      "epoch: 7 step: 1690, loss is 0.0012045527109876275\n",
      "epoch: 7 step: 1691, loss is 0.01618911325931549\n",
      "epoch: 7 step: 1692, loss is 0.009776853956282139\n",
      "epoch: 7 step: 1693, loss is 0.023135749623179436\n",
      "epoch: 7 step: 1694, loss is 0.0004994983319193125\n",
      "epoch: 7 step: 1695, loss is 0.009219188243150711\n",
      "epoch: 7 step: 1696, loss is 0.00025281700072810054\n",
      "epoch: 7 step: 1697, loss is 0.05296633392572403\n",
      "epoch: 7 step: 1698, loss is 0.20861534774303436\n",
      "epoch: 7 step: 1699, loss is 0.29645922780036926\n",
      "epoch: 7 step: 1700, loss is 0.004498457536101341\n",
      "epoch: 7 step: 1701, loss is 0.03877875953912735\n",
      "epoch: 7 step: 1702, loss is 0.007181719411164522\n",
      "epoch: 7 step: 1703, loss is 0.02477453462779522\n",
      "epoch: 7 step: 1704, loss is 0.008779426105320454\n",
      "epoch: 7 step: 1705, loss is 0.0012036749394610524\n",
      "epoch: 7 step: 1706, loss is 0.001000194693915546\n",
      "epoch: 7 step: 1707, loss is 0.01639541983604431\n",
      "epoch: 7 step: 1708, loss is 0.00237987725995481\n",
      "epoch: 7 step: 1709, loss is 0.005600571632385254\n",
      "epoch: 7 step: 1710, loss is 0.0033684000372886658\n",
      "epoch: 7 step: 1711, loss is 0.0010385708883404732\n",
      "epoch: 7 step: 1712, loss is 0.002159436233341694\n",
      "epoch: 7 step: 1713, loss is 0.025132136419415474\n",
      "epoch: 7 step: 1714, loss is 0.006442147772759199\n",
      "epoch: 7 step: 1715, loss is 0.00167661695741117\n",
      "epoch: 7 step: 1716, loss is 0.01501371432095766\n",
      "epoch: 7 step: 1717, loss is 0.040274813771247864\n",
      "epoch: 7 step: 1718, loss is 0.006803909782320261\n",
      "epoch: 7 step: 1719, loss is 0.047564808279275894\n",
      "epoch: 7 step: 1720, loss is 0.00022313189401756972\n",
      "epoch: 7 step: 1721, loss is 0.004930663388222456\n",
      "epoch: 7 step: 1722, loss is 0.01571051962673664\n",
      "epoch: 7 step: 1723, loss is 0.009808517061173916\n",
      "epoch: 7 step: 1724, loss is 0.1189831867814064\n",
      "epoch: 7 step: 1725, loss is 0.0005144444876350462\n",
      "epoch: 7 step: 1726, loss is 0.01880814880132675\n",
      "epoch: 7 step: 1727, loss is 0.0182816032320261\n",
      "epoch: 7 step: 1728, loss is 0.008752763271331787\n",
      "epoch: 7 step: 1729, loss is 0.004174540750682354\n",
      "epoch: 7 step: 1730, loss is 0.06338556855916977\n",
      "epoch: 7 step: 1731, loss is 0.0001194823271362111\n",
      "epoch: 7 step: 1732, loss is 0.05342980846762657\n",
      "epoch: 7 step: 1733, loss is 0.01939702406525612\n",
      "epoch: 7 step: 1734, loss is 0.026192184537649155\n",
      "epoch: 7 step: 1735, loss is 0.004540465772151947\n",
      "epoch: 7 step: 1736, loss is 0.002940577920526266\n",
      "epoch: 7 step: 1737, loss is 0.0002984949678648263\n",
      "epoch: 7 step: 1738, loss is 0.0012589727994054556\n",
      "epoch: 7 step: 1739, loss is 0.05674898251891136\n",
      "epoch: 7 step: 1740, loss is 0.0044318074360489845\n",
      "epoch: 7 step: 1741, loss is 0.0002718966861721128\n",
      "epoch: 7 step: 1742, loss is 0.004740339238196611\n",
      "epoch: 7 step: 1743, loss is 0.00012832626816816628\n",
      "epoch: 7 step: 1744, loss is 0.02004193514585495\n",
      "epoch: 7 step: 1745, loss is 0.0019685649313032627\n",
      "epoch: 7 step: 1746, loss is 0.22908642888069153\n",
      "epoch: 7 step: 1747, loss is 0.21378500759601593\n",
      "epoch: 7 step: 1748, loss is 0.0822979211807251\n",
      "epoch: 7 step: 1749, loss is 0.005125891882926226\n",
      "epoch: 7 step: 1750, loss is 0.003389788558706641\n",
      "epoch: 7 step: 1751, loss is 0.0027335206978023052\n",
      "epoch: 7 step: 1752, loss is 0.0006230212165974081\n",
      "epoch: 7 step: 1753, loss is 0.002726850565522909\n",
      "epoch: 7 step: 1754, loss is 0.02020822837948799\n",
      "epoch: 7 step: 1755, loss is 0.01342706847935915\n",
      "epoch: 7 step: 1756, loss is 0.0018915984546765685\n",
      "epoch: 7 step: 1757, loss is 0.0003114244027528912\n",
      "epoch: 7 step: 1758, loss is 0.00028991803992539644\n",
      "epoch: 7 step: 1759, loss is 0.014580505900084972\n",
      "epoch: 7 step: 1760, loss is 0.015368564985692501\n",
      "epoch: 7 step: 1761, loss is 0.17932477593421936\n",
      "epoch: 7 step: 1762, loss is 0.028953488916158676\n",
      "epoch: 7 step: 1763, loss is 0.013840249739587307\n",
      "epoch: 7 step: 1764, loss is 0.0008264395873993635\n",
      "epoch: 7 step: 1765, loss is 0.0002625470806378871\n",
      "epoch: 7 step: 1766, loss is 0.17374087870121002\n",
      "epoch: 7 step: 1767, loss is 0.0015251003205776215\n",
      "epoch: 7 step: 1768, loss is 0.0027207541279494762\n",
      "epoch: 7 step: 1769, loss is 0.00038756075082346797\n",
      "epoch: 7 step: 1770, loss is 0.0021452493965625763\n",
      "epoch: 7 step: 1771, loss is 0.02647298201918602\n",
      "epoch: 7 step: 1772, loss is 0.00214776792563498\n",
      "epoch: 7 step: 1773, loss is 0.00634831702336669\n",
      "epoch: 7 step: 1774, loss is 0.0010692778741940856\n",
      "epoch: 7 step: 1775, loss is 0.00019688923202920705\n",
      "epoch: 7 step: 1776, loss is 0.0030671763233840466\n",
      "epoch: 7 step: 1777, loss is 0.014919282868504524\n",
      "epoch: 7 step: 1778, loss is 0.004508753307163715\n",
      "epoch: 7 step: 1779, loss is 0.010685663670301437\n",
      "epoch: 7 step: 1780, loss is 0.03044021688401699\n",
      "epoch: 7 step: 1781, loss is 0.0023212998639792204\n",
      "epoch: 7 step: 1782, loss is 0.052153050899505615\n",
      "epoch: 7 step: 1783, loss is 0.0007996828644536436\n",
      "epoch: 7 step: 1784, loss is 0.0021754915360361338\n",
      "epoch: 7 step: 1785, loss is 0.005548038985580206\n",
      "epoch: 7 step: 1786, loss is 0.028465421870350838\n",
      "epoch: 7 step: 1787, loss is 0.005957764573395252\n",
      "epoch: 7 step: 1788, loss is 0.0030268605332821608\n",
      "epoch: 7 step: 1789, loss is 0.0016565716359764338\n",
      "epoch: 7 step: 1790, loss is 0.022850528359413147\n",
      "epoch: 7 step: 1791, loss is 0.0016956442268565297\n",
      "epoch: 7 step: 1792, loss is 0.012927062809467316\n",
      "epoch: 7 step: 1793, loss is 0.0007486346876248717\n",
      "epoch: 7 step: 1794, loss is 0.01026611402630806\n",
      "epoch: 7 step: 1795, loss is 0.086619533598423\n",
      "epoch: 7 step: 1796, loss is 0.004756237845867872\n",
      "epoch: 7 step: 1797, loss is 0.3141637146472931\n",
      "epoch: 7 step: 1798, loss is 0.012642668560147285\n",
      "epoch: 7 step: 1799, loss is 0.02263007126748562\n",
      "epoch: 7 step: 1800, loss is 0.027181416749954224\n",
      "epoch: 7 step: 1801, loss is 0.18967919051647186\n",
      "epoch: 7 step: 1802, loss is 0.089018814265728\n",
      "epoch: 7 step: 1803, loss is 0.0023992841597646475\n",
      "epoch: 7 step: 1804, loss is 0.0044249785132706165\n",
      "epoch: 7 step: 1805, loss is 0.30474281311035156\n",
      "epoch: 7 step: 1806, loss is 0.0019108845153823495\n",
      "epoch: 7 step: 1807, loss is 0.002689759712666273\n",
      "epoch: 7 step: 1808, loss is 0.0008923147106543183\n",
      "epoch: 7 step: 1809, loss is 0.0016106382245197892\n",
      "epoch: 7 step: 1810, loss is 0.013150888495147228\n",
      "epoch: 7 step: 1811, loss is 0.004287299234420061\n",
      "epoch: 7 step: 1812, loss is 0.03314616158604622\n",
      "epoch: 7 step: 1813, loss is 0.012498666532337666\n",
      "epoch: 7 step: 1814, loss is 0.009663895703852177\n",
      "epoch: 7 step: 1815, loss is 0.013689091429114342\n",
      "epoch: 7 step: 1816, loss is 0.02164353057742119\n",
      "epoch: 7 step: 1817, loss is 0.0004554879851639271\n",
      "epoch: 7 step: 1818, loss is 0.01840604841709137\n",
      "epoch: 7 step: 1819, loss is 0.040909383445978165\n",
      "epoch: 7 step: 1820, loss is 0.0017694211564958096\n",
      "epoch: 7 step: 1821, loss is 0.0026509796734899282\n",
      "epoch: 7 step: 1822, loss is 0.0085535803809762\n",
      "epoch: 7 step: 1823, loss is 0.13070669770240784\n",
      "epoch: 7 step: 1824, loss is 0.02689376100897789\n",
      "epoch: 7 step: 1825, loss is 0.0800701230764389\n",
      "epoch: 7 step: 1826, loss is 0.00046016808482818305\n",
      "epoch: 7 step: 1827, loss is 0.0010820429306477308\n",
      "epoch: 7 step: 1828, loss is 0.051611170172691345\n",
      "epoch: 7 step: 1829, loss is 0.14416752755641937\n",
      "epoch: 7 step: 1830, loss is 0.017364954575896263\n",
      "epoch: 7 step: 1831, loss is 0.11158234626054764\n",
      "epoch: 7 step: 1832, loss is 0.13787463307380676\n",
      "epoch: 7 step: 1833, loss is 0.041054319590330124\n",
      "epoch: 7 step: 1834, loss is 0.02369493618607521\n",
      "epoch: 7 step: 1835, loss is 0.012416641227900982\n",
      "epoch: 7 step: 1836, loss is 0.005838834214955568\n",
      "epoch: 7 step: 1837, loss is 0.31189045310020447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 1838, loss is 0.01013482827693224\n",
      "epoch: 7 step: 1839, loss is 0.039262574166059494\n",
      "epoch: 7 step: 1840, loss is 0.0018726550042629242\n",
      "epoch: 7 step: 1841, loss is 0.0005181566230021417\n",
      "epoch: 7 step: 1842, loss is 0.007934591732919216\n",
      "epoch: 7 step: 1843, loss is 0.004244770854711533\n",
      "epoch: 7 step: 1844, loss is 0.007662530057132244\n",
      "epoch: 7 step: 1845, loss is 0.016660872846841812\n",
      "epoch: 7 step: 1846, loss is 0.10087966173887253\n",
      "epoch: 7 step: 1847, loss is 0.0014116084203124046\n",
      "epoch: 7 step: 1848, loss is 0.02718222141265869\n",
      "epoch: 7 step: 1849, loss is 0.002646842272952199\n",
      "epoch: 7 step: 1850, loss is 0.13453920185565948\n",
      "epoch: 7 step: 1851, loss is 0.007589112035930157\n",
      "epoch: 7 step: 1852, loss is 0.01714438386261463\n",
      "epoch: 7 step: 1853, loss is 0.002218689536675811\n",
      "epoch: 7 step: 1854, loss is 0.002988720079883933\n",
      "epoch: 7 step: 1855, loss is 0.00706461351364851\n",
      "epoch: 7 step: 1856, loss is 0.0001490161957917735\n",
      "epoch: 7 step: 1857, loss is 0.00034563211374916136\n",
      "epoch: 7 step: 1858, loss is 0.289311945438385\n",
      "epoch: 7 step: 1859, loss is 0.027753271162509918\n",
      "epoch: 7 step: 1860, loss is 0.017730429768562317\n",
      "epoch: 7 step: 1861, loss is 0.002158848801627755\n",
      "epoch: 7 step: 1862, loss is 0.002691590692847967\n",
      "epoch: 7 step: 1863, loss is 0.1088036447763443\n",
      "epoch: 7 step: 1864, loss is 0.0009696055203676224\n",
      "epoch: 7 step: 1865, loss is 0.001570909982547164\n",
      "epoch: 7 step: 1866, loss is 0.008913707919418812\n",
      "epoch: 7 step: 1867, loss is 0.0011971399653702974\n",
      "epoch: 7 step: 1868, loss is 0.0007169403834268451\n",
      "epoch: 7 step: 1869, loss is 0.1207825317978859\n",
      "epoch: 7 step: 1870, loss is 0.007457228843122721\n",
      "epoch: 7 step: 1871, loss is 0.007785884663462639\n",
      "epoch: 7 step: 1872, loss is 0.03596486151218414\n",
      "epoch: 7 step: 1873, loss is 0.01612613908946514\n",
      "epoch: 7 step: 1874, loss is 0.0009199422784149647\n",
      "epoch: 7 step: 1875, loss is 0.00024277657212223858\n",
      "epoch: 8 step: 1, loss is 0.06221187487244606\n",
      "epoch: 8 step: 2, loss is 0.0005043403361923993\n",
      "epoch: 8 step: 3, loss is 0.0017029274022206664\n",
      "epoch: 8 step: 4, loss is 0.004136503674089909\n",
      "epoch: 8 step: 5, loss is 0.025929003953933716\n",
      "epoch: 8 step: 6, loss is 0.0001442852953914553\n",
      "epoch: 8 step: 7, loss is 0.00293830968439579\n",
      "epoch: 8 step: 8, loss is 0.12923571467399597\n",
      "epoch: 8 step: 9, loss is 0.00038897505146451294\n",
      "epoch: 8 step: 10, loss is 0.00047681224532425404\n",
      "epoch: 8 step: 11, loss is 0.005735947750508785\n",
      "epoch: 8 step: 12, loss is 0.07564830034971237\n",
      "epoch: 8 step: 13, loss is 0.013609102927148342\n",
      "epoch: 8 step: 14, loss is 0.005607145838439465\n",
      "epoch: 8 step: 15, loss is 0.0009949468076229095\n",
      "epoch: 8 step: 16, loss is 0.037221964448690414\n",
      "epoch: 8 step: 17, loss is 0.0002449798630550504\n",
      "epoch: 8 step: 18, loss is 0.0008652403485029936\n",
      "epoch: 8 step: 19, loss is 0.0007449610275216401\n",
      "epoch: 8 step: 20, loss is 0.00039654155261814594\n",
      "epoch: 8 step: 21, loss is 0.0016086441464722157\n",
      "epoch: 8 step: 22, loss is 0.000361951591912657\n",
      "epoch: 8 step: 23, loss is 0.047497060149908066\n",
      "epoch: 8 step: 24, loss is 0.0017346754902973771\n",
      "epoch: 8 step: 25, loss is 0.0048258681781589985\n",
      "epoch: 8 step: 26, loss is 0.05875673517584801\n",
      "epoch: 8 step: 27, loss is 0.003491676179692149\n",
      "epoch: 8 step: 28, loss is 0.0009252188610844314\n",
      "epoch: 8 step: 29, loss is 0.0008963036816567183\n",
      "epoch: 8 step: 30, loss is 0.000661857018712908\n",
      "epoch: 8 step: 31, loss is 0.003069809637963772\n",
      "epoch: 8 step: 32, loss is 0.03949470818042755\n",
      "epoch: 8 step: 33, loss is 0.0007082555675879121\n",
      "epoch: 8 step: 34, loss is 0.010095453821122646\n",
      "epoch: 8 step: 35, loss is 0.006245523225516081\n",
      "epoch: 8 step: 36, loss is 0.0023567983880639076\n",
      "epoch: 8 step: 37, loss is 0.0001861598575487733\n",
      "epoch: 8 step: 38, loss is 0.0005022315890528262\n",
      "epoch: 8 step: 39, loss is 0.024206573143601418\n",
      "epoch: 8 step: 40, loss is 0.0002850749879144132\n",
      "epoch: 8 step: 41, loss is 0.0005955316592007875\n",
      "epoch: 8 step: 42, loss is 0.00304414052516222\n",
      "epoch: 8 step: 43, loss is 0.0007486282847821712\n",
      "epoch: 8 step: 44, loss is 0.03651222214102745\n",
      "epoch: 8 step: 45, loss is 0.00027602503541857004\n",
      "epoch: 8 step: 46, loss is 0.005384309682995081\n",
      "epoch: 8 step: 47, loss is 0.0008689850801602006\n",
      "epoch: 8 step: 48, loss is 0.0026110338512808084\n",
      "epoch: 8 step: 49, loss is 0.0005653136759065092\n",
      "epoch: 8 step: 50, loss is 0.0005228551453910768\n",
      "epoch: 8 step: 51, loss is 0.011532080359756947\n",
      "epoch: 8 step: 52, loss is 0.0003956161090172827\n",
      "epoch: 8 step: 53, loss is 0.00010442985512781888\n",
      "epoch: 8 step: 54, loss is 0.0001924362004501745\n",
      "epoch: 8 step: 55, loss is 0.002010566648095846\n",
      "epoch: 8 step: 56, loss is 0.0023337355814874172\n",
      "epoch: 8 step: 57, loss is 0.00027002280694432557\n",
      "epoch: 8 step: 58, loss is 0.00042075951932929456\n",
      "epoch: 8 step: 59, loss is 0.0064333057962358\n",
      "epoch: 8 step: 60, loss is 0.00267016957513988\n",
      "epoch: 8 step: 61, loss is 0.01598743163049221\n",
      "epoch: 8 step: 62, loss is 0.0010377764701843262\n",
      "epoch: 8 step: 63, loss is 0.001101781497709453\n",
      "epoch: 8 step: 64, loss is 0.0005673763807862997\n",
      "epoch: 8 step: 65, loss is 0.00024359661620110273\n",
      "epoch: 8 step: 66, loss is 0.0037458566948771477\n",
      "epoch: 8 step: 67, loss is 0.03322799131274223\n",
      "epoch: 8 step: 68, loss is 0.039013512432575226\n",
      "epoch: 8 step: 69, loss is 0.0006612867000512779\n",
      "epoch: 8 step: 70, loss is 0.00030069847707636654\n",
      "epoch: 8 step: 71, loss is 0.016140166670084\n",
      "epoch: 8 step: 72, loss is 0.003415403189137578\n",
      "epoch: 8 step: 73, loss is 0.001098412205465138\n",
      "epoch: 8 step: 74, loss is 0.03261983394622803\n",
      "epoch: 8 step: 75, loss is 0.003974479157477617\n",
      "epoch: 8 step: 76, loss is 4.2027550080092624e-05\n",
      "epoch: 8 step: 77, loss is 0.003153447527438402\n",
      "epoch: 8 step: 78, loss is 0.0009283250547014177\n",
      "epoch: 8 step: 79, loss is 0.00012465652253013104\n",
      "epoch: 8 step: 80, loss is 0.0049960799515247345\n",
      "epoch: 8 step: 81, loss is 0.00012070805678376928\n",
      "epoch: 8 step: 82, loss is 0.004613474011421204\n",
      "epoch: 8 step: 83, loss is 0.0007201737607829273\n",
      "epoch: 8 step: 84, loss is 0.0002108255575876683\n",
      "epoch: 8 step: 85, loss is 7.083035598043352e-05\n",
      "epoch: 8 step: 86, loss is 0.005510320421308279\n",
      "epoch: 8 step: 87, loss is 8.825363329378888e-05\n",
      "epoch: 8 step: 88, loss is 0.0011595928808674216\n",
      "epoch: 8 step: 89, loss is 0.1006094440817833\n",
      "epoch: 8 step: 90, loss is 0.0005181633168831468\n",
      "epoch: 8 step: 91, loss is 0.017513040453195572\n",
      "epoch: 8 step: 92, loss is 0.03699425980448723\n",
      "epoch: 8 step: 93, loss is 0.010513476096093655\n",
      "epoch: 8 step: 94, loss is 6.977449083933607e-05\n",
      "epoch: 8 step: 95, loss is 0.0013585627311840653\n",
      "epoch: 8 step: 96, loss is 0.00026014121249318123\n",
      "epoch: 8 step: 97, loss is 0.01660032570362091\n",
      "epoch: 8 step: 98, loss is 0.16719400882720947\n",
      "epoch: 8 step: 99, loss is 0.0003249853034503758\n",
      "epoch: 8 step: 100, loss is 0.004310946445912123\n",
      "epoch: 8 step: 101, loss is 0.041981399059295654\n",
      "epoch: 8 step: 102, loss is 0.03579841926693916\n",
      "epoch: 8 step: 103, loss is 0.001029646024107933\n",
      "epoch: 8 step: 104, loss is 0.16345751285552979\n",
      "epoch: 8 step: 105, loss is 0.030077841132879257\n",
      "epoch: 8 step: 106, loss is 0.008464299142360687\n",
      "epoch: 8 step: 107, loss is 0.0016376400599256158\n",
      "epoch: 8 step: 108, loss is 0.006112988572567701\n",
      "epoch: 8 step: 109, loss is 0.0013069106498733163\n",
      "epoch: 8 step: 110, loss is 0.0011743116192519665\n",
      "epoch: 8 step: 111, loss is 0.0008302627829834819\n",
      "epoch: 8 step: 112, loss is 0.0006737487856298685\n",
      "epoch: 8 step: 113, loss is 0.002818993292748928\n",
      "epoch: 8 step: 114, loss is 0.0009544268832542002\n",
      "epoch: 8 step: 115, loss is 0.03695692494511604\n",
      "epoch: 8 step: 116, loss is 0.0032236180268228054\n",
      "epoch: 8 step: 117, loss is 0.0012209574924781919\n",
      "epoch: 8 step: 118, loss is 0.019581696018576622\n",
      "epoch: 8 step: 119, loss is 0.0021946735214442015\n",
      "epoch: 8 step: 120, loss is 0.008134822361171246\n",
      "epoch: 8 step: 121, loss is 0.00013687368482351303\n",
      "epoch: 8 step: 122, loss is 0.0029715814162045717\n",
      "epoch: 8 step: 123, loss is 0.007111051119863987\n",
      "epoch: 8 step: 124, loss is 0.11010337620973587\n",
      "epoch: 8 step: 125, loss is 0.0010223579593002796\n",
      "epoch: 8 step: 126, loss is 0.07007970660924911\n",
      "epoch: 8 step: 127, loss is 0.0016185501590371132\n",
      "epoch: 8 step: 128, loss is 0.0040259757079184055\n",
      "epoch: 8 step: 129, loss is 0.03345951810479164\n",
      "epoch: 8 step: 130, loss is 0.008331735618412495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 131, loss is 0.00755841750651598\n",
      "epoch: 8 step: 132, loss is 0.0017080073012039065\n",
      "epoch: 8 step: 133, loss is 0.0010318094864487648\n",
      "epoch: 8 step: 134, loss is 0.0004938548081554472\n",
      "epoch: 8 step: 135, loss is 0.012127352878451347\n",
      "epoch: 8 step: 136, loss is 0.009917190298438072\n",
      "epoch: 8 step: 137, loss is 0.00017342927458230406\n",
      "epoch: 8 step: 138, loss is 0.0067322878167033195\n",
      "epoch: 8 step: 139, loss is 0.0012526679784059525\n",
      "epoch: 8 step: 140, loss is 0.00010018346802098677\n",
      "epoch: 8 step: 141, loss is 0.000568658928386867\n",
      "epoch: 8 step: 142, loss is 0.0008803623495623469\n",
      "epoch: 8 step: 143, loss is 0.005257211625576019\n",
      "epoch: 8 step: 144, loss is 0.012272052466869354\n",
      "epoch: 8 step: 145, loss is 5.3160365496296436e-05\n",
      "epoch: 8 step: 146, loss is 0.00449755135923624\n",
      "epoch: 8 step: 147, loss is 4.9363694415660575e-05\n",
      "epoch: 8 step: 148, loss is 0.02871437929570675\n",
      "epoch: 8 step: 149, loss is 0.042264364659786224\n",
      "epoch: 8 step: 150, loss is 0.00011318320321151987\n",
      "epoch: 8 step: 151, loss is 0.001597487716935575\n",
      "epoch: 8 step: 152, loss is 0.00258536241017282\n",
      "epoch: 8 step: 153, loss is 0.0502796396613121\n",
      "epoch: 8 step: 154, loss is 0.0005570905050262809\n",
      "epoch: 8 step: 155, loss is 0.0007987144635990262\n",
      "epoch: 8 step: 156, loss is 0.0009972188854590058\n",
      "epoch: 8 step: 157, loss is 0.001817507785744965\n",
      "epoch: 8 step: 158, loss is 0.00018889197963289917\n",
      "epoch: 8 step: 159, loss is 0.000924518913961947\n",
      "epoch: 8 step: 160, loss is 0.008814819157123566\n",
      "epoch: 8 step: 161, loss is 0.0008184878388419747\n",
      "epoch: 8 step: 162, loss is 0.004849725402891636\n",
      "epoch: 8 step: 163, loss is 0.025582851842045784\n",
      "epoch: 8 step: 164, loss is 8.500387048115954e-05\n",
      "epoch: 8 step: 165, loss is 0.0015839196275919676\n",
      "epoch: 8 step: 166, loss is 0.0004821402835659683\n",
      "epoch: 8 step: 167, loss is 0.034501682966947556\n",
      "epoch: 8 step: 168, loss is 0.03216296434402466\n",
      "epoch: 8 step: 169, loss is 0.003762871026992798\n",
      "epoch: 8 step: 170, loss is 0.006860929541289806\n",
      "epoch: 8 step: 171, loss is 0.00036622307379730046\n",
      "epoch: 8 step: 172, loss is 0.012243605218827724\n",
      "epoch: 8 step: 173, loss is 0.06799782812595367\n",
      "epoch: 8 step: 174, loss is 0.012050950899720192\n",
      "epoch: 8 step: 175, loss is 0.002190368715673685\n",
      "epoch: 8 step: 176, loss is 0.001418630126863718\n",
      "epoch: 8 step: 177, loss is 0.001328070880845189\n",
      "epoch: 8 step: 178, loss is 0.008056986145675182\n",
      "epoch: 8 step: 179, loss is 0.0028588424902409315\n",
      "epoch: 8 step: 180, loss is 0.11847513914108276\n",
      "epoch: 8 step: 181, loss is 0.00022099714260548353\n",
      "epoch: 8 step: 182, loss is 0.00023204844910651445\n",
      "epoch: 8 step: 183, loss is 0.0005595141556113958\n",
      "epoch: 8 step: 184, loss is 6.8420878960751e-05\n",
      "epoch: 8 step: 185, loss is 0.0014815583126619458\n",
      "epoch: 8 step: 186, loss is 0.00151138287037611\n",
      "epoch: 8 step: 187, loss is 0.0005794101161882281\n",
      "epoch: 8 step: 188, loss is 0.0006164611550047994\n",
      "epoch: 8 step: 189, loss is 0.028186555951833725\n",
      "epoch: 8 step: 190, loss is 0.025325832888484\n",
      "epoch: 8 step: 191, loss is 0.02402961254119873\n",
      "epoch: 8 step: 192, loss is 0.0007021110504865646\n",
      "epoch: 8 step: 193, loss is 8.01322894403711e-05\n",
      "epoch: 8 step: 194, loss is 0.00030355213675647974\n",
      "epoch: 8 step: 195, loss is 0.002420086646452546\n",
      "epoch: 8 step: 196, loss is 0.03257923200726509\n",
      "epoch: 8 step: 197, loss is 0.001274483627639711\n",
      "epoch: 8 step: 198, loss is 0.023486291989684105\n",
      "epoch: 8 step: 199, loss is 0.021122116595506668\n",
      "epoch: 8 step: 200, loss is 0.026153121143579483\n",
      "epoch: 8 step: 201, loss is 0.011490794830024242\n",
      "epoch: 8 step: 202, loss is 0.0009810652118176222\n",
      "epoch: 8 step: 203, loss is 6.64990147924982e-05\n",
      "epoch: 8 step: 204, loss is 0.046308860182762146\n",
      "epoch: 8 step: 205, loss is 3.595569796743803e-05\n",
      "epoch: 8 step: 206, loss is 0.033033087849617004\n",
      "epoch: 8 step: 207, loss is 0.0009810151532292366\n",
      "epoch: 8 step: 208, loss is 0.00010165271669393405\n",
      "epoch: 8 step: 209, loss is 0.0005709376418963075\n",
      "epoch: 8 step: 210, loss is 0.0003485741908662021\n",
      "epoch: 8 step: 211, loss is 0.00020386969845276326\n",
      "epoch: 8 step: 212, loss is 0.10167837888002396\n",
      "epoch: 8 step: 213, loss is 0.001057387562468648\n",
      "epoch: 8 step: 214, loss is 0.03203251585364342\n",
      "epoch: 8 step: 215, loss is 0.014758581295609474\n",
      "epoch: 8 step: 216, loss is 0.0060420255176723\n",
      "epoch: 8 step: 217, loss is 8.328934200108051e-05\n",
      "epoch: 8 step: 218, loss is 0.013217930682003498\n",
      "epoch: 8 step: 219, loss is 0.001499867532402277\n",
      "epoch: 8 step: 220, loss is 0.009676298126578331\n",
      "epoch: 8 step: 221, loss is 0.02030724287033081\n",
      "epoch: 8 step: 222, loss is 0.0005850978195667267\n",
      "epoch: 8 step: 223, loss is 0.015701476484537125\n",
      "epoch: 8 step: 224, loss is 5.526127279154025e-05\n",
      "epoch: 8 step: 225, loss is 0.0038369609974324703\n",
      "epoch: 8 step: 226, loss is 0.0023270798847079277\n",
      "epoch: 8 step: 227, loss is 2.589265386632178e-05\n",
      "epoch: 8 step: 228, loss is 0.00016805810446385294\n",
      "epoch: 8 step: 229, loss is 0.0015686142724007368\n",
      "epoch: 8 step: 230, loss is 0.025422291830182076\n",
      "epoch: 8 step: 231, loss is 0.0001349166122963652\n",
      "epoch: 8 step: 232, loss is 0.008849327452480793\n",
      "epoch: 8 step: 233, loss is 0.0001892790023703128\n",
      "epoch: 8 step: 234, loss is 0.0008173073292709887\n",
      "epoch: 8 step: 235, loss is 0.001770160743035376\n",
      "epoch: 8 step: 236, loss is 0.00091625249478966\n",
      "epoch: 8 step: 237, loss is 0.005036265589296818\n",
      "epoch: 8 step: 238, loss is 2.2331541913445108e-05\n",
      "epoch: 8 step: 239, loss is 0.036532770842313766\n",
      "epoch: 8 step: 240, loss is 0.0019432298140600324\n",
      "epoch: 8 step: 241, loss is 0.0010417801095172763\n",
      "epoch: 8 step: 242, loss is 0.00020911236060783267\n",
      "epoch: 8 step: 243, loss is 0.05390515550971031\n",
      "epoch: 8 step: 244, loss is 0.056134674698114395\n",
      "epoch: 8 step: 245, loss is 0.0018439824925735593\n",
      "epoch: 8 step: 246, loss is 0.000747020123526454\n",
      "epoch: 8 step: 247, loss is 0.008744368329644203\n",
      "epoch: 8 step: 248, loss is 0.033044058829545975\n",
      "epoch: 8 step: 249, loss is 0.05440215766429901\n",
      "epoch: 8 step: 250, loss is 8.664303459227085e-05\n",
      "epoch: 8 step: 251, loss is 0.0036838199011981487\n",
      "epoch: 8 step: 252, loss is 1.881416028481908e-05\n",
      "epoch: 8 step: 253, loss is 0.003130400087684393\n",
      "epoch: 8 step: 254, loss is 0.0008773346780799329\n",
      "epoch: 8 step: 255, loss is 0.0009351150947622955\n",
      "epoch: 8 step: 256, loss is 0.0016601936658844352\n",
      "epoch: 8 step: 257, loss is 0.001781604252755642\n",
      "epoch: 8 step: 258, loss is 9.640087955631316e-05\n",
      "epoch: 8 step: 259, loss is 0.0008753964211791754\n",
      "epoch: 8 step: 260, loss is 0.0033008570317178965\n",
      "epoch: 8 step: 261, loss is 0.02359308861196041\n",
      "epoch: 8 step: 262, loss is 0.00018754323536995798\n",
      "epoch: 8 step: 263, loss is 0.0035070155281573534\n",
      "epoch: 8 step: 264, loss is 0.0001570514141349122\n",
      "epoch: 8 step: 265, loss is 0.0005778539925813675\n",
      "epoch: 8 step: 266, loss is 0.02485670894384384\n",
      "epoch: 8 step: 267, loss is 0.0002132493391400203\n",
      "epoch: 8 step: 268, loss is 0.001010529464110732\n",
      "epoch: 8 step: 269, loss is 0.0002093850343953818\n",
      "epoch: 8 step: 270, loss is 0.17515622079372406\n",
      "epoch: 8 step: 271, loss is 0.007886048406362534\n",
      "epoch: 8 step: 272, loss is 0.00038536597276106477\n",
      "epoch: 8 step: 273, loss is 0.1996700018644333\n",
      "epoch: 8 step: 274, loss is 0.006692164111882448\n",
      "epoch: 8 step: 275, loss is 0.004484507720917463\n",
      "epoch: 8 step: 276, loss is 5.4023574193706736e-05\n",
      "epoch: 8 step: 277, loss is 0.0077666728757321835\n",
      "epoch: 8 step: 278, loss is 0.002909258473664522\n",
      "epoch: 8 step: 279, loss is 0.007917103357613087\n",
      "epoch: 8 step: 280, loss is 0.0005942140123806894\n",
      "epoch: 8 step: 281, loss is 0.0009726358694024384\n",
      "epoch: 8 step: 282, loss is 0.00846084300428629\n",
      "epoch: 8 step: 283, loss is 0.0023286319337785244\n",
      "epoch: 8 step: 284, loss is 0.00039380788803100586\n",
      "epoch: 8 step: 285, loss is 0.008571172133088112\n",
      "epoch: 8 step: 286, loss is 0.0020029055885970592\n",
      "epoch: 8 step: 287, loss is 0.004177124239504337\n",
      "epoch: 8 step: 288, loss is 0.0010676580714061856\n",
      "epoch: 8 step: 289, loss is 0.026090728119015694\n",
      "epoch: 8 step: 290, loss is 0.04203551262617111\n",
      "epoch: 8 step: 291, loss is 0.0028890613466501236\n",
      "epoch: 8 step: 292, loss is 0.1966654360294342\n",
      "epoch: 8 step: 293, loss is 0.013436123728752136\n",
      "epoch: 8 step: 294, loss is 5.442295878310688e-05\n",
      "epoch: 8 step: 295, loss is 0.0007356866844929755\n",
      "epoch: 8 step: 296, loss is 0.0029346977826207876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 297, loss is 0.1776977926492691\n",
      "epoch: 8 step: 298, loss is 0.00202614045701921\n",
      "epoch: 8 step: 299, loss is 0.000248430238571018\n",
      "epoch: 8 step: 300, loss is 0.03425213694572449\n",
      "epoch: 8 step: 301, loss is 0.0007531792507506907\n",
      "epoch: 8 step: 302, loss is 0.00018521654419600964\n",
      "epoch: 8 step: 303, loss is 0.03690839186310768\n",
      "epoch: 8 step: 304, loss is 0.0006769287283532321\n",
      "epoch: 8 step: 305, loss is 0.0014165951870381832\n",
      "epoch: 8 step: 306, loss is 0.001152558485046029\n",
      "epoch: 8 step: 307, loss is 7.466992974514142e-05\n",
      "epoch: 8 step: 308, loss is 0.007793759927153587\n",
      "epoch: 8 step: 309, loss is 0.005457059480249882\n",
      "epoch: 8 step: 310, loss is 0.0705924928188324\n",
      "epoch: 8 step: 311, loss is 0.005155615042895079\n",
      "epoch: 8 step: 312, loss is 0.004172582644969225\n",
      "epoch: 8 step: 313, loss is 0.0007412110571749508\n",
      "epoch: 8 step: 314, loss is 0.021980488672852516\n",
      "epoch: 8 step: 315, loss is 8.307164534926414e-05\n",
      "epoch: 8 step: 316, loss is 0.0029395506717264652\n",
      "epoch: 8 step: 317, loss is 0.004886409267783165\n",
      "epoch: 8 step: 318, loss is 0.000584432331379503\n",
      "epoch: 8 step: 319, loss is 0.004275782499462366\n",
      "epoch: 8 step: 320, loss is 0.005613368004560471\n",
      "epoch: 8 step: 321, loss is 0.0020845348481088877\n",
      "epoch: 8 step: 322, loss is 0.001283886842429638\n",
      "epoch: 8 step: 323, loss is 0.014262950047850609\n",
      "epoch: 8 step: 324, loss is 0.007294894196093082\n",
      "epoch: 8 step: 325, loss is 0.02741874009370804\n",
      "epoch: 8 step: 326, loss is 0.010719818994402885\n",
      "epoch: 8 step: 327, loss is 0.0004220271948724985\n",
      "epoch: 8 step: 328, loss is 0.014180206693708897\n",
      "epoch: 8 step: 329, loss is 0.00010924354864982888\n",
      "epoch: 8 step: 330, loss is 0.00974463764578104\n",
      "epoch: 8 step: 331, loss is 2.2668515157420188e-05\n",
      "epoch: 8 step: 332, loss is 0.0006774328066967428\n",
      "epoch: 8 step: 333, loss is 0.009950762614607811\n",
      "epoch: 8 step: 334, loss is 0.004666697233915329\n",
      "epoch: 8 step: 335, loss is 0.00014676291903015226\n",
      "epoch: 8 step: 336, loss is 0.004122117999941111\n",
      "epoch: 8 step: 337, loss is 0.00015627137327101082\n",
      "epoch: 8 step: 338, loss is 0.009636563248932362\n",
      "epoch: 8 step: 339, loss is 0.00016537097690161318\n",
      "epoch: 8 step: 340, loss is 0.011259988881647587\n",
      "epoch: 8 step: 341, loss is 0.00011002401151927188\n",
      "epoch: 8 step: 342, loss is 0.00018403120338916779\n",
      "epoch: 8 step: 343, loss is 0.0009617366595193744\n",
      "epoch: 8 step: 344, loss is 0.00042761798249557614\n",
      "epoch: 8 step: 345, loss is 0.0015153905842453241\n",
      "epoch: 8 step: 346, loss is 0.021768050268292427\n",
      "epoch: 8 step: 347, loss is 0.054019104689359665\n",
      "epoch: 8 step: 348, loss is 0.048835162073373795\n",
      "epoch: 8 step: 349, loss is 0.0002189662045566365\n",
      "epoch: 8 step: 350, loss is 0.005635445937514305\n",
      "epoch: 8 step: 351, loss is 0.00014047826698515564\n",
      "epoch: 8 step: 352, loss is 0.028629977256059647\n",
      "epoch: 8 step: 353, loss is 0.006034430582076311\n",
      "epoch: 8 step: 354, loss is 0.0014199311845004559\n",
      "epoch: 8 step: 355, loss is 0.009218426421284676\n",
      "epoch: 8 step: 356, loss is 0.11914771795272827\n",
      "epoch: 8 step: 357, loss is 0.020080186426639557\n",
      "epoch: 8 step: 358, loss is 0.0007842281484045088\n",
      "epoch: 8 step: 359, loss is 0.00027062735171057284\n",
      "epoch: 8 step: 360, loss is 3.656790795503184e-05\n",
      "epoch: 8 step: 361, loss is 0.014560101553797722\n",
      "epoch: 8 step: 362, loss is 0.05645354837179184\n",
      "epoch: 8 step: 363, loss is 0.0064736343920230865\n",
      "epoch: 8 step: 364, loss is 0.0006341430707834661\n",
      "epoch: 8 step: 365, loss is 0.003396055195480585\n",
      "epoch: 8 step: 366, loss is 0.0005824511172249913\n",
      "epoch: 8 step: 367, loss is 0.0008228264050558209\n",
      "epoch: 8 step: 368, loss is 0.00011187847121618688\n",
      "epoch: 8 step: 369, loss is 0.01775176450610161\n",
      "epoch: 8 step: 370, loss is 0.0001525079715065658\n",
      "epoch: 8 step: 371, loss is 0.09994550049304962\n",
      "epoch: 8 step: 372, loss is 0.04235732555389404\n",
      "epoch: 8 step: 373, loss is 0.00018130827811546624\n",
      "epoch: 8 step: 374, loss is 0.0006400624988600612\n",
      "epoch: 8 step: 375, loss is 0.00012182011414552107\n",
      "epoch: 8 step: 376, loss is 0.0005473524215631187\n",
      "epoch: 8 step: 377, loss is 0.026333676651120186\n",
      "epoch: 8 step: 378, loss is 0.000861817505210638\n",
      "epoch: 8 step: 379, loss is 0.001776359393261373\n",
      "epoch: 8 step: 380, loss is 0.00014431994350161403\n",
      "epoch: 8 step: 381, loss is 0.00109274429269135\n",
      "epoch: 8 step: 382, loss is 0.00011332948633935302\n",
      "epoch: 8 step: 383, loss is 0.002061988925561309\n",
      "epoch: 8 step: 384, loss is 0.05304386094212532\n",
      "epoch: 8 step: 385, loss is 0.007225171197205782\n",
      "epoch: 8 step: 386, loss is 0.011654047295451164\n",
      "epoch: 8 step: 387, loss is 0.08098449558019638\n",
      "epoch: 8 step: 388, loss is 0.0012077894061803818\n",
      "epoch: 8 step: 389, loss is 0.06930562108755112\n",
      "epoch: 8 step: 390, loss is 0.17873278260231018\n",
      "epoch: 8 step: 391, loss is 0.001394977211020887\n",
      "epoch: 8 step: 392, loss is 0.06246102228760719\n",
      "epoch: 8 step: 393, loss is 0.007555982563644648\n",
      "epoch: 8 step: 394, loss is 0.0022182592656463385\n",
      "epoch: 8 step: 395, loss is 0.00014867739810142666\n",
      "epoch: 8 step: 396, loss is 0.0011055921204388142\n",
      "epoch: 8 step: 397, loss is 4.335211997386068e-05\n",
      "epoch: 8 step: 398, loss is 0.007591867353767157\n",
      "epoch: 8 step: 399, loss is 0.0004336701531428844\n",
      "epoch: 8 step: 400, loss is 0.0025243668351322412\n",
      "epoch: 8 step: 401, loss is 0.02302834764122963\n",
      "epoch: 8 step: 402, loss is 0.000271896889898926\n",
      "epoch: 8 step: 403, loss is 0.00010628292511682957\n",
      "epoch: 8 step: 404, loss is 0.00106454745400697\n",
      "epoch: 8 step: 405, loss is 0.004974146373569965\n",
      "epoch: 8 step: 406, loss is 0.004334657918661833\n",
      "epoch: 8 step: 407, loss is 0.00436774268746376\n",
      "epoch: 8 step: 408, loss is 0.0004037109902128577\n",
      "epoch: 8 step: 409, loss is 0.003289266023784876\n",
      "epoch: 8 step: 410, loss is 5.351540676201694e-05\n",
      "epoch: 8 step: 411, loss is 0.0006781675620004535\n",
      "epoch: 8 step: 412, loss is 0.0002586956834420562\n",
      "epoch: 8 step: 413, loss is 0.000236551757552661\n",
      "epoch: 8 step: 414, loss is 0.1457502394914627\n",
      "epoch: 8 step: 415, loss is 0.0048093353398144245\n",
      "epoch: 8 step: 416, loss is 0.12052535265684128\n",
      "epoch: 8 step: 417, loss is 0.0004963367828167975\n",
      "epoch: 8 step: 418, loss is 0.00046676286729052663\n",
      "epoch: 8 step: 419, loss is 0.002216879976913333\n",
      "epoch: 8 step: 420, loss is 0.0013887013774365187\n",
      "epoch: 8 step: 421, loss is 0.01475706696510315\n",
      "epoch: 8 step: 422, loss is 0.000138333227369003\n",
      "epoch: 8 step: 423, loss is 0.002383752493187785\n",
      "epoch: 8 step: 424, loss is 0.03119945153594017\n",
      "epoch: 8 step: 425, loss is 0.022114766761660576\n",
      "epoch: 8 step: 426, loss is 0.0015607505338266492\n",
      "epoch: 8 step: 427, loss is 0.10094231367111206\n",
      "epoch: 8 step: 428, loss is 0.004426922649145126\n",
      "epoch: 8 step: 429, loss is 0.006570357363671064\n",
      "epoch: 8 step: 430, loss is 0.00012362268171273172\n",
      "epoch: 8 step: 431, loss is 0.0008613778627477586\n",
      "epoch: 8 step: 432, loss is 0.0037016612477600574\n",
      "epoch: 8 step: 433, loss is 0.0023906591814011335\n",
      "epoch: 8 step: 434, loss is 0.005513770971447229\n",
      "epoch: 8 step: 435, loss is 0.010265943594276905\n",
      "epoch: 8 step: 436, loss is 0.0029449413996189833\n",
      "epoch: 8 step: 437, loss is 0.012229176238179207\n",
      "epoch: 8 step: 438, loss is 0.018454520031809807\n",
      "epoch: 8 step: 439, loss is 0.02387562394142151\n",
      "epoch: 8 step: 440, loss is 0.0019240380497649312\n",
      "epoch: 8 step: 441, loss is 0.00019193712796550244\n",
      "epoch: 8 step: 442, loss is 0.0002745614910963923\n",
      "epoch: 8 step: 443, loss is 0.004108124412596226\n",
      "epoch: 8 step: 444, loss is 0.011233402416110039\n",
      "epoch: 8 step: 445, loss is 0.0051711848936975\n",
      "epoch: 8 step: 446, loss is 0.0003957283915951848\n",
      "epoch: 8 step: 447, loss is 0.026979470625519753\n",
      "epoch: 8 step: 448, loss is 0.021698005497455597\n",
      "epoch: 8 step: 449, loss is 0.0011558948317542672\n",
      "epoch: 8 step: 450, loss is 0.00543795432895422\n",
      "epoch: 8 step: 451, loss is 0.0005922613199800253\n",
      "epoch: 8 step: 452, loss is 0.025738365948200226\n",
      "epoch: 8 step: 453, loss is 0.07178520411252975\n",
      "epoch: 8 step: 454, loss is 0.00037552506546489894\n",
      "epoch: 8 step: 455, loss is 0.024446500465273857\n",
      "epoch: 8 step: 456, loss is 0.001357114757411182\n",
      "epoch: 8 step: 457, loss is 0.01184131670743227\n",
      "epoch: 8 step: 458, loss is 0.0008500841213390231\n",
      "epoch: 8 step: 459, loss is 0.0004213761130813509\n",
      "epoch: 8 step: 460, loss is 0.021062195301055908\n",
      "epoch: 8 step: 461, loss is 0.00021456459944602102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 462, loss is 0.0008549446356482804\n",
      "epoch: 8 step: 463, loss is 0.0013099537463858724\n",
      "epoch: 8 step: 464, loss is 0.02081041783094406\n",
      "epoch: 8 step: 465, loss is 0.0016199367819353938\n",
      "epoch: 8 step: 466, loss is 0.016662633046507835\n",
      "epoch: 8 step: 467, loss is 0.0003759647661354393\n",
      "epoch: 8 step: 468, loss is 0.05172088369727135\n",
      "epoch: 8 step: 469, loss is 0.09822991490364075\n",
      "epoch: 8 step: 470, loss is 0.003920327872037888\n",
      "epoch: 8 step: 471, loss is 0.015999680384993553\n",
      "epoch: 8 step: 472, loss is 0.019730372354388237\n",
      "epoch: 8 step: 473, loss is 0.08382598310709\n",
      "epoch: 8 step: 474, loss is 0.01760733127593994\n",
      "epoch: 8 step: 475, loss is 0.0005377148045226932\n",
      "epoch: 8 step: 476, loss is 0.00020042066171299666\n",
      "epoch: 8 step: 477, loss is 0.000861596199683845\n",
      "epoch: 8 step: 478, loss is 0.07572754472494125\n",
      "epoch: 8 step: 479, loss is 0.008235949091613293\n",
      "epoch: 8 step: 480, loss is 0.00021089868096169084\n",
      "epoch: 8 step: 481, loss is 0.001258521107956767\n",
      "epoch: 8 step: 482, loss is 0.00231919321231544\n",
      "epoch: 8 step: 483, loss is 0.03923480212688446\n",
      "epoch: 8 step: 484, loss is 0.04080954194068909\n",
      "epoch: 8 step: 485, loss is 0.016728729009628296\n",
      "epoch: 8 step: 486, loss is 0.0028851833194494247\n",
      "epoch: 8 step: 487, loss is 0.0002606181369628757\n",
      "epoch: 8 step: 488, loss is 0.002451674547046423\n",
      "epoch: 8 step: 489, loss is 0.0010173751506954432\n",
      "epoch: 8 step: 490, loss is 0.007618579547852278\n",
      "epoch: 8 step: 491, loss is 0.01602659560739994\n",
      "epoch: 8 step: 492, loss is 0.0003576221934054047\n",
      "epoch: 8 step: 493, loss is 0.003959536552429199\n",
      "epoch: 8 step: 494, loss is 0.0006238084170036018\n",
      "epoch: 8 step: 495, loss is 0.07742065191268921\n",
      "epoch: 8 step: 496, loss is 0.0029161786660552025\n",
      "epoch: 8 step: 497, loss is 0.0013458639150485396\n",
      "epoch: 8 step: 498, loss is 0.00030638399766758084\n",
      "epoch: 8 step: 499, loss is 0.0006220678915269673\n",
      "epoch: 8 step: 500, loss is 0.0006040871958248317\n",
      "epoch: 8 step: 501, loss is 0.006471623200923204\n",
      "epoch: 8 step: 502, loss is 0.0041668307967484\n",
      "epoch: 8 step: 503, loss is 0.0007720993598923087\n",
      "epoch: 8 step: 504, loss is 8.829854050418362e-05\n",
      "epoch: 8 step: 505, loss is 0.1899365931749344\n",
      "epoch: 8 step: 506, loss is 0.0002881847321987152\n",
      "epoch: 8 step: 507, loss is 0.02671511285007\n",
      "epoch: 8 step: 508, loss is 0.00654095783829689\n",
      "epoch: 8 step: 509, loss is 0.0005429068114608526\n",
      "epoch: 8 step: 510, loss is 0.0014826423721387982\n",
      "epoch: 8 step: 511, loss is 0.0002658362500369549\n",
      "epoch: 8 step: 512, loss is 0.05025378614664078\n",
      "epoch: 8 step: 513, loss is 0.0009242026135325432\n",
      "epoch: 8 step: 514, loss is 0.0013014820870012045\n",
      "epoch: 8 step: 515, loss is 0.06787896156311035\n",
      "epoch: 8 step: 516, loss is 0.0014883612748235464\n",
      "epoch: 8 step: 517, loss is 0.005089729558676481\n",
      "epoch: 8 step: 518, loss is 0.008302200585603714\n",
      "epoch: 8 step: 519, loss is 0.0016997678903862834\n",
      "epoch: 8 step: 520, loss is 0.014297273941338062\n",
      "epoch: 8 step: 521, loss is 0.0008220141171477735\n",
      "epoch: 8 step: 522, loss is 0.20657332241535187\n",
      "epoch: 8 step: 523, loss is 0.012106797657907009\n",
      "epoch: 8 step: 524, loss is 0.003061590250581503\n",
      "epoch: 8 step: 525, loss is 0.001788055757060647\n",
      "epoch: 8 step: 526, loss is 0.013322670944035053\n",
      "epoch: 8 step: 527, loss is 0.011284861713647842\n",
      "epoch: 8 step: 528, loss is 7.405217183986679e-05\n",
      "epoch: 8 step: 529, loss is 0.014764929190278053\n",
      "epoch: 8 step: 530, loss is 0.005355740897357464\n",
      "epoch: 8 step: 531, loss is 1.52989559865091e-05\n",
      "epoch: 8 step: 532, loss is 0.00015103566693142056\n",
      "epoch: 8 step: 533, loss is 0.003618161892518401\n",
      "epoch: 8 step: 534, loss is 0.003033129731193185\n",
      "epoch: 8 step: 535, loss is 0.002130813430994749\n",
      "epoch: 8 step: 536, loss is 0.02983625791966915\n",
      "epoch: 8 step: 537, loss is 0.016347119584679604\n",
      "epoch: 8 step: 538, loss is 0.005326906684786081\n",
      "epoch: 8 step: 539, loss is 1.4621264199377038e-05\n",
      "epoch: 8 step: 540, loss is 0.0008396405610255897\n",
      "epoch: 8 step: 541, loss is 0.00014451485185418278\n",
      "epoch: 8 step: 542, loss is 0.006539843510836363\n",
      "epoch: 8 step: 543, loss is 0.0004625563742592931\n",
      "epoch: 8 step: 544, loss is 0.00039519192068837583\n",
      "epoch: 8 step: 545, loss is 4.4005184463458136e-05\n",
      "epoch: 8 step: 546, loss is 2.280227818118874e-05\n",
      "epoch: 8 step: 547, loss is 0.00040337350219488144\n",
      "epoch: 8 step: 548, loss is 0.11413905024528503\n",
      "epoch: 8 step: 549, loss is 0.00026185603928752244\n",
      "epoch: 8 step: 550, loss is 0.006256334017962217\n",
      "epoch: 8 step: 551, loss is 0.02169809304177761\n",
      "epoch: 8 step: 552, loss is 0.028043903410434723\n",
      "epoch: 8 step: 553, loss is 0.11236607283353806\n",
      "epoch: 8 step: 554, loss is 0.0089109493419528\n",
      "epoch: 8 step: 555, loss is 0.0001339304872089997\n",
      "epoch: 8 step: 556, loss is 0.0018903091549873352\n",
      "epoch: 8 step: 557, loss is 0.0016433895798400044\n",
      "epoch: 8 step: 558, loss is 0.006624316330999136\n",
      "epoch: 8 step: 559, loss is 0.10261223465204239\n",
      "epoch: 8 step: 560, loss is 0.00025423895567655563\n",
      "epoch: 8 step: 561, loss is 0.053519584238529205\n",
      "epoch: 8 step: 562, loss is 4.276543404557742e-05\n",
      "epoch: 8 step: 563, loss is 0.001516234129667282\n",
      "epoch: 8 step: 564, loss is 0.07526286691427231\n",
      "epoch: 8 step: 565, loss is 0.0007743352325633168\n",
      "epoch: 8 step: 566, loss is 0.006072801537811756\n",
      "epoch: 8 step: 567, loss is 0.00358721730299294\n",
      "epoch: 8 step: 568, loss is 0.0011794177116826177\n",
      "epoch: 8 step: 569, loss is 0.007157520391047001\n",
      "epoch: 8 step: 570, loss is 0.032601434737443924\n",
      "epoch: 8 step: 571, loss is 0.00025834940606728196\n",
      "epoch: 8 step: 572, loss is 1.00996476248838e-05\n",
      "epoch: 8 step: 573, loss is 0.0011582400184124708\n",
      "epoch: 8 step: 574, loss is 0.12088647484779358\n",
      "epoch: 8 step: 575, loss is 0.10871861129999161\n",
      "epoch: 8 step: 576, loss is 0.0019103987142443657\n",
      "epoch: 8 step: 577, loss is 0.003049941500648856\n",
      "epoch: 8 step: 578, loss is 0.0020076248329132795\n",
      "epoch: 8 step: 579, loss is 0.0001119694352382794\n",
      "epoch: 8 step: 580, loss is 0.02198728546500206\n",
      "epoch: 8 step: 581, loss is 0.0001228061009896919\n",
      "epoch: 8 step: 582, loss is 0.003530725371092558\n",
      "epoch: 8 step: 583, loss is 0.0019224099814891815\n",
      "epoch: 8 step: 584, loss is 0.008401774801313877\n",
      "epoch: 8 step: 585, loss is 0.000754159817006439\n",
      "epoch: 8 step: 586, loss is 0.00040571222780272365\n",
      "epoch: 8 step: 587, loss is 0.00024371118342969567\n",
      "epoch: 8 step: 588, loss is 0.0038334326818585396\n",
      "epoch: 8 step: 589, loss is 0.07926863431930542\n",
      "epoch: 8 step: 590, loss is 0.06579607725143433\n",
      "epoch: 8 step: 591, loss is 0.000713863642886281\n",
      "epoch: 8 step: 592, loss is 0.009382461197674274\n",
      "epoch: 8 step: 593, loss is 0.00024608770036138594\n",
      "epoch: 8 step: 594, loss is 0.013909861445426941\n",
      "epoch: 8 step: 595, loss is 0.001388757606036961\n",
      "epoch: 8 step: 596, loss is 0.003382219234481454\n",
      "epoch: 8 step: 597, loss is 0.002373575000092387\n",
      "epoch: 8 step: 598, loss is 9.299411613028497e-05\n",
      "epoch: 8 step: 599, loss is 0.048116739839315414\n",
      "epoch: 8 step: 600, loss is 0.005759004503488541\n",
      "epoch: 8 step: 601, loss is 0.002972692484036088\n",
      "epoch: 8 step: 602, loss is 0.004192128777503967\n",
      "epoch: 8 step: 603, loss is 0.038581036031246185\n",
      "epoch: 8 step: 604, loss is 0.00023304394562728703\n",
      "epoch: 8 step: 605, loss is 0.007402496412396431\n",
      "epoch: 8 step: 606, loss is 0.003619323717430234\n",
      "epoch: 8 step: 607, loss is 0.0014422681415453553\n",
      "epoch: 8 step: 608, loss is 0.0005608286592178047\n",
      "epoch: 8 step: 609, loss is 4.8354449972976e-05\n",
      "epoch: 8 step: 610, loss is 0.013745906762778759\n",
      "epoch: 8 step: 611, loss is 0.0014180514262989163\n",
      "epoch: 8 step: 612, loss is 0.012740888632833958\n",
      "epoch: 8 step: 613, loss is 0.00011525843729032204\n",
      "epoch: 8 step: 614, loss is 0.00013438078167382628\n",
      "epoch: 8 step: 615, loss is 0.018351539969444275\n",
      "epoch: 8 step: 616, loss is 0.00033907132456079125\n",
      "epoch: 8 step: 617, loss is 0.0015985333593562245\n",
      "epoch: 8 step: 618, loss is 0.015888052061200142\n",
      "epoch: 8 step: 619, loss is 0.008985044434666634\n",
      "epoch: 8 step: 620, loss is 0.002210363047197461\n",
      "epoch: 8 step: 621, loss is 0.0009392054635100067\n",
      "epoch: 8 step: 622, loss is 0.006643228232860565\n",
      "epoch: 8 step: 623, loss is 0.012171591632068157\n",
      "epoch: 8 step: 624, loss is 0.00014115977683104575\n",
      "epoch: 8 step: 625, loss is 0.00031699391547590494\n",
      "epoch: 8 step: 626, loss is 0.0005837264470756054\n",
      "epoch: 8 step: 627, loss is 0.0004975876654498279\n",
      "epoch: 8 step: 628, loss is 0.009602305479347706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 629, loss is 0.005821685306727886\n",
      "epoch: 8 step: 630, loss is 0.4564111828804016\n",
      "epoch: 8 step: 631, loss is 0.09959473460912704\n",
      "epoch: 8 step: 632, loss is 0.08068283647298813\n",
      "epoch: 8 step: 633, loss is 8.010432793525979e-05\n",
      "epoch: 8 step: 634, loss is 0.000991778913885355\n",
      "epoch: 8 step: 635, loss is 0.0001485385000705719\n",
      "epoch: 8 step: 636, loss is 0.00729606207460165\n",
      "epoch: 8 step: 637, loss is 0.164812833070755\n",
      "epoch: 8 step: 638, loss is 0.0007025640225037932\n",
      "epoch: 8 step: 639, loss is 0.008394112810492516\n",
      "epoch: 8 step: 640, loss is 0.04576314985752106\n",
      "epoch: 8 step: 641, loss is 0.07097765803337097\n",
      "epoch: 8 step: 642, loss is 0.0015149404061958194\n",
      "epoch: 8 step: 643, loss is 0.0003813621588051319\n",
      "epoch: 8 step: 644, loss is 0.18097393214702606\n",
      "epoch: 8 step: 645, loss is 0.0012490935623645782\n",
      "epoch: 8 step: 646, loss is 0.0036015280056744814\n",
      "epoch: 8 step: 647, loss is 0.0005566859035752714\n",
      "epoch: 8 step: 648, loss is 0.00063662591855973\n",
      "epoch: 8 step: 649, loss is 0.002615132601931691\n",
      "epoch: 8 step: 650, loss is 0.0013466377276927233\n",
      "epoch: 8 step: 651, loss is 0.0014135231031104922\n",
      "epoch: 8 step: 652, loss is 0.007181697990745306\n",
      "epoch: 8 step: 653, loss is 0.001407111412845552\n",
      "epoch: 8 step: 654, loss is 0.00012246171536389738\n",
      "epoch: 8 step: 655, loss is 0.0004621146072167903\n",
      "epoch: 8 step: 656, loss is 0.004398726858198643\n",
      "epoch: 8 step: 657, loss is 0.07362622022628784\n",
      "epoch: 8 step: 658, loss is 0.004784954246133566\n",
      "epoch: 8 step: 659, loss is 0.0014446091372519732\n",
      "epoch: 8 step: 660, loss is 0.0024568606168031693\n",
      "epoch: 8 step: 661, loss is 0.0005657184519805014\n",
      "epoch: 8 step: 662, loss is 0.0019167523132637143\n",
      "epoch: 8 step: 663, loss is 0.020656438544392586\n",
      "epoch: 8 step: 664, loss is 0.0017753165448084474\n",
      "epoch: 8 step: 665, loss is 4.477916081668809e-05\n",
      "epoch: 8 step: 666, loss is 0.0015709184808656573\n",
      "epoch: 8 step: 667, loss is 0.0004021669155918062\n",
      "epoch: 8 step: 668, loss is 0.008453568443655968\n",
      "epoch: 8 step: 669, loss is 0.00017613636737223715\n",
      "epoch: 8 step: 670, loss is 0.0030253054574131966\n",
      "epoch: 8 step: 671, loss is 0.00030652887653559446\n",
      "epoch: 8 step: 672, loss is 0.0010202472331002355\n",
      "epoch: 8 step: 673, loss is 0.0006856134859845042\n",
      "epoch: 8 step: 674, loss is 0.013734143227338791\n",
      "epoch: 8 step: 675, loss is 0.0470205619931221\n",
      "epoch: 8 step: 676, loss is 0.007075476925820112\n",
      "epoch: 8 step: 677, loss is 0.00040447295759804547\n",
      "epoch: 8 step: 678, loss is 0.004149231594055891\n",
      "epoch: 8 step: 679, loss is 0.017367610707879066\n",
      "epoch: 8 step: 680, loss is 0.0012014912208542228\n",
      "epoch: 8 step: 681, loss is 0.002445421414449811\n",
      "epoch: 8 step: 682, loss is 0.06174793094396591\n",
      "epoch: 8 step: 683, loss is 0.0039029328618198633\n",
      "epoch: 8 step: 684, loss is 0.07245153188705444\n",
      "epoch: 8 step: 685, loss is 0.0729430690407753\n",
      "epoch: 8 step: 686, loss is 0.0013970284489914775\n",
      "epoch: 8 step: 687, loss is 0.10163643956184387\n",
      "epoch: 8 step: 688, loss is 0.006620783358812332\n",
      "epoch: 8 step: 689, loss is 0.005451939068734646\n",
      "epoch: 8 step: 690, loss is 0.0017048424342647195\n",
      "epoch: 8 step: 691, loss is 0.0026437488850206137\n",
      "epoch: 8 step: 692, loss is 0.004810310434550047\n",
      "epoch: 8 step: 693, loss is 0.24706250429153442\n",
      "epoch: 8 step: 694, loss is 0.007562922779470682\n",
      "epoch: 8 step: 695, loss is 0.010180984623730183\n",
      "epoch: 8 step: 696, loss is 0.0034440772142261267\n",
      "epoch: 8 step: 697, loss is 0.01951364241540432\n",
      "epoch: 8 step: 698, loss is 0.003278951859101653\n",
      "epoch: 8 step: 699, loss is 0.10337727516889572\n",
      "epoch: 8 step: 700, loss is 0.00036865996662527323\n",
      "epoch: 8 step: 701, loss is 0.01639370247721672\n",
      "epoch: 8 step: 702, loss is 3.5296274290885776e-05\n",
      "epoch: 8 step: 703, loss is 4.738429925055243e-05\n",
      "epoch: 8 step: 704, loss is 0.00013574169133789837\n",
      "epoch: 8 step: 705, loss is 0.000489294994622469\n",
      "epoch: 8 step: 706, loss is 0.06443975865840912\n",
      "epoch: 8 step: 707, loss is 0.043264638632535934\n",
      "epoch: 8 step: 708, loss is 0.0008186448831111193\n",
      "epoch: 8 step: 709, loss is 0.020353958010673523\n",
      "epoch: 8 step: 710, loss is 0.0023334987927228212\n",
      "epoch: 8 step: 711, loss is 0.0020126982126384974\n",
      "epoch: 8 step: 712, loss is 0.0010317631531506777\n",
      "epoch: 8 step: 713, loss is 0.0020065754652023315\n",
      "epoch: 8 step: 714, loss is 0.0010268499609082937\n",
      "epoch: 8 step: 715, loss is 0.12006781995296478\n",
      "epoch: 8 step: 716, loss is 0.13023410737514496\n",
      "epoch: 8 step: 717, loss is 0.0002694187860470265\n",
      "epoch: 8 step: 718, loss is 0.032433681190013885\n",
      "epoch: 8 step: 719, loss is 0.009120850823819637\n",
      "epoch: 8 step: 720, loss is 0.004679668694734573\n",
      "epoch: 8 step: 721, loss is 0.013778452761471272\n",
      "epoch: 8 step: 722, loss is 0.004241021350026131\n",
      "epoch: 8 step: 723, loss is 0.003064015181735158\n",
      "epoch: 8 step: 724, loss is 0.0009344546706415713\n",
      "epoch: 8 step: 725, loss is 0.0025339440908282995\n",
      "epoch: 8 step: 726, loss is 0.008873816579580307\n",
      "epoch: 8 step: 727, loss is 0.0023916156496852636\n",
      "epoch: 8 step: 728, loss is 0.00025016299332492054\n",
      "epoch: 8 step: 729, loss is 0.0067579541355371475\n",
      "epoch: 8 step: 730, loss is 0.0011991732753813267\n",
      "epoch: 8 step: 731, loss is 0.051232486963272095\n",
      "epoch: 8 step: 732, loss is 0.01107868179678917\n",
      "epoch: 8 step: 733, loss is 0.0007565475534647703\n",
      "epoch: 8 step: 734, loss is 0.0012451125076040626\n",
      "epoch: 8 step: 735, loss is 0.00016765781037975103\n",
      "epoch: 8 step: 736, loss is 0.006064392626285553\n",
      "epoch: 8 step: 737, loss is 0.017734697088599205\n",
      "epoch: 8 step: 738, loss is 0.0002355211618123576\n",
      "epoch: 8 step: 739, loss is 6.871112418593839e-05\n",
      "epoch: 8 step: 740, loss is 0.0005837701610289514\n",
      "epoch: 8 step: 741, loss is 0.002424676436930895\n",
      "epoch: 8 step: 742, loss is 0.0011498016538098454\n",
      "epoch: 8 step: 743, loss is 0.013985520228743553\n",
      "epoch: 8 step: 744, loss is 0.003859646152704954\n",
      "epoch: 8 step: 745, loss is 0.007662678137421608\n",
      "epoch: 8 step: 746, loss is 0.0004073604941368103\n",
      "epoch: 8 step: 747, loss is 0.20295922458171844\n",
      "epoch: 8 step: 748, loss is 0.00045214523561298847\n",
      "epoch: 8 step: 749, loss is 0.006700641009956598\n",
      "epoch: 8 step: 750, loss is 0.0008184823673218489\n",
      "epoch: 8 step: 751, loss is 0.002423434518277645\n",
      "epoch: 8 step: 752, loss is 0.0022167947608977556\n",
      "epoch: 8 step: 753, loss is 0.0002981793077196926\n",
      "epoch: 8 step: 754, loss is 0.036832768470048904\n",
      "epoch: 8 step: 755, loss is 0.0006378724938258529\n",
      "epoch: 8 step: 756, loss is 0.021623043343424797\n",
      "epoch: 8 step: 757, loss is 0.0009643599623814225\n",
      "epoch: 8 step: 758, loss is 0.0010953085729852319\n",
      "epoch: 8 step: 759, loss is 0.00017547501192893833\n",
      "epoch: 8 step: 760, loss is 0.13964784145355225\n",
      "epoch: 8 step: 761, loss is 0.16197513043880463\n",
      "epoch: 8 step: 762, loss is 0.0036345329135656357\n",
      "epoch: 8 step: 763, loss is 0.023046543821692467\n",
      "epoch: 8 step: 764, loss is 0.00037697621155530214\n",
      "epoch: 8 step: 765, loss is 0.00026722377515397966\n",
      "epoch: 8 step: 766, loss is 0.00010291790385963395\n",
      "epoch: 8 step: 767, loss is 0.00018953063408844173\n",
      "epoch: 8 step: 768, loss is 0.09200523793697357\n",
      "epoch: 8 step: 769, loss is 0.007800595369189978\n",
      "epoch: 8 step: 770, loss is 0.017859037965536118\n",
      "epoch: 8 step: 771, loss is 0.00019912919378839433\n",
      "epoch: 8 step: 772, loss is 0.0002723034704104066\n",
      "epoch: 8 step: 773, loss is 0.02930818498134613\n",
      "epoch: 8 step: 774, loss is 0.10126104950904846\n",
      "epoch: 8 step: 775, loss is 0.001957107335329056\n",
      "epoch: 8 step: 776, loss is 0.0006689620786346495\n",
      "epoch: 8 step: 777, loss is 0.00014834164176136255\n",
      "epoch: 8 step: 778, loss is 0.007250106427818537\n",
      "epoch: 8 step: 779, loss is 0.0033248269464820623\n",
      "epoch: 8 step: 780, loss is 0.0006806067540310323\n",
      "epoch: 8 step: 781, loss is 0.05420735478401184\n",
      "epoch: 8 step: 782, loss is 0.003176787169650197\n",
      "epoch: 8 step: 783, loss is 0.0028976427856832743\n",
      "epoch: 8 step: 784, loss is 0.0009271607268601656\n",
      "epoch: 8 step: 785, loss is 0.0021712882444262505\n",
      "epoch: 8 step: 786, loss is 0.019767481833696365\n",
      "epoch: 8 step: 787, loss is 0.06822752952575684\n",
      "epoch: 8 step: 788, loss is 0.0030072866939008236\n",
      "epoch: 8 step: 789, loss is 0.0031021342147141695\n",
      "epoch: 8 step: 790, loss is 0.0738961473107338\n",
      "epoch: 8 step: 791, loss is 0.0020387161057442427\n",
      "epoch: 8 step: 792, loss is 0.005382804200053215\n",
      "epoch: 8 step: 793, loss is 0.0008533893851563334\n",
      "epoch: 8 step: 794, loss is 0.000928834022488445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 795, loss is 0.00972649548202753\n",
      "epoch: 8 step: 796, loss is 0.10500795394182205\n",
      "epoch: 8 step: 797, loss is 0.0006597987958230078\n",
      "epoch: 8 step: 798, loss is 0.00368444318883121\n",
      "epoch: 8 step: 799, loss is 0.002049459842965007\n",
      "epoch: 8 step: 800, loss is 0.0013892221031710505\n",
      "epoch: 8 step: 801, loss is 0.05075572058558464\n",
      "epoch: 8 step: 802, loss is 0.220118448138237\n",
      "epoch: 8 step: 803, loss is 0.010223782621324062\n",
      "epoch: 8 step: 804, loss is 0.04907163232564926\n",
      "epoch: 8 step: 805, loss is 0.0013426002115011215\n",
      "epoch: 8 step: 806, loss is 0.004166711121797562\n",
      "epoch: 8 step: 807, loss is 0.00011968800390604883\n",
      "epoch: 8 step: 808, loss is 0.002366189146414399\n",
      "epoch: 8 step: 809, loss is 0.15636944770812988\n",
      "epoch: 8 step: 810, loss is 0.0006453956593759358\n",
      "epoch: 8 step: 811, loss is 0.0027425007428973913\n",
      "epoch: 8 step: 812, loss is 0.0008472363697364926\n",
      "epoch: 8 step: 813, loss is 0.001435265876352787\n",
      "epoch: 8 step: 814, loss is 0.08610101044178009\n",
      "epoch: 8 step: 815, loss is 0.002783874049782753\n",
      "epoch: 8 step: 816, loss is 0.1532486081123352\n",
      "epoch: 8 step: 817, loss is 0.004087800160050392\n",
      "epoch: 8 step: 818, loss is 0.021089980378746986\n",
      "epoch: 8 step: 819, loss is 0.0008260091999545693\n",
      "epoch: 8 step: 820, loss is 0.00398403313010931\n",
      "epoch: 8 step: 821, loss is 0.03461948782205582\n",
      "epoch: 8 step: 822, loss is 0.0053287227638065815\n",
      "epoch: 8 step: 823, loss is 0.04161610081791878\n",
      "epoch: 8 step: 824, loss is 0.0007142910035327077\n",
      "epoch: 8 step: 825, loss is 0.0063516064547002316\n",
      "epoch: 8 step: 826, loss is 0.0005453263875097036\n",
      "epoch: 8 step: 827, loss is 0.05345907807350159\n",
      "epoch: 8 step: 828, loss is 0.0022951418068259954\n",
      "epoch: 8 step: 829, loss is 0.013269718736410141\n",
      "epoch: 8 step: 830, loss is 5.3064522944623604e-05\n",
      "epoch: 8 step: 831, loss is 0.11848977953195572\n",
      "epoch: 8 step: 832, loss is 0.00680740550160408\n",
      "epoch: 8 step: 833, loss is 9.500480518909171e-05\n",
      "epoch: 8 step: 834, loss is 0.06180968135595322\n",
      "epoch: 8 step: 835, loss is 0.0010937638580799103\n",
      "epoch: 8 step: 836, loss is 5.956555833108723e-05\n",
      "epoch: 8 step: 837, loss is 0.0003796584205701947\n",
      "epoch: 8 step: 838, loss is 0.0034728513564914465\n",
      "epoch: 8 step: 839, loss is 4.519591311691329e-05\n",
      "epoch: 8 step: 840, loss is 0.026979908347129822\n",
      "epoch: 8 step: 841, loss is 0.0007098006899468601\n",
      "epoch: 8 step: 842, loss is 0.007296751253306866\n",
      "epoch: 8 step: 843, loss is 0.029740214347839355\n",
      "epoch: 8 step: 844, loss is 6.437988486140966e-05\n",
      "epoch: 8 step: 845, loss is 0.0017092011403292418\n",
      "epoch: 8 step: 846, loss is 0.00027976249111816287\n",
      "epoch: 8 step: 847, loss is 0.0006020046421326697\n",
      "epoch: 8 step: 848, loss is 0.0011627341154962778\n",
      "epoch: 8 step: 849, loss is 0.0015595729928463697\n",
      "epoch: 8 step: 850, loss is 0.0008443458937108517\n",
      "epoch: 8 step: 851, loss is 0.0014440730446949601\n",
      "epoch: 8 step: 852, loss is 0.00035511786700226367\n",
      "epoch: 8 step: 853, loss is 0.03285886347293854\n",
      "epoch: 8 step: 854, loss is 0.0008071472402662039\n",
      "epoch: 8 step: 855, loss is 0.0201540719717741\n",
      "epoch: 8 step: 856, loss is 0.00012650810822378844\n",
      "epoch: 8 step: 857, loss is 0.01550312526524067\n",
      "epoch: 8 step: 858, loss is 0.029412418603897095\n",
      "epoch: 8 step: 859, loss is 0.0028485730290412903\n",
      "epoch: 8 step: 860, loss is 0.05450984835624695\n",
      "epoch: 8 step: 861, loss is 0.00017939903773367405\n",
      "epoch: 8 step: 862, loss is 0.02076616883277893\n",
      "epoch: 8 step: 863, loss is 0.0005837673670612276\n",
      "epoch: 8 step: 864, loss is 0.0017633659299463034\n",
      "epoch: 8 step: 865, loss is 0.0018139206804335117\n",
      "epoch: 8 step: 866, loss is 0.001912715146318078\n",
      "epoch: 8 step: 867, loss is 0.005974580533802509\n",
      "epoch: 8 step: 868, loss is 0.0003366897872183472\n",
      "epoch: 8 step: 869, loss is 0.01059495098888874\n",
      "epoch: 8 step: 870, loss is 0.00029672784148715436\n",
      "epoch: 8 step: 871, loss is 3.361385824973695e-05\n",
      "epoch: 8 step: 872, loss is 0.0006797821260988712\n",
      "epoch: 8 step: 873, loss is 0.0005513323121704161\n",
      "epoch: 8 step: 874, loss is 0.0002655387797858566\n",
      "epoch: 8 step: 875, loss is 0.0071810586377978325\n",
      "epoch: 8 step: 876, loss is 0.00017314577416982502\n",
      "epoch: 8 step: 877, loss is 0.0005193412653170526\n",
      "epoch: 8 step: 878, loss is 0.00021206655947025865\n",
      "epoch: 8 step: 879, loss is 5.620918091153726e-05\n",
      "epoch: 8 step: 880, loss is 0.03379924222826958\n",
      "epoch: 8 step: 881, loss is 0.023446548730134964\n",
      "epoch: 8 step: 882, loss is 0.00010677906539058313\n",
      "epoch: 8 step: 883, loss is 0.018492044880986214\n",
      "epoch: 8 step: 884, loss is 0.002998087788000703\n",
      "epoch: 8 step: 885, loss is 0.014707430265843868\n",
      "epoch: 8 step: 886, loss is 0.0002738594193942845\n",
      "epoch: 8 step: 887, loss is 0.00010998415382346138\n",
      "epoch: 8 step: 888, loss is 0.006722630467265844\n",
      "epoch: 8 step: 889, loss is 0.012974225915968418\n",
      "epoch: 8 step: 890, loss is 0.0023259965237230062\n",
      "epoch: 8 step: 891, loss is 0.013379519805312157\n",
      "epoch: 8 step: 892, loss is 0.02237379364669323\n",
      "epoch: 8 step: 893, loss is 0.00011590302165132016\n",
      "epoch: 8 step: 894, loss is 0.00019929370319005102\n",
      "epoch: 8 step: 895, loss is 0.0008680344908498228\n",
      "epoch: 8 step: 896, loss is 5.274672730593011e-05\n",
      "epoch: 8 step: 897, loss is 0.00016559832147322595\n",
      "epoch: 8 step: 898, loss is 0.00011639676085906103\n",
      "epoch: 8 step: 899, loss is 0.00047667024773545563\n",
      "epoch: 8 step: 900, loss is 0.00010028373799286783\n",
      "epoch: 8 step: 901, loss is 0.0017093830974772573\n",
      "epoch: 8 step: 902, loss is 0.00019697526295203716\n",
      "epoch: 8 step: 903, loss is 0.0006118594901636243\n",
      "epoch: 8 step: 904, loss is 0.008030803874135017\n",
      "epoch: 8 step: 905, loss is 0.00010094438766827807\n",
      "epoch: 8 step: 906, loss is 7.17194561730139e-05\n",
      "epoch: 8 step: 907, loss is 9.223775123246014e-05\n",
      "epoch: 8 step: 908, loss is 0.000283467787085101\n",
      "epoch: 8 step: 909, loss is 0.0005488795577548444\n",
      "epoch: 8 step: 910, loss is 5.955715096206404e-05\n",
      "epoch: 8 step: 911, loss is 0.00019064397201873362\n",
      "epoch: 8 step: 912, loss is 0.0005575733375735581\n",
      "epoch: 8 step: 913, loss is 0.0032119890674948692\n",
      "epoch: 8 step: 914, loss is 0.0018365065334364772\n",
      "epoch: 8 step: 915, loss is 3.646387631306425e-05\n",
      "epoch: 8 step: 916, loss is 6.310620665317401e-05\n",
      "epoch: 8 step: 917, loss is 0.00057591701624915\n",
      "epoch: 8 step: 918, loss is 0.06575994193553925\n",
      "epoch: 8 step: 919, loss is 0.015773631632328033\n",
      "epoch: 8 step: 920, loss is 0.033154577016830444\n",
      "epoch: 8 step: 921, loss is 0.0016423719935119152\n",
      "epoch: 8 step: 922, loss is 0.009725221432745457\n",
      "epoch: 8 step: 923, loss is 0.08125729113817215\n",
      "epoch: 8 step: 924, loss is 0.0002580391592346132\n",
      "epoch: 8 step: 925, loss is 5.13171216880437e-05\n",
      "epoch: 8 step: 926, loss is 2.0985809896956198e-05\n",
      "epoch: 8 step: 927, loss is 3.340602415846661e-05\n",
      "epoch: 8 step: 928, loss is 0.0029043350368738174\n",
      "epoch: 8 step: 929, loss is 0.0017162191215902567\n",
      "epoch: 8 step: 930, loss is 0.058869633823633194\n",
      "epoch: 8 step: 931, loss is 0.13785669207572937\n",
      "epoch: 8 step: 932, loss is 0.016221556812524796\n",
      "epoch: 8 step: 933, loss is 0.00015067149070091546\n",
      "epoch: 8 step: 934, loss is 0.0003919179434888065\n",
      "epoch: 8 step: 935, loss is 0.0038712655659765005\n",
      "epoch: 8 step: 936, loss is 0.0032659133430570364\n",
      "epoch: 8 step: 937, loss is 2.083679828501772e-05\n",
      "epoch: 8 step: 938, loss is 0.0016441564075648785\n",
      "epoch: 8 step: 939, loss is 0.0013513155281543732\n",
      "epoch: 8 step: 940, loss is 0.056930363178253174\n",
      "epoch: 8 step: 941, loss is 0.0009032143279910088\n",
      "epoch: 8 step: 942, loss is 0.0006408329354599118\n",
      "epoch: 8 step: 943, loss is 0.0004525443946477026\n",
      "epoch: 8 step: 944, loss is 0.0009211481083184481\n",
      "epoch: 8 step: 945, loss is 0.00024252755974885076\n",
      "epoch: 8 step: 946, loss is 0.07065402716398239\n",
      "epoch: 8 step: 947, loss is 0.0007721562869846821\n",
      "epoch: 8 step: 948, loss is 0.001786787761375308\n",
      "epoch: 8 step: 949, loss is 0.005884063430130482\n",
      "epoch: 8 step: 950, loss is 0.10781798511743546\n",
      "epoch: 8 step: 951, loss is 0.006525947712361813\n",
      "epoch: 8 step: 952, loss is 0.07141099870204926\n",
      "epoch: 8 step: 953, loss is 3.746191578102298e-05\n",
      "epoch: 8 step: 954, loss is 0.0013762860326096416\n",
      "epoch: 8 step: 955, loss is 0.0016050911508500576\n",
      "epoch: 8 step: 956, loss is 0.2907664179801941\n",
      "epoch: 8 step: 957, loss is 0.000660535239148885\n",
      "epoch: 8 step: 958, loss is 0.0001825194776756689\n",
      "epoch: 8 step: 959, loss is 4.773592809215188e-05\n",
      "epoch: 8 step: 960, loss is 2.049802424153313e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 961, loss is 0.0008044145652092993\n",
      "epoch: 8 step: 962, loss is 0.0057479823008179665\n",
      "epoch: 8 step: 963, loss is 5.4619918955722824e-05\n",
      "epoch: 8 step: 964, loss is 0.0001901764189824462\n",
      "epoch: 8 step: 965, loss is 0.00016916242020670325\n",
      "epoch: 8 step: 966, loss is 0.009738833643496037\n",
      "epoch: 8 step: 967, loss is 7.651047053514048e-05\n",
      "epoch: 8 step: 968, loss is 0.00018932943930849433\n",
      "epoch: 8 step: 969, loss is 0.12892861664295197\n",
      "epoch: 8 step: 970, loss is 0.0020101163536310196\n",
      "epoch: 8 step: 971, loss is 0.002575809368863702\n",
      "epoch: 8 step: 972, loss is 0.0006317521329037845\n",
      "epoch: 8 step: 973, loss is 0.006955730728805065\n",
      "epoch: 8 step: 974, loss is 0.32218965888023376\n",
      "epoch: 8 step: 975, loss is 0.052677858620882034\n",
      "epoch: 8 step: 976, loss is 0.06243226304650307\n",
      "epoch: 8 step: 977, loss is 0.0033135542180389166\n",
      "epoch: 8 step: 978, loss is 0.003745915135368705\n",
      "epoch: 8 step: 979, loss is 7.466774695785716e-05\n",
      "epoch: 8 step: 980, loss is 0.0023058445658534765\n",
      "epoch: 8 step: 981, loss is 0.022016389295458794\n",
      "epoch: 8 step: 982, loss is 0.006013328675180674\n",
      "epoch: 8 step: 983, loss is 0.0015138330636546016\n",
      "epoch: 8 step: 984, loss is 0.0333915650844574\n",
      "epoch: 8 step: 985, loss is 0.0006010405486449599\n",
      "epoch: 8 step: 986, loss is 0.0493708997964859\n",
      "epoch: 8 step: 987, loss is 0.0001811390684451908\n",
      "epoch: 8 step: 988, loss is 0.03789950907230377\n",
      "epoch: 8 step: 989, loss is 0.002509006066247821\n",
      "epoch: 8 step: 990, loss is 0.0029411683790385723\n",
      "epoch: 8 step: 991, loss is 0.00021375369396992028\n",
      "epoch: 8 step: 992, loss is 0.03311476111412048\n",
      "epoch: 8 step: 993, loss is 0.0009203549125231802\n",
      "epoch: 8 step: 994, loss is 0.0021730896551162004\n",
      "epoch: 8 step: 995, loss is 0.0005809533759020269\n",
      "epoch: 8 step: 996, loss is 0.3030478060245514\n",
      "epoch: 8 step: 997, loss is 0.00037768471520394087\n",
      "epoch: 8 step: 998, loss is 0.0007799401064403355\n",
      "epoch: 8 step: 999, loss is 0.007334613706916571\n",
      "epoch: 8 step: 1000, loss is 0.001436082529835403\n",
      "epoch: 8 step: 1001, loss is 0.0008821974042803049\n",
      "epoch: 8 step: 1002, loss is 0.0004985783016309142\n",
      "epoch: 8 step: 1003, loss is 0.004594603553414345\n",
      "epoch: 8 step: 1004, loss is 0.0034632720053195953\n",
      "epoch: 8 step: 1005, loss is 0.0027433661743998528\n",
      "epoch: 8 step: 1006, loss is 0.06593392044305801\n",
      "epoch: 8 step: 1007, loss is 0.0018456473480910063\n",
      "epoch: 8 step: 1008, loss is 0.0023268775548785925\n",
      "epoch: 8 step: 1009, loss is 0.002307651098817587\n",
      "epoch: 8 step: 1010, loss is 0.0007585874991491437\n",
      "epoch: 8 step: 1011, loss is 0.04093426093459129\n",
      "epoch: 8 step: 1012, loss is 0.005946521181613207\n",
      "epoch: 8 step: 1013, loss is 0.0011330773122608662\n",
      "epoch: 8 step: 1014, loss is 0.006249571684747934\n",
      "epoch: 8 step: 1015, loss is 0.09392797946929932\n",
      "epoch: 8 step: 1016, loss is 3.1962175853550434e-05\n",
      "epoch: 8 step: 1017, loss is 0.0015643073711544275\n",
      "epoch: 8 step: 1018, loss is 0.0009269941365346313\n",
      "epoch: 8 step: 1019, loss is 0.015475341118872166\n",
      "epoch: 8 step: 1020, loss is 0.0009680057992227376\n",
      "epoch: 8 step: 1021, loss is 0.006309486459940672\n",
      "epoch: 8 step: 1022, loss is 0.005946601275354624\n",
      "epoch: 8 step: 1023, loss is 0.003234035335481167\n",
      "epoch: 8 step: 1024, loss is 0.0005275850417092443\n",
      "epoch: 8 step: 1025, loss is 0.005969261284917593\n",
      "epoch: 8 step: 1026, loss is 0.02409518137574196\n",
      "epoch: 8 step: 1027, loss is 7.275163079611957e-05\n",
      "epoch: 8 step: 1028, loss is 0.005595823749899864\n",
      "epoch: 8 step: 1029, loss is 0.0038992350455373526\n",
      "epoch: 8 step: 1030, loss is 0.006831553298979998\n",
      "epoch: 8 step: 1031, loss is 0.00731520913541317\n",
      "epoch: 8 step: 1032, loss is 0.006752076558768749\n",
      "epoch: 8 step: 1033, loss is 0.01107091922312975\n",
      "epoch: 8 step: 1034, loss is 0.005757542327046394\n",
      "epoch: 8 step: 1035, loss is 0.009687788784503937\n",
      "epoch: 8 step: 1036, loss is 0.000380114623112604\n",
      "epoch: 8 step: 1037, loss is 0.0001571081520523876\n",
      "epoch: 8 step: 1038, loss is 0.008711426518857479\n",
      "epoch: 8 step: 1039, loss is 0.07513324916362762\n",
      "epoch: 8 step: 1040, loss is 0.0013453953433781862\n",
      "epoch: 8 step: 1041, loss is 0.06420767307281494\n",
      "epoch: 8 step: 1042, loss is 0.009217265993356705\n",
      "epoch: 8 step: 1043, loss is 5.961282295174897e-05\n",
      "epoch: 8 step: 1044, loss is 0.01984206587076187\n",
      "epoch: 8 step: 1045, loss is 0.00045105753815732896\n",
      "epoch: 8 step: 1046, loss is 0.019092457368969917\n",
      "epoch: 8 step: 1047, loss is 0.03538648039102554\n",
      "epoch: 8 step: 1048, loss is 0.00017528070020489395\n",
      "epoch: 8 step: 1049, loss is 0.0028857183642685413\n",
      "epoch: 8 step: 1050, loss is 0.0004881921922788024\n",
      "epoch: 8 step: 1051, loss is 0.04391688480973244\n",
      "epoch: 8 step: 1052, loss is 0.0518159493803978\n",
      "epoch: 8 step: 1053, loss is 0.0003714043996296823\n",
      "epoch: 8 step: 1054, loss is 0.00010506559192435816\n",
      "epoch: 8 step: 1055, loss is 0.002938996534794569\n",
      "epoch: 8 step: 1056, loss is 0.003186807269230485\n",
      "epoch: 8 step: 1057, loss is 0.0196243729442358\n",
      "epoch: 8 step: 1058, loss is 0.00013053903239779174\n",
      "epoch: 8 step: 1059, loss is 0.004624753724783659\n",
      "epoch: 8 step: 1060, loss is 0.005735805723816156\n",
      "epoch: 8 step: 1061, loss is 0.0020597693510353565\n",
      "epoch: 8 step: 1062, loss is 0.0009141435148194432\n",
      "epoch: 8 step: 1063, loss is 0.002855683444067836\n",
      "epoch: 8 step: 1064, loss is 0.00817022379487753\n",
      "epoch: 8 step: 1065, loss is 0.000548606039956212\n",
      "epoch: 8 step: 1066, loss is 0.006551031954586506\n",
      "epoch: 8 step: 1067, loss is 0.16114160418510437\n",
      "epoch: 8 step: 1068, loss is 0.00012211590365041047\n",
      "epoch: 8 step: 1069, loss is 0.00017197182751260698\n",
      "epoch: 8 step: 1070, loss is 0.019071053713560104\n",
      "epoch: 8 step: 1071, loss is 0.0016220080433413386\n",
      "epoch: 8 step: 1072, loss is 0.0005165199982002378\n",
      "epoch: 8 step: 1073, loss is 0.0025573098100721836\n",
      "epoch: 8 step: 1074, loss is 0.009376496076583862\n",
      "epoch: 8 step: 1075, loss is 0.013852438889443874\n",
      "epoch: 8 step: 1076, loss is 0.00044414770673029125\n",
      "epoch: 8 step: 1077, loss is 0.0017623730236664414\n",
      "epoch: 8 step: 1078, loss is 0.0016021407209336758\n",
      "epoch: 8 step: 1079, loss is 0.000492032035253942\n",
      "epoch: 8 step: 1080, loss is 0.16063576936721802\n",
      "epoch: 8 step: 1081, loss is 0.0001399917819071561\n",
      "epoch: 8 step: 1082, loss is 0.00030289855203591287\n",
      "epoch: 8 step: 1083, loss is 0.0014626438496634364\n",
      "epoch: 8 step: 1084, loss is 0.00010694145748857409\n",
      "epoch: 8 step: 1085, loss is 0.006515766493976116\n",
      "epoch: 8 step: 1086, loss is 0.0006711845635436475\n",
      "epoch: 8 step: 1087, loss is 0.0011937699746340513\n",
      "epoch: 8 step: 1088, loss is 0.0003254673501942307\n",
      "epoch: 8 step: 1089, loss is 0.020945493131875992\n",
      "epoch: 8 step: 1090, loss is 0.018463898450136185\n",
      "epoch: 8 step: 1091, loss is 0.00012810791668016464\n",
      "epoch: 8 step: 1092, loss is 0.000210949860047549\n",
      "epoch: 8 step: 1093, loss is 0.0016425990033894777\n",
      "epoch: 8 step: 1094, loss is 0.0003277170762885362\n",
      "epoch: 8 step: 1095, loss is 9.728850272949785e-05\n",
      "epoch: 8 step: 1096, loss is 0.0012787426821887493\n",
      "epoch: 8 step: 1097, loss is 0.0023910743184387684\n",
      "epoch: 8 step: 1098, loss is 7.804892811691388e-05\n",
      "epoch: 8 step: 1099, loss is 0.017170626670122147\n",
      "epoch: 8 step: 1100, loss is 0.002099717501550913\n",
      "epoch: 8 step: 1101, loss is 0.004189600236713886\n",
      "epoch: 8 step: 1102, loss is 0.0025277482345700264\n",
      "epoch: 8 step: 1103, loss is 0.00044043658999726176\n",
      "epoch: 8 step: 1104, loss is 9.529572707833722e-05\n",
      "epoch: 8 step: 1105, loss is 0.030253633856773376\n",
      "epoch: 8 step: 1106, loss is 0.0027858528774231672\n",
      "epoch: 8 step: 1107, loss is 0.0007933765882626176\n",
      "epoch: 8 step: 1108, loss is 0.00022354250540956855\n",
      "epoch: 8 step: 1109, loss is 0.00219636014662683\n",
      "epoch: 8 step: 1110, loss is 0.03147237375378609\n",
      "epoch: 8 step: 1111, loss is 0.06355704367160797\n",
      "epoch: 8 step: 1112, loss is 0.0028107743710279465\n",
      "epoch: 8 step: 1113, loss is 0.028652070090174675\n",
      "epoch: 8 step: 1114, loss is 0.0005676103755831718\n",
      "epoch: 8 step: 1115, loss is 0.0006364581058733165\n",
      "epoch: 8 step: 1116, loss is 0.0012587371747940779\n",
      "epoch: 8 step: 1117, loss is 0.008415663614869118\n",
      "epoch: 8 step: 1118, loss is 0.0005332604050636292\n",
      "epoch: 8 step: 1119, loss is 0.00025003281189128757\n",
      "epoch: 8 step: 1120, loss is 0.04356105998158455\n",
      "epoch: 8 step: 1121, loss is 0.00013790529919788241\n",
      "epoch: 8 step: 1122, loss is 0.0002845031558535993\n",
      "epoch: 8 step: 1123, loss is 0.04548526182770729\n",
      "epoch: 8 step: 1124, loss is 0.027448976412415504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 1125, loss is 0.001308141858316958\n",
      "epoch: 8 step: 1126, loss is 8.879194501787424e-05\n",
      "epoch: 8 step: 1127, loss is 0.0011175211984664202\n",
      "epoch: 8 step: 1128, loss is 0.0017717083683237433\n",
      "epoch: 8 step: 1129, loss is 0.01291525550186634\n",
      "epoch: 8 step: 1130, loss is 0.0006927766371518373\n",
      "epoch: 8 step: 1131, loss is 0.030118228867650032\n",
      "epoch: 8 step: 1132, loss is 0.2553774118423462\n",
      "epoch: 8 step: 1133, loss is 0.0007641431293450296\n",
      "epoch: 8 step: 1134, loss is 0.0003656094486359507\n",
      "epoch: 8 step: 1135, loss is 0.0003623373922891915\n",
      "epoch: 8 step: 1136, loss is 0.027621222659945488\n",
      "epoch: 8 step: 1137, loss is 0.0007580672390758991\n",
      "epoch: 8 step: 1138, loss is 4.939803147863131e-06\n",
      "epoch: 8 step: 1139, loss is 0.0009508788934908807\n",
      "epoch: 8 step: 1140, loss is 0.0002184515178669244\n",
      "epoch: 8 step: 1141, loss is 0.0017309446120634675\n",
      "epoch: 8 step: 1142, loss is 0.0009408068144693971\n",
      "epoch: 8 step: 1143, loss is 0.008964302949607372\n",
      "epoch: 8 step: 1144, loss is 0.0014532728819176555\n",
      "epoch: 8 step: 1145, loss is 0.006078471429646015\n",
      "epoch: 8 step: 1146, loss is 0.15896004438400269\n",
      "epoch: 8 step: 1147, loss is 0.07568767666816711\n",
      "epoch: 8 step: 1148, loss is 3.845606624963693e-05\n",
      "epoch: 8 step: 1149, loss is 0.0017006122507154942\n",
      "epoch: 8 step: 1150, loss is 0.0014159459387883544\n",
      "epoch: 8 step: 1151, loss is 0.0008553858497180045\n",
      "epoch: 8 step: 1152, loss is 0.001232811133377254\n",
      "epoch: 8 step: 1153, loss is 0.0009169044205918908\n",
      "epoch: 8 step: 1154, loss is 0.0059669907204806805\n",
      "epoch: 8 step: 1155, loss is 0.00012706413690466434\n",
      "epoch: 8 step: 1156, loss is 5.190892989048734e-05\n",
      "epoch: 8 step: 1157, loss is 0.00022025367070455104\n",
      "epoch: 8 step: 1158, loss is 0.002463959390297532\n",
      "epoch: 8 step: 1159, loss is 0.0035155932419002056\n",
      "epoch: 8 step: 1160, loss is 0.05561646446585655\n",
      "epoch: 8 step: 1161, loss is 0.07401782274246216\n",
      "epoch: 8 step: 1162, loss is 0.0036140838637948036\n",
      "epoch: 8 step: 1163, loss is 0.0005564167513512075\n",
      "epoch: 8 step: 1164, loss is 0.0017069859895855188\n",
      "epoch: 8 step: 1165, loss is 0.0015671828296035528\n",
      "epoch: 8 step: 1166, loss is 0.001098587061278522\n",
      "epoch: 8 step: 1167, loss is 0.0002889400057028979\n",
      "epoch: 8 step: 1168, loss is 4.1975417843787e-05\n",
      "epoch: 8 step: 1169, loss is 0.00047185999574139714\n",
      "epoch: 8 step: 1170, loss is 0.0013349313521757722\n",
      "epoch: 8 step: 1171, loss is 0.0011643798789009452\n",
      "epoch: 8 step: 1172, loss is 0.09736616909503937\n",
      "epoch: 8 step: 1173, loss is 0.16762031614780426\n",
      "epoch: 8 step: 1174, loss is 0.0007053336012177169\n",
      "epoch: 8 step: 1175, loss is 0.022322816774249077\n",
      "epoch: 8 step: 1176, loss is 0.5180748701095581\n",
      "epoch: 8 step: 1177, loss is 7.095521141309291e-05\n",
      "epoch: 8 step: 1178, loss is 0.0001059130736393854\n",
      "epoch: 8 step: 1179, loss is 0.00034261643304489553\n",
      "epoch: 8 step: 1180, loss is 0.17018626630306244\n",
      "epoch: 8 step: 1181, loss is 0.0026832586154341698\n",
      "epoch: 8 step: 1182, loss is 0.0011560411658138037\n",
      "epoch: 8 step: 1183, loss is 0.008566351607441902\n",
      "epoch: 8 step: 1184, loss is 0.010164658538997173\n",
      "epoch: 8 step: 1185, loss is 0.02949984185397625\n",
      "epoch: 8 step: 1186, loss is 0.26162123680114746\n",
      "epoch: 8 step: 1187, loss is 0.0009498136932961643\n",
      "epoch: 8 step: 1188, loss is 0.00016049704572651535\n",
      "epoch: 8 step: 1189, loss is 0.0004718389827758074\n",
      "epoch: 8 step: 1190, loss is 0.0004046972026117146\n",
      "epoch: 8 step: 1191, loss is 0.020365707576274872\n",
      "epoch: 8 step: 1192, loss is 0.0054802000522613525\n",
      "epoch: 8 step: 1193, loss is 0.012716543860733509\n",
      "epoch: 8 step: 1194, loss is 0.00017414138710591942\n",
      "epoch: 8 step: 1195, loss is 0.026729634031653404\n",
      "epoch: 8 step: 1196, loss is 0.036701105535030365\n",
      "epoch: 8 step: 1197, loss is 0.005475528538227081\n",
      "epoch: 8 step: 1198, loss is 0.004310800693929195\n",
      "epoch: 8 step: 1199, loss is 0.0017052976181730628\n",
      "epoch: 8 step: 1200, loss is 0.006401895545423031\n",
      "epoch: 8 step: 1201, loss is 0.024650294333696365\n",
      "epoch: 8 step: 1202, loss is 0.015187199227511883\n",
      "epoch: 8 step: 1203, loss is 0.047381021082401276\n",
      "epoch: 8 step: 1204, loss is 0.003191020805388689\n",
      "epoch: 8 step: 1205, loss is 0.0020890722516924143\n",
      "epoch: 8 step: 1206, loss is 0.021200252696871758\n",
      "epoch: 8 step: 1207, loss is 0.007527499459683895\n",
      "epoch: 8 step: 1208, loss is 0.002478655194863677\n",
      "epoch: 8 step: 1209, loss is 0.0350407175719738\n",
      "epoch: 8 step: 1210, loss is 0.01018641795963049\n",
      "epoch: 8 step: 1211, loss is 0.06830061972141266\n",
      "epoch: 8 step: 1212, loss is 0.0015868996270000935\n",
      "epoch: 8 step: 1213, loss is 0.07827813923358917\n",
      "epoch: 8 step: 1214, loss is 0.0011009618174284697\n",
      "epoch: 8 step: 1215, loss is 0.0008548408513888717\n",
      "epoch: 8 step: 1216, loss is 0.0008859108202159405\n",
      "epoch: 8 step: 1217, loss is 0.01010288018733263\n",
      "epoch: 8 step: 1218, loss is 0.0035409186966717243\n",
      "epoch: 8 step: 1219, loss is 0.011864534579217434\n",
      "epoch: 8 step: 1220, loss is 0.0861889198422432\n",
      "epoch: 8 step: 1221, loss is 0.039246074855327606\n",
      "epoch: 8 step: 1222, loss is 0.002007110510021448\n",
      "epoch: 8 step: 1223, loss is 0.004193123895674944\n",
      "epoch: 8 step: 1224, loss is 0.0029965387657284737\n",
      "epoch: 8 step: 1225, loss is 0.006842104718089104\n",
      "epoch: 8 step: 1226, loss is 0.0028360735159367323\n",
      "epoch: 8 step: 1227, loss is 0.024344125762581825\n",
      "epoch: 8 step: 1228, loss is 0.003541625803336501\n",
      "epoch: 8 step: 1229, loss is 0.007784545887261629\n",
      "epoch: 8 step: 1230, loss is 0.010486473329365253\n",
      "epoch: 8 step: 1231, loss is 0.0033330065198242664\n",
      "epoch: 8 step: 1232, loss is 0.003045641118660569\n",
      "epoch: 8 step: 1233, loss is 0.0025017354637384415\n",
      "epoch: 8 step: 1234, loss is 0.000791484082583338\n",
      "epoch: 8 step: 1235, loss is 0.003524947678670287\n",
      "epoch: 8 step: 1236, loss is 0.0021897018887102604\n",
      "epoch: 8 step: 1237, loss is 0.013431650586426258\n",
      "epoch: 8 step: 1238, loss is 0.0002483447315171361\n",
      "epoch: 8 step: 1239, loss is 0.00018057254783343524\n",
      "epoch: 8 step: 1240, loss is 0.014079907909035683\n",
      "epoch: 8 step: 1241, loss is 0.001467929221689701\n",
      "epoch: 8 step: 1242, loss is 0.003760024905204773\n",
      "epoch: 8 step: 1243, loss is 0.0006652255542576313\n",
      "epoch: 8 step: 1244, loss is 0.001504257321357727\n",
      "epoch: 8 step: 1245, loss is 0.0077157034538686275\n",
      "epoch: 8 step: 1246, loss is 0.0011535431258380413\n",
      "epoch: 8 step: 1247, loss is 2.5858251319732517e-05\n",
      "epoch: 8 step: 1248, loss is 0.00011996116518275812\n",
      "epoch: 8 step: 1249, loss is 0.004722926765680313\n",
      "epoch: 8 step: 1250, loss is 0.008814714848995209\n",
      "epoch: 8 step: 1251, loss is 0.0012762212427332997\n",
      "epoch: 8 step: 1252, loss is 0.00981691014021635\n",
      "epoch: 8 step: 1253, loss is 0.0003634416207205504\n",
      "epoch: 8 step: 1254, loss is 0.0006452875677496195\n",
      "epoch: 8 step: 1255, loss is 0.00816290732473135\n",
      "epoch: 8 step: 1256, loss is 0.06345124542713165\n",
      "epoch: 8 step: 1257, loss is 0.020753346383571625\n",
      "epoch: 8 step: 1258, loss is 0.0018750226590782404\n",
      "epoch: 8 step: 1259, loss is 3.451614247751422e-05\n",
      "epoch: 8 step: 1260, loss is 0.00014266790822148323\n",
      "epoch: 8 step: 1261, loss is 8.779358904575929e-05\n",
      "epoch: 8 step: 1262, loss is 0.0011092127533629537\n",
      "epoch: 8 step: 1263, loss is 0.001448895432986319\n",
      "epoch: 8 step: 1264, loss is 0.015803052112460136\n",
      "epoch: 8 step: 1265, loss is 0.005255851894617081\n",
      "epoch: 8 step: 1266, loss is 0.0001515210315119475\n",
      "epoch: 8 step: 1267, loss is 0.00640675937756896\n",
      "epoch: 8 step: 1268, loss is 0.0001454183366149664\n",
      "epoch: 8 step: 1269, loss is 0.00034906610380858183\n",
      "epoch: 8 step: 1270, loss is 0.00019450331456027925\n",
      "epoch: 8 step: 1271, loss is 0.00022348204220179468\n",
      "epoch: 8 step: 1272, loss is 0.0009663149248808622\n",
      "epoch: 8 step: 1273, loss is 0.007796009071171284\n",
      "epoch: 8 step: 1274, loss is 0.14136351644992828\n",
      "epoch: 8 step: 1275, loss is 0.00014646374620497227\n",
      "epoch: 8 step: 1276, loss is 0.15404962003231049\n",
      "epoch: 8 step: 1277, loss is 0.0157042033970356\n",
      "epoch: 8 step: 1278, loss is 0.3908703029155731\n",
      "epoch: 8 step: 1279, loss is 0.0001389103999827057\n",
      "epoch: 8 step: 1280, loss is 0.00012910763325635344\n",
      "epoch: 8 step: 1281, loss is 0.003353115636855364\n",
      "epoch: 8 step: 1282, loss is 0.023519501090049744\n",
      "epoch: 8 step: 1283, loss is 0.0009185722447000444\n",
      "epoch: 8 step: 1284, loss is 0.002855621976777911\n",
      "epoch: 8 step: 1285, loss is 0.0005501423729583621\n",
      "epoch: 8 step: 1286, loss is 0.008893785998225212\n",
      "epoch: 8 step: 1287, loss is 0.0020600564312189817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 1288, loss is 0.000249051081482321\n",
      "epoch: 8 step: 1289, loss is 0.0010475583840161562\n",
      "epoch: 8 step: 1290, loss is 0.006728065200150013\n",
      "epoch: 8 step: 1291, loss is 0.004194782115519047\n",
      "epoch: 8 step: 1292, loss is 0.05223853141069412\n",
      "epoch: 8 step: 1293, loss is 0.0021034558303654194\n",
      "epoch: 8 step: 1294, loss is 0.00011039013043045998\n",
      "epoch: 8 step: 1295, loss is 0.0019688340835273266\n",
      "epoch: 8 step: 1296, loss is 0.09938434511423111\n",
      "epoch: 8 step: 1297, loss is 0.011556467041373253\n",
      "epoch: 8 step: 1298, loss is 0.03238646313548088\n",
      "epoch: 8 step: 1299, loss is 0.0029863507952541113\n",
      "epoch: 8 step: 1300, loss is 0.009735734201967716\n",
      "epoch: 8 step: 1301, loss is 0.09577929228544235\n",
      "epoch: 8 step: 1302, loss is 0.01779608428478241\n",
      "epoch: 8 step: 1303, loss is 0.009758603759109974\n",
      "epoch: 8 step: 1304, loss is 0.0007945385295897722\n",
      "epoch: 8 step: 1305, loss is 4.378245284897275e-05\n",
      "epoch: 8 step: 1306, loss is 0.00027289698482491076\n",
      "epoch: 8 step: 1307, loss is 0.05239211022853851\n",
      "epoch: 8 step: 1308, loss is 0.08480098843574524\n",
      "epoch: 8 step: 1309, loss is 0.0017289145616814494\n",
      "epoch: 8 step: 1310, loss is 0.00010091031435877085\n",
      "epoch: 8 step: 1311, loss is 0.009990891441702843\n",
      "epoch: 8 step: 1312, loss is 0.002939810510724783\n",
      "epoch: 8 step: 1313, loss is 0.0017458979273214936\n",
      "epoch: 8 step: 1314, loss is 0.0016930451383814216\n",
      "epoch: 8 step: 1315, loss is 0.022639429196715355\n",
      "epoch: 8 step: 1316, loss is 0.00022966641699895263\n",
      "epoch: 8 step: 1317, loss is 0.3015163242816925\n",
      "epoch: 8 step: 1318, loss is 0.0002907341404352337\n",
      "epoch: 8 step: 1319, loss is 2.7477386538521387e-05\n",
      "epoch: 8 step: 1320, loss is 0.01172062661498785\n",
      "epoch: 8 step: 1321, loss is 0.0014807074330747128\n",
      "epoch: 8 step: 1322, loss is 0.0021730565931648016\n",
      "epoch: 8 step: 1323, loss is 0.000689782144036144\n",
      "epoch: 8 step: 1324, loss is 0.03614275902509689\n",
      "epoch: 8 step: 1325, loss is 6.771946209482849e-05\n",
      "epoch: 8 step: 1326, loss is 0.027475804090499878\n",
      "epoch: 8 step: 1327, loss is 0.001825436600483954\n",
      "epoch: 8 step: 1328, loss is 0.000401134806452319\n",
      "epoch: 8 step: 1329, loss is 0.0006445649196393788\n",
      "epoch: 8 step: 1330, loss is 0.0013268896145746112\n",
      "epoch: 8 step: 1331, loss is 0.0026377448812127113\n",
      "epoch: 8 step: 1332, loss is 0.001955966930836439\n",
      "epoch: 8 step: 1333, loss is 0.009300689212977886\n",
      "epoch: 8 step: 1334, loss is 7.205615111161023e-05\n",
      "epoch: 8 step: 1335, loss is 0.02099558897316456\n",
      "epoch: 8 step: 1336, loss is 0.00045663461787626147\n",
      "epoch: 8 step: 1337, loss is 0.0016708978218957782\n",
      "epoch: 8 step: 1338, loss is 0.013311916030943394\n",
      "epoch: 8 step: 1339, loss is 0.0004889764240942895\n",
      "epoch: 8 step: 1340, loss is 0.05012816563248634\n",
      "epoch: 8 step: 1341, loss is 0.0057922666892409325\n",
      "epoch: 8 step: 1342, loss is 0.11143641918897629\n",
      "epoch: 8 step: 1343, loss is 0.0005244941567070782\n",
      "epoch: 8 step: 1344, loss is 0.0007338261348195374\n",
      "epoch: 8 step: 1345, loss is 0.0007901632343418896\n",
      "epoch: 8 step: 1346, loss is 0.000255840684985742\n",
      "epoch: 8 step: 1347, loss is 0.24105146527290344\n",
      "epoch: 8 step: 1348, loss is 0.00123753456864506\n",
      "epoch: 8 step: 1349, loss is 0.005888177547603846\n",
      "epoch: 8 step: 1350, loss is 0.00031940595363266766\n",
      "epoch: 8 step: 1351, loss is 3.5682303860085085e-05\n",
      "epoch: 8 step: 1352, loss is 0.06133469194173813\n",
      "epoch: 8 step: 1353, loss is 0.2958357632160187\n",
      "epoch: 8 step: 1354, loss is 0.0005090025370009243\n",
      "epoch: 8 step: 1355, loss is 0.013327104039490223\n",
      "epoch: 8 step: 1356, loss is 0.0023959323298186064\n",
      "epoch: 8 step: 1357, loss is 0.00200664228759706\n",
      "epoch: 8 step: 1358, loss is 0.0003711481986101717\n",
      "epoch: 8 step: 1359, loss is 0.0005924549186602235\n",
      "epoch: 8 step: 1360, loss is 0.00015573305427096784\n",
      "epoch: 8 step: 1361, loss is 0.013030461966991425\n",
      "epoch: 8 step: 1362, loss is 0.09254086017608643\n",
      "epoch: 8 step: 1363, loss is 0.07505254447460175\n",
      "epoch: 8 step: 1364, loss is 0.19904574751853943\n",
      "epoch: 8 step: 1365, loss is 0.000924606341868639\n",
      "epoch: 8 step: 1366, loss is 0.00013742854935117066\n",
      "epoch: 8 step: 1367, loss is 0.0013073140289634466\n",
      "epoch: 8 step: 1368, loss is 0.0013489940902218223\n",
      "epoch: 8 step: 1369, loss is 0.006551728583872318\n",
      "epoch: 8 step: 1370, loss is 0.00045413116458803415\n",
      "epoch: 8 step: 1371, loss is 0.023001551628112793\n",
      "epoch: 8 step: 1372, loss is 0.001084436196833849\n",
      "epoch: 8 step: 1373, loss is 0.02758042886853218\n",
      "epoch: 8 step: 1374, loss is 0.11865269392728806\n",
      "epoch: 8 step: 1375, loss is 0.05343122407793999\n",
      "epoch: 8 step: 1376, loss is 0.1408364623785019\n",
      "epoch: 8 step: 1377, loss is 0.013910689391195774\n",
      "epoch: 8 step: 1378, loss is 0.0005808289861306548\n",
      "epoch: 8 step: 1379, loss is 0.005439430475234985\n",
      "epoch: 8 step: 1380, loss is 0.00031543863588012755\n",
      "epoch: 8 step: 1381, loss is 0.0032516545616090298\n",
      "epoch: 8 step: 1382, loss is 0.004021080676466227\n",
      "epoch: 8 step: 1383, loss is 0.005784898530691862\n",
      "epoch: 8 step: 1384, loss is 0.02739877998828888\n",
      "epoch: 8 step: 1385, loss is 0.027385519817471504\n",
      "epoch: 8 step: 1386, loss is 0.0018558853771537542\n",
      "epoch: 8 step: 1387, loss is 0.01345253549516201\n",
      "epoch: 8 step: 1388, loss is 0.0010527734411880374\n",
      "epoch: 8 step: 1389, loss is 0.0014437774661928415\n",
      "epoch: 8 step: 1390, loss is 0.0022364119067788124\n",
      "epoch: 8 step: 1391, loss is 0.004166984464973211\n",
      "epoch: 8 step: 1392, loss is 0.3134180009365082\n",
      "epoch: 8 step: 1393, loss is 0.07110806554555893\n",
      "epoch: 8 step: 1394, loss is 0.0005555222742259502\n",
      "epoch: 8 step: 1395, loss is 0.0798373594880104\n",
      "epoch: 8 step: 1396, loss is 0.08889785408973694\n",
      "epoch: 8 step: 1397, loss is 0.0029213803354650736\n",
      "epoch: 8 step: 1398, loss is 0.228333979845047\n",
      "epoch: 8 step: 1399, loss is 0.0914982333779335\n",
      "epoch: 8 step: 1400, loss is 0.0028609950095415115\n",
      "epoch: 8 step: 1401, loss is 0.004234156105667353\n",
      "epoch: 8 step: 1402, loss is 0.0009155590669251978\n",
      "epoch: 8 step: 1403, loss is 0.08664119988679886\n",
      "epoch: 8 step: 1404, loss is 0.04538372531533241\n",
      "epoch: 8 step: 1405, loss is 0.03625322878360748\n",
      "epoch: 8 step: 1406, loss is 0.0033621215261518955\n",
      "epoch: 8 step: 1407, loss is 0.003586024045944214\n",
      "epoch: 8 step: 1408, loss is 0.003435734426602721\n",
      "epoch: 8 step: 1409, loss is 0.0007734278333373368\n",
      "epoch: 8 step: 1410, loss is 0.0983966514468193\n",
      "epoch: 8 step: 1411, loss is 0.010689320974051952\n",
      "epoch: 8 step: 1412, loss is 0.0026678312569856644\n",
      "epoch: 8 step: 1413, loss is 0.0019688464235514402\n",
      "epoch: 8 step: 1414, loss is 0.020313270390033722\n",
      "epoch: 8 step: 1415, loss is 0.0003872673842124641\n",
      "epoch: 8 step: 1416, loss is 0.07962620258331299\n",
      "epoch: 8 step: 1417, loss is 0.005469880532473326\n",
      "epoch: 8 step: 1418, loss is 0.001884432160295546\n",
      "epoch: 8 step: 1419, loss is 0.002176546724513173\n",
      "epoch: 8 step: 1420, loss is 0.02531084604561329\n",
      "epoch: 8 step: 1421, loss is 0.034718211740255356\n",
      "epoch: 8 step: 1422, loss is 0.0007156736101023853\n",
      "epoch: 8 step: 1423, loss is 0.006768372375518084\n",
      "epoch: 8 step: 1424, loss is 0.16331854462623596\n",
      "epoch: 8 step: 1425, loss is 0.03261730074882507\n",
      "epoch: 8 step: 1426, loss is 0.01159609667956829\n",
      "epoch: 8 step: 1427, loss is 0.005222788080573082\n",
      "epoch: 8 step: 1428, loss is 0.009439368732273579\n",
      "epoch: 8 step: 1429, loss is 0.002277976833283901\n",
      "epoch: 8 step: 1430, loss is 0.00021018850384280086\n",
      "epoch: 8 step: 1431, loss is 0.013691723346710205\n",
      "epoch: 8 step: 1432, loss is 0.016309360042214394\n",
      "epoch: 8 step: 1433, loss is 0.01768818311393261\n",
      "epoch: 8 step: 1434, loss is 0.0017280599568039179\n",
      "epoch: 8 step: 1435, loss is 0.002523740055039525\n",
      "epoch: 8 step: 1436, loss is 0.0007101508672349155\n",
      "epoch: 8 step: 1437, loss is 0.007556157652288675\n",
      "epoch: 8 step: 1438, loss is 0.05030947923660278\n",
      "epoch: 8 step: 1439, loss is 0.0015909791691228747\n",
      "epoch: 8 step: 1440, loss is 0.0007398542948067188\n",
      "epoch: 8 step: 1441, loss is 0.0016564782708883286\n",
      "epoch: 8 step: 1442, loss is 0.04695216566324234\n",
      "epoch: 8 step: 1443, loss is 0.00021632939751725644\n",
      "epoch: 8 step: 1444, loss is 0.005085074808448553\n",
      "epoch: 8 step: 1445, loss is 0.0017881449311971664\n",
      "epoch: 8 step: 1446, loss is 0.009173566475510597\n",
      "epoch: 8 step: 1447, loss is 0.0009595650481060147\n",
      "epoch: 8 step: 1448, loss is 0.0022401330061256886\n",
      "epoch: 8 step: 1449, loss is 0.03435533121228218\n",
      "epoch: 8 step: 1450, loss is 0.0024914618115872145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 1451, loss is 0.01369740255177021\n",
      "epoch: 8 step: 1452, loss is 0.036334507167339325\n",
      "epoch: 8 step: 1453, loss is 0.003959461580961943\n",
      "epoch: 8 step: 1454, loss is 0.01190250925719738\n",
      "epoch: 8 step: 1455, loss is 0.0011917329393327236\n",
      "epoch: 8 step: 1456, loss is 0.0003078538575209677\n",
      "epoch: 8 step: 1457, loss is 0.001204099738970399\n",
      "epoch: 8 step: 1458, loss is 0.0013951750006526709\n",
      "epoch: 8 step: 1459, loss is 0.014765995554625988\n",
      "epoch: 8 step: 1460, loss is 0.0007857432356104255\n",
      "epoch: 8 step: 1461, loss is 0.0001863691577455029\n",
      "epoch: 8 step: 1462, loss is 0.008769155479967594\n",
      "epoch: 8 step: 1463, loss is 0.0003940769820474088\n",
      "epoch: 8 step: 1464, loss is 0.008150462992489338\n",
      "epoch: 8 step: 1465, loss is 0.1662529706954956\n",
      "epoch: 8 step: 1466, loss is 0.017375214025378227\n",
      "epoch: 8 step: 1467, loss is 0.0003238809294998646\n",
      "epoch: 8 step: 1468, loss is 0.0004934549215249717\n",
      "epoch: 8 step: 1469, loss is 0.011148802004754543\n",
      "epoch: 8 step: 1470, loss is 0.002107335487380624\n",
      "epoch: 8 step: 1471, loss is 0.08927614241838455\n",
      "epoch: 8 step: 1472, loss is 0.02370736375451088\n",
      "epoch: 8 step: 1473, loss is 0.009137245826423168\n",
      "epoch: 8 step: 1474, loss is 0.03085891157388687\n",
      "epoch: 8 step: 1475, loss is 0.0024823585990816355\n",
      "epoch: 8 step: 1476, loss is 0.0007715782849118114\n",
      "epoch: 8 step: 1477, loss is 0.00759946508333087\n",
      "epoch: 8 step: 1478, loss is 0.0006463489844463766\n",
      "epoch: 8 step: 1479, loss is 0.018924204632639885\n",
      "epoch: 8 step: 1480, loss is 0.0013031495036557317\n",
      "epoch: 8 step: 1481, loss is 0.19415311515331268\n",
      "epoch: 8 step: 1482, loss is 0.0917920172214508\n",
      "epoch: 8 step: 1483, loss is 0.007711475249379873\n",
      "epoch: 8 step: 1484, loss is 0.0017759831389412284\n",
      "epoch: 8 step: 1485, loss is 0.010177276097238064\n",
      "epoch: 8 step: 1486, loss is 0.002392794704064727\n",
      "epoch: 8 step: 1487, loss is 0.0010915491729974747\n",
      "epoch: 8 step: 1488, loss is 0.006055326201021671\n",
      "epoch: 8 step: 1489, loss is 0.03005443885922432\n",
      "epoch: 8 step: 1490, loss is 0.0036748540587723255\n",
      "epoch: 8 step: 1491, loss is 0.015444462187588215\n",
      "epoch: 8 step: 1492, loss is 0.0031473368871957064\n",
      "epoch: 8 step: 1493, loss is 0.043273210525512695\n",
      "epoch: 8 step: 1494, loss is 6.910772935952991e-05\n",
      "epoch: 8 step: 1495, loss is 0.004396743606775999\n",
      "epoch: 8 step: 1496, loss is 0.014971258118748665\n",
      "epoch: 8 step: 1497, loss is 0.002662460319697857\n",
      "epoch: 8 step: 1498, loss is 0.028906183317303658\n",
      "epoch: 8 step: 1499, loss is 0.004224326461553574\n",
      "epoch: 8 step: 1500, loss is 0.0001063302843249403\n",
      "epoch: 8 step: 1501, loss is 0.004875656217336655\n",
      "epoch: 8 step: 1502, loss is 0.0001548676227685064\n",
      "epoch: 8 step: 1503, loss is 0.0035968872252851725\n",
      "epoch: 8 step: 1504, loss is 0.004794639069586992\n",
      "epoch: 8 step: 1505, loss is 0.001659497618675232\n",
      "epoch: 8 step: 1506, loss is 0.0012566786026582122\n",
      "epoch: 8 step: 1507, loss is 0.016150031238794327\n",
      "epoch: 8 step: 1508, loss is 0.005310336593538523\n",
      "epoch: 8 step: 1509, loss is 0.0015837911050766706\n",
      "epoch: 8 step: 1510, loss is 0.004965977277606726\n",
      "epoch: 8 step: 1511, loss is 0.00850269291549921\n",
      "epoch: 8 step: 1512, loss is 0.00010531242151046172\n",
      "epoch: 8 step: 1513, loss is 0.018391920253634453\n",
      "epoch: 8 step: 1514, loss is 0.019347652792930603\n",
      "epoch: 8 step: 1515, loss is 0.014749519526958466\n",
      "epoch: 8 step: 1516, loss is 0.00011028796143364161\n",
      "epoch: 8 step: 1517, loss is 0.0004925269167870283\n",
      "epoch: 8 step: 1518, loss is 0.06430819630622864\n",
      "epoch: 8 step: 1519, loss is 0.0021979159209877253\n",
      "epoch: 8 step: 1520, loss is 0.0014196255942806602\n",
      "epoch: 8 step: 1521, loss is 0.031140172854065895\n",
      "epoch: 8 step: 1522, loss is 0.012795249931514263\n",
      "epoch: 8 step: 1523, loss is 0.013173120096325874\n",
      "epoch: 8 step: 1524, loss is 0.0662350133061409\n",
      "epoch: 8 step: 1525, loss is 0.002860449254512787\n",
      "epoch: 8 step: 1526, loss is 0.18634751439094543\n",
      "epoch: 8 step: 1527, loss is 0.02274651639163494\n",
      "epoch: 8 step: 1528, loss is 0.003061317140236497\n",
      "epoch: 8 step: 1529, loss is 0.00010170655878027901\n",
      "epoch: 8 step: 1530, loss is 0.0011298219906166196\n",
      "epoch: 8 step: 1531, loss is 6.036732520442456e-05\n",
      "epoch: 8 step: 1532, loss is 0.0018847642932087183\n",
      "epoch: 8 step: 1533, loss is 0.14801910519599915\n",
      "epoch: 8 step: 1534, loss is 0.002988815540447831\n",
      "epoch: 8 step: 1535, loss is 0.0007608403684571385\n",
      "epoch: 8 step: 1536, loss is 0.0006520567694678903\n",
      "epoch: 8 step: 1537, loss is 0.00011997822002740577\n",
      "epoch: 8 step: 1538, loss is 0.001953212544322014\n",
      "epoch: 8 step: 1539, loss is 0.05850290507078171\n",
      "epoch: 8 step: 1540, loss is 0.0021125548519194126\n",
      "epoch: 8 step: 1541, loss is 0.0036451113410294056\n",
      "epoch: 8 step: 1542, loss is 0.011947151273488998\n",
      "epoch: 8 step: 1543, loss is 0.0002378796343691647\n",
      "epoch: 8 step: 1544, loss is 0.00117455271538347\n",
      "epoch: 8 step: 1545, loss is 0.3509601354598999\n",
      "epoch: 8 step: 1546, loss is 0.08082299679517746\n",
      "epoch: 8 step: 1547, loss is 0.10427728295326233\n",
      "epoch: 8 step: 1548, loss is 0.0034751929342746735\n",
      "epoch: 8 step: 1549, loss is 0.004006531089544296\n",
      "epoch: 8 step: 1550, loss is 0.003942576702684164\n",
      "epoch: 8 step: 1551, loss is 0.0026009194552898407\n",
      "epoch: 8 step: 1552, loss is 0.00015925662592053413\n",
      "epoch: 8 step: 1553, loss is 0.0026788460090756416\n",
      "epoch: 8 step: 1554, loss is 0.0070387255400419235\n",
      "epoch: 8 step: 1555, loss is 0.005396207328885794\n",
      "epoch: 8 step: 1556, loss is 0.050434380769729614\n",
      "epoch: 8 step: 1557, loss is 0.006667567882686853\n",
      "epoch: 8 step: 1558, loss is 0.08946762979030609\n",
      "epoch: 8 step: 1559, loss is 0.0006824618321843445\n",
      "epoch: 8 step: 1560, loss is 0.11381860077381134\n",
      "epoch: 8 step: 1561, loss is 0.10361344367265701\n",
      "epoch: 8 step: 1562, loss is 0.013938014395534992\n",
      "epoch: 8 step: 1563, loss is 0.10689904540777206\n",
      "epoch: 8 step: 1564, loss is 0.0006818155525252223\n",
      "epoch: 8 step: 1565, loss is 0.000870659714564681\n",
      "epoch: 8 step: 1566, loss is 0.0005872544134035707\n",
      "epoch: 8 step: 1567, loss is 0.0012623751536011696\n",
      "epoch: 8 step: 1568, loss is 0.16879750788211823\n",
      "epoch: 8 step: 1569, loss is 0.03078041411936283\n",
      "epoch: 8 step: 1570, loss is 0.004037052392959595\n",
      "epoch: 8 step: 1571, loss is 0.0009341635159216821\n",
      "epoch: 8 step: 1572, loss is 0.003165929578244686\n",
      "epoch: 8 step: 1573, loss is 0.016132136806845665\n",
      "epoch: 8 step: 1574, loss is 0.0010387258371338248\n",
      "epoch: 8 step: 1575, loss is 0.0010846559889614582\n",
      "epoch: 8 step: 1576, loss is 0.01692599058151245\n",
      "epoch: 8 step: 1577, loss is 0.02577533759176731\n",
      "epoch: 8 step: 1578, loss is 0.0015729076694697142\n",
      "epoch: 8 step: 1579, loss is 0.005453050136566162\n",
      "epoch: 8 step: 1580, loss is 0.022713568061590195\n",
      "epoch: 8 step: 1581, loss is 0.004651131574064493\n",
      "epoch: 8 step: 1582, loss is 0.005160462576895952\n",
      "epoch: 8 step: 1583, loss is 0.015209037810564041\n",
      "epoch: 8 step: 1584, loss is 0.05755385756492615\n",
      "epoch: 8 step: 1585, loss is 0.006636340171098709\n",
      "epoch: 8 step: 1586, loss is 0.036800894886255264\n",
      "epoch: 8 step: 1587, loss is 0.0004601858090609312\n",
      "epoch: 8 step: 1588, loss is 0.00836547464132309\n",
      "epoch: 8 step: 1589, loss is 0.0016721999272704124\n",
      "epoch: 8 step: 1590, loss is 0.005674795247614384\n",
      "epoch: 8 step: 1591, loss is 0.0061521586030721664\n",
      "epoch: 8 step: 1592, loss is 0.09231898933649063\n",
      "epoch: 8 step: 1593, loss is 0.007189895026385784\n",
      "epoch: 8 step: 1594, loss is 0.166535422205925\n",
      "epoch: 8 step: 1595, loss is 0.12943309545516968\n",
      "epoch: 8 step: 1596, loss is 0.14345726370811462\n",
      "epoch: 8 step: 1597, loss is 0.018842795863747597\n",
      "epoch: 8 step: 1598, loss is 0.004516498185694218\n",
      "epoch: 8 step: 1599, loss is 0.0004343428008724004\n",
      "epoch: 8 step: 1600, loss is 0.005226130597293377\n",
      "epoch: 8 step: 1601, loss is 0.003832950722426176\n",
      "epoch: 8 step: 1602, loss is 0.00860987976193428\n",
      "epoch: 8 step: 1603, loss is 0.060143839567899704\n",
      "epoch: 8 step: 1604, loss is 0.0010102224769070745\n",
      "epoch: 8 step: 1605, loss is 0.0018675500759854913\n",
      "epoch: 8 step: 1606, loss is 0.00304354983381927\n",
      "epoch: 8 step: 1607, loss is 0.00023618695558980107\n",
      "epoch: 8 step: 1608, loss is 0.0009171547135338187\n",
      "epoch: 8 step: 1609, loss is 0.012133634649217129\n",
      "epoch: 8 step: 1610, loss is 0.0036041487473994493\n",
      "epoch: 8 step: 1611, loss is 0.0032126409932971\n",
      "epoch: 8 step: 1612, loss is 0.017297856509685516\n",
      "epoch: 8 step: 1613, loss is 0.004687347449362278\n",
      "epoch: 8 step: 1614, loss is 0.0033654486760497093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 1615, loss is 0.09702064096927643\n",
      "epoch: 8 step: 1616, loss is 0.036461398005485535\n",
      "epoch: 8 step: 1617, loss is 0.008117865771055222\n",
      "epoch: 8 step: 1618, loss is 0.02047872170805931\n",
      "epoch: 8 step: 1619, loss is 0.011487199924886227\n",
      "epoch: 8 step: 1620, loss is 0.0012792523484677076\n",
      "epoch: 8 step: 1621, loss is 0.006088199559599161\n",
      "epoch: 8 step: 1622, loss is 0.016436198726296425\n",
      "epoch: 8 step: 1623, loss is 0.03632946312427521\n",
      "epoch: 8 step: 1624, loss is 0.0002250856632599607\n",
      "epoch: 8 step: 1625, loss is 0.037399083375930786\n",
      "epoch: 8 step: 1626, loss is 0.00025866329087875783\n",
      "epoch: 8 step: 1627, loss is 0.0053719328716397285\n",
      "epoch: 8 step: 1628, loss is 0.03133925050497055\n",
      "epoch: 8 step: 1629, loss is 0.06874972581863403\n",
      "epoch: 8 step: 1630, loss is 0.001778762903995812\n",
      "epoch: 8 step: 1631, loss is 0.27664005756378174\n",
      "epoch: 8 step: 1632, loss is 0.0012390307383611798\n",
      "epoch: 8 step: 1633, loss is 0.004668128676712513\n",
      "epoch: 8 step: 1634, loss is 0.007320478092879057\n",
      "epoch: 8 step: 1635, loss is 0.03450804948806763\n",
      "epoch: 8 step: 1636, loss is 0.005974743515253067\n",
      "epoch: 8 step: 1637, loss is 0.02216089330613613\n",
      "epoch: 8 step: 1638, loss is 0.002458189381286502\n",
      "epoch: 8 step: 1639, loss is 0.0006102581392042339\n",
      "epoch: 8 step: 1640, loss is 0.0007408856763504446\n",
      "epoch: 8 step: 1641, loss is 0.0003159135230816901\n",
      "epoch: 8 step: 1642, loss is 0.02611609362065792\n",
      "epoch: 8 step: 1643, loss is 0.0017255465500056744\n",
      "epoch: 8 step: 1644, loss is 0.06377872079610825\n",
      "epoch: 8 step: 1645, loss is 0.009805930778384209\n",
      "epoch: 8 step: 1646, loss is 0.0015250249998643994\n",
      "epoch: 8 step: 1647, loss is 0.012858407571911812\n",
      "epoch: 8 step: 1648, loss is 0.0046306876465678215\n",
      "epoch: 8 step: 1649, loss is 0.00923601258546114\n",
      "epoch: 8 step: 1650, loss is 0.08625295758247375\n",
      "epoch: 8 step: 1651, loss is 0.0009290811722166836\n",
      "epoch: 8 step: 1652, loss is 0.18092606961727142\n",
      "epoch: 8 step: 1653, loss is 0.00019928831898141652\n",
      "epoch: 8 step: 1654, loss is 0.0020257942378520966\n",
      "epoch: 8 step: 1655, loss is 0.006544237956404686\n",
      "epoch: 8 step: 1656, loss is 0.001518947072327137\n",
      "epoch: 8 step: 1657, loss is 0.023769229650497437\n",
      "epoch: 8 step: 1658, loss is 0.11865918338298798\n",
      "epoch: 8 step: 1659, loss is 0.0001390670659020543\n",
      "epoch: 8 step: 1660, loss is 0.011966566555202007\n",
      "epoch: 8 step: 1661, loss is 0.01970737800002098\n",
      "epoch: 8 step: 1662, loss is 0.12910997867584229\n",
      "epoch: 8 step: 1663, loss is 0.01581006869673729\n",
      "epoch: 8 step: 1664, loss is 0.00014518316311296076\n",
      "epoch: 8 step: 1665, loss is 0.007453171536326408\n",
      "epoch: 8 step: 1666, loss is 0.0012694651959463954\n",
      "epoch: 8 step: 1667, loss is 0.00010010749974753708\n",
      "epoch: 8 step: 1668, loss is 0.12010476738214493\n",
      "epoch: 8 step: 1669, loss is 0.027043776586651802\n",
      "epoch: 8 step: 1670, loss is 0.00041901718941517174\n",
      "epoch: 8 step: 1671, loss is 0.0012889289064332843\n",
      "epoch: 8 step: 1672, loss is 0.05179480090737343\n",
      "epoch: 8 step: 1673, loss is 0.0021014930680394173\n",
      "epoch: 8 step: 1674, loss is 0.004615881945937872\n",
      "epoch: 8 step: 1675, loss is 0.007903658784925938\n",
      "epoch: 8 step: 1676, loss is 0.009027013555169106\n",
      "epoch: 8 step: 1677, loss is 0.001360650290735066\n",
      "epoch: 8 step: 1678, loss is 0.03953180089592934\n",
      "epoch: 8 step: 1679, loss is 0.003428487107157707\n",
      "epoch: 8 step: 1680, loss is 0.03599081560969353\n",
      "epoch: 8 step: 1681, loss is 0.0026995523367077112\n",
      "epoch: 8 step: 1682, loss is 0.058656979352235794\n",
      "epoch: 8 step: 1683, loss is 0.005007901694625616\n",
      "epoch: 8 step: 1684, loss is 0.003041698597371578\n",
      "epoch: 8 step: 1685, loss is 0.0075997659005224705\n",
      "epoch: 8 step: 1686, loss is 0.042687349021434784\n",
      "epoch: 8 step: 1687, loss is 0.0018341740360483527\n",
      "epoch: 8 step: 1688, loss is 0.005541519727557898\n",
      "epoch: 8 step: 1689, loss is 0.0003317740047350526\n",
      "epoch: 8 step: 1690, loss is 0.0001256453397218138\n",
      "epoch: 8 step: 1691, loss is 0.002875162521377206\n",
      "epoch: 8 step: 1692, loss is 0.0012320444220677018\n",
      "epoch: 8 step: 1693, loss is 0.00284687802195549\n",
      "epoch: 8 step: 1694, loss is 0.00040451515815220773\n",
      "epoch: 8 step: 1695, loss is 0.0076679871417582035\n",
      "epoch: 8 step: 1696, loss is 0.017366573214530945\n",
      "epoch: 8 step: 1697, loss is 0.0023848728742450476\n",
      "epoch: 8 step: 1698, loss is 0.003919858951121569\n",
      "epoch: 8 step: 1699, loss is 0.013394130393862724\n",
      "epoch: 8 step: 1700, loss is 0.0007010430563241243\n",
      "epoch: 8 step: 1701, loss is 0.028922708705067635\n",
      "epoch: 8 step: 1702, loss is 0.006853163708001375\n",
      "epoch: 8 step: 1703, loss is 0.007325407117605209\n",
      "epoch: 8 step: 1704, loss is 0.00599801167845726\n",
      "epoch: 8 step: 1705, loss is 0.002942837541922927\n",
      "epoch: 8 step: 1706, loss is 0.0006983696948736906\n",
      "epoch: 8 step: 1707, loss is 0.00014059523527976125\n",
      "epoch: 8 step: 1708, loss is 0.10468515753746033\n",
      "epoch: 8 step: 1709, loss is 0.0013209065655246377\n",
      "epoch: 8 step: 1710, loss is 0.0001230348862009123\n",
      "epoch: 8 step: 1711, loss is 0.00472604064270854\n",
      "epoch: 8 step: 1712, loss is 0.0011053053895011544\n",
      "epoch: 8 step: 1713, loss is 0.00900967326015234\n",
      "epoch: 8 step: 1714, loss is 0.005488024093210697\n",
      "epoch: 8 step: 1715, loss is 0.12883348762989044\n",
      "epoch: 8 step: 1716, loss is 0.2677941918373108\n",
      "epoch: 8 step: 1717, loss is 0.0001932358427438885\n",
      "epoch: 8 step: 1718, loss is 0.0071818046271800995\n",
      "epoch: 8 step: 1719, loss is 0.0006017130217514932\n",
      "epoch: 8 step: 1720, loss is 0.01500427071005106\n",
      "epoch: 8 step: 1721, loss is 0.012353302910923958\n",
      "epoch: 8 step: 1722, loss is 0.21043212711811066\n",
      "epoch: 8 step: 1723, loss is 0.001395226689055562\n",
      "epoch: 8 step: 1724, loss is 0.06968168914318085\n",
      "epoch: 8 step: 1725, loss is 0.02449898235499859\n",
      "epoch: 8 step: 1726, loss is 0.0004033738514408469\n",
      "epoch: 8 step: 1727, loss is 0.006334841717034578\n",
      "epoch: 8 step: 1728, loss is 0.009501870721578598\n",
      "epoch: 8 step: 1729, loss is 0.0012477454729378223\n",
      "epoch: 8 step: 1730, loss is 0.000860974017996341\n",
      "epoch: 8 step: 1731, loss is 0.001638513640500605\n",
      "epoch: 8 step: 1732, loss is 0.023757336661219597\n",
      "epoch: 8 step: 1733, loss is 0.00170368910767138\n",
      "epoch: 8 step: 1734, loss is 0.08779443800449371\n",
      "epoch: 8 step: 1735, loss is 0.07365535199642181\n",
      "epoch: 8 step: 1736, loss is 0.005987559445202351\n",
      "epoch: 8 step: 1737, loss is 0.004137403331696987\n",
      "epoch: 8 step: 1738, loss is 0.0021074218675494194\n",
      "epoch: 8 step: 1739, loss is 0.045075673609972\n",
      "epoch: 8 step: 1740, loss is 0.005017691291868687\n",
      "epoch: 8 step: 1741, loss is 0.07403859496116638\n",
      "epoch: 8 step: 1742, loss is 0.009813989512622356\n",
      "epoch: 8 step: 1743, loss is 0.00925119686871767\n",
      "epoch: 8 step: 1744, loss is 0.1921500861644745\n",
      "epoch: 8 step: 1745, loss is 0.001566131366416812\n",
      "epoch: 8 step: 1746, loss is 5.192821117816493e-05\n",
      "epoch: 8 step: 1747, loss is 0.026277752593159676\n",
      "epoch: 8 step: 1748, loss is 0.0005157858831807971\n",
      "epoch: 8 step: 1749, loss is 0.002107325242832303\n",
      "epoch: 8 step: 1750, loss is 0.002742969896644354\n",
      "epoch: 8 step: 1751, loss is 0.07433413714170456\n",
      "epoch: 8 step: 1752, loss is 0.006512146443128586\n",
      "epoch: 8 step: 1753, loss is 0.007663194090127945\n",
      "epoch: 8 step: 1754, loss is 0.011518925428390503\n",
      "epoch: 8 step: 1755, loss is 0.006137724034488201\n",
      "epoch: 8 step: 1756, loss is 0.0036140454467386007\n",
      "epoch: 8 step: 1757, loss is 0.0011836881749331951\n",
      "epoch: 8 step: 1758, loss is 0.002387543208897114\n",
      "epoch: 8 step: 1759, loss is 0.0006603118381462991\n",
      "epoch: 8 step: 1760, loss is 0.0010597413638606668\n",
      "epoch: 8 step: 1761, loss is 0.018490495160222054\n",
      "epoch: 8 step: 1762, loss is 0.34689152240753174\n",
      "epoch: 8 step: 1763, loss is 0.002820944180712104\n",
      "epoch: 8 step: 1764, loss is 0.00760680390521884\n",
      "epoch: 8 step: 1765, loss is 0.007115754298865795\n",
      "epoch: 8 step: 1766, loss is 0.0002130437787855044\n",
      "epoch: 8 step: 1767, loss is 0.0013423084747046232\n",
      "epoch: 8 step: 1768, loss is 0.20400647819042206\n",
      "epoch: 8 step: 1769, loss is 0.004619406070560217\n",
      "epoch: 8 step: 1770, loss is 0.03459843620657921\n",
      "epoch: 8 step: 1771, loss is 0.00039230502443388104\n",
      "epoch: 8 step: 1772, loss is 0.01034531369805336\n",
      "epoch: 8 step: 1773, loss is 0.0011813222663477063\n",
      "epoch: 8 step: 1774, loss is 0.009916115552186966\n",
      "epoch: 8 step: 1775, loss is 0.0017993319779634476\n",
      "epoch: 8 step: 1776, loss is 0.00034945207880809903\n",
      "epoch: 8 step: 1777, loss is 0.0013610461028292775\n",
      "epoch: 8 step: 1778, loss is 0.020805079489946365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 1779, loss is 0.00046126393135637045\n",
      "epoch: 8 step: 1780, loss is 0.02129843272268772\n",
      "epoch: 8 step: 1781, loss is 0.03558143600821495\n",
      "epoch: 8 step: 1782, loss is 0.00014688255032524467\n",
      "epoch: 8 step: 1783, loss is 0.01068807952105999\n",
      "epoch: 8 step: 1784, loss is 5.760719068348408e-05\n",
      "epoch: 8 step: 1785, loss is 0.018204765394330025\n",
      "epoch: 8 step: 1786, loss is 0.02932312712073326\n",
      "epoch: 8 step: 1787, loss is 0.0011360114440321922\n",
      "epoch: 8 step: 1788, loss is 0.0002929650654550642\n",
      "epoch: 8 step: 1789, loss is 6.112161645432934e-05\n",
      "epoch: 8 step: 1790, loss is 0.014151511713862419\n",
      "epoch: 8 step: 1791, loss is 0.007131163962185383\n",
      "epoch: 8 step: 1792, loss is 0.004283041227608919\n",
      "epoch: 8 step: 1793, loss is 0.0077545009553432465\n",
      "epoch: 8 step: 1794, loss is 0.007837076671421528\n",
      "epoch: 8 step: 1795, loss is 0.00137574621476233\n",
      "epoch: 8 step: 1796, loss is 0.0018110225209966302\n",
      "epoch: 8 step: 1797, loss is 0.007593792397528887\n",
      "epoch: 8 step: 1798, loss is 0.009799751453101635\n",
      "epoch: 8 step: 1799, loss is 0.0013142690295353532\n",
      "epoch: 8 step: 1800, loss is 0.004868990741670132\n",
      "epoch: 8 step: 1801, loss is 0.00012831648928113282\n",
      "epoch: 8 step: 1802, loss is 0.03471684455871582\n",
      "epoch: 8 step: 1803, loss is 0.0006263044197112322\n",
      "epoch: 8 step: 1804, loss is 0.010855426080524921\n",
      "epoch: 8 step: 1805, loss is 0.025265498086810112\n",
      "epoch: 8 step: 1806, loss is 0.019954649731516838\n",
      "epoch: 8 step: 1807, loss is 0.0020483406260609627\n",
      "epoch: 8 step: 1808, loss is 0.01575077697634697\n",
      "epoch: 8 step: 1809, loss is 0.0016325798351317644\n",
      "epoch: 8 step: 1810, loss is 0.0028602955862879753\n",
      "epoch: 8 step: 1811, loss is 0.00018877570983022451\n",
      "epoch: 8 step: 1812, loss is 0.0011414935579523444\n",
      "epoch: 8 step: 1813, loss is 0.0002386627165833488\n",
      "epoch: 8 step: 1814, loss is 0.0005116302636452019\n",
      "epoch: 8 step: 1815, loss is 0.001164285815320909\n",
      "epoch: 8 step: 1816, loss is 0.0001465795940021053\n",
      "epoch: 8 step: 1817, loss is 0.0016156771453097463\n",
      "epoch: 8 step: 1818, loss is 0.01962023414671421\n",
      "epoch: 8 step: 1819, loss is 7.890389497333672e-06\n",
      "epoch: 8 step: 1820, loss is 0.01175813376903534\n",
      "epoch: 8 step: 1821, loss is 0.061214253306388855\n",
      "epoch: 8 step: 1822, loss is 0.0003476454003248364\n",
      "epoch: 8 step: 1823, loss is 0.001990536693483591\n",
      "epoch: 8 step: 1824, loss is 6.307462899712846e-05\n",
      "epoch: 8 step: 1825, loss is 0.0004589251766446978\n",
      "epoch: 8 step: 1826, loss is 0.024216018617153168\n",
      "epoch: 8 step: 1827, loss is 0.0009223285596817732\n",
      "epoch: 8 step: 1828, loss is 0.001652990933507681\n",
      "epoch: 8 step: 1829, loss is 6.67497661197558e-05\n",
      "epoch: 8 step: 1830, loss is 0.0005333737935870886\n",
      "epoch: 8 step: 1831, loss is 0.0012513650581240654\n",
      "epoch: 8 step: 1832, loss is 0.0003364976728335023\n",
      "epoch: 8 step: 1833, loss is 0.021539079025387764\n",
      "epoch: 8 step: 1834, loss is 0.00012819783296436071\n",
      "epoch: 8 step: 1835, loss is 0.006129266694188118\n",
      "epoch: 8 step: 1836, loss is 0.04574170336127281\n",
      "epoch: 8 step: 1837, loss is 0.0012879505520686507\n",
      "epoch: 8 step: 1838, loss is 0.18000805377960205\n",
      "epoch: 8 step: 1839, loss is 0.014195112511515617\n",
      "epoch: 8 step: 1840, loss is 0.004566599149256945\n",
      "epoch: 8 step: 1841, loss is 0.0020614364184439182\n",
      "epoch: 8 step: 1842, loss is 0.009887718595564365\n",
      "epoch: 8 step: 1843, loss is 0.008479420095682144\n",
      "epoch: 8 step: 1844, loss is 0.0012180842459201813\n",
      "epoch: 8 step: 1845, loss is 0.00946885161101818\n",
      "epoch: 8 step: 1846, loss is 0.040029026567935944\n",
      "epoch: 8 step: 1847, loss is 0.04609761759638786\n",
      "epoch: 8 step: 1848, loss is 0.0005188839859329164\n",
      "epoch: 8 step: 1849, loss is 0.012686723843216896\n",
      "epoch: 8 step: 1850, loss is 0.0068411268293857574\n",
      "epoch: 8 step: 1851, loss is 0.0033449751790612936\n",
      "epoch: 8 step: 1852, loss is 0.000367716362234205\n",
      "epoch: 8 step: 1853, loss is 0.0013132118619978428\n",
      "epoch: 8 step: 1854, loss is 0.001933208666741848\n",
      "epoch: 8 step: 1855, loss is 0.01265083346515894\n",
      "epoch: 8 step: 1856, loss is 0.09909836947917938\n",
      "epoch: 8 step: 1857, loss is 0.012429985217750072\n",
      "epoch: 8 step: 1858, loss is 0.05010277032852173\n",
      "epoch: 8 step: 1859, loss is 0.00039870833279564977\n",
      "epoch: 8 step: 1860, loss is 0.0002423031983198598\n",
      "epoch: 8 step: 1861, loss is 0.032948367297649384\n",
      "epoch: 8 step: 1862, loss is 7.79807087383233e-05\n",
      "epoch: 8 step: 1863, loss is 0.0019359213765710592\n",
      "epoch: 8 step: 1864, loss is 0.025711845606565475\n",
      "epoch: 8 step: 1865, loss is 0.006541996728628874\n",
      "epoch: 8 step: 1866, loss is 9.333035268355161e-05\n",
      "epoch: 8 step: 1867, loss is 0.000952451431658119\n",
      "epoch: 8 step: 1868, loss is 0.00120284513104707\n",
      "epoch: 8 step: 1869, loss is 0.0004597242805175483\n",
      "epoch: 8 step: 1870, loss is 0.04907740652561188\n",
      "epoch: 8 step: 1871, loss is 0.006486068945378065\n",
      "epoch: 8 step: 1872, loss is 0.00249851169064641\n",
      "epoch: 8 step: 1873, loss is 0.0032445802353322506\n",
      "epoch: 8 step: 1874, loss is 0.010132280178368092\n",
      "epoch: 8 step: 1875, loss is 0.0024491185322403908\n",
      "epoch: 9 step: 1, loss is 0.020349718630313873\n",
      "epoch: 9 step: 2, loss is 0.00023116235388442874\n",
      "epoch: 9 step: 3, loss is 0.0029092556796967983\n",
      "epoch: 9 step: 4, loss is 0.1800985187292099\n",
      "epoch: 9 step: 5, loss is 0.00573973823338747\n",
      "epoch: 9 step: 6, loss is 8.580207941122353e-05\n",
      "epoch: 9 step: 7, loss is 0.002242738613858819\n",
      "epoch: 9 step: 8, loss is 0.00012761648395098746\n",
      "epoch: 9 step: 9, loss is 0.0024203970097005367\n",
      "epoch: 9 step: 10, loss is 0.006202007178217173\n",
      "epoch: 9 step: 11, loss is 0.0004007302923128009\n",
      "epoch: 9 step: 12, loss is 7.204350549727678e-05\n",
      "epoch: 9 step: 13, loss is 0.02775849774479866\n",
      "epoch: 9 step: 14, loss is 0.001033375272527337\n",
      "epoch: 9 step: 15, loss is 0.00411608861759305\n",
      "epoch: 9 step: 16, loss is 0.002230488695204258\n",
      "epoch: 9 step: 17, loss is 0.000192088438780047\n",
      "epoch: 9 step: 18, loss is 0.047129251062870026\n",
      "epoch: 9 step: 19, loss is 0.00012376575614325702\n",
      "epoch: 9 step: 20, loss is 0.001335661974735558\n",
      "epoch: 9 step: 21, loss is 0.002923325402662158\n",
      "epoch: 9 step: 22, loss is 0.00013550135190598667\n",
      "epoch: 9 step: 23, loss is 0.006299332249909639\n",
      "epoch: 9 step: 24, loss is 0.008289754390716553\n",
      "epoch: 9 step: 25, loss is 0.0013770201476290822\n",
      "epoch: 9 step: 26, loss is 0.0020147638861089945\n",
      "epoch: 9 step: 27, loss is 2.000177846639417e-05\n",
      "epoch: 9 step: 28, loss is 0.0004536939086392522\n",
      "epoch: 9 step: 29, loss is 0.00315263494849205\n",
      "epoch: 9 step: 30, loss is 0.014311067759990692\n",
      "epoch: 9 step: 31, loss is 0.005287521053105593\n",
      "epoch: 9 step: 32, loss is 0.00031162388040684164\n",
      "epoch: 9 step: 33, loss is 3.7830752262379974e-05\n",
      "epoch: 9 step: 34, loss is 0.0010471041314303875\n",
      "epoch: 9 step: 35, loss is 0.00010927858238574117\n",
      "epoch: 9 step: 36, loss is 0.014512556605041027\n",
      "epoch: 9 step: 37, loss is 3.724752605194226e-05\n",
      "epoch: 9 step: 38, loss is 0.0006720450473949313\n",
      "epoch: 9 step: 39, loss is 0.0028782719746232033\n",
      "epoch: 9 step: 40, loss is 0.0006921876920387149\n",
      "epoch: 9 step: 41, loss is 3.7578931369353086e-05\n",
      "epoch: 9 step: 42, loss is 0.00018330698367208242\n",
      "epoch: 9 step: 43, loss is 0.0002699175965972245\n",
      "epoch: 9 step: 44, loss is 0.057493239641189575\n",
      "epoch: 9 step: 45, loss is 0.002021577674895525\n",
      "epoch: 9 step: 46, loss is 0.0001089072393369861\n",
      "epoch: 9 step: 47, loss is 0.0052076829597353935\n",
      "epoch: 9 step: 48, loss is 0.00018930323130916804\n",
      "epoch: 9 step: 49, loss is 0.0050437916070222855\n",
      "epoch: 9 step: 50, loss is 0.00807900819927454\n",
      "epoch: 9 step: 51, loss is 0.006404101382941008\n",
      "epoch: 9 step: 52, loss is 0.0001368500670650974\n",
      "epoch: 9 step: 53, loss is 4.340042323747184e-06\n",
      "epoch: 9 step: 54, loss is 0.00023855942708905786\n",
      "epoch: 9 step: 55, loss is 0.00016970615251921117\n",
      "epoch: 9 step: 56, loss is 0.002267711330205202\n",
      "epoch: 9 step: 57, loss is 0.0236655380576849\n",
      "epoch: 9 step: 58, loss is 0.0009042742312885821\n",
      "epoch: 9 step: 59, loss is 0.004225722514092922\n",
      "epoch: 9 step: 60, loss is 0.02015766128897667\n",
      "epoch: 9 step: 61, loss is 0.00017917060176841915\n",
      "epoch: 9 step: 62, loss is 0.0007838173187337816\n",
      "epoch: 9 step: 63, loss is 0.00031768830376677215\n",
      "epoch: 9 step: 64, loss is 0.0008325608214363456\n",
      "epoch: 9 step: 65, loss is 0.0026811000425368547\n",
      "epoch: 9 step: 66, loss is 0.004751402419060469\n",
      "epoch: 9 step: 67, loss is 0.0009220822248607874\n",
      "epoch: 9 step: 68, loss is 2.4162320187315345e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 69, loss is 0.002752862870693207\n",
      "epoch: 9 step: 70, loss is 0.0018485591281205416\n",
      "epoch: 9 step: 71, loss is 8.045296999625862e-05\n",
      "epoch: 9 step: 72, loss is 0.00428211921826005\n",
      "epoch: 9 step: 73, loss is 0.048747677356004715\n",
      "epoch: 9 step: 74, loss is 0.012422822415828705\n",
      "epoch: 9 step: 75, loss is 0.020642146468162537\n",
      "epoch: 9 step: 76, loss is 0.010521863587200642\n",
      "epoch: 9 step: 77, loss is 0.0003053656255360693\n",
      "epoch: 9 step: 78, loss is 0.059230681508779526\n",
      "epoch: 9 step: 79, loss is 0.00042351410957053304\n",
      "epoch: 9 step: 80, loss is 0.0008187981438823044\n",
      "epoch: 9 step: 81, loss is 0.0005899542011320591\n",
      "epoch: 9 step: 82, loss is 0.0019346001790836453\n",
      "epoch: 9 step: 83, loss is 0.00037070942926220596\n",
      "epoch: 9 step: 84, loss is 0.003925940487533808\n",
      "epoch: 9 step: 85, loss is 0.00015244934184011072\n",
      "epoch: 9 step: 86, loss is 4.845790681429207e-05\n",
      "epoch: 9 step: 87, loss is 1.3856155419489369e-05\n",
      "epoch: 9 step: 88, loss is 0.00022754495148546994\n",
      "epoch: 9 step: 89, loss is 0.04690179228782654\n",
      "epoch: 9 step: 90, loss is 0.01458176039159298\n",
      "epoch: 9 step: 91, loss is 0.00020164453599136323\n",
      "epoch: 9 step: 92, loss is 0.00037238592631183565\n",
      "epoch: 9 step: 93, loss is 0.00048332850565202534\n",
      "epoch: 9 step: 94, loss is 0.003939367830753326\n",
      "epoch: 9 step: 95, loss is 0.0002613199467305094\n",
      "epoch: 9 step: 96, loss is 0.02323310449719429\n",
      "epoch: 9 step: 97, loss is 0.00015311794413719326\n",
      "epoch: 9 step: 98, loss is 0.00044794793939217925\n",
      "epoch: 9 step: 99, loss is 0.12290679663419724\n",
      "epoch: 9 step: 100, loss is 4.463843652047217e-05\n",
      "epoch: 9 step: 101, loss is 0.002635637065395713\n",
      "epoch: 9 step: 102, loss is 0.0018918317509815097\n",
      "epoch: 9 step: 103, loss is 0.024568887427449226\n",
      "epoch: 9 step: 104, loss is 2.0911234969389625e-05\n",
      "epoch: 9 step: 105, loss is 0.2965482473373413\n",
      "epoch: 9 step: 106, loss is 0.002537506865337491\n",
      "epoch: 9 step: 107, loss is 0.004795154556632042\n",
      "epoch: 9 step: 108, loss is 0.030043931677937508\n",
      "epoch: 9 step: 109, loss is 0.0033387376461178064\n",
      "epoch: 9 step: 110, loss is 1.5263207387761213e-05\n",
      "epoch: 9 step: 111, loss is 0.033084213733673096\n",
      "epoch: 9 step: 112, loss is 6.881431181682274e-05\n",
      "epoch: 9 step: 113, loss is 0.00021482868760358542\n",
      "epoch: 9 step: 114, loss is 0.015437918715178967\n",
      "epoch: 9 step: 115, loss is 0.0001846171508077532\n",
      "epoch: 9 step: 116, loss is 0.010007670149207115\n",
      "epoch: 9 step: 117, loss is 0.0006981270271353424\n",
      "epoch: 9 step: 118, loss is 0.002317504957318306\n",
      "epoch: 9 step: 119, loss is 0.0648825615644455\n",
      "epoch: 9 step: 120, loss is 0.0028393673710525036\n",
      "epoch: 9 step: 121, loss is 0.0069286939688026905\n",
      "epoch: 9 step: 122, loss is 0.0013468819670379162\n",
      "epoch: 9 step: 123, loss is 0.00335383671335876\n",
      "epoch: 9 step: 124, loss is 0.0010459097102284431\n",
      "epoch: 9 step: 125, loss is 0.0005038214731030166\n",
      "epoch: 9 step: 126, loss is 0.0002132211666321382\n",
      "epoch: 9 step: 127, loss is 0.13657839596271515\n",
      "epoch: 9 step: 128, loss is 0.015339896082878113\n",
      "epoch: 9 step: 129, loss is 0.008967462927103043\n",
      "epoch: 9 step: 130, loss is 0.0019130943110212684\n",
      "epoch: 9 step: 131, loss is 0.05841105058789253\n",
      "epoch: 9 step: 132, loss is 0.00014987860049586743\n",
      "epoch: 9 step: 133, loss is 0.000907495035789907\n",
      "epoch: 9 step: 134, loss is 0.0008448947919532657\n",
      "epoch: 9 step: 135, loss is 9.427749319002032e-05\n",
      "epoch: 9 step: 136, loss is 0.00020988429605495185\n",
      "epoch: 9 step: 137, loss is 0.00032120500691235065\n",
      "epoch: 9 step: 138, loss is 0.0371197834610939\n",
      "epoch: 9 step: 139, loss is 0.052169132977724075\n",
      "epoch: 9 step: 140, loss is 0.0001810290850698948\n",
      "epoch: 9 step: 141, loss is 0.002365336287766695\n",
      "epoch: 9 step: 142, loss is 0.021490072831511497\n",
      "epoch: 9 step: 143, loss is 0.049759089946746826\n",
      "epoch: 9 step: 144, loss is 0.0005641293828375638\n",
      "epoch: 9 step: 145, loss is 6.962630868656561e-05\n",
      "epoch: 9 step: 146, loss is 0.002099993173032999\n",
      "epoch: 9 step: 147, loss is 0.0015299224760383368\n",
      "epoch: 9 step: 148, loss is 0.00883407611399889\n",
      "epoch: 9 step: 149, loss is 0.00019155391782987863\n",
      "epoch: 9 step: 150, loss is 0.0017403023084625602\n",
      "epoch: 9 step: 151, loss is 0.003113723360002041\n",
      "epoch: 9 step: 152, loss is 0.002764446660876274\n",
      "epoch: 9 step: 153, loss is 0.03519760072231293\n",
      "epoch: 9 step: 154, loss is 0.0012796205701306462\n",
      "epoch: 9 step: 155, loss is 0.00036315066972747445\n",
      "epoch: 9 step: 156, loss is 0.0005890839966014028\n",
      "epoch: 9 step: 157, loss is 0.00019598094513639808\n",
      "epoch: 9 step: 158, loss is 0.03630819544196129\n",
      "epoch: 9 step: 159, loss is 0.0007909879204817116\n",
      "epoch: 9 step: 160, loss is 0.1399437040090561\n",
      "epoch: 9 step: 161, loss is 0.0006225697579793632\n",
      "epoch: 9 step: 162, loss is 0.0015764677664265037\n",
      "epoch: 9 step: 163, loss is 0.0004267016192898154\n",
      "epoch: 9 step: 164, loss is 0.0006490241503342986\n",
      "epoch: 9 step: 165, loss is 0.0003877846465911716\n",
      "epoch: 9 step: 166, loss is 0.00034401900484226644\n",
      "epoch: 9 step: 167, loss is 0.00010543708776822314\n",
      "epoch: 9 step: 168, loss is 0.00029690933297388256\n",
      "epoch: 9 step: 169, loss is 0.0007495016907341778\n",
      "epoch: 9 step: 170, loss is 0.025973428040742874\n",
      "epoch: 9 step: 171, loss is 0.0004383610503282398\n",
      "epoch: 9 step: 172, loss is 4.3495118006831035e-05\n",
      "epoch: 9 step: 173, loss is 0.06974281370639801\n",
      "epoch: 9 step: 174, loss is 0.0001531459274701774\n",
      "epoch: 9 step: 175, loss is 0.0010027065873146057\n",
      "epoch: 9 step: 176, loss is 0.07010673731565475\n",
      "epoch: 9 step: 177, loss is 0.00039531057700514793\n",
      "epoch: 9 step: 178, loss is 6.385672895703465e-05\n",
      "epoch: 9 step: 179, loss is 6.127045344328508e-05\n",
      "epoch: 9 step: 180, loss is 0.0003406851610634476\n",
      "epoch: 9 step: 181, loss is 0.018390804529190063\n",
      "epoch: 9 step: 182, loss is 0.0020414586178958416\n",
      "epoch: 9 step: 183, loss is 0.0014000260271131992\n",
      "epoch: 9 step: 184, loss is 5.4387775890063494e-05\n",
      "epoch: 9 step: 185, loss is 7.064721285132691e-05\n",
      "epoch: 9 step: 186, loss is 0.0032737916335463524\n",
      "epoch: 9 step: 187, loss is 0.015926845371723175\n",
      "epoch: 9 step: 188, loss is 0.00267776264809072\n",
      "epoch: 9 step: 189, loss is 0.01289194356650114\n",
      "epoch: 9 step: 190, loss is 0.002883798908442259\n",
      "epoch: 9 step: 191, loss is 0.16472148895263672\n",
      "epoch: 9 step: 192, loss is 0.004927009344100952\n",
      "epoch: 9 step: 193, loss is 0.00432473374530673\n",
      "epoch: 9 step: 194, loss is 0.00039617903530597687\n",
      "epoch: 9 step: 195, loss is 0.0010467839892953634\n",
      "epoch: 9 step: 196, loss is 0.0010994289768859744\n",
      "epoch: 9 step: 197, loss is 0.004511138889938593\n",
      "epoch: 9 step: 198, loss is 0.0012896760599687696\n",
      "epoch: 9 step: 199, loss is 0.010451821610331535\n",
      "epoch: 9 step: 200, loss is 0.10055852681398392\n",
      "epoch: 9 step: 201, loss is 9.822368883760646e-05\n",
      "epoch: 9 step: 202, loss is 0.0300492811948061\n",
      "epoch: 9 step: 203, loss is 0.006050412077456713\n",
      "epoch: 9 step: 204, loss is 0.0005365420365706086\n",
      "epoch: 9 step: 205, loss is 0.011879154480993748\n",
      "epoch: 9 step: 206, loss is 0.006869906093925238\n",
      "epoch: 9 step: 207, loss is 0.0003922864270862192\n",
      "epoch: 9 step: 208, loss is 0.002202936913818121\n",
      "epoch: 9 step: 209, loss is 0.0017043623374775052\n",
      "epoch: 9 step: 210, loss is 0.006513538304716349\n",
      "epoch: 9 step: 211, loss is 0.028049824759364128\n",
      "epoch: 9 step: 212, loss is 0.07845646888017654\n",
      "epoch: 9 step: 213, loss is 0.09371183812618256\n",
      "epoch: 9 step: 214, loss is 0.0005558725097216666\n",
      "epoch: 9 step: 215, loss is 0.0026336642913520336\n",
      "epoch: 9 step: 216, loss is 0.0015455021057277918\n",
      "epoch: 9 step: 217, loss is 0.006648088805377483\n",
      "epoch: 9 step: 218, loss is 0.00018875485693570226\n",
      "epoch: 9 step: 219, loss is 0.0029512669425457716\n",
      "epoch: 9 step: 220, loss is 0.0002458732924424112\n",
      "epoch: 9 step: 221, loss is 0.026378516107797623\n",
      "epoch: 9 step: 222, loss is 0.09396995604038239\n",
      "epoch: 9 step: 223, loss is 0.047300249338150024\n",
      "epoch: 9 step: 224, loss is 0.0021320541854947805\n",
      "epoch: 9 step: 225, loss is 0.0008041943656280637\n",
      "epoch: 9 step: 226, loss is 0.0005043116980232298\n",
      "epoch: 9 step: 227, loss is 0.010670740157365799\n",
      "epoch: 9 step: 228, loss is 0.000397112948121503\n",
      "epoch: 9 step: 229, loss is 0.001959463581442833\n",
      "epoch: 9 step: 230, loss is 0.0016656240914016962\n",
      "epoch: 9 step: 231, loss is 0.0005637715803459287\n",
      "epoch: 9 step: 232, loss is 0.01714562438428402\n",
      "epoch: 9 step: 233, loss is 0.0013874453725293279\n",
      "epoch: 9 step: 234, loss is 0.07403413206338882\n",
      "epoch: 9 step: 235, loss is 0.004574075806885958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 236, loss is 0.002525022719055414\n",
      "epoch: 9 step: 237, loss is 0.044202450662851334\n",
      "epoch: 9 step: 238, loss is 0.0018968075746670365\n",
      "epoch: 9 step: 239, loss is 8.739014447201043e-05\n",
      "epoch: 9 step: 240, loss is 0.011626786552369595\n",
      "epoch: 9 step: 241, loss is 0.00040412385715171695\n",
      "epoch: 9 step: 242, loss is 0.0028484410140663385\n",
      "epoch: 9 step: 243, loss is 0.007903289049863815\n",
      "epoch: 9 step: 244, loss is 0.04178305342793465\n",
      "epoch: 9 step: 245, loss is 0.0019065404776483774\n",
      "epoch: 9 step: 246, loss is 0.030914148315787315\n",
      "epoch: 9 step: 247, loss is 0.000667436164803803\n",
      "epoch: 9 step: 248, loss is 0.0003157866303808987\n",
      "epoch: 9 step: 249, loss is 8.206009806599468e-05\n",
      "epoch: 9 step: 250, loss is 0.0056170253083109856\n",
      "epoch: 9 step: 251, loss is 0.0012410293566063046\n",
      "epoch: 9 step: 252, loss is 0.02073996141552925\n",
      "epoch: 9 step: 253, loss is 0.03485209494829178\n",
      "epoch: 9 step: 254, loss is 0.0014457221841439605\n",
      "epoch: 9 step: 255, loss is 0.024500612169504166\n",
      "epoch: 9 step: 256, loss is 0.0029429199639707804\n",
      "epoch: 9 step: 257, loss is 0.009520025923848152\n",
      "epoch: 9 step: 258, loss is 0.0011182508897036314\n",
      "epoch: 9 step: 259, loss is 0.00044045248068869114\n",
      "epoch: 9 step: 260, loss is 5.915348447160795e-05\n",
      "epoch: 9 step: 261, loss is 0.006021510809659958\n",
      "epoch: 9 step: 262, loss is 0.00012208399130031466\n",
      "epoch: 9 step: 263, loss is 0.00019074992451351136\n",
      "epoch: 9 step: 264, loss is 0.006486013997346163\n",
      "epoch: 9 step: 265, loss is 0.10265437513589859\n",
      "epoch: 9 step: 266, loss is 0.0019101905636489391\n",
      "epoch: 9 step: 267, loss is 0.0008616589475423098\n",
      "epoch: 9 step: 268, loss is 0.032295916229486465\n",
      "epoch: 9 step: 269, loss is 0.00041457804036326706\n",
      "epoch: 9 step: 270, loss is 0.0031383719760924578\n",
      "epoch: 9 step: 271, loss is 0.004729283042252064\n",
      "epoch: 9 step: 272, loss is 0.0003857365227304399\n",
      "epoch: 9 step: 273, loss is 0.00371372839435935\n",
      "epoch: 9 step: 274, loss is 4.200889452476986e-05\n",
      "epoch: 9 step: 275, loss is 0.004195051267743111\n",
      "epoch: 9 step: 276, loss is 0.0035165457520633936\n",
      "epoch: 9 step: 277, loss is 0.0010423768544569612\n",
      "epoch: 9 step: 278, loss is 0.00031036222935654223\n",
      "epoch: 9 step: 279, loss is 0.0031588783022016287\n",
      "epoch: 9 step: 280, loss is 0.0003883178869727999\n",
      "epoch: 9 step: 281, loss is 0.00024345923156943172\n",
      "epoch: 9 step: 282, loss is 0.00048549065832048655\n",
      "epoch: 9 step: 283, loss is 0.013104111887514591\n",
      "epoch: 9 step: 284, loss is 0.009096158668398857\n",
      "epoch: 9 step: 285, loss is 0.00015141908079385757\n",
      "epoch: 9 step: 286, loss is 0.06958742439746857\n",
      "epoch: 9 step: 287, loss is 0.0014756553573533893\n",
      "epoch: 9 step: 288, loss is 0.01605900004506111\n",
      "epoch: 9 step: 289, loss is 0.024755027145147324\n",
      "epoch: 9 step: 290, loss is 0.015554198995232582\n",
      "epoch: 9 step: 291, loss is 0.0023588170297443867\n",
      "epoch: 9 step: 292, loss is 0.10867145657539368\n",
      "epoch: 9 step: 293, loss is 0.03468061611056328\n",
      "epoch: 9 step: 294, loss is 0.012867320328950882\n",
      "epoch: 9 step: 295, loss is 0.004606776405125856\n",
      "epoch: 9 step: 296, loss is 0.0014530187472701073\n",
      "epoch: 9 step: 297, loss is 5.498642622114858e-06\n",
      "epoch: 9 step: 298, loss is 0.03178057447075844\n",
      "epoch: 9 step: 299, loss is 0.009522365406155586\n",
      "epoch: 9 step: 300, loss is 0.007954573258757591\n",
      "epoch: 9 step: 301, loss is 0.0011180519359186292\n",
      "epoch: 9 step: 302, loss is 0.0118643157184124\n",
      "epoch: 9 step: 303, loss is 0.00013864683569408953\n",
      "epoch: 9 step: 304, loss is 0.004562309943139553\n",
      "epoch: 9 step: 305, loss is 0.03491929918527603\n",
      "epoch: 9 step: 306, loss is 0.04763483256101608\n",
      "epoch: 9 step: 307, loss is 0.0016666620504111052\n",
      "epoch: 9 step: 308, loss is 0.0011320163030177355\n",
      "epoch: 9 step: 309, loss is 0.0004549404256977141\n",
      "epoch: 9 step: 310, loss is 0.017077500000596046\n",
      "epoch: 9 step: 311, loss is 0.009688650257885456\n",
      "epoch: 9 step: 312, loss is 0.00012897609849460423\n",
      "epoch: 9 step: 313, loss is 0.00032394766458310187\n",
      "epoch: 9 step: 314, loss is 0.007943369448184967\n",
      "epoch: 9 step: 315, loss is 0.0029058060608804226\n",
      "epoch: 9 step: 316, loss is 7.376453140750527e-05\n",
      "epoch: 9 step: 317, loss is 0.003785261884331703\n",
      "epoch: 9 step: 318, loss is 0.003995942883193493\n",
      "epoch: 9 step: 319, loss is 5.853477705386467e-05\n",
      "epoch: 9 step: 320, loss is 0.0007519596256315708\n",
      "epoch: 9 step: 321, loss is 0.0030920556746423244\n",
      "epoch: 9 step: 322, loss is 0.0017571626231074333\n",
      "epoch: 9 step: 323, loss is 0.0010471040150150657\n",
      "epoch: 9 step: 324, loss is 0.006799312774091959\n",
      "epoch: 9 step: 325, loss is 0.08064503222703934\n",
      "epoch: 9 step: 326, loss is 0.01576251909136772\n",
      "epoch: 9 step: 327, loss is 0.0005843436229042709\n",
      "epoch: 9 step: 328, loss is 0.03245730325579643\n",
      "epoch: 9 step: 329, loss is 0.00012133202835684642\n",
      "epoch: 9 step: 330, loss is 1.1533933502505533e-05\n",
      "epoch: 9 step: 331, loss is 5.5340362450806424e-05\n",
      "epoch: 9 step: 332, loss is 0.0012698398204520345\n",
      "epoch: 9 step: 333, loss is 1.3864923857909162e-05\n",
      "epoch: 9 step: 334, loss is 0.0005207954091019928\n",
      "epoch: 9 step: 335, loss is 0.00012512053945101798\n",
      "epoch: 9 step: 336, loss is 0.00342977955006063\n",
      "epoch: 9 step: 337, loss is 0.0014816150069236755\n",
      "epoch: 9 step: 338, loss is 0.0003838871489278972\n",
      "epoch: 9 step: 339, loss is 0.0009798371465876698\n",
      "epoch: 9 step: 340, loss is 8.172580419341102e-05\n",
      "epoch: 9 step: 341, loss is 0.002409429755061865\n",
      "epoch: 9 step: 342, loss is 5.2372124628163874e-05\n",
      "epoch: 9 step: 343, loss is 0.017287006601691246\n",
      "epoch: 9 step: 344, loss is 0.021266944706439972\n",
      "epoch: 9 step: 345, loss is 4.206530502415262e-05\n",
      "epoch: 9 step: 346, loss is 0.0002787162666209042\n",
      "epoch: 9 step: 347, loss is 0.0009250306175090373\n",
      "epoch: 9 step: 348, loss is 8.30353019409813e-05\n",
      "epoch: 9 step: 349, loss is 0.0005344915552996099\n",
      "epoch: 9 step: 350, loss is 0.004427766893059015\n",
      "epoch: 9 step: 351, loss is 4.2223891796311364e-05\n",
      "epoch: 9 step: 352, loss is 0.009760316461324692\n",
      "epoch: 9 step: 353, loss is 0.0005272844573482871\n",
      "epoch: 9 step: 354, loss is 0.00795058161020279\n",
      "epoch: 9 step: 355, loss is 0.008121639490127563\n",
      "epoch: 9 step: 356, loss is 0.023125160485506058\n",
      "epoch: 9 step: 357, loss is 9.408611367689446e-05\n",
      "epoch: 9 step: 358, loss is 5.898219387745485e-05\n",
      "epoch: 9 step: 359, loss is 1.8770033420878462e-05\n",
      "epoch: 9 step: 360, loss is 0.06342263519763947\n",
      "epoch: 9 step: 361, loss is 0.0037519691977649927\n",
      "epoch: 9 step: 362, loss is 0.0037355744279921055\n",
      "epoch: 9 step: 363, loss is 0.002624588320031762\n",
      "epoch: 9 step: 364, loss is 0.00232863356359303\n",
      "epoch: 9 step: 365, loss is 0.001987071242183447\n",
      "epoch: 9 step: 366, loss is 0.00031800332362763584\n",
      "epoch: 9 step: 367, loss is 0.0005610270891338587\n",
      "epoch: 9 step: 368, loss is 0.0044989315792918205\n",
      "epoch: 9 step: 369, loss is 0.002887261100113392\n",
      "epoch: 9 step: 370, loss is 0.014926019124686718\n",
      "epoch: 9 step: 371, loss is 0.006481972523033619\n",
      "epoch: 9 step: 372, loss is 3.488888978608884e-05\n",
      "epoch: 9 step: 373, loss is 0.000617354700807482\n",
      "epoch: 9 step: 374, loss is 0.10420152544975281\n",
      "epoch: 9 step: 375, loss is 0.009778360836207867\n",
      "epoch: 9 step: 376, loss is 0.08668295294046402\n",
      "epoch: 9 step: 377, loss is 0.0007514638127759099\n",
      "epoch: 9 step: 378, loss is 7.95674350229092e-05\n",
      "epoch: 9 step: 379, loss is 0.03104298748075962\n",
      "epoch: 9 step: 380, loss is 0.0032767741940915585\n",
      "epoch: 9 step: 381, loss is 0.1034625768661499\n",
      "epoch: 9 step: 382, loss is 8.833668107399717e-05\n",
      "epoch: 9 step: 383, loss is 0.049856267869472504\n",
      "epoch: 9 step: 384, loss is 0.0009839152917265892\n",
      "epoch: 9 step: 385, loss is 3.848983033094555e-05\n",
      "epoch: 9 step: 386, loss is 0.03299121558666229\n",
      "epoch: 9 step: 387, loss is 4.542435999610461e-05\n",
      "epoch: 9 step: 388, loss is 0.03203734755516052\n",
      "epoch: 9 step: 389, loss is 0.0006885360344313085\n",
      "epoch: 9 step: 390, loss is 0.054691534489393234\n",
      "epoch: 9 step: 391, loss is 0.000938061682973057\n",
      "epoch: 9 step: 392, loss is 3.597171598812565e-05\n",
      "epoch: 9 step: 393, loss is 0.003514643060043454\n",
      "epoch: 9 step: 394, loss is 0.0011907045263797045\n",
      "epoch: 9 step: 395, loss is 0.0004714656388387084\n",
      "epoch: 9 step: 396, loss is 0.0015735068591311574\n",
      "epoch: 9 step: 397, loss is 0.005837434437125921\n",
      "epoch: 9 step: 398, loss is 0.000930539274122566\n",
      "epoch: 9 step: 399, loss is 7.226183515740559e-05\n",
      "epoch: 9 step: 400, loss is 0.00013244242290966213\n",
      "epoch: 9 step: 401, loss is 0.01020730659365654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 402, loss is 0.0025889279786497355\n",
      "epoch: 9 step: 403, loss is 0.07821960002183914\n",
      "epoch: 9 step: 404, loss is 0.01397804357111454\n",
      "epoch: 9 step: 405, loss is 0.0002346443070564419\n",
      "epoch: 9 step: 406, loss is 4.994891060050577e-05\n",
      "epoch: 9 step: 407, loss is 0.0001280635187868029\n",
      "epoch: 9 step: 408, loss is 0.020453376695513725\n",
      "epoch: 9 step: 409, loss is 0.024612508714199066\n",
      "epoch: 9 step: 410, loss is 0.0021481001749634743\n",
      "epoch: 9 step: 411, loss is 0.0011930514592677355\n",
      "epoch: 9 step: 412, loss is 0.0002923448337242007\n",
      "epoch: 9 step: 413, loss is 0.019733194261789322\n",
      "epoch: 9 step: 414, loss is 0.051525238901376724\n",
      "epoch: 9 step: 415, loss is 0.00018461009312886745\n",
      "epoch: 9 step: 416, loss is 0.0019535506144165993\n",
      "epoch: 9 step: 417, loss is 0.0002207073848694563\n",
      "epoch: 9 step: 418, loss is 0.0007143943221308291\n",
      "epoch: 9 step: 419, loss is 0.000582670618314296\n",
      "epoch: 9 step: 420, loss is 0.0007366167264990509\n",
      "epoch: 9 step: 421, loss is 0.0005181082524359226\n",
      "epoch: 9 step: 422, loss is 2.831904566846788e-05\n",
      "epoch: 9 step: 423, loss is 0.0005939314141869545\n",
      "epoch: 9 step: 424, loss is 0.0017301467014476657\n",
      "epoch: 9 step: 425, loss is 0.0002542551374062896\n",
      "epoch: 9 step: 426, loss is 0.010651270858943462\n",
      "epoch: 9 step: 427, loss is 0.00017371488502249122\n",
      "epoch: 9 step: 428, loss is 0.03737754374742508\n",
      "epoch: 9 step: 429, loss is 0.047798871994018555\n",
      "epoch: 9 step: 430, loss is 0.0007580055389553308\n",
      "epoch: 9 step: 431, loss is 0.0006002412410452962\n",
      "epoch: 9 step: 432, loss is 0.0014959040563553572\n",
      "epoch: 9 step: 433, loss is 0.00030313365277834237\n",
      "epoch: 9 step: 434, loss is 1.8527996871853247e-05\n",
      "epoch: 9 step: 435, loss is 0.00031664856942370534\n",
      "epoch: 9 step: 436, loss is 0.0075149331241846085\n",
      "epoch: 9 step: 437, loss is 0.07582809776067734\n",
      "epoch: 9 step: 438, loss is 0.010219796560704708\n",
      "epoch: 9 step: 439, loss is 0.0008522581192664802\n",
      "epoch: 9 step: 440, loss is 0.0005305274971760809\n",
      "epoch: 9 step: 441, loss is 4.491015351959504e-05\n",
      "epoch: 9 step: 442, loss is 0.0007938485941849649\n",
      "epoch: 9 step: 443, loss is 3.773550997721031e-05\n",
      "epoch: 9 step: 444, loss is 0.0027788798324763775\n",
      "epoch: 9 step: 445, loss is 0.004604000598192215\n",
      "epoch: 9 step: 446, loss is 0.0323789082467556\n",
      "epoch: 9 step: 447, loss is 8.726028318051249e-05\n",
      "epoch: 9 step: 448, loss is 0.00029540082323364913\n",
      "epoch: 9 step: 449, loss is 0.02619975432753563\n",
      "epoch: 9 step: 450, loss is 0.008395997807383537\n",
      "epoch: 9 step: 451, loss is 0.021387869492173195\n",
      "epoch: 9 step: 452, loss is 0.00040344742592424154\n",
      "epoch: 9 step: 453, loss is 0.007665871176868677\n",
      "epoch: 9 step: 454, loss is 0.044928476214408875\n",
      "epoch: 9 step: 455, loss is 0.020340699702501297\n",
      "epoch: 9 step: 456, loss is 0.000201278759050183\n",
      "epoch: 9 step: 457, loss is 0.0018268588464707136\n",
      "epoch: 9 step: 458, loss is 9.674242028268054e-05\n",
      "epoch: 9 step: 459, loss is 0.074600450694561\n",
      "epoch: 9 step: 460, loss is 0.00036829407326877117\n",
      "epoch: 9 step: 461, loss is 0.011051907204091549\n",
      "epoch: 9 step: 462, loss is 0.030227061361074448\n",
      "epoch: 9 step: 463, loss is 0.0007119589135982096\n",
      "epoch: 9 step: 464, loss is 0.0006897805142216384\n",
      "epoch: 9 step: 465, loss is 0.0023192758671939373\n",
      "epoch: 9 step: 466, loss is 0.07097005844116211\n",
      "epoch: 9 step: 467, loss is 0.00016066223906818777\n",
      "epoch: 9 step: 468, loss is 0.0007889339467510581\n",
      "epoch: 9 step: 469, loss is 0.00019540685752872378\n",
      "epoch: 9 step: 470, loss is 0.003612667787820101\n",
      "epoch: 9 step: 471, loss is 0.07751306146383286\n",
      "epoch: 9 step: 472, loss is 0.00010091041622217745\n",
      "epoch: 9 step: 473, loss is 0.005925755947828293\n",
      "epoch: 9 step: 474, loss is 0.05178403854370117\n",
      "epoch: 9 step: 475, loss is 2.4692279112059623e-05\n",
      "epoch: 9 step: 476, loss is 0.0007941914955154061\n",
      "epoch: 9 step: 477, loss is 0.0009059272124432027\n",
      "epoch: 9 step: 478, loss is 9.37373042688705e-05\n",
      "epoch: 9 step: 479, loss is 0.0057141282595694065\n",
      "epoch: 9 step: 480, loss is 0.00011606582120293751\n",
      "epoch: 9 step: 481, loss is 0.00783168338239193\n",
      "epoch: 9 step: 482, loss is 0.02248348854482174\n",
      "epoch: 9 step: 483, loss is 0.005587232764810324\n",
      "epoch: 9 step: 484, loss is 0.02430903911590576\n",
      "epoch: 9 step: 485, loss is 0.008096401579678059\n",
      "epoch: 9 step: 486, loss is 0.00273903482593596\n",
      "epoch: 9 step: 487, loss is 0.0003259454679209739\n",
      "epoch: 9 step: 488, loss is 0.00018216363969258964\n",
      "epoch: 9 step: 489, loss is 0.048738278448581696\n",
      "epoch: 9 step: 490, loss is 0.0006853356026113033\n",
      "epoch: 9 step: 491, loss is 0.03824024274945259\n",
      "epoch: 9 step: 492, loss is 0.00017867003043647856\n",
      "epoch: 9 step: 493, loss is 0.0012743659317493439\n",
      "epoch: 9 step: 494, loss is 0.0009692457970231771\n",
      "epoch: 9 step: 495, loss is 0.0016041083727031946\n",
      "epoch: 9 step: 496, loss is 0.017589043825864792\n",
      "epoch: 9 step: 497, loss is 0.00031723236315883696\n",
      "epoch: 9 step: 498, loss is 0.0652359202504158\n",
      "epoch: 9 step: 499, loss is 0.010143977589905262\n",
      "epoch: 9 step: 500, loss is 0.0009098636801354587\n",
      "epoch: 9 step: 501, loss is 0.0007459372282028198\n",
      "epoch: 9 step: 502, loss is 0.08217036724090576\n",
      "epoch: 9 step: 503, loss is 0.12350833415985107\n",
      "epoch: 9 step: 504, loss is 7.730979996267706e-05\n",
      "epoch: 9 step: 505, loss is 0.000660381861962378\n",
      "epoch: 9 step: 506, loss is 0.001071146922186017\n",
      "epoch: 9 step: 507, loss is 0.018435822799801826\n",
      "epoch: 9 step: 508, loss is 0.00030670914566144347\n",
      "epoch: 9 step: 509, loss is 0.18461260199546814\n",
      "epoch: 9 step: 510, loss is 0.00021606068185064942\n",
      "epoch: 9 step: 511, loss is 1.6097781553980894e-05\n",
      "epoch: 9 step: 512, loss is 0.002080723410472274\n",
      "epoch: 9 step: 513, loss is 0.001702666049823165\n",
      "epoch: 9 step: 514, loss is 0.0030450487975031137\n",
      "epoch: 9 step: 515, loss is 0.000504878000356257\n",
      "epoch: 9 step: 516, loss is 0.0009902134770527482\n",
      "epoch: 9 step: 517, loss is 0.00044583098497241735\n",
      "epoch: 9 step: 518, loss is 0.004511652514338493\n",
      "epoch: 9 step: 519, loss is 0.0007697015535086393\n",
      "epoch: 9 step: 520, loss is 0.16221953928470612\n",
      "epoch: 9 step: 521, loss is 2.266774208692368e-05\n",
      "epoch: 9 step: 522, loss is 0.001405696733854711\n",
      "epoch: 9 step: 523, loss is 0.010978076606988907\n",
      "epoch: 9 step: 524, loss is 0.00019595579942688346\n",
      "epoch: 9 step: 525, loss is 0.0001937160996021703\n",
      "epoch: 9 step: 526, loss is 0.0010569618316367269\n",
      "epoch: 9 step: 527, loss is 0.028234321624040604\n",
      "epoch: 9 step: 528, loss is 0.00018908422498498112\n",
      "epoch: 9 step: 529, loss is 0.004448607098311186\n",
      "epoch: 9 step: 530, loss is 6.99900119798258e-05\n",
      "epoch: 9 step: 531, loss is 0.032910849899053574\n",
      "epoch: 9 step: 532, loss is 0.0050213392823934555\n",
      "epoch: 9 step: 533, loss is 0.0002727634273469448\n",
      "epoch: 9 step: 534, loss is 0.0002565843751654029\n",
      "epoch: 9 step: 535, loss is 0.0031607598066329956\n",
      "epoch: 9 step: 536, loss is 0.0002711474662646651\n",
      "epoch: 9 step: 537, loss is 0.0030725630931556225\n",
      "epoch: 9 step: 538, loss is 0.015526038594543934\n",
      "epoch: 9 step: 539, loss is 0.048733726143836975\n",
      "epoch: 9 step: 540, loss is 0.0002834917395375669\n",
      "epoch: 9 step: 541, loss is 0.0007307010819204152\n",
      "epoch: 9 step: 542, loss is 0.00023301703913602978\n",
      "epoch: 9 step: 543, loss is 0.0006777913076803088\n",
      "epoch: 9 step: 544, loss is 0.0007698084227740765\n",
      "epoch: 9 step: 545, loss is 0.018936637789011\n",
      "epoch: 9 step: 546, loss is 7.814294804120436e-05\n",
      "epoch: 9 step: 547, loss is 0.024956777691841125\n",
      "epoch: 9 step: 548, loss is 0.0018795774085447192\n",
      "epoch: 9 step: 549, loss is 0.0033505959436297417\n",
      "epoch: 9 step: 550, loss is 0.0004909199196845293\n",
      "epoch: 9 step: 551, loss is 0.02019626460969448\n",
      "epoch: 9 step: 552, loss is 0.0006820156704634428\n",
      "epoch: 9 step: 553, loss is 0.00026711547980085015\n",
      "epoch: 9 step: 554, loss is 0.0003887939383275807\n",
      "epoch: 9 step: 555, loss is 0.06980711221694946\n",
      "epoch: 9 step: 556, loss is 0.0024004203733056784\n",
      "epoch: 9 step: 557, loss is 0.045080166310071945\n",
      "epoch: 9 step: 558, loss is 6.918013968970627e-05\n",
      "epoch: 9 step: 559, loss is 0.004721201024949551\n",
      "epoch: 9 step: 560, loss is 0.0007619863608852029\n",
      "epoch: 9 step: 561, loss is 0.0050342995673418045\n",
      "epoch: 9 step: 562, loss is 0.001111082499846816\n",
      "epoch: 9 step: 563, loss is 0.0003497422148939222\n",
      "epoch: 9 step: 564, loss is 0.00585152069106698\n",
      "epoch: 9 step: 565, loss is 0.00026007663109339774\n",
      "epoch: 9 step: 566, loss is 5.381683149607852e-05\n",
      "epoch: 9 step: 567, loss is 0.020288195461034775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 568, loss is 0.004021038766950369\n",
      "epoch: 9 step: 569, loss is 0.0034289048053324223\n",
      "epoch: 9 step: 570, loss is 4.752120730699971e-05\n",
      "epoch: 9 step: 571, loss is 0.0005698584136553109\n",
      "epoch: 9 step: 572, loss is 0.0008612364763393998\n",
      "epoch: 9 step: 573, loss is 0.001933033810928464\n",
      "epoch: 9 step: 574, loss is 0.0003415481769479811\n",
      "epoch: 9 step: 575, loss is 0.06834205985069275\n",
      "epoch: 9 step: 576, loss is 8.775365859037265e-05\n",
      "epoch: 9 step: 577, loss is 0.00010220713738817722\n",
      "epoch: 9 step: 578, loss is 0.0008986161556094885\n",
      "epoch: 9 step: 579, loss is 0.17960050702095032\n",
      "epoch: 9 step: 580, loss is 0.002246258547529578\n",
      "epoch: 9 step: 581, loss is 0.03885004669427872\n",
      "epoch: 9 step: 582, loss is 3.822460348601453e-05\n",
      "epoch: 9 step: 583, loss is 0.23399733006954193\n",
      "epoch: 9 step: 584, loss is 0.0002456352231092751\n",
      "epoch: 9 step: 585, loss is 0.0025978535413742065\n",
      "epoch: 9 step: 586, loss is 0.00034869115916080773\n",
      "epoch: 9 step: 587, loss is 0.0033081774599850178\n",
      "epoch: 9 step: 588, loss is 0.000319474667776376\n",
      "epoch: 9 step: 589, loss is 0.0060736434534192085\n",
      "epoch: 9 step: 590, loss is 0.09965689480304718\n",
      "epoch: 9 step: 591, loss is 0.011460154317319393\n",
      "epoch: 9 step: 592, loss is 0.0001235194649780169\n",
      "epoch: 9 step: 593, loss is 0.0003628900449257344\n",
      "epoch: 9 step: 594, loss is 0.0020664245821535587\n",
      "epoch: 9 step: 595, loss is 0.00037995402817614377\n",
      "epoch: 9 step: 596, loss is 0.0018896827241405845\n",
      "epoch: 9 step: 597, loss is 0.003498280653730035\n",
      "epoch: 9 step: 598, loss is 0.004112901631742716\n",
      "epoch: 9 step: 599, loss is 0.06055469810962677\n",
      "epoch: 9 step: 600, loss is 0.0002028183953370899\n",
      "epoch: 9 step: 601, loss is 0.0006391110364347696\n",
      "epoch: 9 step: 602, loss is 0.0007850154652260244\n",
      "epoch: 9 step: 603, loss is 0.0037676640786230564\n",
      "epoch: 9 step: 604, loss is 0.009885288774967194\n",
      "epoch: 9 step: 605, loss is 0.00041793257696554065\n",
      "epoch: 9 step: 606, loss is 0.0021780466195195913\n",
      "epoch: 9 step: 607, loss is 9.165912342723459e-05\n",
      "epoch: 9 step: 608, loss is 0.007143862545490265\n",
      "epoch: 9 step: 609, loss is 0.0019244318827986717\n",
      "epoch: 9 step: 610, loss is 0.020709402859210968\n",
      "epoch: 9 step: 611, loss is 0.04802674800157547\n",
      "epoch: 9 step: 612, loss is 0.002466535894200206\n",
      "epoch: 9 step: 613, loss is 0.1581054925918579\n",
      "epoch: 9 step: 614, loss is 2.3798840629751794e-05\n",
      "epoch: 9 step: 615, loss is 0.056490208953619\n",
      "epoch: 9 step: 616, loss is 0.16012299060821533\n",
      "epoch: 9 step: 617, loss is 0.006988303270190954\n",
      "epoch: 9 step: 618, loss is 0.0008691658731549978\n",
      "epoch: 9 step: 619, loss is 0.0002603580942377448\n",
      "epoch: 9 step: 620, loss is 0.0002589200739748776\n",
      "epoch: 9 step: 621, loss is 0.008971565403044224\n",
      "epoch: 9 step: 622, loss is 0.0006575482548214495\n",
      "epoch: 9 step: 623, loss is 0.00030080435681156814\n",
      "epoch: 9 step: 624, loss is 0.00011627099593169987\n",
      "epoch: 9 step: 625, loss is 0.004124093800783157\n",
      "epoch: 9 step: 626, loss is 0.0004924673121422529\n",
      "epoch: 9 step: 627, loss is 0.002693460788577795\n",
      "epoch: 9 step: 628, loss is 0.009893102571368217\n",
      "epoch: 9 step: 629, loss is 0.0022824187763035297\n",
      "epoch: 9 step: 630, loss is 4.582041219691746e-05\n",
      "epoch: 9 step: 631, loss is 0.00010510673746466637\n",
      "epoch: 9 step: 632, loss is 7.080291106831282e-05\n",
      "epoch: 9 step: 633, loss is 0.004682480823248625\n",
      "epoch: 9 step: 634, loss is 0.0003036555426660925\n",
      "epoch: 9 step: 635, loss is 0.004174082074314356\n",
      "epoch: 9 step: 636, loss is 0.00031699149985797703\n",
      "epoch: 9 step: 637, loss is 0.00026058522053062916\n",
      "epoch: 9 step: 638, loss is 0.04154091328382492\n",
      "epoch: 9 step: 639, loss is 0.001010858453810215\n",
      "epoch: 9 step: 640, loss is 0.0017803931841626763\n",
      "epoch: 9 step: 641, loss is 0.0006413165829144418\n",
      "epoch: 9 step: 642, loss is 0.026951905339956284\n",
      "epoch: 9 step: 643, loss is 0.065100759267807\n",
      "epoch: 9 step: 644, loss is 1.8437462131259963e-05\n",
      "epoch: 9 step: 645, loss is 0.00757420202717185\n",
      "epoch: 9 step: 646, loss is 0.00024544482585042715\n",
      "epoch: 9 step: 647, loss is 0.007823643274605274\n",
      "epoch: 9 step: 648, loss is 0.26589998602867126\n",
      "epoch: 9 step: 649, loss is 0.01598907634615898\n",
      "epoch: 9 step: 650, loss is 0.00027473553200252354\n",
      "epoch: 9 step: 651, loss is 0.0010071832221001387\n",
      "epoch: 9 step: 652, loss is 0.00011748057295335457\n",
      "epoch: 9 step: 653, loss is 0.0021633764263242483\n",
      "epoch: 9 step: 654, loss is 0.00961628183722496\n",
      "epoch: 9 step: 655, loss is 0.015584000386297703\n",
      "epoch: 9 step: 656, loss is 0.00012856663670390844\n",
      "epoch: 9 step: 657, loss is 0.0013116244226694107\n",
      "epoch: 9 step: 658, loss is 0.0001453230215702206\n",
      "epoch: 9 step: 659, loss is 0.0022996149491518736\n",
      "epoch: 9 step: 660, loss is 0.0006404106388799846\n",
      "epoch: 9 step: 661, loss is 7.110981096047908e-05\n",
      "epoch: 9 step: 662, loss is 0.03132512420415878\n",
      "epoch: 9 step: 663, loss is 0.010258846916258335\n",
      "epoch: 9 step: 664, loss is 0.00012646563118323684\n",
      "epoch: 9 step: 665, loss is 0.0007767083588987589\n",
      "epoch: 9 step: 666, loss is 0.013712371699512005\n",
      "epoch: 9 step: 667, loss is 0.002108744578436017\n",
      "epoch: 9 step: 668, loss is 0.009121724404394627\n",
      "epoch: 9 step: 669, loss is 0.0006154289003461599\n",
      "epoch: 9 step: 670, loss is 0.008528531529009342\n",
      "epoch: 9 step: 671, loss is 0.0031960811465978622\n",
      "epoch: 9 step: 672, loss is 0.00014445243868976831\n",
      "epoch: 9 step: 673, loss is 0.0014814542373642325\n",
      "epoch: 9 step: 674, loss is 0.0019576363265514374\n",
      "epoch: 9 step: 675, loss is 0.009803295135498047\n",
      "epoch: 9 step: 676, loss is 0.05556492134928703\n",
      "epoch: 9 step: 677, loss is 0.014496552757918835\n",
      "epoch: 9 step: 678, loss is 0.0076732817105948925\n",
      "epoch: 9 step: 679, loss is 0.0004786155477631837\n",
      "epoch: 9 step: 680, loss is 0.0007006942178122699\n",
      "epoch: 9 step: 681, loss is 0.00935925543308258\n",
      "epoch: 9 step: 682, loss is 0.006118331104516983\n",
      "epoch: 9 step: 683, loss is 0.000574969279114157\n",
      "epoch: 9 step: 684, loss is 0.001253156689926982\n",
      "epoch: 9 step: 685, loss is 0.0005738945910707116\n",
      "epoch: 9 step: 686, loss is 0.015045476146042347\n",
      "epoch: 9 step: 687, loss is 0.00045198772568255663\n",
      "epoch: 9 step: 688, loss is 0.0006807756726630032\n",
      "epoch: 9 step: 689, loss is 0.00039019063115119934\n",
      "epoch: 9 step: 690, loss is 0.0013896254822611809\n",
      "epoch: 9 step: 691, loss is 0.004788629710674286\n",
      "epoch: 9 step: 692, loss is 0.00674589304253459\n",
      "epoch: 9 step: 693, loss is 0.0010159746743738651\n",
      "epoch: 9 step: 694, loss is 0.005173307843506336\n",
      "epoch: 9 step: 695, loss is 6.005178511259146e-05\n",
      "epoch: 9 step: 696, loss is 5.510710252565332e-05\n",
      "epoch: 9 step: 697, loss is 0.00909341312944889\n",
      "epoch: 9 step: 698, loss is 0.03929289057850838\n",
      "epoch: 9 step: 699, loss is 0.06556045264005661\n",
      "epoch: 9 step: 700, loss is 0.0036837891675531864\n",
      "epoch: 9 step: 701, loss is 0.00019675599469337612\n",
      "epoch: 9 step: 702, loss is 0.0030101474840193987\n",
      "epoch: 9 step: 703, loss is 0.0018249691929668188\n",
      "epoch: 9 step: 704, loss is 0.00046724246931262314\n",
      "epoch: 9 step: 705, loss is 0.008101248182356358\n",
      "epoch: 9 step: 706, loss is 0.014392612501978874\n",
      "epoch: 9 step: 707, loss is 0.09778179973363876\n",
      "epoch: 9 step: 708, loss is 0.0004276265390217304\n",
      "epoch: 9 step: 709, loss is 0.0015103238401934505\n",
      "epoch: 9 step: 710, loss is 0.00035491655580699444\n",
      "epoch: 9 step: 711, loss is 0.004393321927636862\n",
      "epoch: 9 step: 712, loss is 0.007321739569306374\n",
      "epoch: 9 step: 713, loss is 0.013618767261505127\n",
      "epoch: 9 step: 714, loss is 0.0003168676339555532\n",
      "epoch: 9 step: 715, loss is 0.0019262471469119191\n",
      "epoch: 9 step: 716, loss is 0.002339053899049759\n",
      "epoch: 9 step: 717, loss is 0.09559289366006851\n",
      "epoch: 9 step: 718, loss is 0.018397681415081024\n",
      "epoch: 9 step: 719, loss is 0.003873497946187854\n",
      "epoch: 9 step: 720, loss is 0.10998911410570145\n",
      "epoch: 9 step: 721, loss is 0.0639108344912529\n",
      "epoch: 9 step: 722, loss is 0.001529793837107718\n",
      "epoch: 9 step: 723, loss is 0.0038510500453412533\n",
      "epoch: 9 step: 724, loss is 0.02254711277782917\n",
      "epoch: 9 step: 725, loss is 0.005485615227371454\n",
      "epoch: 9 step: 726, loss is 0.001248791697435081\n",
      "epoch: 9 step: 727, loss is 0.0038123021367937326\n",
      "epoch: 9 step: 728, loss is 0.0012615708401426673\n",
      "epoch: 9 step: 729, loss is 0.0002511649508960545\n",
      "epoch: 9 step: 730, loss is 0.026879779994487762\n",
      "epoch: 9 step: 731, loss is 0.00019332543888594955\n",
      "epoch: 9 step: 732, loss is 0.017292503267526627\n",
      "epoch: 9 step: 733, loss is 0.006357901729643345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 734, loss is 0.003325642319396138\n",
      "epoch: 9 step: 735, loss is 0.016358917579054832\n",
      "epoch: 9 step: 736, loss is 0.009790336713194847\n",
      "epoch: 9 step: 737, loss is 0.00013044416846241802\n",
      "epoch: 9 step: 738, loss is 0.00019343363237567246\n",
      "epoch: 9 step: 739, loss is 0.0068430365063250065\n",
      "epoch: 9 step: 740, loss is 0.011085662990808487\n",
      "epoch: 9 step: 741, loss is 0.006425485946238041\n",
      "epoch: 9 step: 742, loss is 9.44333296502009e-05\n",
      "epoch: 9 step: 743, loss is 0.00024279276840388775\n",
      "epoch: 9 step: 744, loss is 0.00411547115072608\n",
      "epoch: 9 step: 745, loss is 0.0004445043159648776\n",
      "epoch: 9 step: 746, loss is 0.0008653176482766867\n",
      "epoch: 9 step: 747, loss is 0.0007136886706575751\n",
      "epoch: 9 step: 748, loss is 0.07969360053539276\n",
      "epoch: 9 step: 749, loss is 0.006066580768674612\n",
      "epoch: 9 step: 750, loss is 0.0004613000201061368\n",
      "epoch: 9 step: 751, loss is 0.001821833779104054\n",
      "epoch: 9 step: 752, loss is 0.017091412097215652\n",
      "epoch: 9 step: 753, loss is 0.045583851635456085\n",
      "epoch: 9 step: 754, loss is 3.992670463048853e-05\n",
      "epoch: 9 step: 755, loss is 0.0002355212054681033\n",
      "epoch: 9 step: 756, loss is 0.0001826000661822036\n",
      "epoch: 9 step: 757, loss is 0.005918086972087622\n",
      "epoch: 9 step: 758, loss is 0.13433848321437836\n",
      "epoch: 9 step: 759, loss is 4.622753112926148e-05\n",
      "epoch: 9 step: 760, loss is 2.1519050278584473e-05\n",
      "epoch: 9 step: 761, loss is 0.0016758162528276443\n",
      "epoch: 9 step: 762, loss is 0.12026809900999069\n",
      "epoch: 9 step: 763, loss is 0.0013314558891579509\n",
      "epoch: 9 step: 764, loss is 0.03130829334259033\n",
      "epoch: 9 step: 765, loss is 0.014332638122141361\n",
      "epoch: 9 step: 766, loss is 1.8572642147773877e-05\n",
      "epoch: 9 step: 767, loss is 0.02483115717768669\n",
      "epoch: 9 step: 768, loss is 0.0013448413228616118\n",
      "epoch: 9 step: 769, loss is 0.00029873885796405375\n",
      "epoch: 9 step: 770, loss is 0.012622964568436146\n",
      "epoch: 9 step: 771, loss is 0.000240412526181899\n",
      "epoch: 9 step: 772, loss is 0.1206723153591156\n",
      "epoch: 9 step: 773, loss is 0.0006147430976852775\n",
      "epoch: 9 step: 774, loss is 0.018744470551609993\n",
      "epoch: 9 step: 775, loss is 0.011650538071990013\n",
      "epoch: 9 step: 776, loss is 0.08692062646150589\n",
      "epoch: 9 step: 777, loss is 0.010576700791716576\n",
      "epoch: 9 step: 778, loss is 0.0011269517708569765\n",
      "epoch: 9 step: 779, loss is 0.002546850126236677\n",
      "epoch: 9 step: 780, loss is 0.00012263951066415757\n",
      "epoch: 9 step: 781, loss is 0.03354533389210701\n",
      "epoch: 9 step: 782, loss is 0.03620356321334839\n",
      "epoch: 9 step: 783, loss is 0.00041962898103520274\n",
      "epoch: 9 step: 784, loss is 1.991486715269275e-05\n",
      "epoch: 9 step: 785, loss is 0.08122873306274414\n",
      "epoch: 9 step: 786, loss is 0.003673611208796501\n",
      "epoch: 9 step: 787, loss is 0.0014207360800355673\n",
      "epoch: 9 step: 788, loss is 0.0005030026659369469\n",
      "epoch: 9 step: 789, loss is 0.049117788672447205\n",
      "epoch: 9 step: 790, loss is 0.00045712123392149806\n",
      "epoch: 9 step: 791, loss is 0.0017016720958054066\n",
      "epoch: 9 step: 792, loss is 0.0020284480415284634\n",
      "epoch: 9 step: 793, loss is 0.0012341018300503492\n",
      "epoch: 9 step: 794, loss is 0.0008575068786740303\n",
      "epoch: 9 step: 795, loss is 0.0021991245448589325\n",
      "epoch: 9 step: 796, loss is 0.0015536705031991005\n",
      "epoch: 9 step: 797, loss is 0.00047554686898365617\n",
      "epoch: 9 step: 798, loss is 0.019886886700987816\n",
      "epoch: 9 step: 799, loss is 0.011177653446793556\n",
      "epoch: 9 step: 800, loss is 0.0010812339605763555\n",
      "epoch: 9 step: 801, loss is 0.0001317968562943861\n",
      "epoch: 9 step: 802, loss is 0.006700905039906502\n",
      "epoch: 9 step: 803, loss is 6.764885620214045e-05\n",
      "epoch: 9 step: 804, loss is 0.002435823902487755\n",
      "epoch: 9 step: 805, loss is 0.0011088153114542365\n",
      "epoch: 9 step: 806, loss is 0.005427114199846983\n",
      "epoch: 9 step: 807, loss is 0.005305357277393341\n",
      "epoch: 9 step: 808, loss is 0.004960678517818451\n",
      "epoch: 9 step: 809, loss is 0.0006520839524455369\n",
      "epoch: 9 step: 810, loss is 0.00038165514706633985\n",
      "epoch: 9 step: 811, loss is 0.001243887236341834\n",
      "epoch: 9 step: 812, loss is 0.002746249781921506\n",
      "epoch: 9 step: 813, loss is 0.010725356638431549\n",
      "epoch: 9 step: 814, loss is 5.336604226613417e-05\n",
      "epoch: 9 step: 815, loss is 4.034628364024684e-05\n",
      "epoch: 9 step: 816, loss is 0.00010469628614373505\n",
      "epoch: 9 step: 817, loss is 0.02695470117032528\n",
      "epoch: 9 step: 818, loss is 0.0010492652654647827\n",
      "epoch: 9 step: 819, loss is 0.0004753049579448998\n",
      "epoch: 9 step: 820, loss is 4.712614099844359e-06\n",
      "epoch: 9 step: 821, loss is 0.015314397402107716\n",
      "epoch: 9 step: 822, loss is 0.03251054510474205\n",
      "epoch: 9 step: 823, loss is 0.0005877920775674284\n",
      "epoch: 9 step: 824, loss is 0.009741815738379955\n",
      "epoch: 9 step: 825, loss is 9.755758219398558e-05\n",
      "epoch: 9 step: 826, loss is 6.391170609276742e-05\n",
      "epoch: 9 step: 827, loss is 0.19566859304904938\n",
      "epoch: 9 step: 828, loss is 0.0020448067225515842\n",
      "epoch: 9 step: 829, loss is 8.230437379097566e-05\n",
      "epoch: 9 step: 830, loss is 0.019945569336414337\n",
      "epoch: 9 step: 831, loss is 0.00024600280448794365\n",
      "epoch: 9 step: 832, loss is 6.083327753003687e-05\n",
      "epoch: 9 step: 833, loss is 0.0011750231496989727\n",
      "epoch: 9 step: 834, loss is 0.005927891470491886\n",
      "epoch: 9 step: 835, loss is 0.01052736397832632\n",
      "epoch: 9 step: 836, loss is 0.0027225918602198362\n",
      "epoch: 9 step: 837, loss is 0.005141299683600664\n",
      "epoch: 9 step: 838, loss is 0.0001103146787500009\n",
      "epoch: 9 step: 839, loss is 0.0006440610159188509\n",
      "epoch: 9 step: 840, loss is 0.017870454117655754\n",
      "epoch: 9 step: 841, loss is 1.981216155400034e-05\n",
      "epoch: 9 step: 842, loss is 4.444280421012081e-05\n",
      "epoch: 9 step: 843, loss is 0.1878526359796524\n",
      "epoch: 9 step: 844, loss is 0.0001570163294672966\n",
      "epoch: 9 step: 845, loss is 0.0034628771245479584\n",
      "epoch: 9 step: 846, loss is 0.08891434967517853\n",
      "epoch: 9 step: 847, loss is 0.004862941335886717\n",
      "epoch: 9 step: 848, loss is 0.00037564418744295835\n",
      "epoch: 9 step: 849, loss is 0.0180406142026186\n",
      "epoch: 9 step: 850, loss is 0.004885151516646147\n",
      "epoch: 9 step: 851, loss is 0.0012365833390504122\n",
      "epoch: 9 step: 852, loss is 0.002337345154955983\n",
      "epoch: 9 step: 853, loss is 0.0005575517425313592\n",
      "epoch: 9 step: 854, loss is 0.005379038862884045\n",
      "epoch: 9 step: 855, loss is 0.0007186422008089721\n",
      "epoch: 9 step: 856, loss is 0.0004800287133548409\n",
      "epoch: 9 step: 857, loss is 0.001177357044070959\n",
      "epoch: 9 step: 858, loss is 0.059276945888996124\n",
      "epoch: 9 step: 859, loss is 9.641566430218518e-05\n",
      "epoch: 9 step: 860, loss is 0.219892218708992\n",
      "epoch: 9 step: 861, loss is 0.0036507865879684687\n",
      "epoch: 9 step: 862, loss is 0.11246953159570694\n",
      "epoch: 9 step: 863, loss is 0.02802010253071785\n",
      "epoch: 9 step: 864, loss is 0.0346711166203022\n",
      "epoch: 9 step: 865, loss is 0.001894850516691804\n",
      "epoch: 9 step: 866, loss is 0.14509059488773346\n",
      "epoch: 9 step: 867, loss is 0.007126090582460165\n",
      "epoch: 9 step: 868, loss is 0.00041497734491713345\n",
      "epoch: 9 step: 869, loss is 3.300420939922333e-05\n",
      "epoch: 9 step: 870, loss is 0.00013408303493633866\n",
      "epoch: 9 step: 871, loss is 0.016064949333667755\n",
      "epoch: 9 step: 872, loss is 0.061495617032051086\n",
      "epoch: 9 step: 873, loss is 0.01858527585864067\n",
      "epoch: 9 step: 874, loss is 0.011346484534442425\n",
      "epoch: 9 step: 875, loss is 0.00028340378776192665\n",
      "epoch: 9 step: 876, loss is 0.0002235021092928946\n",
      "epoch: 9 step: 877, loss is 0.02947971224784851\n",
      "epoch: 9 step: 878, loss is 0.13767948746681213\n",
      "epoch: 9 step: 879, loss is 0.1105661541223526\n",
      "epoch: 9 step: 880, loss is 0.005012878682464361\n",
      "epoch: 9 step: 881, loss is 0.0005934032378718257\n",
      "epoch: 9 step: 882, loss is 0.011681834235787392\n",
      "epoch: 9 step: 883, loss is 0.00873737782239914\n",
      "epoch: 9 step: 884, loss is 0.06031179428100586\n",
      "epoch: 9 step: 885, loss is 0.001911774161271751\n",
      "epoch: 9 step: 886, loss is 0.008262356743216515\n",
      "epoch: 9 step: 887, loss is 0.00017505904543213546\n",
      "epoch: 9 step: 888, loss is 6.726007268298417e-05\n",
      "epoch: 9 step: 889, loss is 8.617175626568496e-05\n",
      "epoch: 9 step: 890, loss is 0.016337191686034203\n",
      "epoch: 9 step: 891, loss is 0.08063264191150665\n",
      "epoch: 9 step: 892, loss is 0.0029953529592603445\n",
      "epoch: 9 step: 893, loss is 0.00028154501342214644\n",
      "epoch: 9 step: 894, loss is 0.04892617464065552\n",
      "epoch: 9 step: 895, loss is 0.0063069104216992855\n",
      "epoch: 9 step: 896, loss is 0.00367228826507926\n",
      "epoch: 9 step: 897, loss is 0.0004526979464571923\n",
      "epoch: 9 step: 898, loss is 0.0017064849380403757\n",
      "epoch: 9 step: 899, loss is 0.0003737358492799103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 900, loss is 0.0683378353714943\n",
      "epoch: 9 step: 901, loss is 0.023104531690478325\n",
      "epoch: 9 step: 902, loss is 0.0007105064578354359\n",
      "epoch: 9 step: 903, loss is 0.03569534793496132\n",
      "epoch: 9 step: 904, loss is 0.0013937328476458788\n",
      "epoch: 9 step: 905, loss is 0.0012765851570293307\n",
      "epoch: 9 step: 906, loss is 0.003689208999276161\n",
      "epoch: 9 step: 907, loss is 0.003852620953693986\n",
      "epoch: 9 step: 908, loss is 0.12290748953819275\n",
      "epoch: 9 step: 909, loss is 0.00045072659850120544\n",
      "epoch: 9 step: 910, loss is 0.00034019147278741\n",
      "epoch: 9 step: 911, loss is 0.004950978327542543\n",
      "epoch: 9 step: 912, loss is 0.04128621518611908\n",
      "epoch: 9 step: 913, loss is 0.15707360208034515\n",
      "epoch: 9 step: 914, loss is 0.001757737947627902\n",
      "epoch: 9 step: 915, loss is 0.0007847909582778811\n",
      "epoch: 9 step: 916, loss is 0.01152619905769825\n",
      "epoch: 9 step: 917, loss is 0.00899064913392067\n",
      "epoch: 9 step: 918, loss is 0.0016548112034797668\n",
      "epoch: 9 step: 919, loss is 0.006157505325973034\n",
      "epoch: 9 step: 920, loss is 0.07235459238290787\n",
      "epoch: 9 step: 921, loss is 0.0017877198988571763\n",
      "epoch: 9 step: 922, loss is 0.0703098401427269\n",
      "epoch: 9 step: 923, loss is 0.003752254182472825\n",
      "epoch: 9 step: 924, loss is 0.0005787548143416643\n",
      "epoch: 9 step: 925, loss is 0.00019208395679015666\n",
      "epoch: 9 step: 926, loss is 0.037806425243616104\n",
      "epoch: 9 step: 927, loss is 0.00013649050379171968\n",
      "epoch: 9 step: 928, loss is 0.07404984533786774\n",
      "epoch: 9 step: 929, loss is 0.010661699809134007\n",
      "epoch: 9 step: 930, loss is 8.367581176571548e-05\n",
      "epoch: 9 step: 931, loss is 0.0025842401664704084\n",
      "epoch: 9 step: 932, loss is 0.01938258670270443\n",
      "epoch: 9 step: 933, loss is 0.07141699641942978\n",
      "epoch: 9 step: 934, loss is 0.00808084849268198\n",
      "epoch: 9 step: 935, loss is 0.007233062759041786\n",
      "epoch: 9 step: 936, loss is 0.012893921695649624\n",
      "epoch: 9 step: 937, loss is 0.00028843022300861776\n",
      "epoch: 9 step: 938, loss is 0.012226013466715813\n",
      "epoch: 9 step: 939, loss is 0.0004034867451991886\n",
      "epoch: 9 step: 940, loss is 0.08377261459827423\n",
      "epoch: 9 step: 941, loss is 0.0016938823973760009\n",
      "epoch: 9 step: 942, loss is 0.0014975182712078094\n",
      "epoch: 9 step: 943, loss is 0.024125348776578903\n",
      "epoch: 9 step: 944, loss is 0.007085666060447693\n",
      "epoch: 9 step: 945, loss is 0.0030162225011736155\n",
      "epoch: 9 step: 946, loss is 0.0019515205640345812\n",
      "epoch: 9 step: 947, loss is 0.04082228243350983\n",
      "epoch: 9 step: 948, loss is 0.004954649601131678\n",
      "epoch: 9 step: 949, loss is 0.0004234942316543311\n",
      "epoch: 9 step: 950, loss is 0.0008027818985283375\n",
      "epoch: 9 step: 951, loss is 0.07726876437664032\n",
      "epoch: 9 step: 952, loss is 0.004430380649864674\n",
      "epoch: 9 step: 953, loss is 0.007082061842083931\n",
      "epoch: 9 step: 954, loss is 0.00288793514482677\n",
      "epoch: 9 step: 955, loss is 0.06399711966514587\n",
      "epoch: 9 step: 956, loss is 0.0024927095510065556\n",
      "epoch: 9 step: 957, loss is 0.01868855394423008\n",
      "epoch: 9 step: 958, loss is 0.0018424437148496509\n",
      "epoch: 9 step: 959, loss is 0.00020803126972168684\n",
      "epoch: 9 step: 960, loss is 0.009899580851197243\n",
      "epoch: 9 step: 961, loss is 0.005166998133063316\n",
      "epoch: 9 step: 962, loss is 0.0003932237159460783\n",
      "epoch: 9 step: 963, loss is 0.0015664435923099518\n",
      "epoch: 9 step: 964, loss is 0.00567147321999073\n",
      "epoch: 9 step: 965, loss is 0.0019233336206525564\n",
      "epoch: 9 step: 966, loss is 0.008053174242377281\n",
      "epoch: 9 step: 967, loss is 0.008586624637246132\n",
      "epoch: 9 step: 968, loss is 0.0005303221987560391\n",
      "epoch: 9 step: 969, loss is 0.11457457393407822\n",
      "epoch: 9 step: 970, loss is 0.015617065131664276\n",
      "epoch: 9 step: 971, loss is 0.007088830694556236\n",
      "epoch: 9 step: 972, loss is 1.317516671406338e-05\n",
      "epoch: 9 step: 973, loss is 0.03445468470454216\n",
      "epoch: 9 step: 974, loss is 0.008160758763551712\n",
      "epoch: 9 step: 975, loss is 0.017660846933722496\n",
      "epoch: 9 step: 976, loss is 0.03144816681742668\n",
      "epoch: 9 step: 977, loss is 9.23388433875516e-05\n",
      "epoch: 9 step: 978, loss is 0.00019349483773112297\n",
      "epoch: 9 step: 979, loss is 0.0006552558625116944\n",
      "epoch: 9 step: 980, loss is 0.0010012175189331174\n",
      "epoch: 9 step: 981, loss is 0.0014960167463868856\n",
      "epoch: 9 step: 982, loss is 0.005577308591455221\n",
      "epoch: 9 step: 983, loss is 0.0004699978744611144\n",
      "epoch: 9 step: 984, loss is 0.00033979254658333957\n",
      "epoch: 9 step: 985, loss is 0.0005581897567026317\n",
      "epoch: 9 step: 986, loss is 0.0006529513630084693\n",
      "epoch: 9 step: 987, loss is 0.00928194634616375\n",
      "epoch: 9 step: 988, loss is 0.0004578374500852078\n",
      "epoch: 9 step: 989, loss is 0.006090537644922733\n",
      "epoch: 9 step: 990, loss is 0.1005033478140831\n",
      "epoch: 9 step: 991, loss is 0.05608822777867317\n",
      "epoch: 9 step: 992, loss is 0.0005312669673003256\n",
      "epoch: 9 step: 993, loss is 0.046726398169994354\n",
      "epoch: 9 step: 994, loss is 0.0043261246755719185\n",
      "epoch: 9 step: 995, loss is 0.00017367904365528375\n",
      "epoch: 9 step: 996, loss is 0.0005876037757843733\n",
      "epoch: 9 step: 997, loss is 6.79798613418825e-05\n",
      "epoch: 9 step: 998, loss is 6.080106686567888e-05\n",
      "epoch: 9 step: 999, loss is 0.008945111185312271\n",
      "epoch: 9 step: 1000, loss is 0.004606151953339577\n",
      "epoch: 9 step: 1001, loss is 0.0005216423305682838\n",
      "epoch: 9 step: 1002, loss is 0.11122346669435501\n",
      "epoch: 9 step: 1003, loss is 0.00035057420609518886\n",
      "epoch: 9 step: 1004, loss is 0.0042778379283845425\n",
      "epoch: 9 step: 1005, loss is 0.0050818659365177155\n",
      "epoch: 9 step: 1006, loss is 4.224952863296494e-05\n",
      "epoch: 9 step: 1007, loss is 0.019883915781974792\n",
      "epoch: 9 step: 1008, loss is 0.02418157458305359\n",
      "epoch: 9 step: 1009, loss is 0.005424779839813709\n",
      "epoch: 9 step: 1010, loss is 0.001751537318341434\n",
      "epoch: 9 step: 1011, loss is 0.0010639760876074433\n",
      "epoch: 9 step: 1012, loss is 0.029947474598884583\n",
      "epoch: 9 step: 1013, loss is 0.00012638609041459858\n",
      "epoch: 9 step: 1014, loss is 0.0008073521894402802\n",
      "epoch: 9 step: 1015, loss is 0.00037501438055187464\n",
      "epoch: 9 step: 1016, loss is 0.000239462053286843\n",
      "epoch: 9 step: 1017, loss is 0.007392560597509146\n",
      "epoch: 9 step: 1018, loss is 0.0028972395230084658\n",
      "epoch: 9 step: 1019, loss is 0.0001553533656988293\n",
      "epoch: 9 step: 1020, loss is 0.124485082924366\n",
      "epoch: 9 step: 1021, loss is 7.868270040489733e-05\n",
      "epoch: 9 step: 1022, loss is 2.7208723622607067e-05\n",
      "epoch: 9 step: 1023, loss is 1.6871113984961994e-05\n",
      "epoch: 9 step: 1024, loss is 7.605640712426975e-05\n",
      "epoch: 9 step: 1025, loss is 0.018539391458034515\n",
      "epoch: 9 step: 1026, loss is 0.05814750865101814\n",
      "epoch: 9 step: 1027, loss is 9.508258517598733e-05\n",
      "epoch: 9 step: 1028, loss is 0.0007680976414121687\n",
      "epoch: 9 step: 1029, loss is 0.006440762896090746\n",
      "epoch: 9 step: 1030, loss is 3.021190786967054e-05\n",
      "epoch: 9 step: 1031, loss is 7.875529263401404e-06\n",
      "epoch: 9 step: 1032, loss is 0.00016062524809967726\n",
      "epoch: 9 step: 1033, loss is 0.005204921588301659\n",
      "epoch: 9 step: 1034, loss is 0.00014764575462322682\n",
      "epoch: 9 step: 1035, loss is 0.0003830589121207595\n",
      "epoch: 9 step: 1036, loss is 0.00010165302228415385\n",
      "epoch: 9 step: 1037, loss is 9.080918243853375e-05\n",
      "epoch: 9 step: 1038, loss is 0.00023444845282938331\n",
      "epoch: 9 step: 1039, loss is 0.031806107610464096\n",
      "epoch: 9 step: 1040, loss is 0.0006160859484225512\n",
      "epoch: 9 step: 1041, loss is 0.001274189562536776\n",
      "epoch: 9 step: 1042, loss is 0.10607503354549408\n",
      "epoch: 9 step: 1043, loss is 0.002149956300854683\n",
      "epoch: 9 step: 1044, loss is 0.0023266118951141834\n",
      "epoch: 9 step: 1045, loss is 0.00013337295968085527\n",
      "epoch: 9 step: 1046, loss is 0.007315858732908964\n",
      "epoch: 9 step: 1047, loss is 0.001147064263932407\n",
      "epoch: 9 step: 1048, loss is 0.10446394979953766\n",
      "epoch: 9 step: 1049, loss is 0.003778985468670726\n",
      "epoch: 9 step: 1050, loss is 0.0038785100914537907\n",
      "epoch: 9 step: 1051, loss is 0.002352894051000476\n",
      "epoch: 9 step: 1052, loss is 0.001992634031921625\n",
      "epoch: 9 step: 1053, loss is 0.005997820757329464\n",
      "epoch: 9 step: 1054, loss is 0.0009753924096003175\n",
      "epoch: 9 step: 1055, loss is 0.061578407883644104\n",
      "epoch: 9 step: 1056, loss is 0.016138985753059387\n",
      "epoch: 9 step: 1057, loss is 0.0027188421227037907\n",
      "epoch: 9 step: 1058, loss is 0.24882447719573975\n",
      "epoch: 9 step: 1059, loss is 0.0007631581393070519\n",
      "epoch: 9 step: 1060, loss is 0.0003997921012341976\n",
      "epoch: 9 step: 1061, loss is 0.0007674445514567196\n",
      "epoch: 9 step: 1062, loss is 1.773477197275497e-05\n",
      "epoch: 9 step: 1063, loss is 6.212051812326536e-05\n",
      "epoch: 9 step: 1064, loss is 0.0002077323879348114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 1065, loss is 0.0023512477055191994\n",
      "epoch: 9 step: 1066, loss is 0.000609091657679528\n",
      "epoch: 9 step: 1067, loss is 0.008626759983599186\n",
      "epoch: 9 step: 1068, loss is 0.04831360653042793\n",
      "epoch: 9 step: 1069, loss is 0.004131415393203497\n",
      "epoch: 9 step: 1070, loss is 0.1680571436882019\n",
      "epoch: 9 step: 1071, loss is 0.000886918802279979\n",
      "epoch: 9 step: 1072, loss is 0.022140126675367355\n",
      "epoch: 9 step: 1073, loss is 0.03213575854897499\n",
      "epoch: 9 step: 1074, loss is 0.0001612779451534152\n",
      "epoch: 9 step: 1075, loss is 0.007108766119927168\n",
      "epoch: 9 step: 1076, loss is 0.000133532055770047\n",
      "epoch: 9 step: 1077, loss is 0.0003217834164388478\n",
      "epoch: 9 step: 1078, loss is 0.014234877191483974\n",
      "epoch: 9 step: 1079, loss is 0.013496970757842064\n",
      "epoch: 9 step: 1080, loss is 0.029283657670021057\n",
      "epoch: 9 step: 1081, loss is 0.06377702206373215\n",
      "epoch: 9 step: 1082, loss is 0.004683715291321278\n",
      "epoch: 9 step: 1083, loss is 5.583516031038016e-05\n",
      "epoch: 9 step: 1084, loss is 0.0004097583587281406\n",
      "epoch: 9 step: 1085, loss is 0.0011225179769098759\n",
      "epoch: 9 step: 1086, loss is 0.0011029555462300777\n",
      "epoch: 9 step: 1087, loss is 0.017832498997449875\n",
      "epoch: 9 step: 1088, loss is 0.0006843568990007043\n",
      "epoch: 9 step: 1089, loss is 0.012328103184700012\n",
      "epoch: 9 step: 1090, loss is 0.0002761296054814011\n",
      "epoch: 9 step: 1091, loss is 0.009738396853208542\n",
      "epoch: 9 step: 1092, loss is 0.013395499438047409\n",
      "epoch: 9 step: 1093, loss is 0.0026882875245064497\n",
      "epoch: 9 step: 1094, loss is 0.0003746529691852629\n",
      "epoch: 9 step: 1095, loss is 0.015916595235466957\n",
      "epoch: 9 step: 1096, loss is 8.54696991154924e-05\n",
      "epoch: 9 step: 1097, loss is 0.0002955565578304231\n",
      "epoch: 9 step: 1098, loss is 0.007687757257372141\n",
      "epoch: 9 step: 1099, loss is 5.7259367167716846e-05\n",
      "epoch: 9 step: 1100, loss is 0.0038923113606870174\n",
      "epoch: 9 step: 1101, loss is 0.0011312621645629406\n",
      "epoch: 9 step: 1102, loss is 0.07997430860996246\n",
      "epoch: 9 step: 1103, loss is 0.04199186712503433\n",
      "epoch: 9 step: 1104, loss is 0.39917999505996704\n",
      "epoch: 9 step: 1105, loss is 0.0011377966729924083\n",
      "epoch: 9 step: 1106, loss is 0.00533632468432188\n",
      "epoch: 9 step: 1107, loss is 0.0011534960940480232\n",
      "epoch: 9 step: 1108, loss is 0.0005190861993469298\n",
      "epoch: 9 step: 1109, loss is 0.006257746368646622\n",
      "epoch: 9 step: 1110, loss is 0.0002721672644838691\n",
      "epoch: 9 step: 1111, loss is 0.00879408698529005\n",
      "epoch: 9 step: 1112, loss is 0.0028198999352753162\n",
      "epoch: 9 step: 1113, loss is 0.005151725374162197\n",
      "epoch: 9 step: 1114, loss is 0.0008216206333599985\n",
      "epoch: 9 step: 1115, loss is 0.04440152645111084\n",
      "epoch: 9 step: 1116, loss is 0.00011732026177924126\n",
      "epoch: 9 step: 1117, loss is 0.0015421557473018765\n",
      "epoch: 9 step: 1118, loss is 0.0003893364919349551\n",
      "epoch: 9 step: 1119, loss is 0.003921457100659609\n",
      "epoch: 9 step: 1120, loss is 0.0002831891179084778\n",
      "epoch: 9 step: 1121, loss is 0.04165642708539963\n",
      "epoch: 9 step: 1122, loss is 0.00022548716515302658\n",
      "epoch: 9 step: 1123, loss is 0.01261363085359335\n",
      "epoch: 9 step: 1124, loss is 0.018030643463134766\n",
      "epoch: 9 step: 1125, loss is 0.0007233080104924738\n",
      "epoch: 9 step: 1126, loss is 0.0026689255610108376\n",
      "epoch: 9 step: 1127, loss is 0.0023087337613105774\n",
      "epoch: 9 step: 1128, loss is 0.0008048359886743128\n",
      "epoch: 9 step: 1129, loss is 0.039878588169813156\n",
      "epoch: 9 step: 1130, loss is 0.0004752577224280685\n",
      "epoch: 9 step: 1131, loss is 0.2577238976955414\n",
      "epoch: 9 step: 1132, loss is 0.001359567861072719\n",
      "epoch: 9 step: 1133, loss is 0.001282789628021419\n",
      "epoch: 9 step: 1134, loss is 0.0009492996614426374\n",
      "epoch: 9 step: 1135, loss is 0.00036423420533537865\n",
      "epoch: 9 step: 1136, loss is 0.029510073363780975\n",
      "epoch: 9 step: 1137, loss is 0.01221069972962141\n",
      "epoch: 9 step: 1138, loss is 0.039098046720027924\n",
      "epoch: 9 step: 1139, loss is 6.352344644255936e-05\n",
      "epoch: 9 step: 1140, loss is 0.0020655605476349592\n",
      "epoch: 9 step: 1141, loss is 0.00010158412624150515\n",
      "epoch: 9 step: 1142, loss is 0.013769890181720257\n",
      "epoch: 9 step: 1143, loss is 0.0022500227205455303\n",
      "epoch: 9 step: 1144, loss is 0.047594841569662094\n",
      "epoch: 9 step: 1145, loss is 0.0065573761239647865\n",
      "epoch: 9 step: 1146, loss is 0.00016083793889265507\n",
      "epoch: 9 step: 1147, loss is 0.11037860810756683\n",
      "epoch: 9 step: 1148, loss is 0.0028540310449898243\n",
      "epoch: 9 step: 1149, loss is 0.00014000652299728245\n",
      "epoch: 9 step: 1150, loss is 0.15696100890636444\n",
      "epoch: 9 step: 1151, loss is 0.00015149930550251156\n",
      "epoch: 9 step: 1152, loss is 0.001548455678857863\n",
      "epoch: 9 step: 1153, loss is 0.007126612588763237\n",
      "epoch: 9 step: 1154, loss is 0.018783435225486755\n",
      "epoch: 9 step: 1155, loss is 0.007045319769531488\n",
      "epoch: 9 step: 1156, loss is 0.004287689458578825\n",
      "epoch: 9 step: 1157, loss is 0.014928974211215973\n",
      "epoch: 9 step: 1158, loss is 0.05660853534936905\n",
      "epoch: 9 step: 1159, loss is 0.0025728093460202217\n",
      "epoch: 9 step: 1160, loss is 0.0019674422219395638\n",
      "epoch: 9 step: 1161, loss is 0.06260643899440765\n",
      "epoch: 9 step: 1162, loss is 0.0072050453163683414\n",
      "epoch: 9 step: 1163, loss is 0.002330024726688862\n",
      "epoch: 9 step: 1164, loss is 0.016404621303081512\n",
      "epoch: 9 step: 1165, loss is 0.00034151392173953354\n",
      "epoch: 9 step: 1166, loss is 0.004814259707927704\n",
      "epoch: 9 step: 1167, loss is 0.003462248481810093\n",
      "epoch: 9 step: 1168, loss is 0.00015450945647899061\n",
      "epoch: 9 step: 1169, loss is 0.0003532877890393138\n",
      "epoch: 9 step: 1170, loss is 0.0010981815867125988\n",
      "epoch: 9 step: 1171, loss is 0.009911619126796722\n",
      "epoch: 9 step: 1172, loss is 0.007079415023326874\n",
      "epoch: 9 step: 1173, loss is 0.0004207128949929029\n",
      "epoch: 9 step: 1174, loss is 0.00821632333099842\n",
      "epoch: 9 step: 1175, loss is 0.0009334626374766231\n",
      "epoch: 9 step: 1176, loss is 0.0015197207685559988\n",
      "epoch: 9 step: 1177, loss is 0.27527445554733276\n",
      "epoch: 9 step: 1178, loss is 0.0023704583290964365\n",
      "epoch: 9 step: 1179, loss is 0.0003710371383931488\n",
      "epoch: 9 step: 1180, loss is 0.004886243026703596\n",
      "epoch: 9 step: 1181, loss is 8.226262434618548e-05\n",
      "epoch: 9 step: 1182, loss is 0.0004160109965596348\n",
      "epoch: 9 step: 1183, loss is 0.0009624118683859706\n",
      "epoch: 9 step: 1184, loss is 0.00530635192990303\n",
      "epoch: 9 step: 1185, loss is 0.0005159360007382929\n",
      "epoch: 9 step: 1186, loss is 0.005136437714099884\n",
      "epoch: 9 step: 1187, loss is 0.012512894347310066\n",
      "epoch: 9 step: 1188, loss is 0.0010885043302550912\n",
      "epoch: 9 step: 1189, loss is 0.0005257843877188861\n",
      "epoch: 9 step: 1190, loss is 0.0037299522664397955\n",
      "epoch: 9 step: 1191, loss is 0.0016021758783608675\n",
      "epoch: 9 step: 1192, loss is 0.0372459776699543\n",
      "epoch: 9 step: 1193, loss is 0.004029192961752415\n",
      "epoch: 9 step: 1194, loss is 0.010862361639738083\n",
      "epoch: 9 step: 1195, loss is 0.028339078649878502\n",
      "epoch: 9 step: 1196, loss is 0.0004962527309544384\n",
      "epoch: 9 step: 1197, loss is 0.0959910899400711\n",
      "epoch: 9 step: 1198, loss is 0.0012030367506667972\n",
      "epoch: 9 step: 1199, loss is 0.0005198315484449267\n",
      "epoch: 9 step: 1200, loss is 0.024721357971429825\n",
      "epoch: 9 step: 1201, loss is 0.04409394785761833\n",
      "epoch: 9 step: 1202, loss is 0.029386794194579124\n",
      "epoch: 9 step: 1203, loss is 0.01651201955974102\n",
      "epoch: 9 step: 1204, loss is 0.00021260154608171433\n",
      "epoch: 9 step: 1205, loss is 0.025926221162080765\n",
      "epoch: 9 step: 1206, loss is 0.009813053533434868\n",
      "epoch: 9 step: 1207, loss is 0.01671081781387329\n",
      "epoch: 9 step: 1208, loss is 6.594825390493497e-05\n",
      "epoch: 9 step: 1209, loss is 0.0013791280798614025\n",
      "epoch: 9 step: 1210, loss is 0.006233546417206526\n",
      "epoch: 9 step: 1211, loss is 0.0012918810825794935\n",
      "epoch: 9 step: 1212, loss is 0.0014613380189985037\n",
      "epoch: 9 step: 1213, loss is 0.003302408382296562\n",
      "epoch: 9 step: 1214, loss is 0.0020317849703133106\n",
      "epoch: 9 step: 1215, loss is 0.0036807493306696415\n",
      "epoch: 9 step: 1216, loss is 0.0003077966102864593\n",
      "epoch: 9 step: 1217, loss is 0.010003626346588135\n",
      "epoch: 9 step: 1218, loss is 0.0008857658831402659\n",
      "epoch: 9 step: 1219, loss is 0.0004835090949200094\n",
      "epoch: 9 step: 1220, loss is 0.007549864239990711\n",
      "epoch: 9 step: 1221, loss is 0.00017784369993023574\n",
      "epoch: 9 step: 1222, loss is 3.88684420613572e-05\n",
      "epoch: 9 step: 1223, loss is 0.02072438783943653\n",
      "epoch: 9 step: 1224, loss is 0.008896773681044579\n",
      "epoch: 9 step: 1225, loss is 0.07102639973163605\n",
      "epoch: 9 step: 1226, loss is 0.001524074119515717\n",
      "epoch: 9 step: 1227, loss is 0.008774570189416409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 1228, loss is 0.005196115467697382\n",
      "epoch: 9 step: 1229, loss is 0.004058632533997297\n",
      "epoch: 9 step: 1230, loss is 0.0007473264122381806\n",
      "epoch: 9 step: 1231, loss is 0.0008675691788084805\n",
      "epoch: 9 step: 1232, loss is 0.0012056645937263966\n",
      "epoch: 9 step: 1233, loss is 0.000609987007919699\n",
      "epoch: 9 step: 1234, loss is 7.695567182963714e-05\n",
      "epoch: 9 step: 1235, loss is 0.00010410817776573822\n",
      "epoch: 9 step: 1236, loss is 0.0022450052201747894\n",
      "epoch: 9 step: 1237, loss is 0.001052230829373002\n",
      "epoch: 9 step: 1238, loss is 2.5630097297835164e-06\n",
      "epoch: 9 step: 1239, loss is 0.006441249046474695\n",
      "epoch: 9 step: 1240, loss is 2.1526926502701826e-05\n",
      "epoch: 9 step: 1241, loss is 0.004297508858144283\n",
      "epoch: 9 step: 1242, loss is 0.0004417169839143753\n",
      "epoch: 9 step: 1243, loss is 0.006128864828497171\n",
      "epoch: 9 step: 1244, loss is 0.022052623331546783\n",
      "epoch: 9 step: 1245, loss is 6.735588249284774e-05\n",
      "epoch: 9 step: 1246, loss is 0.011594105511903763\n",
      "epoch: 9 step: 1247, loss is 8.583076123613864e-05\n",
      "epoch: 9 step: 1248, loss is 0.00027376407524570823\n",
      "epoch: 9 step: 1249, loss is 4.536604683380574e-05\n",
      "epoch: 9 step: 1250, loss is 0.0013149594888091087\n",
      "epoch: 9 step: 1251, loss is 0.0005958979018032551\n",
      "epoch: 9 step: 1252, loss is 0.006376656237989664\n",
      "epoch: 9 step: 1253, loss is 0.10002867877483368\n",
      "epoch: 9 step: 1254, loss is 0.0004968428402207792\n",
      "epoch: 9 step: 1255, loss is 0.002549592638388276\n",
      "epoch: 9 step: 1256, loss is 0.0029089022427797318\n",
      "epoch: 9 step: 1257, loss is 0.026640931144356728\n",
      "epoch: 9 step: 1258, loss is 0.03214217722415924\n",
      "epoch: 9 step: 1259, loss is 0.0030532339587807655\n",
      "epoch: 9 step: 1260, loss is 0.02030063420534134\n",
      "epoch: 9 step: 1261, loss is 0.0003436411207076162\n",
      "epoch: 9 step: 1262, loss is 0.008937468752264977\n",
      "epoch: 9 step: 1263, loss is 0.0021564518101513386\n",
      "epoch: 9 step: 1264, loss is 0.07721343636512756\n",
      "epoch: 9 step: 1265, loss is 0.03129398450255394\n",
      "epoch: 9 step: 1266, loss is 0.006159564014524221\n",
      "epoch: 9 step: 1267, loss is 0.0005083821597509086\n",
      "epoch: 9 step: 1268, loss is 0.0008968450711108744\n",
      "epoch: 9 step: 1269, loss is 0.26085394620895386\n",
      "epoch: 9 step: 1270, loss is 0.13268350064754486\n",
      "epoch: 9 step: 1271, loss is 0.46650615334510803\n",
      "epoch: 9 step: 1272, loss is 0.0007584428531117737\n",
      "epoch: 9 step: 1273, loss is 0.0008490384789183736\n",
      "epoch: 9 step: 1274, loss is 0.05848637595772743\n",
      "epoch: 9 step: 1275, loss is 0.020602412521839142\n",
      "epoch: 9 step: 1276, loss is 0.022838307544589043\n",
      "epoch: 9 step: 1277, loss is 0.0018449942581355572\n",
      "epoch: 9 step: 1278, loss is 0.01640424318611622\n",
      "epoch: 9 step: 1279, loss is 0.001163893030025065\n",
      "epoch: 9 step: 1280, loss is 0.02601270191371441\n",
      "epoch: 9 step: 1281, loss is 0.0028058199677616358\n",
      "epoch: 9 step: 1282, loss is 0.007866009138524532\n",
      "epoch: 9 step: 1283, loss is 0.026707138866186142\n",
      "epoch: 9 step: 1284, loss is 0.1223154067993164\n",
      "epoch: 9 step: 1285, loss is 0.009289058856666088\n",
      "epoch: 9 step: 1286, loss is 0.017887059599161148\n",
      "epoch: 9 step: 1287, loss is 0.0018663827795535326\n",
      "epoch: 9 step: 1288, loss is 0.0002818452485371381\n",
      "epoch: 9 step: 1289, loss is 0.0015086365165188909\n",
      "epoch: 9 step: 1290, loss is 0.1349945068359375\n",
      "epoch: 9 step: 1291, loss is 0.11814509332180023\n",
      "epoch: 9 step: 1292, loss is 0.04292115941643715\n",
      "epoch: 9 step: 1293, loss is 0.06808946281671524\n",
      "epoch: 9 step: 1294, loss is 0.009122466668486595\n",
      "epoch: 9 step: 1295, loss is 0.0040339152328670025\n",
      "epoch: 9 step: 1296, loss is 0.0010928926058113575\n",
      "epoch: 9 step: 1297, loss is 0.0003727251314558089\n",
      "epoch: 9 step: 1298, loss is 0.0022387055214494467\n",
      "epoch: 9 step: 1299, loss is 0.0025441383477300406\n",
      "epoch: 9 step: 1300, loss is 0.0002582039451226592\n",
      "epoch: 9 step: 1301, loss is 0.00045083879376761615\n",
      "epoch: 9 step: 1302, loss is 0.0007762967725284398\n",
      "epoch: 9 step: 1303, loss is 0.0018742592073976994\n",
      "epoch: 9 step: 1304, loss is 0.003635802771896124\n",
      "epoch: 9 step: 1305, loss is 0.00014829114661552012\n",
      "epoch: 9 step: 1306, loss is 0.00044291611993685365\n",
      "epoch: 9 step: 1307, loss is 0.05799471586942673\n",
      "epoch: 9 step: 1308, loss is 0.00283813220448792\n",
      "epoch: 9 step: 1309, loss is 0.005730294622480869\n",
      "epoch: 9 step: 1310, loss is 0.00019288848852738738\n",
      "epoch: 9 step: 1311, loss is 0.0010689659975469112\n",
      "epoch: 9 step: 1312, loss is 0.03542586416006088\n",
      "epoch: 9 step: 1313, loss is 0.0014561814023181796\n",
      "epoch: 9 step: 1314, loss is 0.0030782639514654875\n",
      "epoch: 9 step: 1315, loss is 6.183236109791324e-05\n",
      "epoch: 9 step: 1316, loss is 0.009049121290445328\n",
      "epoch: 9 step: 1317, loss is 0.047259870916604996\n",
      "epoch: 9 step: 1318, loss is 0.0014439522055909038\n",
      "epoch: 9 step: 1319, loss is 0.005281464196741581\n",
      "epoch: 9 step: 1320, loss is 0.006092685274779797\n",
      "epoch: 9 step: 1321, loss is 0.0009084087796509266\n",
      "epoch: 9 step: 1322, loss is 0.004073194693773985\n",
      "epoch: 9 step: 1323, loss is 0.0016930095152929425\n",
      "epoch: 9 step: 1324, loss is 3.064811244257726e-05\n",
      "epoch: 9 step: 1325, loss is 0.015303914435207844\n",
      "epoch: 9 step: 1326, loss is 0.003211835166439414\n",
      "epoch: 9 step: 1327, loss is 0.0001380982284899801\n",
      "epoch: 9 step: 1328, loss is 0.019752856343984604\n",
      "epoch: 9 step: 1329, loss is 0.01856520213186741\n",
      "epoch: 9 step: 1330, loss is 0.003831711132079363\n",
      "epoch: 9 step: 1331, loss is 0.0008233032422140241\n",
      "epoch: 9 step: 1332, loss is 9.10424132598564e-05\n",
      "epoch: 9 step: 1333, loss is 0.03941617161035538\n",
      "epoch: 9 step: 1334, loss is 0.0719342827796936\n",
      "epoch: 9 step: 1335, loss is 0.007172802463173866\n",
      "epoch: 9 step: 1336, loss is 0.09287291020154953\n",
      "epoch: 9 step: 1337, loss is 8.345047535840422e-05\n",
      "epoch: 9 step: 1338, loss is 0.0005808985442854464\n",
      "epoch: 9 step: 1339, loss is 0.0010768899228423834\n",
      "epoch: 9 step: 1340, loss is 0.0003338162205182016\n",
      "epoch: 9 step: 1341, loss is 0.00020360869530122727\n",
      "epoch: 9 step: 1342, loss is 0.001848009298555553\n",
      "epoch: 9 step: 1343, loss is 0.0016164579428732395\n",
      "epoch: 9 step: 1344, loss is 0.007398681715130806\n",
      "epoch: 9 step: 1345, loss is 0.0010252654319629073\n",
      "epoch: 9 step: 1346, loss is 0.0014321362832561135\n",
      "epoch: 9 step: 1347, loss is 0.006718259304761887\n",
      "epoch: 9 step: 1348, loss is 0.0003076086286455393\n",
      "epoch: 9 step: 1349, loss is 0.024180755019187927\n",
      "epoch: 9 step: 1350, loss is 0.06339868158102036\n",
      "epoch: 9 step: 1351, loss is 0.0014783133519813418\n",
      "epoch: 9 step: 1352, loss is 0.1528649479150772\n",
      "epoch: 9 step: 1353, loss is 0.1500922590494156\n",
      "epoch: 9 step: 1354, loss is 0.0009501475724391639\n",
      "epoch: 9 step: 1355, loss is 4.529249054030515e-05\n",
      "epoch: 9 step: 1356, loss is 0.0013688525650650263\n",
      "epoch: 9 step: 1357, loss is 0.00020879962539765984\n",
      "epoch: 9 step: 1358, loss is 0.014113739132881165\n",
      "epoch: 9 step: 1359, loss is 0.003987202420830727\n",
      "epoch: 9 step: 1360, loss is 0.005923534277826548\n",
      "epoch: 9 step: 1361, loss is 0.0032662537414580584\n",
      "epoch: 9 step: 1362, loss is 0.0003246758715249598\n",
      "epoch: 9 step: 1363, loss is 0.051867250353097916\n",
      "epoch: 9 step: 1364, loss is 0.017562834545969963\n",
      "epoch: 9 step: 1365, loss is 0.1444109082221985\n",
      "epoch: 9 step: 1366, loss is 0.0007773597608320415\n",
      "epoch: 9 step: 1367, loss is 0.006833198945969343\n",
      "epoch: 9 step: 1368, loss is 0.000246626470470801\n",
      "epoch: 9 step: 1369, loss is 0.0006875160615891218\n",
      "epoch: 9 step: 1370, loss is 0.08006302267313004\n",
      "epoch: 9 step: 1371, loss is 0.0006915272679179907\n",
      "epoch: 9 step: 1372, loss is 0.03243310749530792\n",
      "epoch: 9 step: 1373, loss is 0.0026136974338442087\n",
      "epoch: 9 step: 1374, loss is 0.017228323966264725\n",
      "epoch: 9 step: 1375, loss is 0.0007218030514195561\n",
      "epoch: 9 step: 1376, loss is 0.0006000340799801052\n",
      "epoch: 9 step: 1377, loss is 0.0003165494417771697\n",
      "epoch: 9 step: 1378, loss is 0.008788886480033398\n",
      "epoch: 9 step: 1379, loss is 0.048538681119680405\n",
      "epoch: 9 step: 1380, loss is 0.09321509301662445\n",
      "epoch: 9 step: 1381, loss is 0.01529780589044094\n",
      "epoch: 9 step: 1382, loss is 0.0013500968925654888\n",
      "epoch: 9 step: 1383, loss is 0.0005087209865450859\n",
      "epoch: 9 step: 1384, loss is 1.661183705436997e-05\n",
      "epoch: 9 step: 1385, loss is 0.005602447781711817\n",
      "epoch: 9 step: 1386, loss is 0.0006497607100754976\n",
      "epoch: 9 step: 1387, loss is 0.0003584082587622106\n",
      "epoch: 9 step: 1388, loss is 0.003953592386096716\n",
      "epoch: 9 step: 1389, loss is 0.000674779643304646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 1390, loss is 0.004378915764391422\n",
      "epoch: 9 step: 1391, loss is 0.0005379630019888282\n",
      "epoch: 9 step: 1392, loss is 0.021129077300429344\n",
      "epoch: 9 step: 1393, loss is 0.006999436765909195\n",
      "epoch: 9 step: 1394, loss is 0.3233596682548523\n",
      "epoch: 9 step: 1395, loss is 0.012947848066687584\n",
      "epoch: 9 step: 1396, loss is 0.008521992713212967\n",
      "epoch: 9 step: 1397, loss is 0.0017035696655511856\n",
      "epoch: 9 step: 1398, loss is 0.005427800118923187\n",
      "epoch: 9 step: 1399, loss is 0.02766338735818863\n",
      "epoch: 9 step: 1400, loss is 0.0006640791543759406\n",
      "epoch: 9 step: 1401, loss is 0.0020327968522906303\n",
      "epoch: 9 step: 1402, loss is 0.0006112761911936104\n",
      "epoch: 9 step: 1403, loss is 0.0003278504591435194\n",
      "epoch: 9 step: 1404, loss is 0.04093795269727707\n",
      "epoch: 9 step: 1405, loss is 0.001929222489707172\n",
      "epoch: 9 step: 1406, loss is 0.0010748328641057014\n",
      "epoch: 9 step: 1407, loss is 0.0006930971867404878\n",
      "epoch: 9 step: 1408, loss is 0.002868572948500514\n",
      "epoch: 9 step: 1409, loss is 0.0013609954621642828\n",
      "epoch: 9 step: 1410, loss is 0.00022844993509352207\n",
      "epoch: 9 step: 1411, loss is 0.001039421884343028\n",
      "epoch: 9 step: 1412, loss is 0.0017430644948035479\n",
      "epoch: 9 step: 1413, loss is 0.0018974022241309285\n",
      "epoch: 9 step: 1414, loss is 0.002340185223147273\n",
      "epoch: 9 step: 1415, loss is 0.004333053715527058\n",
      "epoch: 9 step: 1416, loss is 0.0009407135657966137\n",
      "epoch: 9 step: 1417, loss is 6.748360465280712e-05\n",
      "epoch: 9 step: 1418, loss is 0.24071192741394043\n",
      "epoch: 9 step: 1419, loss is 0.0011698507005348802\n",
      "epoch: 9 step: 1420, loss is 0.002186989411711693\n",
      "epoch: 9 step: 1421, loss is 0.0057154456153512\n",
      "epoch: 9 step: 1422, loss is 0.00023116184456739575\n",
      "epoch: 9 step: 1423, loss is 0.0007755468832328916\n",
      "epoch: 9 step: 1424, loss is 0.03214481845498085\n",
      "epoch: 9 step: 1425, loss is 0.01463712565600872\n",
      "epoch: 9 step: 1426, loss is 0.0007133846520446241\n",
      "epoch: 9 step: 1427, loss is 0.0007225134177133441\n",
      "epoch: 9 step: 1428, loss is 0.0011629110667854548\n",
      "epoch: 9 step: 1429, loss is 0.07267531752586365\n",
      "epoch: 9 step: 1430, loss is 0.00756005709990859\n",
      "epoch: 9 step: 1431, loss is 0.01469092071056366\n",
      "epoch: 9 step: 1432, loss is 0.0018808272434398532\n",
      "epoch: 9 step: 1433, loss is 0.004677746910601854\n",
      "epoch: 9 step: 1434, loss is 0.043165307492017746\n",
      "epoch: 9 step: 1435, loss is 0.000558940228074789\n",
      "epoch: 9 step: 1436, loss is 0.013016260229051113\n",
      "epoch: 9 step: 1437, loss is 0.00034651701571419835\n",
      "epoch: 9 step: 1438, loss is 0.0034987665712833405\n",
      "epoch: 9 step: 1439, loss is 0.012439115904271603\n",
      "epoch: 9 step: 1440, loss is 0.00684807263314724\n",
      "epoch: 9 step: 1441, loss is 0.0010353748220950365\n",
      "epoch: 9 step: 1442, loss is 0.01791590452194214\n",
      "epoch: 9 step: 1443, loss is 0.0023656634148210287\n",
      "epoch: 9 step: 1444, loss is 0.0009731216705404222\n",
      "epoch: 9 step: 1445, loss is 0.0002230558602605015\n",
      "epoch: 9 step: 1446, loss is 0.0031654888298362494\n",
      "epoch: 9 step: 1447, loss is 0.02445828542113304\n",
      "epoch: 9 step: 1448, loss is 0.004330328665673733\n",
      "epoch: 9 step: 1449, loss is 0.00025283536524511874\n",
      "epoch: 9 step: 1450, loss is 0.0002765880199149251\n",
      "epoch: 9 step: 1451, loss is 0.0004039843042846769\n",
      "epoch: 9 step: 1452, loss is 0.00814239401370287\n",
      "epoch: 9 step: 1453, loss is 0.0008684694766998291\n",
      "epoch: 9 step: 1454, loss is 0.028266217559576035\n",
      "epoch: 9 step: 1455, loss is 0.0005771092255599797\n",
      "epoch: 9 step: 1456, loss is 3.764551001950167e-05\n",
      "epoch: 9 step: 1457, loss is 0.010475295595824718\n",
      "epoch: 9 step: 1458, loss is 0.0003781431005336344\n",
      "epoch: 9 step: 1459, loss is 0.0006554642459377646\n",
      "epoch: 9 step: 1460, loss is 0.025055093690752983\n",
      "epoch: 9 step: 1461, loss is 0.002209808910265565\n",
      "epoch: 9 step: 1462, loss is 0.08303911238908768\n",
      "epoch: 9 step: 1463, loss is 0.00033324171090498567\n",
      "epoch: 9 step: 1464, loss is 0.006874805316329002\n",
      "epoch: 9 step: 1465, loss is 0.0006076697609387338\n",
      "epoch: 9 step: 1466, loss is 0.0008292457787320018\n",
      "epoch: 9 step: 1467, loss is 0.000758173584472388\n",
      "epoch: 9 step: 1468, loss is 0.003625529818236828\n",
      "epoch: 9 step: 1469, loss is 0.0001885782548924908\n",
      "epoch: 9 step: 1470, loss is 0.0005023978883400559\n",
      "epoch: 9 step: 1471, loss is 4.035425081383437e-05\n",
      "epoch: 9 step: 1472, loss is 0.00027597701409831643\n",
      "epoch: 9 step: 1473, loss is 0.002672327682375908\n",
      "epoch: 9 step: 1474, loss is 0.0002044810971710831\n",
      "epoch: 9 step: 1475, loss is 0.009144836105406284\n",
      "epoch: 9 step: 1476, loss is 0.00010944240784738213\n",
      "epoch: 9 step: 1477, loss is 7.205151632661e-05\n",
      "epoch: 9 step: 1478, loss is 0.002603979082778096\n",
      "epoch: 9 step: 1479, loss is 0.024675926193594933\n",
      "epoch: 9 step: 1480, loss is 0.006083185784518719\n",
      "epoch: 9 step: 1481, loss is 0.035490188747644424\n",
      "epoch: 9 step: 1482, loss is 0.03413400426506996\n",
      "epoch: 9 step: 1483, loss is 0.029530107975006104\n",
      "epoch: 9 step: 1484, loss is 0.002011036267504096\n",
      "epoch: 9 step: 1485, loss is 0.0004492545558605343\n",
      "epoch: 9 step: 1486, loss is 0.00030977249843999743\n",
      "epoch: 9 step: 1487, loss is 0.049734365195035934\n",
      "epoch: 9 step: 1488, loss is 0.00016399819287471473\n",
      "epoch: 9 step: 1489, loss is 0.0003813994990196079\n",
      "epoch: 9 step: 1490, loss is 6.741780089214444e-05\n",
      "epoch: 9 step: 1491, loss is 0.01411504577845335\n",
      "epoch: 9 step: 1492, loss is 3.8547503208974376e-05\n",
      "epoch: 9 step: 1493, loss is 0.00013175973435863853\n",
      "epoch: 9 step: 1494, loss is 0.0020576613023877144\n",
      "epoch: 9 step: 1495, loss is 0.003747362643480301\n",
      "epoch: 9 step: 1496, loss is 0.0003680395893752575\n",
      "epoch: 9 step: 1497, loss is 0.0003734753408934921\n",
      "epoch: 9 step: 1498, loss is 0.00022115083993412554\n",
      "epoch: 9 step: 1499, loss is 0.0008378942729905248\n",
      "epoch: 9 step: 1500, loss is 0.00016722612781450152\n",
      "epoch: 9 step: 1501, loss is 0.02771928906440735\n",
      "epoch: 9 step: 1502, loss is 7.614011701662093e-05\n",
      "epoch: 9 step: 1503, loss is 0.14415495097637177\n",
      "epoch: 9 step: 1504, loss is 0.027602974325418472\n",
      "epoch: 9 step: 1505, loss is 0.005544555839151144\n",
      "epoch: 9 step: 1506, loss is 6.042320092092268e-05\n",
      "epoch: 9 step: 1507, loss is 0.03414761647582054\n",
      "epoch: 9 step: 1508, loss is 1.1066533261328004e-05\n",
      "epoch: 9 step: 1509, loss is 0.0029469707515090704\n",
      "epoch: 9 step: 1510, loss is 0.008125067688524723\n",
      "epoch: 9 step: 1511, loss is 0.014862043783068657\n",
      "epoch: 9 step: 1512, loss is 0.0006960507016628981\n",
      "epoch: 9 step: 1513, loss is 0.0014323674840852618\n",
      "epoch: 9 step: 1514, loss is 0.0004203462158329785\n",
      "epoch: 9 step: 1515, loss is 0.0001122277244576253\n",
      "epoch: 9 step: 1516, loss is 0.012758564203977585\n",
      "epoch: 9 step: 1517, loss is 0.0006311626057140529\n",
      "epoch: 9 step: 1518, loss is 0.00013766842312179506\n",
      "epoch: 9 step: 1519, loss is 8.581187285017222e-05\n",
      "epoch: 9 step: 1520, loss is 7.03941987012513e-05\n",
      "epoch: 9 step: 1521, loss is 1.2010841601295397e-05\n",
      "epoch: 9 step: 1522, loss is 0.0006259743240661919\n",
      "epoch: 9 step: 1523, loss is 0.0017244432820007205\n",
      "epoch: 9 step: 1524, loss is 3.610555359045975e-05\n",
      "epoch: 9 step: 1525, loss is 0.018898772075772285\n",
      "epoch: 9 step: 1526, loss is 0.3697880506515503\n",
      "epoch: 9 step: 1527, loss is 0.0015816539525985718\n",
      "epoch: 9 step: 1528, loss is 0.00015230831922963262\n",
      "epoch: 9 step: 1529, loss is 3.6894914956064895e-05\n",
      "epoch: 9 step: 1530, loss is 0.08298233151435852\n",
      "epoch: 9 step: 1531, loss is 0.0035397594328969717\n",
      "epoch: 9 step: 1532, loss is 0.0003455930855125189\n",
      "epoch: 9 step: 1533, loss is 0.0002794917381834239\n",
      "epoch: 9 step: 1534, loss is 0.04022098705172539\n",
      "epoch: 9 step: 1535, loss is 0.008776857517659664\n",
      "epoch: 9 step: 1536, loss is 0.009283586405217648\n",
      "epoch: 9 step: 1537, loss is 0.0006427390035241842\n",
      "epoch: 9 step: 1538, loss is 0.008891630917787552\n",
      "epoch: 9 step: 1539, loss is 0.10220620036125183\n",
      "epoch: 9 step: 1540, loss is 0.12386660277843475\n",
      "epoch: 9 step: 1541, loss is 0.0006452756933867931\n",
      "epoch: 9 step: 1542, loss is 0.0011231605894863605\n",
      "epoch: 9 step: 1543, loss is 0.04499034583568573\n",
      "epoch: 9 step: 1544, loss is 0.0020816365722566843\n",
      "epoch: 9 step: 1545, loss is 0.10433375090360641\n",
      "epoch: 9 step: 1546, loss is 3.0521387088811025e-05\n",
      "epoch: 9 step: 1547, loss is 0.00076173065463081\n",
      "epoch: 9 step: 1548, loss is 0.01778308115899563\n",
      "epoch: 9 step: 1549, loss is 0.010539365001022816\n",
      "epoch: 9 step: 1550, loss is 0.007573686074465513\n",
      "epoch: 9 step: 1551, loss is 5.5661403166595846e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 1552, loss is 0.11632930487394333\n",
      "epoch: 9 step: 1553, loss is 0.006864360999315977\n",
      "epoch: 9 step: 1554, loss is 0.007534767501056194\n",
      "epoch: 9 step: 1555, loss is 0.041971445083618164\n",
      "epoch: 9 step: 1556, loss is 0.0007649898761883378\n",
      "epoch: 9 step: 1557, loss is 0.002344106324017048\n",
      "epoch: 9 step: 1558, loss is 0.017883209511637688\n",
      "epoch: 9 step: 1559, loss is 0.05022658035159111\n",
      "epoch: 9 step: 1560, loss is 8.568384509999305e-05\n",
      "epoch: 9 step: 1561, loss is 0.009304177947342396\n",
      "epoch: 9 step: 1562, loss is 0.00028598750941455364\n",
      "epoch: 9 step: 1563, loss is 0.014551877044141293\n",
      "epoch: 9 step: 1564, loss is 0.0006313423509709537\n",
      "epoch: 9 step: 1565, loss is 0.002497059991583228\n",
      "epoch: 9 step: 1566, loss is 0.0026110648177564144\n",
      "epoch: 9 step: 1567, loss is 0.002671529073268175\n",
      "epoch: 9 step: 1568, loss is 0.06794627755880356\n",
      "epoch: 9 step: 1569, loss is 0.0020412474405020475\n",
      "epoch: 9 step: 1570, loss is 0.0007663450669497252\n",
      "epoch: 9 step: 1571, loss is 0.20244845747947693\n",
      "epoch: 9 step: 1572, loss is 0.0006419747951440513\n",
      "epoch: 9 step: 1573, loss is 0.007784535177052021\n",
      "epoch: 9 step: 1574, loss is 8.256862929556519e-05\n",
      "epoch: 9 step: 1575, loss is 0.038014866411685944\n",
      "epoch: 9 step: 1576, loss is 0.004510644823312759\n",
      "epoch: 9 step: 1577, loss is 0.007701558526605368\n",
      "epoch: 9 step: 1578, loss is 0.007194505538791418\n",
      "epoch: 9 step: 1579, loss is 0.004404871258884668\n",
      "epoch: 9 step: 1580, loss is 0.015646586194634438\n",
      "epoch: 9 step: 1581, loss is 0.06208698824048042\n",
      "epoch: 9 step: 1582, loss is 0.0003269507724326104\n",
      "epoch: 9 step: 1583, loss is 0.01871206983923912\n",
      "epoch: 9 step: 1584, loss is 0.00340930069796741\n",
      "epoch: 9 step: 1585, loss is 0.0017026594141498208\n",
      "epoch: 9 step: 1586, loss is 0.029434045776724815\n",
      "epoch: 9 step: 1587, loss is 0.0307946614921093\n",
      "epoch: 9 step: 1588, loss is 0.0012542862677946687\n",
      "epoch: 9 step: 1589, loss is 3.827436739811674e-05\n",
      "epoch: 9 step: 1590, loss is 0.0014884626725688577\n",
      "epoch: 9 step: 1591, loss is 0.0007710582576692104\n",
      "epoch: 9 step: 1592, loss is 0.0014835539041087031\n",
      "epoch: 9 step: 1593, loss is 0.005866970866918564\n",
      "epoch: 9 step: 1594, loss is 0.0006593124126084149\n",
      "epoch: 9 step: 1595, loss is 0.002592853968963027\n",
      "epoch: 9 step: 1596, loss is 0.0785510316491127\n",
      "epoch: 9 step: 1597, loss is 0.0011143158189952374\n",
      "epoch: 9 step: 1598, loss is 1.4093302524997853e-05\n",
      "epoch: 9 step: 1599, loss is 0.0013400842435657978\n",
      "epoch: 9 step: 1600, loss is 0.047209057956933975\n",
      "epoch: 9 step: 1601, loss is 0.007409028708934784\n",
      "epoch: 9 step: 1602, loss is 0.0067498269490897655\n",
      "epoch: 9 step: 1603, loss is 0.003945165779441595\n",
      "epoch: 9 step: 1604, loss is 7.569378067273647e-05\n",
      "epoch: 9 step: 1605, loss is 0.0021044332534074783\n",
      "epoch: 9 step: 1606, loss is 0.00023018168576527387\n",
      "epoch: 9 step: 1607, loss is 0.0021417865063995123\n",
      "epoch: 9 step: 1608, loss is 0.28512808680534363\n",
      "epoch: 9 step: 1609, loss is 0.00019600045925471932\n",
      "epoch: 9 step: 1610, loss is 0.0009446673793718219\n",
      "epoch: 9 step: 1611, loss is 3.5277611459605396e-05\n",
      "epoch: 9 step: 1612, loss is 0.0006144289509393275\n",
      "epoch: 9 step: 1613, loss is 0.0001563319528941065\n",
      "epoch: 9 step: 1614, loss is 0.0004248941841069609\n",
      "epoch: 9 step: 1615, loss is 0.1444285660982132\n",
      "epoch: 9 step: 1616, loss is 0.0011084281140938401\n",
      "epoch: 9 step: 1617, loss is 0.005835545249283314\n",
      "epoch: 9 step: 1618, loss is 0.012178587727248669\n",
      "epoch: 9 step: 1619, loss is 0.003986099734902382\n",
      "epoch: 9 step: 1620, loss is 0.0001434719597455114\n",
      "epoch: 9 step: 1621, loss is 0.0012731601018458605\n",
      "epoch: 9 step: 1622, loss is 0.009635758586227894\n",
      "epoch: 9 step: 1623, loss is 0.006325031630694866\n",
      "epoch: 9 step: 1624, loss is 0.006187185179442167\n",
      "epoch: 9 step: 1625, loss is 0.08634098619222641\n",
      "epoch: 9 step: 1626, loss is 0.04255538433790207\n",
      "epoch: 9 step: 1627, loss is 0.0001660287962295115\n",
      "epoch: 9 step: 1628, loss is 0.0032434563618153334\n",
      "epoch: 9 step: 1629, loss is 0.001381922629661858\n",
      "epoch: 9 step: 1630, loss is 0.0003177933394908905\n",
      "epoch: 9 step: 1631, loss is 0.00120696728117764\n",
      "epoch: 9 step: 1632, loss is 0.00688351271674037\n",
      "epoch: 9 step: 1633, loss is 0.01622086390852928\n",
      "epoch: 9 step: 1634, loss is 0.15256209671497345\n",
      "epoch: 9 step: 1635, loss is 0.006249221973121166\n",
      "epoch: 9 step: 1636, loss is 0.10804231464862823\n",
      "epoch: 9 step: 1637, loss is 7.147283758968115e-05\n",
      "epoch: 9 step: 1638, loss is 0.017844418063759804\n",
      "epoch: 9 step: 1639, loss is 0.000554006895981729\n",
      "epoch: 9 step: 1640, loss is 0.002781251212581992\n",
      "epoch: 9 step: 1641, loss is 0.022544359788298607\n",
      "epoch: 9 step: 1642, loss is 0.0005560707650147378\n",
      "epoch: 9 step: 1643, loss is 0.002043580636382103\n",
      "epoch: 9 step: 1644, loss is 0.0036429129540920258\n",
      "epoch: 9 step: 1645, loss is 0.0006962432526051998\n",
      "epoch: 9 step: 1646, loss is 0.1318618357181549\n",
      "epoch: 9 step: 1647, loss is 0.012044613249599934\n",
      "epoch: 9 step: 1648, loss is 0.006369966547936201\n",
      "epoch: 9 step: 1649, loss is 0.04023640230298042\n",
      "epoch: 9 step: 1650, loss is 0.00031499259057454765\n",
      "epoch: 9 step: 1651, loss is 0.0026208041235804558\n",
      "epoch: 9 step: 1652, loss is 0.0010161317186430097\n",
      "epoch: 9 step: 1653, loss is 0.007260072510689497\n",
      "epoch: 9 step: 1654, loss is 0.004513448104262352\n",
      "epoch: 9 step: 1655, loss is 0.007022585719823837\n",
      "epoch: 9 step: 1656, loss is 0.01890455186367035\n",
      "epoch: 9 step: 1657, loss is 0.04820552468299866\n",
      "epoch: 9 step: 1658, loss is 0.00018468864436727017\n",
      "epoch: 9 step: 1659, loss is 3.9191669202409685e-05\n",
      "epoch: 9 step: 1660, loss is 0.002028158400207758\n",
      "epoch: 9 step: 1661, loss is 0.00347543484531343\n",
      "epoch: 9 step: 1662, loss is 0.10066557675600052\n",
      "epoch: 9 step: 1663, loss is 0.0003858644049614668\n",
      "epoch: 9 step: 1664, loss is 0.047169048339128494\n",
      "epoch: 9 step: 1665, loss is 0.0034819068387150764\n",
      "epoch: 9 step: 1666, loss is 0.0018042437732219696\n",
      "epoch: 9 step: 1667, loss is 0.0029413574375212193\n",
      "epoch: 9 step: 1668, loss is 0.10068351030349731\n",
      "epoch: 9 step: 1669, loss is 0.01700022630393505\n",
      "epoch: 9 step: 1670, loss is 0.0003816604148596525\n",
      "epoch: 9 step: 1671, loss is 0.0003802200371865183\n",
      "epoch: 9 step: 1672, loss is 0.08143584430217743\n",
      "epoch: 9 step: 1673, loss is 5.9940161008853465e-05\n",
      "epoch: 9 step: 1674, loss is 0.0001149659656221047\n",
      "epoch: 9 step: 1675, loss is 0.0011280688922852278\n",
      "epoch: 9 step: 1676, loss is 0.09585282951593399\n",
      "epoch: 9 step: 1677, loss is 0.013930047862231731\n",
      "epoch: 9 step: 1678, loss is 0.00016735393728595227\n",
      "epoch: 9 step: 1679, loss is 0.1182374507188797\n",
      "epoch: 9 step: 1680, loss is 0.0951961874961853\n",
      "epoch: 9 step: 1681, loss is 0.00028044157079420984\n",
      "epoch: 9 step: 1682, loss is 0.00026354784495197237\n",
      "epoch: 9 step: 1683, loss is 0.10711309313774109\n",
      "epoch: 9 step: 1684, loss is 0.13957667350769043\n",
      "epoch: 9 step: 1685, loss is 0.0002170281659346074\n",
      "epoch: 9 step: 1686, loss is 0.01334797777235508\n",
      "epoch: 9 step: 1687, loss is 5.0492755690356717e-05\n",
      "epoch: 9 step: 1688, loss is 0.00010638560343068093\n",
      "epoch: 9 step: 1689, loss is 0.00048010415048338473\n",
      "epoch: 9 step: 1690, loss is 0.01038491539657116\n",
      "epoch: 9 step: 1691, loss is 0.007440625689923763\n",
      "epoch: 9 step: 1692, loss is 0.002542011672630906\n",
      "epoch: 9 step: 1693, loss is 0.000967507075984031\n",
      "epoch: 9 step: 1694, loss is 0.000868660572450608\n",
      "epoch: 9 step: 1695, loss is 0.10114652663469315\n",
      "epoch: 9 step: 1696, loss is 0.0039388942532241344\n",
      "epoch: 9 step: 1697, loss is 8.181906014215201e-05\n",
      "epoch: 9 step: 1698, loss is 0.004500202834606171\n",
      "epoch: 9 step: 1699, loss is 5.5490068916697055e-05\n",
      "epoch: 9 step: 1700, loss is 0.00011739556794054806\n",
      "epoch: 9 step: 1701, loss is 0.0037472727708518505\n",
      "epoch: 9 step: 1702, loss is 0.04727262258529663\n",
      "epoch: 9 step: 1703, loss is 0.05560740455985069\n",
      "epoch: 9 step: 1704, loss is 0.006131026893854141\n",
      "epoch: 9 step: 1705, loss is 0.0008799149072729051\n",
      "epoch: 9 step: 1706, loss is 0.0007931147702038288\n",
      "epoch: 9 step: 1707, loss is 0.0008576858090236783\n",
      "epoch: 9 step: 1708, loss is 0.0014409542782232165\n",
      "epoch: 9 step: 1709, loss is 0.03574449568986893\n",
      "epoch: 9 step: 1710, loss is 0.00047092261957004666\n",
      "epoch: 9 step: 1711, loss is 0.003853320376947522\n",
      "epoch: 9 step: 1712, loss is 0.003554695285856724\n",
      "epoch: 9 step: 1713, loss is 0.006339989136904478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 1714, loss is 0.04622754082083702\n",
      "epoch: 9 step: 1715, loss is 0.0002788296842481941\n",
      "epoch: 9 step: 1716, loss is 0.11538806557655334\n",
      "epoch: 9 step: 1717, loss is 0.12366919964551926\n",
      "epoch: 9 step: 1718, loss is 0.001968289725482464\n",
      "epoch: 9 step: 1719, loss is 0.0001716881524771452\n",
      "epoch: 9 step: 1720, loss is 0.0012422737199813128\n",
      "epoch: 9 step: 1721, loss is 0.0006148170796222985\n",
      "epoch: 9 step: 1722, loss is 0.11818017810583115\n",
      "epoch: 9 step: 1723, loss is 0.0022391134407371283\n",
      "epoch: 9 step: 1724, loss is 0.0191801767796278\n",
      "epoch: 9 step: 1725, loss is 0.001853581634350121\n",
      "epoch: 9 step: 1726, loss is 0.004168404266238213\n",
      "epoch: 9 step: 1727, loss is 0.00892903096973896\n",
      "epoch: 9 step: 1728, loss is 0.003555534640327096\n",
      "epoch: 9 step: 1729, loss is 0.004799903370440006\n",
      "epoch: 9 step: 1730, loss is 0.12336549162864685\n",
      "epoch: 9 step: 1731, loss is 0.08547946810722351\n",
      "epoch: 9 step: 1732, loss is 0.11801917850971222\n",
      "epoch: 9 step: 1733, loss is 0.0024106299970299006\n",
      "epoch: 9 step: 1734, loss is 0.03532685711979866\n",
      "epoch: 9 step: 1735, loss is 0.004742293618619442\n",
      "epoch: 9 step: 1736, loss is 0.008209757506847382\n",
      "epoch: 9 step: 1737, loss is 0.0009769364260137081\n",
      "epoch: 9 step: 1738, loss is 0.004070112947374582\n",
      "epoch: 9 step: 1739, loss is 0.036831628531217575\n",
      "epoch: 9 step: 1740, loss is 0.00013585909618996084\n",
      "epoch: 9 step: 1741, loss is 0.02082437463104725\n",
      "epoch: 9 step: 1742, loss is 0.003578498028218746\n",
      "epoch: 9 step: 1743, loss is 0.031010815873742104\n",
      "epoch: 9 step: 1744, loss is 0.23901812732219696\n",
      "epoch: 9 step: 1745, loss is 0.0002943575382232666\n",
      "epoch: 9 step: 1746, loss is 0.0038098320364952087\n",
      "epoch: 9 step: 1747, loss is 0.00023794974549673498\n",
      "epoch: 9 step: 1748, loss is 0.0030264363158494234\n",
      "epoch: 9 step: 1749, loss is 0.014015939086675644\n",
      "epoch: 9 step: 1750, loss is 0.005276746582239866\n",
      "epoch: 9 step: 1751, loss is 0.00032353162532672286\n",
      "epoch: 9 step: 1752, loss is 0.00014540742267854512\n",
      "epoch: 9 step: 1753, loss is 0.019375290721654892\n",
      "epoch: 9 step: 1754, loss is 0.022603079676628113\n",
      "epoch: 9 step: 1755, loss is 0.00013928962289355695\n",
      "epoch: 9 step: 1756, loss is 0.001123634516261518\n",
      "epoch: 9 step: 1757, loss is 0.0015329198213294148\n",
      "epoch: 9 step: 1758, loss is 0.0021726929116994143\n",
      "epoch: 9 step: 1759, loss is 0.00011894282943103462\n",
      "epoch: 9 step: 1760, loss is 0.003751276060938835\n",
      "epoch: 9 step: 1761, loss is 0.055514849722385406\n",
      "epoch: 9 step: 1762, loss is 0.09350386261940002\n",
      "epoch: 9 step: 1763, loss is 0.06361649185419083\n",
      "epoch: 9 step: 1764, loss is 0.0062149264849722385\n",
      "epoch: 9 step: 1765, loss is 0.0007019643671810627\n",
      "epoch: 9 step: 1766, loss is 0.0030102217569947243\n",
      "epoch: 9 step: 1767, loss is 0.004746719263494015\n",
      "epoch: 9 step: 1768, loss is 0.0008650748059153557\n",
      "epoch: 9 step: 1769, loss is 0.0003948296362068504\n",
      "epoch: 9 step: 1770, loss is 0.0009874082170426846\n",
      "epoch: 9 step: 1771, loss is 0.016222119331359863\n",
      "epoch: 9 step: 1772, loss is 9.674431203166023e-05\n",
      "epoch: 9 step: 1773, loss is 0.003134479746222496\n",
      "epoch: 9 step: 1774, loss is 0.000371959205949679\n",
      "epoch: 9 step: 1775, loss is 0.026925833895802498\n",
      "epoch: 9 step: 1776, loss is 0.00010062569344881922\n",
      "epoch: 9 step: 1777, loss is 0.0003299710515420884\n",
      "epoch: 9 step: 1778, loss is 0.000640219368506223\n",
      "epoch: 9 step: 1779, loss is 0.0089398343116045\n",
      "epoch: 9 step: 1780, loss is 0.003709245240315795\n",
      "epoch: 9 step: 1781, loss is 0.00023235684784594923\n",
      "epoch: 9 step: 1782, loss is 0.0004680129641201347\n",
      "epoch: 9 step: 1783, loss is 0.0031341835856437683\n",
      "epoch: 9 step: 1784, loss is 0.17487528920173645\n",
      "epoch: 9 step: 1785, loss is 0.0024838487152010202\n",
      "epoch: 9 step: 1786, loss is 0.0009218999184668064\n",
      "epoch: 9 step: 1787, loss is 0.00011649750376818702\n",
      "epoch: 9 step: 1788, loss is 0.00032889225985854864\n",
      "epoch: 9 step: 1789, loss is 0.002396838739514351\n",
      "epoch: 9 step: 1790, loss is 0.0022903422359377146\n",
      "epoch: 9 step: 1791, loss is 0.09627000987529755\n",
      "epoch: 9 step: 1792, loss is 0.00013657640374731272\n",
      "epoch: 9 step: 1793, loss is 0.0005396013148128986\n",
      "epoch: 9 step: 1794, loss is 0.005076660308986902\n",
      "epoch: 9 step: 1795, loss is 0.00212941225618124\n",
      "epoch: 9 step: 1796, loss is 0.0034814493265002966\n",
      "epoch: 9 step: 1797, loss is 0.009563691914081573\n",
      "epoch: 9 step: 1798, loss is 0.0034938189201056957\n",
      "epoch: 9 step: 1799, loss is 0.00825215969234705\n",
      "epoch: 9 step: 1800, loss is 0.005502752959728241\n",
      "epoch: 9 step: 1801, loss is 0.0003909954975824803\n",
      "epoch: 9 step: 1802, loss is 0.0008329384145326912\n",
      "epoch: 9 step: 1803, loss is 0.0005436697974801064\n",
      "epoch: 9 step: 1804, loss is 0.0197617020457983\n",
      "epoch: 9 step: 1805, loss is 0.0005293205031193793\n",
      "epoch: 9 step: 1806, loss is 0.0002632118994370103\n",
      "epoch: 9 step: 1807, loss is 0.0036305193789303303\n",
      "epoch: 9 step: 1808, loss is 0.11024338752031326\n",
      "epoch: 9 step: 1809, loss is 0.00022570492001250386\n",
      "epoch: 9 step: 1810, loss is 0.05762485787272453\n",
      "epoch: 9 step: 1811, loss is 0.004757301416248083\n",
      "epoch: 9 step: 1812, loss is 0.003447245340794325\n",
      "epoch: 9 step: 1813, loss is 0.001123737427406013\n",
      "epoch: 9 step: 1814, loss is 0.0005253772251307964\n",
      "epoch: 9 step: 1815, loss is 0.00204519578255713\n",
      "epoch: 9 step: 1816, loss is 0.09957754611968994\n",
      "epoch: 9 step: 1817, loss is 0.011128492653369904\n",
      "epoch: 9 step: 1818, loss is 0.0003079956804867834\n",
      "epoch: 9 step: 1819, loss is 0.0003803220752160996\n",
      "epoch: 9 step: 1820, loss is 0.007066773716360331\n",
      "epoch: 9 step: 1821, loss is 0.0007710171048529446\n",
      "epoch: 9 step: 1822, loss is 0.0010722216684371233\n",
      "epoch: 9 step: 1823, loss is 0.001448900788091123\n",
      "epoch: 9 step: 1824, loss is 0.0018433151999488473\n",
      "epoch: 9 step: 1825, loss is 0.002860755892470479\n",
      "epoch: 9 step: 1826, loss is 0.0037166173569858074\n",
      "epoch: 9 step: 1827, loss is 0.0003834511444438249\n",
      "epoch: 9 step: 1828, loss is 0.006110555958002806\n",
      "epoch: 9 step: 1829, loss is 0.016644073650240898\n",
      "epoch: 9 step: 1830, loss is 0.006758526433259249\n",
      "epoch: 9 step: 1831, loss is 0.0031561320647597313\n",
      "epoch: 9 step: 1832, loss is 0.0003222734376322478\n",
      "epoch: 9 step: 1833, loss is 0.0038206400349736214\n",
      "epoch: 9 step: 1834, loss is 0.0005985458265058696\n",
      "epoch: 9 step: 1835, loss is 0.0007708006887696683\n",
      "epoch: 9 step: 1836, loss is 0.14068235456943512\n",
      "epoch: 9 step: 1837, loss is 0.0007682149880565703\n",
      "epoch: 9 step: 1838, loss is 0.0006035492406226695\n",
      "epoch: 9 step: 1839, loss is 0.0007722945883870125\n",
      "epoch: 9 step: 1840, loss is 0.00022524384257849306\n",
      "epoch: 9 step: 1841, loss is 0.004859349690377712\n",
      "epoch: 9 step: 1842, loss is 0.00022201426327228546\n",
      "epoch: 9 step: 1843, loss is 0.0002244512870674953\n",
      "epoch: 9 step: 1844, loss is 0.0009803916327655315\n",
      "epoch: 9 step: 1845, loss is 0.0004917900660075247\n",
      "epoch: 9 step: 1846, loss is 0.01731283590197563\n",
      "epoch: 9 step: 1847, loss is 4.229466139804572e-05\n",
      "epoch: 9 step: 1848, loss is 5.9084934036945924e-05\n",
      "epoch: 9 step: 1849, loss is 0.011717462912201881\n",
      "epoch: 9 step: 1850, loss is 0.03713786229491234\n",
      "epoch: 9 step: 1851, loss is 0.00040158035699278116\n",
      "epoch: 9 step: 1852, loss is 0.00013777974527329206\n",
      "epoch: 9 step: 1853, loss is 0.005629785358905792\n",
      "epoch: 9 step: 1854, loss is 0.009007436223328114\n",
      "epoch: 9 step: 1855, loss is 0.05158787965774536\n",
      "epoch: 9 step: 1856, loss is 5.967659308225848e-05\n",
      "epoch: 9 step: 1857, loss is 0.005736926570534706\n",
      "epoch: 9 step: 1858, loss is 0.07541386038064957\n",
      "epoch: 9 step: 1859, loss is 0.0034096345771104097\n",
      "epoch: 9 step: 1860, loss is 0.0002637782599776983\n",
      "epoch: 9 step: 1861, loss is 1.0992121133313049e-05\n",
      "epoch: 9 step: 1862, loss is 0.008956645615398884\n",
      "epoch: 9 step: 1863, loss is 0.05894581601023674\n",
      "epoch: 9 step: 1864, loss is 0.0008785551763139665\n",
      "epoch: 9 step: 1865, loss is 0.0011648571817204356\n",
      "epoch: 9 step: 1866, loss is 9.121775656240061e-05\n",
      "epoch: 9 step: 1867, loss is 0.22636808454990387\n",
      "epoch: 9 step: 1868, loss is 0.06333131343126297\n",
      "epoch: 9 step: 1869, loss is 0.000283038301859051\n",
      "epoch: 9 step: 1870, loss is 0.009430983103811741\n",
      "epoch: 9 step: 1871, loss is 0.0003331918269395828\n",
      "epoch: 9 step: 1872, loss is 0.003751171287149191\n",
      "epoch: 9 step: 1873, loss is 0.00533420592546463\n",
      "epoch: 9 step: 1874, loss is 0.008288310840725899\n",
      "epoch: 9 step: 1875, loss is 9.340453834738582e-05\n",
      "epoch: 10 step: 1, loss is 0.00028321819263510406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 2, loss is 0.0017790242563933134\n",
      "epoch: 10 step: 3, loss is 0.0008220146410167217\n",
      "epoch: 10 step: 4, loss is 0.00408735778182745\n",
      "epoch: 10 step: 5, loss is 0.004950948059558868\n",
      "epoch: 10 step: 6, loss is 0.007486929651349783\n",
      "epoch: 10 step: 7, loss is 0.006736778188496828\n",
      "epoch: 10 step: 8, loss is 0.0007643639110028744\n",
      "epoch: 10 step: 9, loss is 0.03341351076960564\n",
      "epoch: 10 step: 10, loss is 0.0002517849497962743\n",
      "epoch: 10 step: 11, loss is 0.014859210699796677\n",
      "epoch: 10 step: 12, loss is 0.0029297969304025173\n",
      "epoch: 10 step: 13, loss is 8.36588442325592e-05\n",
      "epoch: 10 step: 14, loss is 0.09709437936544418\n",
      "epoch: 10 step: 15, loss is 0.0009347901795990765\n",
      "epoch: 10 step: 16, loss is 0.07691703736782074\n",
      "epoch: 10 step: 17, loss is 0.005207360722124577\n",
      "epoch: 10 step: 18, loss is 0.00012861347931902856\n",
      "epoch: 10 step: 19, loss is 0.004045222885906696\n",
      "epoch: 10 step: 20, loss is 0.001229192828759551\n",
      "epoch: 10 step: 21, loss is 0.00878721009939909\n",
      "epoch: 10 step: 22, loss is 0.00783606804907322\n",
      "epoch: 10 step: 23, loss is 0.00012657829211093485\n",
      "epoch: 10 step: 24, loss is 0.010747979395091534\n",
      "epoch: 10 step: 25, loss is 0.0008867762517184019\n",
      "epoch: 10 step: 26, loss is 6.831373320892453e-05\n",
      "epoch: 10 step: 27, loss is 0.001470279647037387\n",
      "epoch: 10 step: 28, loss is 0.0004962879465892911\n",
      "epoch: 10 step: 29, loss is 0.0055516487918794155\n",
      "epoch: 10 step: 30, loss is 0.00617974204942584\n",
      "epoch: 10 step: 31, loss is 0.0013065298553556204\n",
      "epoch: 10 step: 32, loss is 0.0009492908720858395\n",
      "epoch: 10 step: 33, loss is 0.0001781318715075031\n",
      "epoch: 10 step: 34, loss is 0.037841543555259705\n",
      "epoch: 10 step: 35, loss is 0.011277200654149055\n",
      "epoch: 10 step: 36, loss is 0.00042745412793010473\n",
      "epoch: 10 step: 37, loss is 0.0152670219540596\n",
      "epoch: 10 step: 38, loss is 0.001547515275888145\n",
      "epoch: 10 step: 39, loss is 0.11126451194286346\n",
      "epoch: 10 step: 40, loss is 0.017618004232645035\n",
      "epoch: 10 step: 41, loss is 0.03756040334701538\n",
      "epoch: 10 step: 42, loss is 0.0010055305901914835\n",
      "epoch: 10 step: 43, loss is 0.00011385874677216634\n",
      "epoch: 10 step: 44, loss is 0.0017382933292537928\n",
      "epoch: 10 step: 45, loss is 0.0013731991639360785\n",
      "epoch: 10 step: 46, loss is 0.0071715423837304115\n",
      "epoch: 10 step: 47, loss is 0.0013264283770695329\n",
      "epoch: 10 step: 48, loss is 0.0019776448607444763\n",
      "epoch: 10 step: 49, loss is 0.0011622094316408038\n",
      "epoch: 10 step: 50, loss is 0.13159078359603882\n",
      "epoch: 10 step: 51, loss is 0.004268902353942394\n",
      "epoch: 10 step: 52, loss is 0.0007376392022706568\n",
      "epoch: 10 step: 53, loss is 0.04629773274064064\n",
      "epoch: 10 step: 54, loss is 0.0004483245429582894\n",
      "epoch: 10 step: 55, loss is 0.07067609578371048\n",
      "epoch: 10 step: 56, loss is 0.0007243931759148836\n",
      "epoch: 10 step: 57, loss is 0.060469549149274826\n",
      "epoch: 10 step: 58, loss is 0.0004178077797405422\n",
      "epoch: 10 step: 59, loss is 0.0030076070688664913\n",
      "epoch: 10 step: 60, loss is 0.004891057498753071\n",
      "epoch: 10 step: 61, loss is 0.06815655529499054\n",
      "epoch: 10 step: 62, loss is 0.007162253372371197\n",
      "epoch: 10 step: 63, loss is 0.00031096331076696515\n",
      "epoch: 10 step: 64, loss is 0.009147688746452332\n",
      "epoch: 10 step: 65, loss is 0.021623937413096428\n",
      "epoch: 10 step: 66, loss is 0.000295669975457713\n",
      "epoch: 10 step: 67, loss is 0.0027740534860640764\n",
      "epoch: 10 step: 68, loss is 0.00111196911893785\n",
      "epoch: 10 step: 69, loss is 0.011358861811459064\n",
      "epoch: 10 step: 70, loss is 0.0009866236941888928\n",
      "epoch: 10 step: 71, loss is 0.022501938045024872\n",
      "epoch: 10 step: 72, loss is 0.006632668431848288\n",
      "epoch: 10 step: 73, loss is 0.0014443532563745975\n",
      "epoch: 10 step: 74, loss is 0.0007591087487526238\n",
      "epoch: 10 step: 75, loss is 0.00014712411211803555\n",
      "epoch: 10 step: 76, loss is 0.0021758871152997017\n",
      "epoch: 10 step: 77, loss is 0.001891972846351564\n",
      "epoch: 10 step: 78, loss is 0.0014152830699458718\n",
      "epoch: 10 step: 79, loss is 0.0002778293564915657\n",
      "epoch: 10 step: 80, loss is 0.0008782243239693344\n",
      "epoch: 10 step: 81, loss is 0.00651528500020504\n",
      "epoch: 10 step: 82, loss is 0.001274341600947082\n",
      "epoch: 10 step: 83, loss is 0.17976239323616028\n",
      "epoch: 10 step: 84, loss is 0.000856984406709671\n",
      "epoch: 10 step: 85, loss is 0.034277673810720444\n",
      "epoch: 10 step: 86, loss is 8.753669681027532e-05\n",
      "epoch: 10 step: 87, loss is 0.0010878161992877722\n",
      "epoch: 10 step: 88, loss is 0.0013005899963900447\n",
      "epoch: 10 step: 89, loss is 0.0004411746049299836\n",
      "epoch: 10 step: 90, loss is 0.07130905240774155\n",
      "epoch: 10 step: 91, loss is 0.027239521965384483\n",
      "epoch: 10 step: 92, loss is 0.0015482221497222781\n",
      "epoch: 10 step: 93, loss is 0.020888013765215874\n",
      "epoch: 10 step: 94, loss is 0.0008589349454268813\n",
      "epoch: 10 step: 95, loss is 2.421172393951565e-05\n",
      "epoch: 10 step: 96, loss is 0.0013834309065714478\n",
      "epoch: 10 step: 97, loss is 0.002731401240453124\n",
      "epoch: 10 step: 98, loss is 0.000895391043741256\n",
      "epoch: 10 step: 99, loss is 0.000835393846500665\n",
      "epoch: 10 step: 100, loss is 0.0003206870751455426\n",
      "epoch: 10 step: 101, loss is 0.003475571284070611\n",
      "epoch: 10 step: 102, loss is 0.0005809210706502199\n",
      "epoch: 10 step: 103, loss is 0.0007316267001442611\n",
      "epoch: 10 step: 104, loss is 0.0017319502076134086\n",
      "epoch: 10 step: 105, loss is 0.001137981889769435\n",
      "epoch: 10 step: 106, loss is 0.0015998175367712975\n",
      "epoch: 10 step: 107, loss is 0.0005184444016776979\n",
      "epoch: 10 step: 108, loss is 0.0004374234122224152\n",
      "epoch: 10 step: 109, loss is 0.0005842049140483141\n",
      "epoch: 10 step: 110, loss is 2.8312237191130407e-05\n",
      "epoch: 10 step: 111, loss is 0.00020210427464917302\n",
      "epoch: 10 step: 112, loss is 0.0057762013748288155\n",
      "epoch: 10 step: 113, loss is 0.0011449719313532114\n",
      "epoch: 10 step: 114, loss is 0.0004678273107856512\n",
      "epoch: 10 step: 115, loss is 0.04445093870162964\n",
      "epoch: 10 step: 116, loss is 0.03578532859683037\n",
      "epoch: 10 step: 117, loss is 0.013981957919895649\n",
      "epoch: 10 step: 118, loss is 0.044214844703674316\n",
      "epoch: 10 step: 119, loss is 3.237753116991371e-05\n",
      "epoch: 10 step: 120, loss is 0.000256591709330678\n",
      "epoch: 10 step: 121, loss is 0.00016116700135171413\n",
      "epoch: 10 step: 122, loss is 0.0015136030269786716\n",
      "epoch: 10 step: 123, loss is 0.005310365930199623\n",
      "epoch: 10 step: 124, loss is 0.008449454791843891\n",
      "epoch: 10 step: 125, loss is 0.0002900069812312722\n",
      "epoch: 10 step: 126, loss is 0.01990656368434429\n",
      "epoch: 10 step: 127, loss is 0.0076206582598388195\n",
      "epoch: 10 step: 128, loss is 0.0001721792941680178\n",
      "epoch: 10 step: 129, loss is 0.000757363741286099\n",
      "epoch: 10 step: 130, loss is 0.0976739227771759\n",
      "epoch: 10 step: 131, loss is 0.0009791257325559855\n",
      "epoch: 10 step: 132, loss is 0.027680521830916405\n",
      "epoch: 10 step: 133, loss is 0.009871043264865875\n",
      "epoch: 10 step: 134, loss is 0.004502341151237488\n",
      "epoch: 10 step: 135, loss is 0.03932928293943405\n",
      "epoch: 10 step: 136, loss is 0.0005286588566377759\n",
      "epoch: 10 step: 137, loss is 0.0011483076959848404\n",
      "epoch: 10 step: 138, loss is 0.00018735058256424963\n",
      "epoch: 10 step: 139, loss is 0.004655538126826286\n",
      "epoch: 10 step: 140, loss is 0.017325859516859055\n",
      "epoch: 10 step: 141, loss is 0.001615088083781302\n",
      "epoch: 10 step: 142, loss is 0.010258020833134651\n",
      "epoch: 10 step: 143, loss is 0.00019294202502351254\n",
      "epoch: 10 step: 144, loss is 0.029317686334252357\n",
      "epoch: 10 step: 145, loss is 0.0025173185858875513\n",
      "epoch: 10 step: 146, loss is 5.270122710498981e-05\n",
      "epoch: 10 step: 147, loss is 0.02187340334057808\n",
      "epoch: 10 step: 148, loss is 0.0026030424050986767\n",
      "epoch: 10 step: 149, loss is 0.00013882186613045633\n",
      "epoch: 10 step: 150, loss is 0.0026522132102400064\n",
      "epoch: 10 step: 151, loss is 0.12750354409217834\n",
      "epoch: 10 step: 152, loss is 0.07919035851955414\n",
      "epoch: 10 step: 153, loss is 0.013996581546962261\n",
      "epoch: 10 step: 154, loss is 0.00580228678882122\n",
      "epoch: 10 step: 155, loss is 0.000625214132014662\n",
      "epoch: 10 step: 156, loss is 0.004263400565832853\n",
      "epoch: 10 step: 157, loss is 0.0011778586776927114\n",
      "epoch: 10 step: 158, loss is 0.0049575925804674625\n",
      "epoch: 10 step: 159, loss is 0.00020046732970513403\n",
      "epoch: 10 step: 160, loss is 0.01228458620607853\n",
      "epoch: 10 step: 161, loss is 0.0011830762960016727\n",
      "epoch: 10 step: 162, loss is 0.00043360816198401153\n",
      "epoch: 10 step: 163, loss is 6.118138117017224e-05\n",
      "epoch: 10 step: 164, loss is 0.0007169638993218541\n",
      "epoch: 10 step: 165, loss is 0.0004432388232089579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 166, loss is 0.00024311906599905342\n",
      "epoch: 10 step: 167, loss is 0.006804607808589935\n",
      "epoch: 10 step: 168, loss is 0.12665726244449615\n",
      "epoch: 10 step: 169, loss is 0.012178637087345123\n",
      "epoch: 10 step: 170, loss is 0.00020809730631299317\n",
      "epoch: 10 step: 171, loss is 0.0007891895365901291\n",
      "epoch: 10 step: 172, loss is 9.709283767733723e-05\n",
      "epoch: 10 step: 173, loss is 0.008591906167566776\n",
      "epoch: 10 step: 174, loss is 0.02482718601822853\n",
      "epoch: 10 step: 175, loss is 0.01537654735147953\n",
      "epoch: 10 step: 176, loss is 0.04191862419247627\n",
      "epoch: 10 step: 177, loss is 0.001106636947952211\n",
      "epoch: 10 step: 178, loss is 0.0014460758538916707\n",
      "epoch: 10 step: 179, loss is 0.17372648417949677\n",
      "epoch: 10 step: 180, loss is 0.014350838959217072\n",
      "epoch: 10 step: 181, loss is 0.0009192245779559016\n",
      "epoch: 10 step: 182, loss is 0.000672376190777868\n",
      "epoch: 10 step: 183, loss is 0.028369607403874397\n",
      "epoch: 10 step: 184, loss is 0.0034174472093582153\n",
      "epoch: 10 step: 185, loss is 0.008980871178209782\n",
      "epoch: 10 step: 186, loss is 0.0004934223834425211\n",
      "epoch: 10 step: 187, loss is 0.005021817982196808\n",
      "epoch: 10 step: 188, loss is 0.0007798164733685553\n",
      "epoch: 10 step: 189, loss is 0.0001957337954081595\n",
      "epoch: 10 step: 190, loss is 0.05644983425736427\n",
      "epoch: 10 step: 191, loss is 0.05583863705396652\n",
      "epoch: 10 step: 192, loss is 0.014183007180690765\n",
      "epoch: 10 step: 193, loss is 0.015352286398410797\n",
      "epoch: 10 step: 194, loss is 0.0015321533428505063\n",
      "epoch: 10 step: 195, loss is 0.0007602383266203105\n",
      "epoch: 10 step: 196, loss is 0.012561000883579254\n",
      "epoch: 10 step: 197, loss is 0.00019572618475649506\n",
      "epoch: 10 step: 198, loss is 0.1252562403678894\n",
      "epoch: 10 step: 199, loss is 0.04968102276325226\n",
      "epoch: 10 step: 200, loss is 0.0009484244510531425\n",
      "epoch: 10 step: 201, loss is 0.0025722659192979336\n",
      "epoch: 10 step: 202, loss is 0.00038294741534627974\n",
      "epoch: 10 step: 203, loss is 4.40300973423291e-05\n",
      "epoch: 10 step: 204, loss is 0.005406408570706844\n",
      "epoch: 10 step: 205, loss is 0.13044913113117218\n",
      "epoch: 10 step: 206, loss is 5.741495988331735e-05\n",
      "epoch: 10 step: 207, loss is 2.47419757215539e-05\n",
      "epoch: 10 step: 208, loss is 0.004115663468837738\n",
      "epoch: 10 step: 209, loss is 0.0018609768012538552\n",
      "epoch: 10 step: 210, loss is 0.13345302641391754\n",
      "epoch: 10 step: 211, loss is 0.0010212658671662211\n",
      "epoch: 10 step: 212, loss is 0.001256692223250866\n",
      "epoch: 10 step: 213, loss is 0.015917973592877388\n",
      "epoch: 10 step: 214, loss is 0.0013596294447779655\n",
      "epoch: 10 step: 215, loss is 0.0018768907757475972\n",
      "epoch: 10 step: 216, loss is 0.005802932195365429\n",
      "epoch: 10 step: 217, loss is 0.004713204223662615\n",
      "epoch: 10 step: 218, loss is 6.6348520704195835e-06\n",
      "epoch: 10 step: 219, loss is 0.01309232134371996\n",
      "epoch: 10 step: 220, loss is 0.0006793876527808607\n",
      "epoch: 10 step: 221, loss is 5.338101982488297e-05\n",
      "epoch: 10 step: 222, loss is 0.0002530083293095231\n",
      "epoch: 10 step: 223, loss is 0.0057182214222848415\n",
      "epoch: 10 step: 224, loss is 0.00014384706446435302\n",
      "epoch: 10 step: 225, loss is 0.00700001185759902\n",
      "epoch: 10 step: 226, loss is 0.025951560586690903\n",
      "epoch: 10 step: 227, loss is 0.00036094209644943476\n",
      "epoch: 10 step: 228, loss is 0.001011780695989728\n",
      "epoch: 10 step: 229, loss is 0.0030426497105509043\n",
      "epoch: 10 step: 230, loss is 0.0006085599889047444\n",
      "epoch: 10 step: 231, loss is 0.02076972834765911\n",
      "epoch: 10 step: 232, loss is 0.00011535314115462825\n",
      "epoch: 10 step: 233, loss is 0.0005543775041587651\n",
      "epoch: 10 step: 234, loss is 0.0005315651651471853\n",
      "epoch: 10 step: 235, loss is 0.000563465291634202\n",
      "epoch: 10 step: 236, loss is 0.0003232002491131425\n",
      "epoch: 10 step: 237, loss is 0.033317480236291885\n",
      "epoch: 10 step: 238, loss is 0.0021791544277220964\n",
      "epoch: 10 step: 239, loss is 0.006560752633959055\n",
      "epoch: 10 step: 240, loss is 0.17516985535621643\n",
      "epoch: 10 step: 241, loss is 0.000493484316393733\n",
      "epoch: 10 step: 242, loss is 0.00011903139966307208\n",
      "epoch: 10 step: 243, loss is 0.03435150533914566\n",
      "epoch: 10 step: 244, loss is 1.250167770194821e-05\n",
      "epoch: 10 step: 245, loss is 0.0036363834515213966\n",
      "epoch: 10 step: 246, loss is 0.006176544353365898\n",
      "epoch: 10 step: 247, loss is 0.0012941667810082436\n",
      "epoch: 10 step: 248, loss is 0.0006261219386942685\n",
      "epoch: 10 step: 249, loss is 0.02217717468738556\n",
      "epoch: 10 step: 250, loss is 0.00018933179671876132\n",
      "epoch: 10 step: 251, loss is 0.0014493941562250257\n",
      "epoch: 10 step: 252, loss is 0.019897807389497757\n",
      "epoch: 10 step: 253, loss is 0.08852635324001312\n",
      "epoch: 10 step: 254, loss is 0.005574564449489117\n",
      "epoch: 10 step: 255, loss is 0.00011174745304742828\n",
      "epoch: 10 step: 256, loss is 0.0008564341114833951\n",
      "epoch: 10 step: 257, loss is 0.0012815605150535703\n",
      "epoch: 10 step: 258, loss is 0.00011320842168061063\n",
      "epoch: 10 step: 259, loss is 0.0008219408919103444\n",
      "epoch: 10 step: 260, loss is 0.0004987021093256772\n",
      "epoch: 10 step: 261, loss is 0.0007316711125895381\n",
      "epoch: 10 step: 262, loss is 0.003384972922503948\n",
      "epoch: 10 step: 263, loss is 0.006811095867305994\n",
      "epoch: 10 step: 264, loss is 0.00041173704084940255\n",
      "epoch: 10 step: 265, loss is 0.000399402721086517\n",
      "epoch: 10 step: 266, loss is 0.0010285972384735942\n",
      "epoch: 10 step: 267, loss is 0.0009411672945134342\n",
      "epoch: 10 step: 268, loss is 0.1041329950094223\n",
      "epoch: 10 step: 269, loss is 0.0014723611529916525\n",
      "epoch: 10 step: 270, loss is 0.0017105898587033153\n",
      "epoch: 10 step: 271, loss is 0.0030851124320179224\n",
      "epoch: 10 step: 272, loss is 0.08074203133583069\n",
      "epoch: 10 step: 273, loss is 0.0013123846147209406\n",
      "epoch: 10 step: 274, loss is 0.0006570987170562148\n",
      "epoch: 10 step: 275, loss is 6.703846156597137e-05\n",
      "epoch: 10 step: 276, loss is 0.002400859259068966\n",
      "epoch: 10 step: 277, loss is 0.0008908619056455791\n",
      "epoch: 10 step: 278, loss is 1.9804314433713444e-05\n",
      "epoch: 10 step: 279, loss is 0.0007074126624502242\n",
      "epoch: 10 step: 280, loss is 0.013108053244650364\n",
      "epoch: 10 step: 281, loss is 0.0019615073688328266\n",
      "epoch: 10 step: 282, loss is 8.485034049954265e-05\n",
      "epoch: 10 step: 283, loss is 0.0009272551978938282\n",
      "epoch: 10 step: 284, loss is 0.01409212313592434\n",
      "epoch: 10 step: 285, loss is 0.0004925689427182078\n",
      "epoch: 10 step: 286, loss is 0.01526014693081379\n",
      "epoch: 10 step: 287, loss is 0.0016127979615703225\n",
      "epoch: 10 step: 288, loss is 0.1846170276403427\n",
      "epoch: 10 step: 289, loss is 0.08314507454633713\n",
      "epoch: 10 step: 290, loss is 0.0008054105564951897\n",
      "epoch: 10 step: 291, loss is 0.059840887784957886\n",
      "epoch: 10 step: 292, loss is 0.0005020188982598484\n",
      "epoch: 10 step: 293, loss is 0.04391082003712654\n",
      "epoch: 10 step: 294, loss is 0.000671291840262711\n",
      "epoch: 10 step: 295, loss is 0.0009861626895144582\n",
      "epoch: 10 step: 296, loss is 0.005427084863185883\n",
      "epoch: 10 step: 297, loss is 0.00023408682318404317\n",
      "epoch: 10 step: 298, loss is 0.03168416768312454\n",
      "epoch: 10 step: 299, loss is 0.000358325254637748\n",
      "epoch: 10 step: 300, loss is 0.00018590335093904287\n",
      "epoch: 10 step: 301, loss is 9.448243508813903e-05\n",
      "epoch: 10 step: 302, loss is 0.000305459660012275\n",
      "epoch: 10 step: 303, loss is 0.12716738879680634\n",
      "epoch: 10 step: 304, loss is 5.306315506459214e-05\n",
      "epoch: 10 step: 305, loss is 0.0019507508259266615\n",
      "epoch: 10 step: 306, loss is 0.005745279602706432\n",
      "epoch: 10 step: 307, loss is 0.0009846786269918084\n",
      "epoch: 10 step: 308, loss is 0.0012873461237177253\n",
      "epoch: 10 step: 309, loss is 0.008975869975984097\n",
      "epoch: 10 step: 310, loss is 0.0005000318051315844\n",
      "epoch: 10 step: 311, loss is 0.00011645664199022576\n",
      "epoch: 10 step: 312, loss is 0.0018270497675985098\n",
      "epoch: 10 step: 313, loss is 0.0007213377975858748\n",
      "epoch: 10 step: 314, loss is 0.0011392398737370968\n",
      "epoch: 10 step: 315, loss is 0.00032503396505489945\n",
      "epoch: 10 step: 316, loss is 0.004172777757048607\n",
      "epoch: 10 step: 317, loss is 0.00097337068291381\n",
      "epoch: 10 step: 318, loss is 0.0013903275830671191\n",
      "epoch: 10 step: 319, loss is 0.0027415461372584105\n",
      "epoch: 10 step: 320, loss is 0.018657617270946503\n",
      "epoch: 10 step: 321, loss is 0.00014767340326216072\n",
      "epoch: 10 step: 322, loss is 0.008913707919418812\n",
      "epoch: 10 step: 323, loss is 0.000159121846081689\n",
      "epoch: 10 step: 324, loss is 0.0019291508942842484\n",
      "epoch: 10 step: 325, loss is 0.012402378022670746\n",
      "epoch: 10 step: 326, loss is 0.0017125043086707592\n",
      "epoch: 10 step: 327, loss is 0.0036608397495001554\n",
      "epoch: 10 step: 328, loss is 0.001513693598099053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 329, loss is 0.00044977577636018395\n",
      "epoch: 10 step: 330, loss is 0.001334992703050375\n",
      "epoch: 10 step: 331, loss is 0.06787578761577606\n",
      "epoch: 10 step: 332, loss is 0.0002160961157642305\n",
      "epoch: 10 step: 333, loss is 0.027934519574046135\n",
      "epoch: 10 step: 334, loss is 9.183584188576788e-05\n",
      "epoch: 10 step: 335, loss is 0.013164338655769825\n",
      "epoch: 10 step: 336, loss is 0.05427724868059158\n",
      "epoch: 10 step: 337, loss is 0.007530006114393473\n",
      "epoch: 10 step: 338, loss is 0.0002765447134152055\n",
      "epoch: 10 step: 339, loss is 0.0004802114563062787\n",
      "epoch: 10 step: 340, loss is 0.0013727452605962753\n",
      "epoch: 10 step: 341, loss is 0.10620994865894318\n",
      "epoch: 10 step: 342, loss is 0.0006789917824789882\n",
      "epoch: 10 step: 343, loss is 0.015157866291701794\n",
      "epoch: 10 step: 344, loss is 0.0010307045886293054\n",
      "epoch: 10 step: 345, loss is 0.0836840495467186\n",
      "epoch: 10 step: 346, loss is 0.0001785024651326239\n",
      "epoch: 10 step: 347, loss is 0.00016122614033520222\n",
      "epoch: 10 step: 348, loss is 0.00013211961777415127\n",
      "epoch: 10 step: 349, loss is 0.0004905115347355604\n",
      "epoch: 10 step: 350, loss is 0.001387290540151298\n",
      "epoch: 10 step: 351, loss is 0.008457726798951626\n",
      "epoch: 10 step: 352, loss is 0.00241382559761405\n",
      "epoch: 10 step: 353, loss is 7.726538024144247e-05\n",
      "epoch: 10 step: 354, loss is 0.03263873606920242\n",
      "epoch: 10 step: 355, loss is 0.0001934728497872129\n",
      "epoch: 10 step: 356, loss is 0.000183731724973768\n",
      "epoch: 10 step: 357, loss is 0.012155145406723022\n",
      "epoch: 10 step: 358, loss is 0.0004856504965573549\n",
      "epoch: 10 step: 359, loss is 0.0011738762259483337\n",
      "epoch: 10 step: 360, loss is 0.0727185308933258\n",
      "epoch: 10 step: 361, loss is 0.00030965154292061925\n",
      "epoch: 10 step: 362, loss is 0.011089744977653027\n",
      "epoch: 10 step: 363, loss is 0.0001211171766044572\n",
      "epoch: 10 step: 364, loss is 0.0014008713187649846\n",
      "epoch: 10 step: 365, loss is 1.8311491658096202e-05\n",
      "epoch: 10 step: 366, loss is 0.008737662807106972\n",
      "epoch: 10 step: 367, loss is 0.00481690838932991\n",
      "epoch: 10 step: 368, loss is 0.0005834424518980086\n",
      "epoch: 10 step: 369, loss is 0.005154339596629143\n",
      "epoch: 10 step: 370, loss is 0.0011729905381798744\n",
      "epoch: 10 step: 371, loss is 0.0013806973583996296\n",
      "epoch: 10 step: 372, loss is 4.227152749081142e-05\n",
      "epoch: 10 step: 373, loss is 0.014448907226324081\n",
      "epoch: 10 step: 374, loss is 0.10336477309465408\n",
      "epoch: 10 step: 375, loss is 0.0007892594439908862\n",
      "epoch: 10 step: 376, loss is 0.008426539599895477\n",
      "epoch: 10 step: 377, loss is 0.0002685098734218627\n",
      "epoch: 10 step: 378, loss is 0.0009833346121013165\n",
      "epoch: 10 step: 379, loss is 0.09290894120931625\n",
      "epoch: 10 step: 380, loss is 9.74598151515238e-05\n",
      "epoch: 10 step: 381, loss is 0.03952890634536743\n",
      "epoch: 10 step: 382, loss is 0.010815577581524849\n",
      "epoch: 10 step: 383, loss is 0.0017627784982323647\n",
      "epoch: 10 step: 384, loss is 0.0002726646780502051\n",
      "epoch: 10 step: 385, loss is 0.0005520654376596212\n",
      "epoch: 10 step: 386, loss is 0.0019375309348106384\n",
      "epoch: 10 step: 387, loss is 0.03980213776230812\n",
      "epoch: 10 step: 388, loss is 0.0009171467972919345\n",
      "epoch: 10 step: 389, loss is 0.002908539492636919\n",
      "epoch: 10 step: 390, loss is 0.029138565063476562\n",
      "epoch: 10 step: 391, loss is 0.0193894375115633\n",
      "epoch: 10 step: 392, loss is 0.0004308103525545448\n",
      "epoch: 10 step: 393, loss is 0.000817267457023263\n",
      "epoch: 10 step: 394, loss is 0.0005052355118095875\n",
      "epoch: 10 step: 395, loss is 0.006628931500017643\n",
      "epoch: 10 step: 396, loss is 0.0001504526735516265\n",
      "epoch: 10 step: 397, loss is 4.6386288886424154e-05\n",
      "epoch: 10 step: 398, loss is 0.00022557521879207343\n",
      "epoch: 10 step: 399, loss is 0.022898154333233833\n",
      "epoch: 10 step: 400, loss is 0.001503588049672544\n",
      "epoch: 10 step: 401, loss is 0.013280077837407589\n",
      "epoch: 10 step: 402, loss is 0.0001547621504869312\n",
      "epoch: 10 step: 403, loss is 0.005472339689731598\n",
      "epoch: 10 step: 404, loss is 0.0069359890185296535\n",
      "epoch: 10 step: 405, loss is 0.04134348779916763\n",
      "epoch: 10 step: 406, loss is 3.764458233490586e-05\n",
      "epoch: 10 step: 407, loss is 5.2751060138689354e-05\n",
      "epoch: 10 step: 408, loss is 0.0021726018749177456\n",
      "epoch: 10 step: 409, loss is 0.0002580577274784446\n",
      "epoch: 10 step: 410, loss is 0.0002969864581245929\n",
      "epoch: 10 step: 411, loss is 0.0012513878755271435\n",
      "epoch: 10 step: 412, loss is 0.0016294433735311031\n",
      "epoch: 10 step: 413, loss is 0.0008680691244080663\n",
      "epoch: 10 step: 414, loss is 0.0012644616654142737\n",
      "epoch: 10 step: 415, loss is 0.0006835723179392517\n",
      "epoch: 10 step: 416, loss is 0.00011666696809697896\n",
      "epoch: 10 step: 417, loss is 0.0016845471691340208\n",
      "epoch: 10 step: 418, loss is 0.00016895779117476195\n",
      "epoch: 10 step: 419, loss is 0.00044353894190862775\n",
      "epoch: 10 step: 420, loss is 0.007295699790120125\n",
      "epoch: 10 step: 421, loss is 0.00015930374502204359\n",
      "epoch: 10 step: 422, loss is 0.0003689380537252873\n",
      "epoch: 10 step: 423, loss is 0.00139819853939116\n",
      "epoch: 10 step: 424, loss is 0.002671339549124241\n",
      "epoch: 10 step: 425, loss is 0.000237736152485013\n",
      "epoch: 10 step: 426, loss is 0.005208490416407585\n",
      "epoch: 10 step: 427, loss is 0.0010219907853752375\n",
      "epoch: 10 step: 428, loss is 0.015470175072550774\n",
      "epoch: 10 step: 429, loss is 0.0005278137978166342\n",
      "epoch: 10 step: 430, loss is 0.007707622367888689\n",
      "epoch: 10 step: 431, loss is 0.054561711847782135\n",
      "epoch: 10 step: 432, loss is 0.00020288316591177136\n",
      "epoch: 10 step: 433, loss is 3.342049240018241e-05\n",
      "epoch: 10 step: 434, loss is 0.02321586012840271\n",
      "epoch: 10 step: 435, loss is 0.00012202538346173242\n",
      "epoch: 10 step: 436, loss is 0.0008258933667093515\n",
      "epoch: 10 step: 437, loss is 0.002149018691852689\n",
      "epoch: 10 step: 438, loss is 0.0007743620662949979\n",
      "epoch: 10 step: 439, loss is 0.000576064339838922\n",
      "epoch: 10 step: 440, loss is 0.00359066529199481\n",
      "epoch: 10 step: 441, loss is 4.44847100879997e-05\n",
      "epoch: 10 step: 442, loss is 0.008769399486482143\n",
      "epoch: 10 step: 443, loss is 0.00011154157255077735\n",
      "epoch: 10 step: 444, loss is 0.0017921989783644676\n",
      "epoch: 10 step: 445, loss is 0.0014989236369729042\n",
      "epoch: 10 step: 446, loss is 8.834651089273393e-05\n",
      "epoch: 10 step: 447, loss is 0.0016181180253624916\n",
      "epoch: 10 step: 448, loss is 0.0012934986734762788\n",
      "epoch: 10 step: 449, loss is 0.03052980825304985\n",
      "epoch: 10 step: 450, loss is 0.013998394832015038\n",
      "epoch: 10 step: 451, loss is 0.0021684777457267046\n",
      "epoch: 10 step: 452, loss is 0.002604743232950568\n",
      "epoch: 10 step: 453, loss is 0.00113361154217273\n",
      "epoch: 10 step: 454, loss is 4.393848212203011e-05\n",
      "epoch: 10 step: 455, loss is 0.0012161963386461139\n",
      "epoch: 10 step: 456, loss is 0.0019410436507314444\n",
      "epoch: 10 step: 457, loss is 0.00520994421094656\n",
      "epoch: 10 step: 458, loss is 1.7461346942582168e-05\n",
      "epoch: 10 step: 459, loss is 1.6442632841062732e-05\n",
      "epoch: 10 step: 460, loss is 5.8455178077565506e-05\n",
      "epoch: 10 step: 461, loss is 7.067279511829838e-05\n",
      "epoch: 10 step: 462, loss is 4.3388004996813834e-05\n",
      "epoch: 10 step: 463, loss is 0.00014719291357323527\n",
      "epoch: 10 step: 464, loss is 0.00012713528121821582\n",
      "epoch: 10 step: 465, loss is 1.8971562894876115e-05\n",
      "epoch: 10 step: 466, loss is 0.00013463648792821914\n",
      "epoch: 10 step: 467, loss is 0.006413061171770096\n",
      "epoch: 10 step: 468, loss is 0.0006889014039188623\n",
      "epoch: 10 step: 469, loss is 6.591288547497243e-05\n",
      "epoch: 10 step: 470, loss is 0.07011371850967407\n",
      "epoch: 10 step: 471, loss is 0.00014274260320235044\n",
      "epoch: 10 step: 472, loss is 0.004443373065441847\n",
      "epoch: 10 step: 473, loss is 0.021074773743748665\n",
      "epoch: 10 step: 474, loss is 0.0024975964333862066\n",
      "epoch: 10 step: 475, loss is 0.0452408604323864\n",
      "epoch: 10 step: 476, loss is 0.0041414666920900345\n",
      "epoch: 10 step: 477, loss is 0.00035902467789128423\n",
      "epoch: 10 step: 478, loss is 0.00044097911450080574\n",
      "epoch: 10 step: 479, loss is 0.08465459197759628\n",
      "epoch: 10 step: 480, loss is 0.0363803394138813\n",
      "epoch: 10 step: 481, loss is 0.00023986656742636114\n",
      "epoch: 10 step: 482, loss is 0.005390678066760302\n",
      "epoch: 10 step: 483, loss is 0.24837060272693634\n",
      "epoch: 10 step: 484, loss is 0.028592530637979507\n",
      "epoch: 10 step: 485, loss is 0.0008623381145298481\n",
      "epoch: 10 step: 486, loss is 0.005168404895812273\n",
      "epoch: 10 step: 487, loss is 0.0013735434040427208\n",
      "epoch: 10 step: 488, loss is 0.012620614841580391\n",
      "epoch: 10 step: 489, loss is 0.001999650616198778\n",
      "epoch: 10 step: 490, loss is 0.00708051910623908\n",
      "epoch: 10 step: 491, loss is 0.0004598888335749507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 492, loss is 0.00012763679842464626\n",
      "epoch: 10 step: 493, loss is 0.005108239594846964\n",
      "epoch: 10 step: 494, loss is 0.08099033683538437\n",
      "epoch: 10 step: 495, loss is 0.0003940521855838597\n",
      "epoch: 10 step: 496, loss is 0.00015797189553268254\n",
      "epoch: 10 step: 497, loss is 0.28013375401496887\n",
      "epoch: 10 step: 498, loss is 0.003927262034267187\n",
      "epoch: 10 step: 499, loss is 0.0006480375304818153\n",
      "epoch: 10 step: 500, loss is 0.0005155595135875046\n",
      "epoch: 10 step: 501, loss is 0.000165397155797109\n",
      "epoch: 10 step: 502, loss is 0.0005244265194050968\n",
      "epoch: 10 step: 503, loss is 0.0064610824920237064\n",
      "epoch: 10 step: 504, loss is 0.0060720741748809814\n",
      "epoch: 10 step: 505, loss is 0.002140945754945278\n",
      "epoch: 10 step: 506, loss is 0.00013501122884918004\n",
      "epoch: 10 step: 507, loss is 0.0007570255547761917\n",
      "epoch: 10 step: 508, loss is 0.006183306220918894\n",
      "epoch: 10 step: 509, loss is 0.01277565211057663\n",
      "epoch: 10 step: 510, loss is 0.0010711632203310728\n",
      "epoch: 10 step: 511, loss is 0.1673033982515335\n",
      "epoch: 10 step: 512, loss is 0.0021173504646867514\n",
      "epoch: 10 step: 513, loss is 0.00045722845243290067\n",
      "epoch: 10 step: 514, loss is 0.00031596768531017005\n",
      "epoch: 10 step: 515, loss is 0.013368031941354275\n",
      "epoch: 10 step: 516, loss is 0.0003023923491127789\n",
      "epoch: 10 step: 517, loss is 0.016803942620754242\n",
      "epoch: 10 step: 518, loss is 0.0013087511761114001\n",
      "epoch: 10 step: 519, loss is 0.0010072033619508147\n",
      "epoch: 10 step: 520, loss is 0.007898355834186077\n",
      "epoch: 10 step: 521, loss is 0.00010792894318001345\n",
      "epoch: 10 step: 522, loss is 0.0005599144496954978\n",
      "epoch: 10 step: 523, loss is 0.0011296105803921819\n",
      "epoch: 10 step: 524, loss is 0.012991944327950478\n",
      "epoch: 10 step: 525, loss is 0.02661730721592903\n",
      "epoch: 10 step: 526, loss is 0.00023826045799069107\n",
      "epoch: 10 step: 527, loss is 0.0012179219629615545\n",
      "epoch: 10 step: 528, loss is 0.002928155940026045\n",
      "epoch: 10 step: 529, loss is 0.0004045159730594605\n",
      "epoch: 10 step: 530, loss is 0.004539196845144033\n",
      "epoch: 10 step: 531, loss is 0.0017477540532127023\n",
      "epoch: 10 step: 532, loss is 0.003050488419830799\n",
      "epoch: 10 step: 533, loss is 0.0013048568507656455\n",
      "epoch: 10 step: 534, loss is 0.0014556421665474772\n",
      "epoch: 10 step: 535, loss is 0.0006451027002185583\n",
      "epoch: 10 step: 536, loss is 0.0009558924357406795\n",
      "epoch: 10 step: 537, loss is 0.0508059598505497\n",
      "epoch: 10 step: 538, loss is 0.0008114444208331406\n",
      "epoch: 10 step: 539, loss is 0.0018763465341180563\n",
      "epoch: 10 step: 540, loss is 0.0004438195610418916\n",
      "epoch: 10 step: 541, loss is 0.0018420825945213437\n",
      "epoch: 10 step: 542, loss is 0.007759196683764458\n",
      "epoch: 10 step: 543, loss is 0.00834483653306961\n",
      "epoch: 10 step: 544, loss is 0.0009387035388499498\n",
      "epoch: 10 step: 545, loss is 0.007818314246833324\n",
      "epoch: 10 step: 546, loss is 0.004593041725456715\n",
      "epoch: 10 step: 547, loss is 0.002020737621933222\n",
      "epoch: 10 step: 548, loss is 0.0005376541521400213\n",
      "epoch: 10 step: 549, loss is 0.001498385681770742\n",
      "epoch: 10 step: 550, loss is 0.0005581264849752188\n",
      "epoch: 10 step: 551, loss is 0.03302814066410065\n",
      "epoch: 10 step: 552, loss is 4.900754720438272e-05\n",
      "epoch: 10 step: 553, loss is 0.0002837969805113971\n",
      "epoch: 10 step: 554, loss is 0.007758887019008398\n",
      "epoch: 10 step: 555, loss is 0.00015410278865601867\n",
      "epoch: 10 step: 556, loss is 0.022693093866109848\n",
      "epoch: 10 step: 557, loss is 0.00032694044057279825\n",
      "epoch: 10 step: 558, loss is 0.003268152941018343\n",
      "epoch: 10 step: 559, loss is 0.00158607994671911\n",
      "epoch: 10 step: 560, loss is 0.0001494524913141504\n",
      "epoch: 10 step: 561, loss is 0.04082746431231499\n",
      "epoch: 10 step: 562, loss is 0.0006247784476727247\n",
      "epoch: 10 step: 563, loss is 0.0014535101363435388\n",
      "epoch: 10 step: 564, loss is 0.002710666973143816\n",
      "epoch: 10 step: 565, loss is 0.022552544251084328\n",
      "epoch: 10 step: 566, loss is 0.010825123637914658\n",
      "epoch: 10 step: 567, loss is 0.0009814713848754764\n",
      "epoch: 10 step: 568, loss is 0.00013325738837011158\n",
      "epoch: 10 step: 569, loss is 0.0015189589466899633\n",
      "epoch: 10 step: 570, loss is 0.00024196204321924597\n",
      "epoch: 10 step: 571, loss is 0.00018718290084507316\n",
      "epoch: 10 step: 572, loss is 0.002038730774074793\n",
      "epoch: 10 step: 573, loss is 4.568420263240114e-05\n",
      "epoch: 10 step: 574, loss is 2.9857634217478335e-05\n",
      "epoch: 10 step: 575, loss is 0.0031489813700318336\n",
      "epoch: 10 step: 576, loss is 0.00042245365330018103\n",
      "epoch: 10 step: 577, loss is 0.00013580956147052348\n",
      "epoch: 10 step: 578, loss is 0.0008418615907430649\n",
      "epoch: 10 step: 579, loss is 0.00012017618428217247\n",
      "epoch: 10 step: 580, loss is 0.0888604149222374\n",
      "epoch: 10 step: 581, loss is 0.00047456444008275867\n",
      "epoch: 10 step: 582, loss is 0.00849742628633976\n",
      "epoch: 10 step: 583, loss is 0.00021157665469218045\n",
      "epoch: 10 step: 584, loss is 0.00043173262383788824\n",
      "epoch: 10 step: 585, loss is 5.688371311407536e-05\n",
      "epoch: 10 step: 586, loss is 0.039381083101034164\n",
      "epoch: 10 step: 587, loss is 0.00042618357110768557\n",
      "epoch: 10 step: 588, loss is 0.05228118970990181\n",
      "epoch: 10 step: 589, loss is 0.0229976624250412\n",
      "epoch: 10 step: 590, loss is 0.005046915262937546\n",
      "epoch: 10 step: 591, loss is 5.433380283648148e-05\n",
      "epoch: 10 step: 592, loss is 0.0018726178677752614\n",
      "epoch: 10 step: 593, loss is 0.00021445221500471234\n",
      "epoch: 10 step: 594, loss is 0.002635381883010268\n",
      "epoch: 10 step: 595, loss is 0.00018037539848592132\n",
      "epoch: 10 step: 596, loss is 0.05024156719446182\n",
      "epoch: 10 step: 597, loss is 0.002596292644739151\n",
      "epoch: 10 step: 598, loss is 0.00019507431716192514\n",
      "epoch: 10 step: 599, loss is 0.0019893301650881767\n",
      "epoch: 10 step: 600, loss is 0.0007769783842377365\n",
      "epoch: 10 step: 601, loss is 0.007685605436563492\n",
      "epoch: 10 step: 602, loss is 2.7485404643812217e-05\n",
      "epoch: 10 step: 603, loss is 0.003596812253817916\n",
      "epoch: 10 step: 604, loss is 0.0003707987198140472\n",
      "epoch: 10 step: 605, loss is 0.0016463000793009996\n",
      "epoch: 10 step: 606, loss is 0.000502782582771033\n",
      "epoch: 10 step: 607, loss is 0.0015578486490994692\n",
      "epoch: 10 step: 608, loss is 0.00022395930136553943\n",
      "epoch: 10 step: 609, loss is 0.00018933130195364356\n",
      "epoch: 10 step: 610, loss is 0.06126270443201065\n",
      "epoch: 10 step: 611, loss is 0.002062727464362979\n",
      "epoch: 10 step: 612, loss is 0.00299317785538733\n",
      "epoch: 10 step: 613, loss is 0.002098947996273637\n",
      "epoch: 10 step: 614, loss is 0.0003532982664182782\n",
      "epoch: 10 step: 615, loss is 0.05597224831581116\n",
      "epoch: 10 step: 616, loss is 0.06219480559229851\n",
      "epoch: 10 step: 617, loss is 0.003553990740329027\n",
      "epoch: 10 step: 618, loss is 0.0006724317790940404\n",
      "epoch: 10 step: 619, loss is 0.001992887118831277\n",
      "epoch: 10 step: 620, loss is 0.0025573144666850567\n",
      "epoch: 10 step: 621, loss is 0.002255713567137718\n",
      "epoch: 10 step: 622, loss is 0.0003323238343000412\n",
      "epoch: 10 step: 623, loss is 0.0034538130275905132\n",
      "epoch: 10 step: 624, loss is 0.198695570230484\n",
      "epoch: 10 step: 625, loss is 0.04135172441601753\n",
      "epoch: 10 step: 626, loss is 0.0020688464865088463\n",
      "epoch: 10 step: 627, loss is 0.003161234315484762\n",
      "epoch: 10 step: 628, loss is 0.0006570663535967469\n",
      "epoch: 10 step: 629, loss is 0.0026998603716492653\n",
      "epoch: 10 step: 630, loss is 0.20788130164146423\n",
      "epoch: 10 step: 631, loss is 0.04690682888031006\n",
      "epoch: 10 step: 632, loss is 0.007373228669166565\n",
      "epoch: 10 step: 633, loss is 0.010195532813668251\n",
      "epoch: 10 step: 634, loss is 0.013020467944443226\n",
      "epoch: 10 step: 635, loss is 0.0008184899925254285\n",
      "epoch: 10 step: 636, loss is 0.015130577608942986\n",
      "epoch: 10 step: 637, loss is 0.07844169437885284\n",
      "epoch: 10 step: 638, loss is 0.0003511968534439802\n",
      "epoch: 10 step: 639, loss is 0.004192549269646406\n",
      "epoch: 10 step: 640, loss is 0.07179451733827591\n",
      "epoch: 10 step: 641, loss is 0.0006177249597385526\n",
      "epoch: 10 step: 642, loss is 0.09627925604581833\n",
      "epoch: 10 step: 643, loss is 0.016967302188277245\n",
      "epoch: 10 step: 644, loss is 0.12748785316944122\n",
      "epoch: 10 step: 645, loss is 0.0038622997235506773\n",
      "epoch: 10 step: 646, loss is 0.0008572734659537673\n",
      "epoch: 10 step: 647, loss is 0.0048367902636528015\n",
      "epoch: 10 step: 648, loss is 0.0005161690060049295\n",
      "epoch: 10 step: 649, loss is 0.000495171349029988\n",
      "epoch: 10 step: 650, loss is 0.0029143367428332567\n",
      "epoch: 10 step: 651, loss is 0.0003432031662669033\n",
      "epoch: 10 step: 652, loss is 0.0005039177485741675\n",
      "epoch: 10 step: 653, loss is 0.0035105727147310972\n",
      "epoch: 10 step: 654, loss is 0.023649441078305244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 655, loss is 0.0016563624376431108\n",
      "epoch: 10 step: 656, loss is 0.009475688450038433\n",
      "epoch: 10 step: 657, loss is 0.0021497393026947975\n",
      "epoch: 10 step: 658, loss is 0.0001650243648327887\n",
      "epoch: 10 step: 659, loss is 0.00915520265698433\n",
      "epoch: 10 step: 660, loss is 4.886091119260527e-05\n",
      "epoch: 10 step: 661, loss is 0.039457496255636215\n",
      "epoch: 10 step: 662, loss is 2.3705993953626603e-05\n",
      "epoch: 10 step: 663, loss is 0.0053260778076946735\n",
      "epoch: 10 step: 664, loss is 0.0005660767201334238\n",
      "epoch: 10 step: 665, loss is 0.004920364823192358\n",
      "epoch: 10 step: 666, loss is 0.0017752975691109896\n",
      "epoch: 10 step: 667, loss is 0.00030187121592462063\n",
      "epoch: 10 step: 668, loss is 0.002896325197070837\n",
      "epoch: 10 step: 669, loss is 0.006100361701101065\n",
      "epoch: 10 step: 670, loss is 0.005970172584056854\n",
      "epoch: 10 step: 671, loss is 0.00023095270444173366\n",
      "epoch: 10 step: 672, loss is 0.0037209203001111746\n",
      "epoch: 10 step: 673, loss is 0.00040336520760320127\n",
      "epoch: 10 step: 674, loss is 0.0006696835625916719\n",
      "epoch: 10 step: 675, loss is 0.0007548189605586231\n",
      "epoch: 10 step: 676, loss is 0.00010209585161646828\n",
      "epoch: 10 step: 677, loss is 0.04129572585225105\n",
      "epoch: 10 step: 678, loss is 0.0004790287930518389\n",
      "epoch: 10 step: 679, loss is 0.1648859679698944\n",
      "epoch: 10 step: 680, loss is 0.009726040065288544\n",
      "epoch: 10 step: 681, loss is 0.0003033268731087446\n",
      "epoch: 10 step: 682, loss is 0.003974780440330505\n",
      "epoch: 10 step: 683, loss is 0.00010757279233075678\n",
      "epoch: 10 step: 684, loss is 0.0051697492599487305\n",
      "epoch: 10 step: 685, loss is 0.004427715670317411\n",
      "epoch: 10 step: 686, loss is 0.01293992530554533\n",
      "epoch: 10 step: 687, loss is 0.003713632468134165\n",
      "epoch: 10 step: 688, loss is 0.0001423055218765512\n",
      "epoch: 10 step: 689, loss is 0.004199744667857885\n",
      "epoch: 10 step: 690, loss is 0.002062815008684993\n",
      "epoch: 10 step: 691, loss is 0.003202487248927355\n",
      "epoch: 10 step: 692, loss is 0.0017366755055263638\n",
      "epoch: 10 step: 693, loss is 0.0012232856824994087\n",
      "epoch: 10 step: 694, loss is 0.008907366544008255\n",
      "epoch: 10 step: 695, loss is 0.0014625590993091464\n",
      "epoch: 10 step: 696, loss is 1.990495547943283e-05\n",
      "epoch: 10 step: 697, loss is 0.0018935921834781766\n",
      "epoch: 10 step: 698, loss is 0.0002644164487719536\n",
      "epoch: 10 step: 699, loss is 0.2501084506511688\n",
      "epoch: 10 step: 700, loss is 8.33746980788419e-06\n",
      "epoch: 10 step: 701, loss is 0.00017253773694392294\n",
      "epoch: 10 step: 702, loss is 0.008122237399220467\n",
      "epoch: 10 step: 703, loss is 0.0021816217340528965\n",
      "epoch: 10 step: 704, loss is 0.00933188758790493\n",
      "epoch: 10 step: 705, loss is 0.0005566489999182522\n",
      "epoch: 10 step: 706, loss is 0.002360065234825015\n",
      "epoch: 10 step: 707, loss is 0.0021785907447338104\n",
      "epoch: 10 step: 708, loss is 3.414619277464226e-05\n",
      "epoch: 10 step: 709, loss is 0.009772121906280518\n",
      "epoch: 10 step: 710, loss is 0.0005437721847556531\n",
      "epoch: 10 step: 711, loss is 0.00021175022993702441\n",
      "epoch: 10 step: 712, loss is 0.0399179570376873\n",
      "epoch: 10 step: 713, loss is 0.00037276753573678434\n",
      "epoch: 10 step: 714, loss is 0.012432971969246864\n",
      "epoch: 10 step: 715, loss is 0.007043255027383566\n",
      "epoch: 10 step: 716, loss is 0.0024007365573197603\n",
      "epoch: 10 step: 717, loss is 0.010464263148605824\n",
      "epoch: 10 step: 718, loss is 0.0030301420483738184\n",
      "epoch: 10 step: 719, loss is 0.0009708022698760033\n",
      "epoch: 10 step: 720, loss is 0.00608689384534955\n",
      "epoch: 10 step: 721, loss is 0.015616532415151596\n",
      "epoch: 10 step: 722, loss is 0.00018725112022366375\n",
      "epoch: 10 step: 723, loss is 0.001442464767023921\n",
      "epoch: 10 step: 724, loss is 0.0035530757158994675\n",
      "epoch: 10 step: 725, loss is 0.0007631870685145259\n",
      "epoch: 10 step: 726, loss is 0.0002849419543053955\n",
      "epoch: 10 step: 727, loss is 0.00014638616994488984\n",
      "epoch: 10 step: 728, loss is 0.00014616281259804964\n",
      "epoch: 10 step: 729, loss is 3.1276344088837504e-05\n",
      "epoch: 10 step: 730, loss is 7.598936645081267e-05\n",
      "epoch: 10 step: 731, loss is 0.0028260252438485622\n",
      "epoch: 10 step: 732, loss is 0.00025349744828417897\n",
      "epoch: 10 step: 733, loss is 0.0001428985851816833\n",
      "epoch: 10 step: 734, loss is 0.008315583691000938\n",
      "epoch: 10 step: 735, loss is 0.00040841399459168315\n",
      "epoch: 10 step: 736, loss is 0.0004987751017324626\n",
      "epoch: 10 step: 737, loss is 0.01204046793282032\n",
      "epoch: 10 step: 738, loss is 0.0003710567834787071\n",
      "epoch: 10 step: 739, loss is 0.03416992351412773\n",
      "epoch: 10 step: 740, loss is 0.0020015742629766464\n",
      "epoch: 10 step: 741, loss is 0.0009876773692667484\n",
      "epoch: 10 step: 742, loss is 0.007267081178724766\n",
      "epoch: 10 step: 743, loss is 0.007262473460286856\n",
      "epoch: 10 step: 744, loss is 0.00014070582983549684\n",
      "epoch: 10 step: 745, loss is 0.005333432927727699\n",
      "epoch: 10 step: 746, loss is 0.0018310745945200324\n",
      "epoch: 10 step: 747, loss is 0.019380023702979088\n",
      "epoch: 10 step: 748, loss is 0.06401197612285614\n",
      "epoch: 10 step: 749, loss is 0.0029169025365263224\n",
      "epoch: 10 step: 750, loss is 0.04509074613451958\n",
      "epoch: 10 step: 751, loss is 0.0032010728027671576\n",
      "epoch: 10 step: 752, loss is 0.07440271228551865\n",
      "epoch: 10 step: 753, loss is 7.229029142763466e-05\n",
      "epoch: 10 step: 754, loss is 0.0003804579610005021\n",
      "epoch: 10 step: 755, loss is 0.006435623858124018\n",
      "epoch: 10 step: 756, loss is 0.024667147547006607\n",
      "epoch: 10 step: 757, loss is 0.007251688279211521\n",
      "epoch: 10 step: 758, loss is 0.0020121431443840265\n",
      "epoch: 10 step: 759, loss is 0.008456554263830185\n",
      "epoch: 10 step: 760, loss is 2.9406710382318124e-05\n",
      "epoch: 10 step: 761, loss is 0.0009881757432594895\n",
      "epoch: 10 step: 762, loss is 1.4510806067846715e-05\n",
      "epoch: 10 step: 763, loss is 0.0038675889372825623\n",
      "epoch: 10 step: 764, loss is 0.0005399625515565276\n",
      "epoch: 10 step: 765, loss is 0.020959854125976562\n",
      "epoch: 10 step: 766, loss is 0.1155923455953598\n",
      "epoch: 10 step: 767, loss is 0.0001583349658176303\n",
      "epoch: 10 step: 768, loss is 0.009283610619604588\n",
      "epoch: 10 step: 769, loss is 8.501912088831887e-05\n",
      "epoch: 10 step: 770, loss is 0.025085324421525\n",
      "epoch: 10 step: 771, loss is 0.027880962938070297\n",
      "epoch: 10 step: 772, loss is 0.007528731133788824\n",
      "epoch: 10 step: 773, loss is 0.00013226440933067352\n",
      "epoch: 10 step: 774, loss is 0.0023943905252963305\n",
      "epoch: 10 step: 775, loss is 0.006429603323340416\n",
      "epoch: 10 step: 776, loss is 0.001469851122237742\n",
      "epoch: 10 step: 777, loss is 3.635759276221506e-05\n",
      "epoch: 10 step: 778, loss is 1.8332419131183997e-05\n",
      "epoch: 10 step: 779, loss is 0.005815691780298948\n",
      "epoch: 10 step: 780, loss is 0.06687572598457336\n",
      "epoch: 10 step: 781, loss is 3.082544935750775e-05\n",
      "epoch: 10 step: 782, loss is 0.08587051182985306\n",
      "epoch: 10 step: 783, loss is 0.00421815225854516\n",
      "epoch: 10 step: 784, loss is 0.0016336552798748016\n",
      "epoch: 10 step: 785, loss is 0.09276332706212997\n",
      "epoch: 10 step: 786, loss is 9.401634451933205e-05\n",
      "epoch: 10 step: 787, loss is 0.00018451025243848562\n",
      "epoch: 10 step: 788, loss is 0.001998934429138899\n",
      "epoch: 10 step: 789, loss is 2.1657404431607574e-05\n",
      "epoch: 10 step: 790, loss is 0.0004527657583821565\n",
      "epoch: 10 step: 791, loss is 0.023881420493125916\n",
      "epoch: 10 step: 792, loss is 0.00014836630725767463\n",
      "epoch: 10 step: 793, loss is 0.01851692609488964\n",
      "epoch: 10 step: 794, loss is 0.006987082771956921\n",
      "epoch: 10 step: 795, loss is 9.039964061230421e-05\n",
      "epoch: 10 step: 796, loss is 0.000996934249997139\n",
      "epoch: 10 step: 797, loss is 0.006160923279821873\n",
      "epoch: 10 step: 798, loss is 0.034790538251399994\n",
      "epoch: 10 step: 799, loss is 0.0017619632417336106\n",
      "epoch: 10 step: 800, loss is 0.007652961649000645\n",
      "epoch: 10 step: 801, loss is 0.03446296975016594\n",
      "epoch: 10 step: 802, loss is 0.00046737605589441955\n",
      "epoch: 10 step: 803, loss is 0.00013916999159846455\n",
      "epoch: 10 step: 804, loss is 0.1451878696680069\n",
      "epoch: 10 step: 805, loss is 3.864490645355545e-05\n",
      "epoch: 10 step: 806, loss is 0.013098380528390408\n",
      "epoch: 10 step: 807, loss is 0.0025336972903460264\n",
      "epoch: 10 step: 808, loss is 0.0022510099224746227\n",
      "epoch: 10 step: 809, loss is 0.0011925838189199567\n",
      "epoch: 10 step: 810, loss is 2.346802102692891e-05\n",
      "epoch: 10 step: 811, loss is 2.7859705369337462e-05\n",
      "epoch: 10 step: 812, loss is 0.0003592010180000216\n",
      "epoch: 10 step: 813, loss is 0.000977586372755468\n",
      "epoch: 10 step: 814, loss is 0.04993751645088196\n",
      "epoch: 10 step: 815, loss is 0.07983221113681793\n",
      "epoch: 10 step: 816, loss is 0.0276217982172966\n",
      "epoch: 10 step: 817, loss is 0.00038613067590631545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 818, loss is 6.33999879937619e-05\n",
      "epoch: 10 step: 819, loss is 5.212988253333606e-05\n",
      "epoch: 10 step: 820, loss is 0.004489351529628038\n",
      "epoch: 10 step: 821, loss is 0.01112532988190651\n",
      "epoch: 10 step: 822, loss is 0.0003135396691504866\n",
      "epoch: 10 step: 823, loss is 0.020682767033576965\n",
      "epoch: 10 step: 824, loss is 2.926717934315093e-05\n",
      "epoch: 10 step: 825, loss is 0.000165631077834405\n",
      "epoch: 10 step: 826, loss is 0.0012456728145480156\n",
      "epoch: 10 step: 827, loss is 0.1266377717256546\n",
      "epoch: 10 step: 828, loss is 0.004907643422484398\n",
      "epoch: 10 step: 829, loss is 0.07778547704219818\n",
      "epoch: 10 step: 830, loss is 0.12772591412067413\n",
      "epoch: 10 step: 831, loss is 7.536855264334008e-05\n",
      "epoch: 10 step: 832, loss is 0.004594188183546066\n",
      "epoch: 10 step: 833, loss is 0.005190455354750156\n",
      "epoch: 10 step: 834, loss is 8.865830022841692e-05\n",
      "epoch: 10 step: 835, loss is 0.0037588661070913076\n",
      "epoch: 10 step: 836, loss is 0.004540709313005209\n",
      "epoch: 10 step: 837, loss is 4.679873018176295e-05\n",
      "epoch: 10 step: 838, loss is 0.003927184268832207\n",
      "epoch: 10 step: 839, loss is 0.10417543351650238\n",
      "epoch: 10 step: 840, loss is 0.004024652298539877\n",
      "epoch: 10 step: 841, loss is 0.0009114803979173303\n",
      "epoch: 10 step: 842, loss is 0.00440439535304904\n",
      "epoch: 10 step: 843, loss is 0.0055946107022464275\n",
      "epoch: 10 step: 844, loss is 0.2609724700450897\n",
      "epoch: 10 step: 845, loss is 0.00031904224306344986\n",
      "epoch: 10 step: 846, loss is 0.0007064919918775558\n",
      "epoch: 10 step: 847, loss is 0.01550283096730709\n",
      "epoch: 10 step: 848, loss is 0.0007197973900474608\n",
      "epoch: 10 step: 849, loss is 0.0002231426042271778\n",
      "epoch: 10 step: 850, loss is 0.0028520976193249226\n",
      "epoch: 10 step: 851, loss is 0.001174555509351194\n",
      "epoch: 10 step: 852, loss is 0.024075333029031754\n",
      "epoch: 10 step: 853, loss is 0.0011320371413603425\n",
      "epoch: 10 step: 854, loss is 0.0012882750015705824\n",
      "epoch: 10 step: 855, loss is 0.0006702655227854848\n",
      "epoch: 10 step: 856, loss is 0.008023384027183056\n",
      "epoch: 10 step: 857, loss is 0.0013852129923179746\n",
      "epoch: 10 step: 858, loss is 7.383423508144915e-05\n",
      "epoch: 10 step: 859, loss is 0.0008574847597628832\n",
      "epoch: 10 step: 860, loss is 0.007421014364808798\n",
      "epoch: 10 step: 861, loss is 0.08612526208162308\n",
      "epoch: 10 step: 862, loss is 0.0005522706196643412\n",
      "epoch: 10 step: 863, loss is 0.012728684581816196\n",
      "epoch: 10 step: 864, loss is 0.0020959728863090277\n",
      "epoch: 10 step: 865, loss is 0.006160111166536808\n",
      "epoch: 10 step: 866, loss is 0.007008319720625877\n",
      "epoch: 10 step: 867, loss is 0.0007377814035862684\n",
      "epoch: 10 step: 868, loss is 0.02685151994228363\n",
      "epoch: 10 step: 869, loss is 0.00023717377916909754\n",
      "epoch: 10 step: 870, loss is 0.0010909463744610548\n",
      "epoch: 10 step: 871, loss is 2.653641058714129e-05\n",
      "epoch: 10 step: 872, loss is 0.0005594394169747829\n",
      "epoch: 10 step: 873, loss is 0.0003105658106505871\n",
      "epoch: 10 step: 874, loss is 0.000814931292552501\n",
      "epoch: 10 step: 875, loss is 0.0021607568487524986\n",
      "epoch: 10 step: 876, loss is 0.055518485605716705\n",
      "epoch: 10 step: 877, loss is 0.019611798226833344\n",
      "epoch: 10 step: 878, loss is 0.001284824451431632\n",
      "epoch: 10 step: 879, loss is 0.006655985489487648\n",
      "epoch: 10 step: 880, loss is 0.023439183831214905\n",
      "epoch: 10 step: 881, loss is 0.0001041284849634394\n",
      "epoch: 10 step: 882, loss is 0.0006420257268473506\n",
      "epoch: 10 step: 883, loss is 0.004852426704019308\n",
      "epoch: 10 step: 884, loss is 0.015406651422381401\n",
      "epoch: 10 step: 885, loss is 0.006588051561266184\n",
      "epoch: 10 step: 886, loss is 1.784534651960712e-05\n",
      "epoch: 10 step: 887, loss is 0.021402928978204727\n",
      "epoch: 10 step: 888, loss is 0.004742236342281103\n",
      "epoch: 10 step: 889, loss is 0.0006343776476569474\n",
      "epoch: 10 step: 890, loss is 0.0002620650047902018\n",
      "epoch: 10 step: 891, loss is 0.00244655879214406\n",
      "epoch: 10 step: 892, loss is 0.0007869826513342559\n",
      "epoch: 10 step: 893, loss is 0.0003958055458497256\n",
      "epoch: 10 step: 894, loss is 0.0008152289083227515\n",
      "epoch: 10 step: 895, loss is 0.00014700672181788832\n",
      "epoch: 10 step: 896, loss is 0.01039083395153284\n",
      "epoch: 10 step: 897, loss is 0.0001685751776676625\n",
      "epoch: 10 step: 898, loss is 0.0057422686368227005\n",
      "epoch: 10 step: 899, loss is 0.02069091610610485\n",
      "epoch: 10 step: 900, loss is 0.03239748254418373\n",
      "epoch: 10 step: 901, loss is 0.00046432105591520667\n",
      "epoch: 10 step: 902, loss is 0.00105016038287431\n",
      "epoch: 10 step: 903, loss is 0.00023749595857225358\n",
      "epoch: 10 step: 904, loss is 0.004058193881064653\n",
      "epoch: 10 step: 905, loss is 0.04577236250042915\n",
      "epoch: 10 step: 906, loss is 0.005109136458486319\n",
      "epoch: 10 step: 907, loss is 0.04948271065950394\n",
      "epoch: 10 step: 908, loss is 0.003845629282295704\n",
      "epoch: 10 step: 909, loss is 0.1944376528263092\n",
      "epoch: 10 step: 910, loss is 2.387396671110764e-05\n",
      "epoch: 10 step: 911, loss is 0.0012746284483000636\n",
      "epoch: 10 step: 912, loss is 0.0011500995606184006\n",
      "epoch: 10 step: 913, loss is 0.0005113211809657514\n",
      "epoch: 10 step: 914, loss is 0.0005890201427973807\n",
      "epoch: 10 step: 915, loss is 0.0012076247949153185\n",
      "epoch: 10 step: 916, loss is 7.38763264962472e-05\n",
      "epoch: 10 step: 917, loss is 0.005580460652709007\n",
      "epoch: 10 step: 918, loss is 0.00015149670070968568\n",
      "epoch: 10 step: 919, loss is 0.003349577309563756\n",
      "epoch: 10 step: 920, loss is 0.047430459409952164\n",
      "epoch: 10 step: 921, loss is 0.0024460740387439728\n",
      "epoch: 10 step: 922, loss is 0.000826086790766567\n",
      "epoch: 10 step: 923, loss is 0.006896945182234049\n",
      "epoch: 10 step: 924, loss is 9.646014223108068e-05\n",
      "epoch: 10 step: 925, loss is 0.008311879821121693\n",
      "epoch: 10 step: 926, loss is 0.01233554445207119\n",
      "epoch: 10 step: 927, loss is 0.007058895193040371\n",
      "epoch: 10 step: 928, loss is 0.2338724285364151\n",
      "epoch: 10 step: 929, loss is 0.056980330497026443\n",
      "epoch: 10 step: 930, loss is 0.0017973353387787938\n",
      "epoch: 10 step: 931, loss is 0.0062103839591145515\n",
      "epoch: 10 step: 932, loss is 0.0003275427734479308\n",
      "epoch: 10 step: 933, loss is 0.003323364071547985\n",
      "epoch: 10 step: 934, loss is 0.13782209157943726\n",
      "epoch: 10 step: 935, loss is 0.00017019151709973812\n",
      "epoch: 10 step: 936, loss is 0.0017285918584093451\n",
      "epoch: 10 step: 937, loss is 0.01187954843044281\n",
      "epoch: 10 step: 938, loss is 0.00032181182177737355\n",
      "epoch: 10 step: 939, loss is 0.1186821460723877\n",
      "epoch: 10 step: 940, loss is 0.00040554258157499135\n",
      "epoch: 10 step: 941, loss is 0.00037318587419576943\n",
      "epoch: 10 step: 942, loss is 0.000494607025757432\n",
      "epoch: 10 step: 943, loss is 0.0012220792705193162\n",
      "epoch: 10 step: 944, loss is 0.00035995346843264997\n",
      "epoch: 10 step: 945, loss is 0.001724153640680015\n",
      "epoch: 10 step: 946, loss is 0.005564696155488491\n",
      "epoch: 10 step: 947, loss is 0.010488331317901611\n",
      "epoch: 10 step: 948, loss is 0.0011712707346305251\n",
      "epoch: 10 step: 949, loss is 0.009978579357266426\n",
      "epoch: 10 step: 950, loss is 0.02270779199898243\n",
      "epoch: 10 step: 951, loss is 0.00517160352319479\n",
      "epoch: 10 step: 952, loss is 0.0002212040708400309\n",
      "epoch: 10 step: 953, loss is 0.030021511018276215\n",
      "epoch: 10 step: 954, loss is 0.0007037643226794899\n",
      "epoch: 10 step: 955, loss is 0.0017390772700309753\n",
      "epoch: 10 step: 956, loss is 0.0015815935330465436\n",
      "epoch: 10 step: 957, loss is 1.4978263607190456e-05\n",
      "epoch: 10 step: 958, loss is 0.01505750510841608\n",
      "epoch: 10 step: 959, loss is 0.010889895260334015\n",
      "epoch: 10 step: 960, loss is 0.0037395504768937826\n",
      "epoch: 10 step: 961, loss is 0.003865642473101616\n",
      "epoch: 10 step: 962, loss is 0.024990076199173927\n",
      "epoch: 10 step: 963, loss is 0.0005546746542677283\n",
      "epoch: 10 step: 964, loss is 0.004544617608189583\n",
      "epoch: 10 step: 965, loss is 0.018467813730239868\n",
      "epoch: 10 step: 966, loss is 0.0786856934428215\n",
      "epoch: 10 step: 967, loss is 0.0007582152029499412\n",
      "epoch: 10 step: 968, loss is 0.011236019432544708\n",
      "epoch: 10 step: 969, loss is 0.0008108094916678965\n",
      "epoch: 10 step: 970, loss is 0.015812398865818977\n",
      "epoch: 10 step: 971, loss is 0.0010786546627059579\n",
      "epoch: 10 step: 972, loss is 0.06538286805152893\n",
      "epoch: 10 step: 973, loss is 0.00012387603055685759\n",
      "epoch: 10 step: 974, loss is 0.00011819876817753538\n",
      "epoch: 10 step: 975, loss is 0.00025420833844691515\n",
      "epoch: 10 step: 976, loss is 0.020435331389307976\n",
      "epoch: 10 step: 977, loss is 0.0001444864901714027\n",
      "epoch: 10 step: 978, loss is 0.0004302234447095543\n",
      "epoch: 10 step: 979, loss is 0.00010994482727255672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 980, loss is 0.00298818526789546\n",
      "epoch: 10 step: 981, loss is 0.0005348889972083271\n",
      "epoch: 10 step: 982, loss is 0.0028604483231902122\n",
      "epoch: 10 step: 983, loss is 0.0010775638511404395\n",
      "epoch: 10 step: 984, loss is 3.675506741274148e-05\n",
      "epoch: 10 step: 985, loss is 0.0018853768706321716\n",
      "epoch: 10 step: 986, loss is 0.0004714313254225999\n",
      "epoch: 10 step: 987, loss is 0.0015118959126994014\n",
      "epoch: 10 step: 988, loss is 0.004483895841985941\n",
      "epoch: 10 step: 989, loss is 0.007182420697063208\n",
      "epoch: 10 step: 990, loss is 0.0034768639598041773\n",
      "epoch: 10 step: 991, loss is 0.00010348133218940347\n",
      "epoch: 10 step: 992, loss is 0.005740347318351269\n",
      "epoch: 10 step: 993, loss is 0.0014521684497594833\n",
      "epoch: 10 step: 994, loss is 1.6140991647262126e-05\n",
      "epoch: 10 step: 995, loss is 0.0005672345287166536\n",
      "epoch: 10 step: 996, loss is 0.0024847632739692926\n",
      "epoch: 10 step: 997, loss is 0.00458679161965847\n",
      "epoch: 10 step: 998, loss is 0.005304605700075626\n",
      "epoch: 10 step: 999, loss is 0.00011183654714841396\n",
      "epoch: 10 step: 1000, loss is 0.00022823733161203563\n",
      "epoch: 10 step: 1001, loss is 0.014304213225841522\n",
      "epoch: 10 step: 1002, loss is 0.002552752150222659\n",
      "epoch: 10 step: 1003, loss is 9.82051933533512e-05\n",
      "epoch: 10 step: 1004, loss is 0.021621620282530785\n",
      "epoch: 10 step: 1005, loss is 0.0016186508582904935\n",
      "epoch: 10 step: 1006, loss is 0.0024175350554287434\n",
      "epoch: 10 step: 1007, loss is 0.0036571372766047716\n",
      "epoch: 10 step: 1008, loss is 0.00019289397459942847\n",
      "epoch: 10 step: 1009, loss is 0.00374813680537045\n",
      "epoch: 10 step: 1010, loss is 9.740290261106566e-05\n",
      "epoch: 10 step: 1011, loss is 0.0022213496267795563\n",
      "epoch: 10 step: 1012, loss is 0.0001476404577260837\n",
      "epoch: 10 step: 1013, loss is 0.0013116103364154696\n",
      "epoch: 10 step: 1014, loss is 0.024386083707213402\n",
      "epoch: 10 step: 1015, loss is 0.003273609559983015\n",
      "epoch: 10 step: 1016, loss is 0.00041139184031635523\n",
      "epoch: 10 step: 1017, loss is 0.00040079368045553565\n",
      "epoch: 10 step: 1018, loss is 0.00013829863746650517\n",
      "epoch: 10 step: 1019, loss is 0.0009686312405392528\n",
      "epoch: 10 step: 1020, loss is 0.002517940476536751\n",
      "epoch: 10 step: 1021, loss is 0.00241110660135746\n",
      "epoch: 10 step: 1022, loss is 0.09949962794780731\n",
      "epoch: 10 step: 1023, loss is 0.0032781970221549273\n",
      "epoch: 10 step: 1024, loss is 5.485062501975335e-05\n",
      "epoch: 10 step: 1025, loss is 0.00035186862805858254\n",
      "epoch: 10 step: 1026, loss is 0.008685039356350899\n",
      "epoch: 10 step: 1027, loss is 8.117911056615412e-05\n",
      "epoch: 10 step: 1028, loss is 0.0025707425083965063\n",
      "epoch: 10 step: 1029, loss is 2.5750086933840066e-05\n",
      "epoch: 10 step: 1030, loss is 0.0004969339352101088\n",
      "epoch: 10 step: 1031, loss is 0.0005491889896802604\n",
      "epoch: 10 step: 1032, loss is 0.00221847603097558\n",
      "epoch: 10 step: 1033, loss is 3.838375414488837e-05\n",
      "epoch: 10 step: 1034, loss is 0.004785447381436825\n",
      "epoch: 10 step: 1035, loss is 6.758350355084985e-05\n",
      "epoch: 10 step: 1036, loss is 0.00017936810036189854\n",
      "epoch: 10 step: 1037, loss is 0.048833105713129044\n",
      "epoch: 10 step: 1038, loss is 0.04733576253056526\n",
      "epoch: 10 step: 1039, loss is 0.004871416371315718\n",
      "epoch: 10 step: 1040, loss is 9.117550507653505e-05\n",
      "epoch: 10 step: 1041, loss is 0.008248730562627316\n",
      "epoch: 10 step: 1042, loss is 0.0029341888148337603\n",
      "epoch: 10 step: 1043, loss is 0.0002606918860692531\n",
      "epoch: 10 step: 1044, loss is 6.264401599764824e-05\n",
      "epoch: 10 step: 1045, loss is 1.4868255675537512e-05\n",
      "epoch: 10 step: 1046, loss is 0.00036351699964143336\n",
      "epoch: 10 step: 1047, loss is 0.0221257321536541\n",
      "epoch: 10 step: 1048, loss is 2.215060158050619e-05\n",
      "epoch: 10 step: 1049, loss is 0.06915728002786636\n",
      "epoch: 10 step: 1050, loss is 0.03231630101799965\n",
      "epoch: 10 step: 1051, loss is 0.014161535538733006\n",
      "epoch: 10 step: 1052, loss is 0.019152378663420677\n",
      "epoch: 10 step: 1053, loss is 0.0006758639356121421\n",
      "epoch: 10 step: 1054, loss is 9.638552000978962e-05\n",
      "epoch: 10 step: 1055, loss is 0.003944156225770712\n",
      "epoch: 10 step: 1056, loss is 0.04643610119819641\n",
      "epoch: 10 step: 1057, loss is 0.0008515000808984041\n",
      "epoch: 10 step: 1058, loss is 0.020590072497725487\n",
      "epoch: 10 step: 1059, loss is 0.0005265767686069012\n",
      "epoch: 10 step: 1060, loss is 0.00017119038966484368\n",
      "epoch: 10 step: 1061, loss is 0.033073730766773224\n",
      "epoch: 10 step: 1062, loss is 0.008740514516830444\n",
      "epoch: 10 step: 1063, loss is 0.026829343289136887\n",
      "epoch: 10 step: 1064, loss is 0.010870011523365974\n",
      "epoch: 10 step: 1065, loss is 9.122235496761277e-05\n",
      "epoch: 10 step: 1066, loss is 0.010822451673448086\n",
      "epoch: 10 step: 1067, loss is 4.0275361243402585e-05\n",
      "epoch: 10 step: 1068, loss is 0.0002687892410904169\n",
      "epoch: 10 step: 1069, loss is 0.0016040183836594224\n",
      "epoch: 10 step: 1070, loss is 0.00012404329027049243\n",
      "epoch: 10 step: 1071, loss is 0.004876776598393917\n",
      "epoch: 10 step: 1072, loss is 0.027038060128688812\n",
      "epoch: 10 step: 1073, loss is 0.1332908570766449\n",
      "epoch: 10 step: 1074, loss is 0.001162808039225638\n",
      "epoch: 10 step: 1075, loss is 0.003213064279407263\n",
      "epoch: 10 step: 1076, loss is 0.0007373765110969543\n",
      "epoch: 10 step: 1077, loss is 0.00036114570684731007\n",
      "epoch: 10 step: 1078, loss is 0.0011731174308806658\n",
      "epoch: 10 step: 1079, loss is 5.555438110604882e-05\n",
      "epoch: 10 step: 1080, loss is 0.002557728672400117\n",
      "epoch: 10 step: 1081, loss is 0.0006302156252786517\n",
      "epoch: 10 step: 1082, loss is 0.0023194740060716867\n",
      "epoch: 10 step: 1083, loss is 0.00018106764764524996\n",
      "epoch: 10 step: 1084, loss is 0.0005405833944678307\n",
      "epoch: 10 step: 1085, loss is 0.0005314944428391755\n",
      "epoch: 10 step: 1086, loss is 0.002926755929365754\n",
      "epoch: 10 step: 1087, loss is 0.0034908095840364695\n",
      "epoch: 10 step: 1088, loss is 0.004123408813029528\n",
      "epoch: 10 step: 1089, loss is 0.005225800443440676\n",
      "epoch: 10 step: 1090, loss is 0.06965411454439163\n",
      "epoch: 10 step: 1091, loss is 0.0007121380767785013\n",
      "epoch: 10 step: 1092, loss is 0.001044803997501731\n",
      "epoch: 10 step: 1093, loss is 0.001562396064400673\n",
      "epoch: 10 step: 1094, loss is 0.05014878511428833\n",
      "epoch: 10 step: 1095, loss is 0.0015172315761446953\n",
      "epoch: 10 step: 1096, loss is 0.09741277992725372\n",
      "epoch: 10 step: 1097, loss is 0.0027316694613546133\n",
      "epoch: 10 step: 1098, loss is 0.0014328951947391033\n",
      "epoch: 10 step: 1099, loss is 0.1191171258687973\n",
      "epoch: 10 step: 1100, loss is 0.02870786376297474\n",
      "epoch: 10 step: 1101, loss is 0.21356748044490814\n",
      "epoch: 10 step: 1102, loss is 0.02587120421230793\n",
      "epoch: 10 step: 1103, loss is 0.00037712170160375535\n",
      "epoch: 10 step: 1104, loss is 0.0016768580535426736\n",
      "epoch: 10 step: 1105, loss is 0.000388737244065851\n",
      "epoch: 10 step: 1106, loss is 0.002501111477613449\n",
      "epoch: 10 step: 1107, loss is 0.01267331000417471\n",
      "epoch: 10 step: 1108, loss is 0.0013756692642346025\n",
      "epoch: 10 step: 1109, loss is 0.0009177918545901775\n",
      "epoch: 10 step: 1110, loss is 0.00021044115419499576\n",
      "epoch: 10 step: 1111, loss is 0.00025219577946700156\n",
      "epoch: 10 step: 1112, loss is 1.0420160833746195e-05\n",
      "epoch: 10 step: 1113, loss is 0.00029870844446122646\n",
      "epoch: 10 step: 1114, loss is 0.0007229680777527392\n",
      "epoch: 10 step: 1115, loss is 0.004290541168302298\n",
      "epoch: 10 step: 1116, loss is 0.000768552883528173\n",
      "epoch: 10 step: 1117, loss is 0.0006587659008800983\n",
      "epoch: 10 step: 1118, loss is 0.0006659668870270252\n",
      "epoch: 10 step: 1119, loss is 0.00042026746086776257\n",
      "epoch: 10 step: 1120, loss is 0.04083918780088425\n",
      "epoch: 10 step: 1121, loss is 0.0006110100075602531\n",
      "epoch: 10 step: 1122, loss is 0.005499804858118296\n",
      "epoch: 10 step: 1123, loss is 0.0015736721688881516\n",
      "epoch: 10 step: 1124, loss is 0.0031565516255795956\n",
      "epoch: 10 step: 1125, loss is 0.0005671248654834926\n",
      "epoch: 10 step: 1126, loss is 0.0014725398505106568\n",
      "epoch: 10 step: 1127, loss is 0.0008505978621542454\n",
      "epoch: 10 step: 1128, loss is 0.005151672754436731\n",
      "epoch: 10 step: 1129, loss is 0.15056633949279785\n",
      "epoch: 10 step: 1130, loss is 0.018110673874616623\n",
      "epoch: 10 step: 1131, loss is 0.051776643842458725\n",
      "epoch: 10 step: 1132, loss is 0.004134290385991335\n",
      "epoch: 10 step: 1133, loss is 8.197860734071583e-05\n",
      "epoch: 10 step: 1134, loss is 7.564412953797728e-05\n",
      "epoch: 10 step: 1135, loss is 0.01677205227315426\n",
      "epoch: 10 step: 1136, loss is 0.000711366010364145\n",
      "epoch: 10 step: 1137, loss is 0.00115174800157547\n",
      "epoch: 10 step: 1138, loss is 0.009854165837168694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 1139, loss is 0.0025571524165570736\n",
      "epoch: 10 step: 1140, loss is 0.004298401065170765\n",
      "epoch: 10 step: 1141, loss is 0.001097103813663125\n",
      "epoch: 10 step: 1142, loss is 0.04685336351394653\n",
      "epoch: 10 step: 1143, loss is 0.0772925391793251\n",
      "epoch: 10 step: 1144, loss is 0.010878394357860088\n",
      "epoch: 10 step: 1145, loss is 0.00216749613173306\n",
      "epoch: 10 step: 1146, loss is 9.100091847358271e-05\n",
      "epoch: 10 step: 1147, loss is 0.33287695050239563\n",
      "epoch: 10 step: 1148, loss is 0.004490428604185581\n",
      "epoch: 10 step: 1149, loss is 0.010460075922310352\n",
      "epoch: 10 step: 1150, loss is 0.022839060053229332\n",
      "epoch: 10 step: 1151, loss is 0.009475382044911385\n",
      "epoch: 10 step: 1152, loss is 0.012549685314297676\n",
      "epoch: 10 step: 1153, loss is 0.00016225100262090564\n",
      "epoch: 10 step: 1154, loss is 0.25529149174690247\n",
      "epoch: 10 step: 1155, loss is 0.0011241909814998507\n",
      "epoch: 10 step: 1156, loss is 0.000433573208283633\n",
      "epoch: 10 step: 1157, loss is 0.009773818776011467\n",
      "epoch: 10 step: 1158, loss is 1.7487242075731046e-05\n",
      "epoch: 10 step: 1159, loss is 0.020628636702895164\n",
      "epoch: 10 step: 1160, loss is 0.0008356210892088711\n",
      "epoch: 10 step: 1161, loss is 0.00010042056965176016\n",
      "epoch: 10 step: 1162, loss is 0.01786477491259575\n",
      "epoch: 10 step: 1163, loss is 0.00036502612056210637\n",
      "epoch: 10 step: 1164, loss is 0.018770340830087662\n",
      "epoch: 10 step: 1165, loss is 0.06690645962953568\n",
      "epoch: 10 step: 1166, loss is 0.0007321829907596111\n",
      "epoch: 10 step: 1167, loss is 0.004854577127844095\n",
      "epoch: 10 step: 1168, loss is 9.345733997179195e-05\n",
      "epoch: 10 step: 1169, loss is 0.019418109208345413\n",
      "epoch: 10 step: 1170, loss is 0.0002652374096214771\n",
      "epoch: 10 step: 1171, loss is 0.0006745937280356884\n",
      "epoch: 10 step: 1172, loss is 4.772259126184508e-05\n",
      "epoch: 10 step: 1173, loss is 0.0004132176109123975\n",
      "epoch: 10 step: 1174, loss is 0.00017566053429618478\n",
      "epoch: 10 step: 1175, loss is 0.0008939061663113534\n",
      "epoch: 10 step: 1176, loss is 0.01180532667785883\n",
      "epoch: 10 step: 1177, loss is 0.01623370498418808\n",
      "epoch: 10 step: 1178, loss is 0.01004572119563818\n",
      "epoch: 10 step: 1179, loss is 0.01083158329129219\n",
      "epoch: 10 step: 1180, loss is 0.0031590904109179974\n",
      "epoch: 10 step: 1181, loss is 0.0003156316524837166\n",
      "epoch: 10 step: 1182, loss is 0.00025002684560604393\n",
      "epoch: 10 step: 1183, loss is 0.0022495149169117212\n",
      "epoch: 10 step: 1184, loss is 0.0006846665637567639\n",
      "epoch: 10 step: 1185, loss is 0.001394603867083788\n",
      "epoch: 10 step: 1186, loss is 0.0024400113616138697\n",
      "epoch: 10 step: 1187, loss is 0.00012082253670087084\n",
      "epoch: 10 step: 1188, loss is 0.0006983960629440844\n",
      "epoch: 10 step: 1189, loss is 0.0031916110310703516\n",
      "epoch: 10 step: 1190, loss is 0.0007344547193497419\n",
      "epoch: 10 step: 1191, loss is 0.0009932754328474402\n",
      "epoch: 10 step: 1192, loss is 0.026575908064842224\n",
      "epoch: 10 step: 1193, loss is 0.00014638682478107512\n",
      "epoch: 10 step: 1194, loss is 0.003184678265824914\n",
      "epoch: 10 step: 1195, loss is 0.010321768932044506\n",
      "epoch: 10 step: 1196, loss is 0.00010827679216163233\n",
      "epoch: 10 step: 1197, loss is 0.033619124442338943\n",
      "epoch: 10 step: 1198, loss is 0.03193670138716698\n",
      "epoch: 10 step: 1199, loss is 0.017182007431983948\n",
      "epoch: 10 step: 1200, loss is 0.006555519066751003\n",
      "epoch: 10 step: 1201, loss is 0.001167811919003725\n",
      "epoch: 10 step: 1202, loss is 0.0013674867805093527\n",
      "epoch: 10 step: 1203, loss is 0.0009743840200826526\n",
      "epoch: 10 step: 1204, loss is 4.9021560698747635e-05\n",
      "epoch: 10 step: 1205, loss is 0.0019182296236976981\n",
      "epoch: 10 step: 1206, loss is 0.00014387840928975493\n",
      "epoch: 10 step: 1207, loss is 0.00013631340698339045\n",
      "epoch: 10 step: 1208, loss is 0.000584292400162667\n",
      "epoch: 10 step: 1209, loss is 0.014483307488262653\n",
      "epoch: 10 step: 1210, loss is 0.14081241190433502\n",
      "epoch: 10 step: 1211, loss is 2.2359125068760477e-05\n",
      "epoch: 10 step: 1212, loss is 0.00022756864200346172\n",
      "epoch: 10 step: 1213, loss is 0.030516166239976883\n",
      "epoch: 10 step: 1214, loss is 0.00340915541164577\n",
      "epoch: 10 step: 1215, loss is 0.02352903038263321\n",
      "epoch: 10 step: 1216, loss is 0.019633132964372635\n",
      "epoch: 10 step: 1217, loss is 0.013464920222759247\n",
      "epoch: 10 step: 1218, loss is 0.0036880681291222572\n",
      "epoch: 10 step: 1219, loss is 0.0001259901764569804\n",
      "epoch: 10 step: 1220, loss is 8.892695404938422e-06\n",
      "epoch: 10 step: 1221, loss is 0.00013690566993318498\n",
      "epoch: 10 step: 1222, loss is 0.014244083315134048\n",
      "epoch: 10 step: 1223, loss is 0.02850138209760189\n",
      "epoch: 10 step: 1224, loss is 0.014265578240156174\n",
      "epoch: 10 step: 1225, loss is 0.06042870879173279\n",
      "epoch: 10 step: 1226, loss is 0.0007531373994424939\n",
      "epoch: 10 step: 1227, loss is 0.00023956733639352024\n",
      "epoch: 10 step: 1228, loss is 0.0018774316413328052\n",
      "epoch: 10 step: 1229, loss is 0.004579151514917612\n",
      "epoch: 10 step: 1230, loss is 0.0004257654945831746\n",
      "epoch: 10 step: 1231, loss is 0.003398355795070529\n",
      "epoch: 10 step: 1232, loss is 0.014425350353121758\n",
      "epoch: 10 step: 1233, loss is 0.0021224659867584705\n",
      "epoch: 10 step: 1234, loss is 0.0026724471244961023\n",
      "epoch: 10 step: 1235, loss is 0.012204634957015514\n",
      "epoch: 10 step: 1236, loss is 4.641876785171917e-06\n",
      "epoch: 10 step: 1237, loss is 2.554981256253086e-05\n",
      "epoch: 10 step: 1238, loss is 0.0002450426109135151\n",
      "epoch: 10 step: 1239, loss is 0.00011336946045048535\n",
      "epoch: 10 step: 1240, loss is 0.0034092285204678774\n",
      "epoch: 10 step: 1241, loss is 0.03138066083192825\n",
      "epoch: 10 step: 1242, loss is 2.8800634026993066e-05\n",
      "epoch: 10 step: 1243, loss is 0.04437608644366264\n",
      "epoch: 10 step: 1244, loss is 0.0013015902368351817\n",
      "epoch: 10 step: 1245, loss is 0.0014502514386549592\n",
      "epoch: 10 step: 1246, loss is 0.01091983076184988\n",
      "epoch: 10 step: 1247, loss is 0.00033383379923179746\n",
      "epoch: 10 step: 1248, loss is 0.008320360444486141\n",
      "epoch: 10 step: 1249, loss is 0.0002828939468599856\n",
      "epoch: 10 step: 1250, loss is 0.0004945411928929389\n",
      "epoch: 10 step: 1251, loss is 0.00012820935808122158\n",
      "epoch: 10 step: 1252, loss is 0.0046084849163889885\n",
      "epoch: 10 step: 1253, loss is 0.0009623975493013859\n",
      "epoch: 10 step: 1254, loss is 0.0006923063192516565\n",
      "epoch: 10 step: 1255, loss is 0.0019767244812101126\n",
      "epoch: 10 step: 1256, loss is 8.113394142128527e-05\n",
      "epoch: 10 step: 1257, loss is 0.0011486465809866786\n",
      "epoch: 10 step: 1258, loss is 0.012288249097764492\n",
      "epoch: 10 step: 1259, loss is 0.011943018063902855\n",
      "epoch: 10 step: 1260, loss is 0.0009154601721093059\n",
      "epoch: 10 step: 1261, loss is 0.0009474463295191526\n",
      "epoch: 10 step: 1262, loss is 0.0002872890036087483\n",
      "epoch: 10 step: 1263, loss is 0.0006058156723156571\n",
      "epoch: 10 step: 1264, loss is 0.08582449704408646\n",
      "epoch: 10 step: 1265, loss is 0.0002844714908860624\n",
      "epoch: 10 step: 1266, loss is 1.0419865247968119e-05\n",
      "epoch: 10 step: 1267, loss is 0.0078010568395257\n",
      "epoch: 10 step: 1268, loss is 0.00010560263763181865\n",
      "epoch: 10 step: 1269, loss is 0.03593391552567482\n",
      "epoch: 10 step: 1270, loss is 0.027734320610761642\n",
      "epoch: 10 step: 1271, loss is 0.004595437087118626\n",
      "epoch: 10 step: 1272, loss is 0.002517749322578311\n",
      "epoch: 10 step: 1273, loss is 2.914411925303284e-05\n",
      "epoch: 10 step: 1274, loss is 0.003776336321607232\n",
      "epoch: 10 step: 1275, loss is 0.0007133964681997895\n",
      "epoch: 10 step: 1276, loss is 1.1108218131994363e-05\n",
      "epoch: 10 step: 1277, loss is 0.00628463551402092\n",
      "epoch: 10 step: 1278, loss is 0.0009972838452085853\n",
      "epoch: 10 step: 1279, loss is 0.05302845314145088\n",
      "epoch: 10 step: 1280, loss is 0.10419905930757523\n",
      "epoch: 10 step: 1281, loss is 0.0013185563730075955\n",
      "epoch: 10 step: 1282, loss is 0.00018706735863815993\n",
      "epoch: 10 step: 1283, loss is 0.014412371441721916\n",
      "epoch: 10 step: 1284, loss is 0.0019087382825091481\n",
      "epoch: 10 step: 1285, loss is 0.0036119618453085423\n",
      "epoch: 10 step: 1286, loss is 0.0016105009708553553\n",
      "epoch: 10 step: 1287, loss is 0.00011263031046837568\n",
      "epoch: 10 step: 1288, loss is 0.006638408172875643\n",
      "epoch: 10 step: 1289, loss is 8.3769133198075e-05\n",
      "epoch: 10 step: 1290, loss is 0.06138920038938522\n",
      "epoch: 10 step: 1291, loss is 0.0002659632882568985\n",
      "epoch: 10 step: 1292, loss is 0.022917060181498528\n",
      "epoch: 10 step: 1293, loss is 0.0002832902828231454\n",
      "epoch: 10 step: 1294, loss is 0.0007063896046020091\n",
      "epoch: 10 step: 1295, loss is 0.00030888774199411273\n",
      "epoch: 10 step: 1296, loss is 0.04669461399316788\n",
      "epoch: 10 step: 1297, loss is 0.00032075081253424287\n",
      "epoch: 10 step: 1298, loss is 0.0011709040263667703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 1299, loss is 0.004165033344179392\n",
      "epoch: 10 step: 1300, loss is 0.0003266693383920938\n",
      "epoch: 10 step: 1301, loss is 0.0007934356108307838\n",
      "epoch: 10 step: 1302, loss is 0.0027706054970622063\n",
      "epoch: 10 step: 1303, loss is 0.00018189317779615521\n",
      "epoch: 10 step: 1304, loss is 0.0005442864494398236\n",
      "epoch: 10 step: 1305, loss is 0.0010318444110453129\n",
      "epoch: 10 step: 1306, loss is 0.0014356207102537155\n",
      "epoch: 10 step: 1307, loss is 0.005870625376701355\n",
      "epoch: 10 step: 1308, loss is 0.0036206557415425777\n",
      "epoch: 10 step: 1309, loss is 0.001553027075715363\n",
      "epoch: 10 step: 1310, loss is 0.09230053424835205\n",
      "epoch: 10 step: 1311, loss is 0.00015617822646163404\n",
      "epoch: 10 step: 1312, loss is 0.00015245485701598227\n",
      "epoch: 10 step: 1313, loss is 1.99177138711093e-05\n",
      "epoch: 10 step: 1314, loss is 0.03722868487238884\n",
      "epoch: 10 step: 1315, loss is 0.004144842736423016\n",
      "epoch: 10 step: 1316, loss is 0.016907768324017525\n",
      "epoch: 10 step: 1317, loss is 0.0009353742352686822\n",
      "epoch: 10 step: 1318, loss is 0.0015514843398705125\n",
      "epoch: 10 step: 1319, loss is 0.09065845608711243\n",
      "epoch: 10 step: 1320, loss is 0.004465875681489706\n",
      "epoch: 10 step: 1321, loss is 0.002464029472321272\n",
      "epoch: 10 step: 1322, loss is 0.0007337210117839277\n",
      "epoch: 10 step: 1323, loss is 0.05737565830349922\n",
      "epoch: 10 step: 1324, loss is 0.00010036166349891573\n",
      "epoch: 10 step: 1325, loss is 0.00014269148232415318\n",
      "epoch: 10 step: 1326, loss is 0.028995366767048836\n",
      "epoch: 10 step: 1327, loss is 6.346693407977e-05\n",
      "epoch: 10 step: 1328, loss is 0.08113881200551987\n",
      "epoch: 10 step: 1329, loss is 0.023052068427205086\n",
      "epoch: 10 step: 1330, loss is 0.00010743202437879518\n",
      "epoch: 10 step: 1331, loss is 0.015531206503510475\n",
      "epoch: 10 step: 1332, loss is 0.04400402307510376\n",
      "epoch: 10 step: 1333, loss is 0.0001550058223074302\n",
      "epoch: 10 step: 1334, loss is 0.0011480770772323012\n",
      "epoch: 10 step: 1335, loss is 0.00013057661999482661\n",
      "epoch: 10 step: 1336, loss is 1.4886916687828489e-05\n",
      "epoch: 10 step: 1337, loss is 0.00014270689280238003\n",
      "epoch: 10 step: 1338, loss is 0.03576623648405075\n",
      "epoch: 10 step: 1339, loss is 0.00017110069165937603\n",
      "epoch: 10 step: 1340, loss is 0.010712498798966408\n",
      "epoch: 10 step: 1341, loss is 0.001617962378077209\n",
      "epoch: 10 step: 1342, loss is 0.0030402843840420246\n",
      "epoch: 10 step: 1343, loss is 0.05600148066878319\n",
      "epoch: 10 step: 1344, loss is 3.453114186413586e-05\n",
      "epoch: 10 step: 1345, loss is 0.017852745950222015\n",
      "epoch: 10 step: 1346, loss is 0.00015536599676124752\n",
      "epoch: 10 step: 1347, loss is 0.0001734041579766199\n",
      "epoch: 10 step: 1348, loss is 0.010362616740167141\n",
      "epoch: 10 step: 1349, loss is 0.0019213944906368852\n",
      "epoch: 10 step: 1350, loss is 0.10411318391561508\n",
      "epoch: 10 step: 1351, loss is 0.0002986716863233596\n",
      "epoch: 10 step: 1352, loss is 0.00117950898129493\n",
      "epoch: 10 step: 1353, loss is 0.030495163053274155\n",
      "epoch: 10 step: 1354, loss is 0.0031492251437157393\n",
      "epoch: 10 step: 1355, loss is 2.367101660638582e-05\n",
      "epoch: 10 step: 1356, loss is 0.0020694166887551546\n",
      "epoch: 10 step: 1357, loss is 0.0002823927206918597\n",
      "epoch: 10 step: 1358, loss is 0.005445917136967182\n",
      "epoch: 10 step: 1359, loss is 0.05196550115942955\n",
      "epoch: 10 step: 1360, loss is 0.01605827733874321\n",
      "epoch: 10 step: 1361, loss is 0.0063917930237948895\n",
      "epoch: 10 step: 1362, loss is 0.00010262069554300979\n",
      "epoch: 10 step: 1363, loss is 9.45005813264288e-05\n",
      "epoch: 10 step: 1364, loss is 0.0021722621750086546\n",
      "epoch: 10 step: 1365, loss is 0.019629232585430145\n",
      "epoch: 10 step: 1366, loss is 0.010902667418122292\n",
      "epoch: 10 step: 1367, loss is 0.0015365612925961614\n",
      "epoch: 10 step: 1368, loss is 0.00010389664385002106\n",
      "epoch: 10 step: 1369, loss is 0.0009525016648694873\n",
      "epoch: 10 step: 1370, loss is 0.0020210302900522947\n",
      "epoch: 10 step: 1371, loss is 0.000775677792262286\n",
      "epoch: 10 step: 1372, loss is 0.029487023130059242\n",
      "epoch: 10 step: 1373, loss is 0.0032176896929740906\n",
      "epoch: 10 step: 1374, loss is 0.0631050392985344\n",
      "epoch: 10 step: 1375, loss is 9.252544259652495e-05\n",
      "epoch: 10 step: 1376, loss is 0.0009586266241967678\n",
      "epoch: 10 step: 1377, loss is 0.0007124159601517022\n",
      "epoch: 10 step: 1378, loss is 0.002661160659044981\n",
      "epoch: 10 step: 1379, loss is 0.004481120966374874\n",
      "epoch: 10 step: 1380, loss is 0.0054852161556482315\n",
      "epoch: 10 step: 1381, loss is 0.07066476345062256\n",
      "epoch: 10 step: 1382, loss is 0.00170136580709368\n",
      "epoch: 10 step: 1383, loss is 0.01875891722738743\n",
      "epoch: 10 step: 1384, loss is 0.010066352784633636\n",
      "epoch: 10 step: 1385, loss is 0.14171913266181946\n",
      "epoch: 10 step: 1386, loss is 0.04258901625871658\n",
      "epoch: 10 step: 1387, loss is 0.000591253163293004\n",
      "epoch: 10 step: 1388, loss is 0.0862799882888794\n",
      "epoch: 10 step: 1389, loss is 0.0006896187551319599\n",
      "epoch: 10 step: 1390, loss is 0.26872190833091736\n",
      "epoch: 10 step: 1391, loss is 0.012168590910732746\n",
      "epoch: 10 step: 1392, loss is 0.015446349047124386\n",
      "epoch: 10 step: 1393, loss is 0.009855111129581928\n",
      "epoch: 10 step: 1394, loss is 0.002019000705331564\n",
      "epoch: 10 step: 1395, loss is 0.00041194408549927175\n",
      "epoch: 10 step: 1396, loss is 0.005025060847401619\n",
      "epoch: 10 step: 1397, loss is 0.002747924067080021\n",
      "epoch: 10 step: 1398, loss is 6.452421803260222e-05\n",
      "epoch: 10 step: 1399, loss is 0.17595040798187256\n",
      "epoch: 10 step: 1400, loss is 0.007639950606971979\n",
      "epoch: 10 step: 1401, loss is 0.0001898704213090241\n",
      "epoch: 10 step: 1402, loss is 0.22923825681209564\n",
      "epoch: 10 step: 1403, loss is 0.05255049094557762\n",
      "epoch: 10 step: 1404, loss is 0.047424059361219406\n",
      "epoch: 10 step: 1405, loss is 0.005251748487353325\n",
      "epoch: 10 step: 1406, loss is 0.15316537022590637\n",
      "epoch: 10 step: 1407, loss is 0.00041255203541368246\n",
      "epoch: 10 step: 1408, loss is 0.11019661277532578\n",
      "epoch: 10 step: 1409, loss is 0.0022736885584890842\n",
      "epoch: 10 step: 1410, loss is 0.0007532480522058904\n",
      "epoch: 10 step: 1411, loss is 6.339049286907539e-05\n",
      "epoch: 10 step: 1412, loss is 6.98057992849499e-05\n",
      "epoch: 10 step: 1413, loss is 0.0009973279666155577\n",
      "epoch: 10 step: 1414, loss is 0.0024894066154956818\n",
      "epoch: 10 step: 1415, loss is 1.2493539543356746e-05\n",
      "epoch: 10 step: 1416, loss is 0.0007701225695200264\n",
      "epoch: 10 step: 1417, loss is 0.0014523424906656146\n",
      "epoch: 10 step: 1418, loss is 0.00043835261021740735\n",
      "epoch: 10 step: 1419, loss is 0.0006510120001621544\n",
      "epoch: 10 step: 1420, loss is 0.01690145954489708\n",
      "epoch: 10 step: 1421, loss is 0.0068093957379460335\n",
      "epoch: 10 step: 1422, loss is 0.0008288240642286837\n",
      "epoch: 10 step: 1423, loss is 0.005208921153098345\n",
      "epoch: 10 step: 1424, loss is 8.242254261858761e-05\n",
      "epoch: 10 step: 1425, loss is 0.0012723199324682355\n",
      "epoch: 10 step: 1426, loss is 0.07246075570583344\n",
      "epoch: 10 step: 1427, loss is 0.005743236746639013\n",
      "epoch: 10 step: 1428, loss is 0.00015041616279631853\n",
      "epoch: 10 step: 1429, loss is 0.003377928165718913\n",
      "epoch: 10 step: 1430, loss is 0.03410738706588745\n",
      "epoch: 10 step: 1431, loss is 0.08788632601499557\n",
      "epoch: 10 step: 1432, loss is 0.004594922531396151\n",
      "epoch: 10 step: 1433, loss is 0.0006796336965635419\n",
      "epoch: 10 step: 1434, loss is 0.00047805625945329666\n",
      "epoch: 10 step: 1435, loss is 0.0007984458352439106\n",
      "epoch: 10 step: 1436, loss is 0.00010852513514691964\n",
      "epoch: 10 step: 1437, loss is 0.0015881817089393735\n",
      "epoch: 10 step: 1438, loss is 0.00018464085587766021\n",
      "epoch: 10 step: 1439, loss is 0.12928567826747894\n",
      "epoch: 10 step: 1440, loss is 0.0001526635023765266\n",
      "epoch: 10 step: 1441, loss is 0.01601681485772133\n",
      "epoch: 10 step: 1442, loss is 0.0052528646774590015\n",
      "epoch: 10 step: 1443, loss is 0.001831014291383326\n",
      "epoch: 10 step: 1444, loss is 0.01961623877286911\n",
      "epoch: 10 step: 1445, loss is 0.0002688412496354431\n",
      "epoch: 10 step: 1446, loss is 1.9605602574301884e-05\n",
      "epoch: 10 step: 1447, loss is 0.00039255604497157037\n",
      "epoch: 10 step: 1448, loss is 0.010127266868948936\n",
      "epoch: 10 step: 1449, loss is 0.050311390310525894\n",
      "epoch: 10 step: 1450, loss is 0.0002589681535027921\n",
      "epoch: 10 step: 1451, loss is 0.0002149120846297592\n",
      "epoch: 10 step: 1452, loss is 0.0010299974819645286\n",
      "epoch: 10 step: 1453, loss is 0.01473003625869751\n",
      "epoch: 10 step: 1454, loss is 0.0003088999364990741\n",
      "epoch: 10 step: 1455, loss is 0.0027766325511038303\n",
      "epoch: 10 step: 1456, loss is 0.013839609920978546\n",
      "epoch: 10 step: 1457, loss is 0.0004980938974767923\n",
      "epoch: 10 step: 1458, loss is 7.378907321253791e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 1459, loss is 0.15526573359966278\n",
      "epoch: 10 step: 1460, loss is 0.0007460461929440498\n",
      "epoch: 10 step: 1461, loss is 0.005232520867139101\n",
      "epoch: 10 step: 1462, loss is 0.0010404452914372087\n",
      "epoch: 10 step: 1463, loss is 0.021012181416153908\n",
      "epoch: 10 step: 1464, loss is 3.766763984458521e-05\n",
      "epoch: 10 step: 1465, loss is 0.02046220190823078\n",
      "epoch: 10 step: 1466, loss is 0.0015890770591795444\n",
      "epoch: 10 step: 1467, loss is 0.0001863118086475879\n",
      "epoch: 10 step: 1468, loss is 0.023547792807221413\n",
      "epoch: 10 step: 1469, loss is 0.0003161282220389694\n",
      "epoch: 10 step: 1470, loss is 0.0002707935927901417\n",
      "epoch: 10 step: 1471, loss is 5.4225471103563905e-05\n",
      "epoch: 10 step: 1472, loss is 0.00039920289418660104\n",
      "epoch: 10 step: 1473, loss is 0.00870593823492527\n",
      "epoch: 10 step: 1474, loss is 0.0004728729254566133\n",
      "epoch: 10 step: 1475, loss is 0.0008719633333384991\n",
      "epoch: 10 step: 1476, loss is 0.0008384222746826708\n",
      "epoch: 10 step: 1477, loss is 0.07734894007444382\n",
      "epoch: 10 step: 1478, loss is 0.00028517734608612955\n",
      "epoch: 10 step: 1479, loss is 0.03863096237182617\n",
      "epoch: 10 step: 1480, loss is 0.09402129799127579\n",
      "epoch: 10 step: 1481, loss is 0.00012052665988449007\n",
      "epoch: 10 step: 1482, loss is 0.0014726766385138035\n",
      "epoch: 10 step: 1483, loss is 0.00967230461537838\n",
      "epoch: 10 step: 1484, loss is 0.009186114184558392\n",
      "epoch: 10 step: 1485, loss is 2.0071327526238747e-05\n",
      "epoch: 10 step: 1486, loss is 0.00043790985364466906\n",
      "epoch: 10 step: 1487, loss is 0.0008941392879933119\n",
      "epoch: 10 step: 1488, loss is 0.0002671332040335983\n",
      "epoch: 10 step: 1489, loss is 0.009910588152706623\n",
      "epoch: 10 step: 1490, loss is 0.0013426680816337466\n",
      "epoch: 10 step: 1491, loss is 0.01354196760803461\n",
      "epoch: 10 step: 1492, loss is 0.0011122627183794975\n",
      "epoch: 10 step: 1493, loss is 6.817140092607588e-05\n",
      "epoch: 10 step: 1494, loss is 0.12578758597373962\n",
      "epoch: 10 step: 1495, loss is 0.01639549806714058\n",
      "epoch: 10 step: 1496, loss is 0.01079539954662323\n",
      "epoch: 10 step: 1497, loss is 0.006016519386321306\n",
      "epoch: 10 step: 1498, loss is 0.2484530508518219\n",
      "epoch: 10 step: 1499, loss is 0.040277961641550064\n",
      "epoch: 10 step: 1500, loss is 2.513560866646003e-05\n",
      "epoch: 10 step: 1501, loss is 0.025212721899151802\n",
      "epoch: 10 step: 1502, loss is 0.11971082538366318\n",
      "epoch: 10 step: 1503, loss is 0.0001480995415477082\n",
      "epoch: 10 step: 1504, loss is 0.00012799145770259202\n",
      "epoch: 10 step: 1505, loss is 0.00014308461686596274\n",
      "epoch: 10 step: 1506, loss is 6.716149800922722e-05\n",
      "epoch: 10 step: 1507, loss is 0.0004130021552555263\n",
      "epoch: 10 step: 1508, loss is 0.13345101475715637\n",
      "epoch: 10 step: 1509, loss is 0.002220400609076023\n",
      "epoch: 10 step: 1510, loss is 0.0014178336132317781\n",
      "epoch: 10 step: 1511, loss is 0.0030725826509296894\n",
      "epoch: 10 step: 1512, loss is 0.0003168662078678608\n",
      "epoch: 10 step: 1513, loss is 0.002851409139111638\n",
      "epoch: 10 step: 1514, loss is 0.0032127941958606243\n",
      "epoch: 10 step: 1515, loss is 0.0005720314220525324\n",
      "epoch: 10 step: 1516, loss is 0.0038261583540588617\n",
      "epoch: 10 step: 1517, loss is 0.009263157844543457\n",
      "epoch: 10 step: 1518, loss is 3.124569411738776e-05\n",
      "epoch: 10 step: 1519, loss is 0.014216887764632702\n",
      "epoch: 10 step: 1520, loss is 0.003981722518801689\n",
      "epoch: 10 step: 1521, loss is 0.004988472908735275\n",
      "epoch: 10 step: 1522, loss is 0.0012283611577004194\n",
      "epoch: 10 step: 1523, loss is 0.00256385188549757\n",
      "epoch: 10 step: 1524, loss is 2.7210999178350903e-05\n",
      "epoch: 10 step: 1525, loss is 0.005763193126767874\n",
      "epoch: 10 step: 1526, loss is 0.007833420298993587\n",
      "epoch: 10 step: 1527, loss is 0.004562104120850563\n",
      "epoch: 10 step: 1528, loss is 0.00018268977873958647\n",
      "epoch: 10 step: 1529, loss is 0.1351565420627594\n",
      "epoch: 10 step: 1530, loss is 0.0015416928799822927\n",
      "epoch: 10 step: 1531, loss is 0.0008574914536438882\n",
      "epoch: 10 step: 1532, loss is 0.00010858374298550189\n",
      "epoch: 10 step: 1533, loss is 0.022764675319194794\n",
      "epoch: 10 step: 1534, loss is 0.014535462483763695\n",
      "epoch: 10 step: 1535, loss is 0.06269729137420654\n",
      "epoch: 10 step: 1536, loss is 0.0002176203124690801\n",
      "epoch: 10 step: 1537, loss is 0.005556097254157066\n",
      "epoch: 10 step: 1538, loss is 0.0002691142144612968\n",
      "epoch: 10 step: 1539, loss is 0.0004933023592457175\n",
      "epoch: 10 step: 1540, loss is 0.09893455356359482\n",
      "epoch: 10 step: 1541, loss is 0.1980040967464447\n",
      "epoch: 10 step: 1542, loss is 0.00028636149363592267\n",
      "epoch: 10 step: 1543, loss is 0.006651955656707287\n",
      "epoch: 10 step: 1544, loss is 0.0007174632046371698\n",
      "epoch: 10 step: 1545, loss is 8.064870053203776e-05\n",
      "epoch: 10 step: 1546, loss is 0.012652920559048653\n",
      "epoch: 10 step: 1547, loss is 0.0006734160124324262\n",
      "epoch: 10 step: 1548, loss is 0.007232848089188337\n",
      "epoch: 10 step: 1549, loss is 0.00858292542397976\n",
      "epoch: 10 step: 1550, loss is 0.1400662213563919\n",
      "epoch: 10 step: 1551, loss is 0.0003302865079604089\n",
      "epoch: 10 step: 1552, loss is 0.0014640731969848275\n",
      "epoch: 10 step: 1553, loss is 0.0002457587979733944\n",
      "epoch: 10 step: 1554, loss is 0.05549129843711853\n",
      "epoch: 10 step: 1555, loss is 0.008408634923398495\n",
      "epoch: 10 step: 1556, loss is 0.020993100479245186\n",
      "epoch: 10 step: 1557, loss is 0.001863442244939506\n",
      "epoch: 10 step: 1558, loss is 0.004214574582874775\n",
      "epoch: 10 step: 1559, loss is 0.029441555961966515\n",
      "epoch: 10 step: 1560, loss is 0.01342561561614275\n",
      "epoch: 10 step: 1561, loss is 0.00014482464757747948\n",
      "epoch: 10 step: 1562, loss is 0.006666701287031174\n",
      "epoch: 10 step: 1563, loss is 0.0011830320581793785\n",
      "epoch: 10 step: 1564, loss is 0.030092235654592514\n",
      "epoch: 10 step: 1565, loss is 0.1355767399072647\n",
      "epoch: 10 step: 1566, loss is 0.08056043088436127\n",
      "epoch: 10 step: 1567, loss is 6.298248626990244e-05\n",
      "epoch: 10 step: 1568, loss is 0.041899651288986206\n",
      "epoch: 10 step: 1569, loss is 0.1787116974592209\n",
      "epoch: 10 step: 1570, loss is 0.007179148029536009\n",
      "epoch: 10 step: 1571, loss is 0.0017294306308031082\n",
      "epoch: 10 step: 1572, loss is 0.006284832023084164\n",
      "epoch: 10 step: 1573, loss is 0.0587301105260849\n",
      "epoch: 10 step: 1574, loss is 0.034182626754045486\n",
      "epoch: 10 step: 1575, loss is 0.00108678441029042\n",
      "epoch: 10 step: 1576, loss is 0.018029388040304184\n",
      "epoch: 10 step: 1577, loss is 0.0033755747135728598\n",
      "epoch: 10 step: 1578, loss is 0.0005861021345481277\n",
      "epoch: 10 step: 1579, loss is 0.0008062702836468816\n",
      "epoch: 10 step: 1580, loss is 0.0018454085802659392\n",
      "epoch: 10 step: 1581, loss is 0.004975060932338238\n",
      "epoch: 10 step: 1582, loss is 0.008335908874869347\n",
      "epoch: 10 step: 1583, loss is 0.01789659634232521\n",
      "epoch: 10 step: 1584, loss is 0.002397072734311223\n",
      "epoch: 10 step: 1585, loss is 0.0005249872338026762\n",
      "epoch: 10 step: 1586, loss is 0.00023625715402886271\n",
      "epoch: 10 step: 1587, loss is 0.001111342920921743\n",
      "epoch: 10 step: 1588, loss is 0.0002551486250013113\n",
      "epoch: 10 step: 1589, loss is 0.03533700853586197\n",
      "epoch: 10 step: 1590, loss is 0.005812934599816799\n",
      "epoch: 10 step: 1591, loss is 0.0002856649807654321\n",
      "epoch: 10 step: 1592, loss is 0.0012204846134409308\n",
      "epoch: 10 step: 1593, loss is 0.018388651311397552\n",
      "epoch: 10 step: 1594, loss is 0.00600906927138567\n",
      "epoch: 10 step: 1595, loss is 0.00344430492259562\n",
      "epoch: 10 step: 1596, loss is 0.0014438806101679802\n",
      "epoch: 10 step: 1597, loss is 0.05428337678313255\n",
      "epoch: 10 step: 1598, loss is 0.00959700532257557\n",
      "epoch: 10 step: 1599, loss is 0.0004694282542914152\n",
      "epoch: 10 step: 1600, loss is 0.00017498715897090733\n",
      "epoch: 10 step: 1601, loss is 0.026235463097691536\n",
      "epoch: 10 step: 1602, loss is 0.03794281557202339\n",
      "epoch: 10 step: 1603, loss is 0.0019573813769966364\n",
      "epoch: 10 step: 1604, loss is 0.015983352437615395\n",
      "epoch: 10 step: 1605, loss is 0.0010816624853760004\n",
      "epoch: 10 step: 1606, loss is 0.01038062572479248\n",
      "epoch: 10 step: 1607, loss is 0.0004487162223085761\n",
      "epoch: 10 step: 1608, loss is 8.922455890569836e-05\n",
      "epoch: 10 step: 1609, loss is 0.000959065044298768\n",
      "epoch: 10 step: 1610, loss is 0.026463504880666733\n",
      "epoch: 10 step: 1611, loss is 0.0001619429385755211\n",
      "epoch: 10 step: 1612, loss is 6.0128222685307264e-06\n",
      "epoch: 10 step: 1613, loss is 0.031542278826236725\n",
      "epoch: 10 step: 1614, loss is 7.825945795048028e-05\n",
      "epoch: 10 step: 1615, loss is 0.0025928528048098087\n",
      "epoch: 10 step: 1616, loss is 0.008851943537592888\n",
      "epoch: 10 step: 1617, loss is 8.904341666493565e-05\n",
      "epoch: 10 step: 1618, loss is 0.027965927496552467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 1619, loss is 0.0023413747549057007\n",
      "epoch: 10 step: 1620, loss is 0.16145165264606476\n",
      "epoch: 10 step: 1621, loss is 0.00047236389946192503\n",
      "epoch: 10 step: 1622, loss is 0.0003643927921075374\n",
      "epoch: 10 step: 1623, loss is 0.0018668553093448281\n",
      "epoch: 10 step: 1624, loss is 0.005471337120980024\n",
      "epoch: 10 step: 1625, loss is 0.0022411243990063667\n",
      "epoch: 10 step: 1626, loss is 0.027820255607366562\n",
      "epoch: 10 step: 1627, loss is 0.0006723321857862175\n",
      "epoch: 10 step: 1628, loss is 0.1124219223856926\n",
      "epoch: 10 step: 1629, loss is 0.004491967149078846\n",
      "epoch: 10 step: 1630, loss is 0.00026832844014279544\n",
      "epoch: 10 step: 1631, loss is 0.00298953615128994\n",
      "epoch: 10 step: 1632, loss is 0.0026308398228138685\n",
      "epoch: 10 step: 1633, loss is 0.10103042423725128\n",
      "epoch: 10 step: 1634, loss is 0.00012058511492796242\n",
      "epoch: 10 step: 1635, loss is 0.0005597076960839331\n",
      "epoch: 10 step: 1636, loss is 0.00010210742766503245\n",
      "epoch: 10 step: 1637, loss is 0.0014328769175335765\n",
      "epoch: 10 step: 1638, loss is 0.004381522536277771\n",
      "epoch: 10 step: 1639, loss is 0.0009637058828957379\n",
      "epoch: 10 step: 1640, loss is 0.005812423303723335\n",
      "epoch: 10 step: 1641, loss is 0.1783294975757599\n",
      "epoch: 10 step: 1642, loss is 0.0007872244459576905\n",
      "epoch: 10 step: 1643, loss is 0.0019776890985667706\n",
      "epoch: 10 step: 1644, loss is 0.00034576444886624813\n",
      "epoch: 10 step: 1645, loss is 0.005946282763034105\n",
      "epoch: 10 step: 1646, loss is 0.004015776328742504\n",
      "epoch: 10 step: 1647, loss is 0.00012618585606105626\n",
      "epoch: 10 step: 1648, loss is 0.03152332454919815\n",
      "epoch: 10 step: 1649, loss is 0.009831758216023445\n",
      "epoch: 10 step: 1650, loss is 0.017363863065838814\n",
      "epoch: 10 step: 1651, loss is 0.003260268596932292\n",
      "epoch: 10 step: 1652, loss is 0.08598821610212326\n",
      "epoch: 10 step: 1653, loss is 0.030717555433511734\n",
      "epoch: 10 step: 1654, loss is 0.0009145134245045483\n",
      "epoch: 10 step: 1655, loss is 0.010215534828603268\n",
      "epoch: 10 step: 1656, loss is 0.00017424343968741596\n",
      "epoch: 10 step: 1657, loss is 0.011813070625066757\n",
      "epoch: 10 step: 1658, loss is 0.0006868066266179085\n",
      "epoch: 10 step: 1659, loss is 0.00015892030205577612\n",
      "epoch: 10 step: 1660, loss is 0.00020738618331961334\n",
      "epoch: 10 step: 1661, loss is 0.014255776070058346\n",
      "epoch: 10 step: 1662, loss is 0.0023527080193161964\n",
      "epoch: 10 step: 1663, loss is 0.08198244869709015\n",
      "epoch: 10 step: 1664, loss is 0.0007113183382898569\n",
      "epoch: 10 step: 1665, loss is 0.01542915403842926\n",
      "epoch: 10 step: 1666, loss is 0.00016040254558902234\n",
      "epoch: 10 step: 1667, loss is 0.0005377448396757245\n",
      "epoch: 10 step: 1668, loss is 2.635207783896476e-05\n",
      "epoch: 10 step: 1669, loss is 0.002544653369113803\n",
      "epoch: 10 step: 1670, loss is 0.0066844201646745205\n",
      "epoch: 10 step: 1671, loss is 0.13113941252231598\n",
      "epoch: 10 step: 1672, loss is 0.004869110882282257\n",
      "epoch: 10 step: 1673, loss is 0.15987001359462738\n",
      "epoch: 10 step: 1674, loss is 0.013921293430030346\n",
      "epoch: 10 step: 1675, loss is 6.094350101193413e-05\n",
      "epoch: 10 step: 1676, loss is 0.011104411445558071\n",
      "epoch: 10 step: 1677, loss is 0.0024610604159533978\n",
      "epoch: 10 step: 1678, loss is 0.0030517196282744408\n",
      "epoch: 10 step: 1679, loss is 0.0005642258329316974\n",
      "epoch: 10 step: 1680, loss is 0.0008121370919980109\n",
      "epoch: 10 step: 1681, loss is 0.0018497188575565815\n",
      "epoch: 10 step: 1682, loss is 9.496204438619316e-05\n",
      "epoch: 10 step: 1683, loss is 0.0014231031527742743\n",
      "epoch: 10 step: 1684, loss is 0.0010416924487799406\n",
      "epoch: 10 step: 1685, loss is 0.0001063955933204852\n",
      "epoch: 10 step: 1686, loss is 1.5686749975429848e-05\n",
      "epoch: 10 step: 1687, loss is 9.294823030359112e-06\n",
      "epoch: 10 step: 1688, loss is 0.0016487077809870243\n",
      "epoch: 10 step: 1689, loss is 0.0005263638449832797\n",
      "epoch: 10 step: 1690, loss is 0.010586940683424473\n",
      "epoch: 10 step: 1691, loss is 0.02689436636865139\n",
      "epoch: 10 step: 1692, loss is 0.12213058024644852\n",
      "epoch: 10 step: 1693, loss is 0.0004827677912544459\n",
      "epoch: 10 step: 1694, loss is 1.8447040929459035e-05\n",
      "epoch: 10 step: 1695, loss is 0.023132149130105972\n",
      "epoch: 10 step: 1696, loss is 0.00041701749432832\n",
      "epoch: 10 step: 1697, loss is 0.09430897980928421\n",
      "epoch: 10 step: 1698, loss is 0.0021593007259070873\n",
      "epoch: 10 step: 1699, loss is 6.717895303154364e-05\n",
      "epoch: 10 step: 1700, loss is 0.0011896787909790874\n",
      "epoch: 10 step: 1701, loss is 0.0013785798801109195\n",
      "epoch: 10 step: 1702, loss is 0.009782720357179642\n",
      "epoch: 10 step: 1703, loss is 0.001397029496729374\n",
      "epoch: 10 step: 1704, loss is 0.003079501446336508\n",
      "epoch: 10 step: 1705, loss is 0.03152221441268921\n",
      "epoch: 10 step: 1706, loss is 0.003660579677671194\n",
      "epoch: 10 step: 1707, loss is 0.06284620612859726\n",
      "epoch: 10 step: 1708, loss is 4.64942968392279e-05\n",
      "epoch: 10 step: 1709, loss is 0.0001501251826994121\n",
      "epoch: 10 step: 1710, loss is 0.00040655588964000344\n",
      "epoch: 10 step: 1711, loss is 0.0022102075163275003\n",
      "epoch: 10 step: 1712, loss is 0.000299740640912205\n",
      "epoch: 10 step: 1713, loss is 0.002516087843105197\n",
      "epoch: 10 step: 1714, loss is 6.022705201758072e-05\n",
      "epoch: 10 step: 1715, loss is 6.612513243453577e-05\n",
      "epoch: 10 step: 1716, loss is 7.348231883952394e-05\n",
      "epoch: 10 step: 1717, loss is 0.00033038394758477807\n",
      "epoch: 10 step: 1718, loss is 0.0001585992140462622\n",
      "epoch: 10 step: 1719, loss is 0.0009399359114468098\n",
      "epoch: 10 step: 1720, loss is 0.0010251745115965605\n",
      "epoch: 10 step: 1721, loss is 0.0003195656754542142\n",
      "epoch: 10 step: 1722, loss is 0.0006435545510612428\n",
      "epoch: 10 step: 1723, loss is 0.0006816749810241163\n",
      "epoch: 10 step: 1724, loss is 0.0007871020352467895\n",
      "epoch: 10 step: 1725, loss is 0.006565313786268234\n",
      "epoch: 10 step: 1726, loss is 0.00023033907928038388\n",
      "epoch: 10 step: 1727, loss is 0.0014743859646841884\n",
      "epoch: 10 step: 1728, loss is 0.003361856797710061\n",
      "epoch: 10 step: 1729, loss is 0.006854369770735502\n",
      "epoch: 10 step: 1730, loss is 0.00027550224331207573\n",
      "epoch: 10 step: 1731, loss is 0.00012025204341625795\n",
      "epoch: 10 step: 1732, loss is 0.0018298067152500153\n",
      "epoch: 10 step: 1733, loss is 4.148000880377367e-05\n",
      "epoch: 10 step: 1734, loss is 0.004108847118914127\n",
      "epoch: 10 step: 1735, loss is 5.770593816123437e-06\n",
      "epoch: 10 step: 1736, loss is 0.00037680851528421044\n",
      "epoch: 10 step: 1737, loss is 0.002264624461531639\n",
      "epoch: 10 step: 1738, loss is 6.28149209660478e-05\n",
      "epoch: 10 step: 1739, loss is 0.003737447550520301\n",
      "epoch: 10 step: 1740, loss is 0.002231389516964555\n",
      "epoch: 10 step: 1741, loss is 0.017640339210629463\n",
      "epoch: 10 step: 1742, loss is 0.10016559809446335\n",
      "epoch: 10 step: 1743, loss is 0.001877444563433528\n",
      "epoch: 10 step: 1744, loss is 0.008682060055434704\n",
      "epoch: 10 step: 1745, loss is 0.00022204697597771883\n",
      "epoch: 10 step: 1746, loss is 9.452194353798404e-05\n",
      "epoch: 10 step: 1747, loss is 0.0004557899956125766\n",
      "epoch: 10 step: 1748, loss is 0.0035824382212013006\n",
      "epoch: 10 step: 1749, loss is 0.005011023487895727\n",
      "epoch: 10 step: 1750, loss is 0.21206006407737732\n",
      "epoch: 10 step: 1751, loss is 0.00012128280650358647\n",
      "epoch: 10 step: 1752, loss is 7.20859388820827e-05\n",
      "epoch: 10 step: 1753, loss is 6.595687591470778e-05\n",
      "epoch: 10 step: 1754, loss is 3.0697337933816016e-05\n",
      "epoch: 10 step: 1755, loss is 0.000902919506188482\n",
      "epoch: 10 step: 1756, loss is 0.0016048937104642391\n",
      "epoch: 10 step: 1757, loss is 0.030892228707671165\n",
      "epoch: 10 step: 1758, loss is 0.0003286849532742053\n",
      "epoch: 10 step: 1759, loss is 3.640741124399938e-05\n",
      "epoch: 10 step: 1760, loss is 0.02902698889374733\n",
      "epoch: 10 step: 1761, loss is 0.010368695482611656\n",
      "epoch: 10 step: 1762, loss is 0.00562243303284049\n",
      "epoch: 10 step: 1763, loss is 0.0033716217149049044\n",
      "epoch: 10 step: 1764, loss is 0.03565644472837448\n",
      "epoch: 10 step: 1765, loss is 0.014761985279619694\n",
      "epoch: 10 step: 1766, loss is 0.00012151103874202818\n",
      "epoch: 10 step: 1767, loss is 0.0018151518888771534\n",
      "epoch: 10 step: 1768, loss is 0.08807520568370819\n",
      "epoch: 10 step: 1769, loss is 0.009327608160674572\n",
      "epoch: 10 step: 1770, loss is 7.495759928133339e-05\n",
      "epoch: 10 step: 1771, loss is 0.0002127805055351928\n",
      "epoch: 10 step: 1772, loss is 0.02466079592704773\n",
      "epoch: 10 step: 1773, loss is 7.346952770603821e-05\n",
      "epoch: 10 step: 1774, loss is 0.017438387498259544\n",
      "epoch: 10 step: 1775, loss is 0.000660636113025248\n",
      "epoch: 10 step: 1776, loss is 0.0038307318463921547\n",
      "epoch: 10 step: 1777, loss is 0.005604282487183809\n",
      "epoch: 10 step: 1778, loss is 0.00017018592916429043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 1779, loss is 0.0017236062558367848\n",
      "epoch: 10 step: 1780, loss is 7.12703840690665e-05\n",
      "epoch: 10 step: 1781, loss is 0.00022731830540578812\n",
      "epoch: 10 step: 1782, loss is 0.002778115216642618\n",
      "epoch: 10 step: 1783, loss is 0.017130747437477112\n",
      "epoch: 10 step: 1784, loss is 0.06145531311631203\n",
      "epoch: 10 step: 1785, loss is 0.05780275911092758\n",
      "epoch: 10 step: 1786, loss is 0.057549141347408295\n",
      "epoch: 10 step: 1787, loss is 0.0023237939458340406\n",
      "epoch: 10 step: 1788, loss is 0.00015396725211758167\n",
      "epoch: 10 step: 1789, loss is 0.0020556971430778503\n",
      "epoch: 10 step: 1790, loss is 0.006788415368646383\n",
      "epoch: 10 step: 1791, loss is 0.1338430941104889\n",
      "epoch: 10 step: 1792, loss is 0.00011677457223413512\n",
      "epoch: 10 step: 1793, loss is 0.05382306128740311\n",
      "epoch: 10 step: 1794, loss is 0.004693131428211927\n",
      "epoch: 10 step: 1795, loss is 4.1986630094470456e-05\n",
      "epoch: 10 step: 1796, loss is 2.9826778700225987e-05\n",
      "epoch: 10 step: 1797, loss is 0.005199802573770285\n",
      "epoch: 10 step: 1798, loss is 0.003039633622393012\n",
      "epoch: 10 step: 1799, loss is 0.025505483150482178\n",
      "epoch: 10 step: 1800, loss is 0.00021695364557672292\n",
      "epoch: 10 step: 1801, loss is 0.019085140898823738\n",
      "epoch: 10 step: 1802, loss is 2.856756873370614e-05\n",
      "epoch: 10 step: 1803, loss is 0.0071687232702970505\n",
      "epoch: 10 step: 1804, loss is 0.0036157371941953897\n",
      "epoch: 10 step: 1805, loss is 0.03251278027892113\n",
      "epoch: 10 step: 1806, loss is 0.0354791097342968\n",
      "epoch: 10 step: 1807, loss is 0.009292248636484146\n",
      "epoch: 10 step: 1808, loss is 0.27539727091789246\n",
      "epoch: 10 step: 1809, loss is 0.012869949452579021\n",
      "epoch: 10 step: 1810, loss is 0.0008408752037212253\n",
      "epoch: 10 step: 1811, loss is 0.0010339663131162524\n",
      "epoch: 10 step: 1812, loss is 0.050482310354709625\n",
      "epoch: 10 step: 1813, loss is 0.0002650235255714506\n",
      "epoch: 10 step: 1814, loss is 0.00017076054064091295\n",
      "epoch: 10 step: 1815, loss is 6.417397526092827e-05\n",
      "epoch: 10 step: 1816, loss is 0.0008759803604334593\n",
      "epoch: 10 step: 1817, loss is 5.7089284382527694e-05\n",
      "epoch: 10 step: 1818, loss is 0.014640551060438156\n",
      "epoch: 10 step: 1819, loss is 0.001570792170241475\n",
      "epoch: 10 step: 1820, loss is 0.02250172756612301\n",
      "epoch: 10 step: 1821, loss is 0.0008735875599086285\n",
      "epoch: 10 step: 1822, loss is 0.016782155260443687\n",
      "epoch: 10 step: 1823, loss is 0.0027987821958959103\n",
      "epoch: 10 step: 1824, loss is 3.11607655021362e-05\n",
      "epoch: 10 step: 1825, loss is 0.002256817650049925\n",
      "epoch: 10 step: 1826, loss is 0.0009545205975882709\n",
      "epoch: 10 step: 1827, loss is 0.02469259314239025\n",
      "epoch: 10 step: 1828, loss is 0.06932085752487183\n",
      "epoch: 10 step: 1829, loss is 0.056081622838974\n",
      "epoch: 10 step: 1830, loss is 0.010745840147137642\n",
      "epoch: 10 step: 1831, loss is 0.009302669204771519\n",
      "epoch: 10 step: 1832, loss is 2.736051828833297e-05\n",
      "epoch: 10 step: 1833, loss is 2.5027720766956918e-05\n",
      "epoch: 10 step: 1834, loss is 0.006678244564682245\n",
      "epoch: 10 step: 1835, loss is 0.010352466255426407\n",
      "epoch: 10 step: 1836, loss is 0.001377804554067552\n",
      "epoch: 10 step: 1837, loss is 0.02418455109000206\n",
      "epoch: 10 step: 1838, loss is 0.013978386297821999\n",
      "epoch: 10 step: 1839, loss is 0.0008340166532434523\n",
      "epoch: 10 step: 1840, loss is 0.00010270701022818685\n",
      "epoch: 10 step: 1841, loss is 0.13472940027713776\n",
      "epoch: 10 step: 1842, loss is 0.012070578522980213\n",
      "epoch: 10 step: 1843, loss is 0.007408683653920889\n",
      "epoch: 10 step: 1844, loss is 0.0001249189954251051\n",
      "epoch: 10 step: 1845, loss is 0.00010006984666688368\n",
      "epoch: 10 step: 1846, loss is 0.026170218363404274\n",
      "epoch: 10 step: 1847, loss is 0.001376960426568985\n",
      "epoch: 10 step: 1848, loss is 0.0013279577251523733\n",
      "epoch: 10 step: 1849, loss is 3.5344211937626824e-05\n",
      "epoch: 10 step: 1850, loss is 0.0013483298243954778\n",
      "epoch: 10 step: 1851, loss is 0.006032916717231274\n",
      "epoch: 10 step: 1852, loss is 0.0023375581949949265\n",
      "epoch: 10 step: 1853, loss is 0.006993541959673166\n",
      "epoch: 10 step: 1854, loss is 0.1913081556558609\n",
      "epoch: 10 step: 1855, loss is 0.0005921922856941819\n",
      "epoch: 10 step: 1856, loss is 0.000242590467678383\n",
      "epoch: 10 step: 1857, loss is 0.0006136849988251925\n",
      "epoch: 10 step: 1858, loss is 0.023971693590283394\n",
      "epoch: 10 step: 1859, loss is 0.00012045923358527943\n",
      "epoch: 10 step: 1860, loss is 0.0004542874521575868\n",
      "epoch: 10 step: 1861, loss is 0.00022891288972459733\n",
      "epoch: 10 step: 1862, loss is 0.007246953435242176\n",
      "epoch: 10 step: 1863, loss is 0.0032594415824860334\n",
      "epoch: 10 step: 1864, loss is 0.013482055626809597\n",
      "epoch: 10 step: 1865, loss is 0.002117833122611046\n",
      "epoch: 10 step: 1866, loss is 8.954044460551813e-05\n",
      "epoch: 10 step: 1867, loss is 7.329081563511863e-05\n",
      "epoch: 10 step: 1868, loss is 0.011166214942932129\n",
      "epoch: 10 step: 1869, loss is 0.001950878300704062\n",
      "epoch: 10 step: 1870, loss is 0.008701407350599766\n",
      "epoch: 10 step: 1871, loss is 4.0839357097866014e-05\n",
      "epoch: 10 step: 1872, loss is 0.0005709777469746768\n",
      "epoch: 10 step: 1873, loss is 0.00019472204439807683\n",
      "epoch: 10 step: 1874, loss is 0.000846459181047976\n",
      "epoch: 10 step: 1875, loss is 0.002047502901405096\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()} )\n",
    "\n",
    "\n",
    "epoch_size = 10\n",
    "mnist_path = \"./MNIST_Data\"\n",
    "\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=125, keep_checkpoint_max=16)\n",
    "# save the network model and parameters for subsequence fine-tuning\n",
    "\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)\n",
    "# group layers into an object with training and evaluation features\n",
    "step_loss = {\"step\": [], \"loss_value\": []}\n",
    "# step_ Loss dictionary for saving loss value and step number information\n",
    "step_loss_info = Step_loss_info()\n",
    "# save the steps and loss value\n",
    "repeat_size = 1\n",
    "train_net(model, epoch_size, mnist_path, repeat_size, ckpoint_cb, step_loss_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f970176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = step_loss[\"step\"]\n",
    "loss_value = step_loss[\"loss_value\"]\n",
    "steps = list(map(int, steps))\n",
    "loss_value = list(map(float, loss_value))\n",
    "plt.plot(steps, loss_value, color=\"red\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss_value\")\n",
    "plt.title(\"Loss function value change chart\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fdabb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:25:44.238.843 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:25:44.239.439 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:25:44.240.454 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:25:44.241.448 [mindspore\\dataset\\core\\validator_helpers.py:806] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:25:44.241.448 [mindspore\\dataset\\core\\validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Testing ==============\n",
      "============== Accuracy:{'Accuracy': 0.9897836538461539} ==============\n"
     ]
    }
   ],
   "source": [
    "def test_net(network, model, mnist_path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    #param_dict = load_checkpoint(\"checkpoint_lenet-1_{}.ckpt\".format(str(i*125)))\n",
    "    param_dict = load_checkpoint(\"checkpoint_lenet-10_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))\n",
    "\n",
    "test_net(network, model, mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e2073fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For 'load_checkpoint', the checkpoint file: D:\\GitCode\\MindSporeTest\\checkpoint_lenet-1_1875.ckpt does not exist, please check whether the 'ckpt_file_name' is correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\1512482725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Draw line chart according to training steps and model accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc_model_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model of Steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\1512482725.py\u001b[0m in \u001b[0;36macc_model_info\u001b[1;34m(network, model, mnist_path, model_numbers, epoch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#加载不同一个模型得到的模型训练步数变化，精度随之变化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mparam_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"checkpoint_lenet-{}_1875.ckpt\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m# load parameter to the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mload_param_into_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\MindSpore\\lib\\site-packages\\mindspore\\train\\serialization.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[1;34m(ckpt_file_name, net, strict_load, filter_prefix, dec_key, dec_mode, specify_prefix)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[0mParameter\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \"\"\"\n\u001b[1;32m--> 507\u001b[1;33m     \u001b[0mckpt_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_ckpt_file_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m     \u001b[0mspecify_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecify_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[0mfilter_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\MindSpore\\lib\\site-packages\\mindspore\\train\\serialization.py\u001b[0m in \u001b[0;36m_check_ckpt_file_name\u001b[1;34m(ckpt_file_name)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         raise ValueError(\"For 'load_checkpoint', the checkpoint file: {} does not exist, please check \"\n\u001b[1;32m--> 576\u001b[1;33m                          \"whether the 'ckpt_file_name' is correct.\".format(ckpt_file_name))\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mckpt_file_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: For 'load_checkpoint', the checkpoint file: D:\\GitCode\\MindSporeTest\\checkpoint_lenet-1_1875.ckpt does not exist, please check whether the 'ckpt_file_name' is correct."
     ]
    }
   ],
   "source": [
    "def acc_model_info(network, model, mnist_path, model_numbers, epoch_size):\n",
    "    \"\"\"Define the plot info method\"\"\"\n",
    "    step_list = []\n",
    "    acc_list = []\n",
    "    for i in range(1, epoch_size+1):\n",
    "        # load the saved model for evaluation\n",
    "        #加载同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        #param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
    "        #加载不同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        param_dict = load_checkpoint(\"checkpoint_lenet-{}_1875.ckpt\".format(str(i)))\n",
    "        # load parameter to the network\n",
    "        load_param_into_net(network, param_dict)\n",
    "        # load testing dataset\n",
    "    for i in range(1, model_numbers+1):\n",
    "        ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "        acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "        acc_list.append(acc['Accuracy'])\n",
    "        step_list.append(i*125)\n",
    "    return step_list,acc_list\n",
    "\n",
    "# Draw line chart according to training steps and model accuracy\n",
    "l1,l2 = acc_model_info(network, model, mnist_path, 15, 10)\n",
    "plt.xlabel(\"Model of Steps\")\n",
    "plt.ylabel(\"Model accuracy\")\n",
    "plt.title(\"Model accuracy variation chart\")\n",
    "plt.plot(l1, l2, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14f93b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:57:11.970.371 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:57:11.971.368 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:57:11.971.368 [mindspore\\dataset\\core\\validator_helpers.py:806] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:57:11.972.420 [mindspore\\dataset\\core\\validator_helpers.py:806] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(15604:13072,MainProcess):2023-06-02-19:57:11.972.420 [mindspore\\dataset\\core\\validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the figures in this group are predicted correctly！\n",
      "[0 9 6 6 4 2 7 4 8 8 8 5 3 7 1 0 0 9 3 4 4 6 2 0 9 6 1 9 5 9 6 9] <--Predicted figures\n",
      "[0 9 6 6 4 2 7 4 8 8 8 5 3 7 1 0 0 9 3 4 4 6 2 0 9 6 1 9 5 9 6 9] <--The right number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_test = create_dataset(test_data_path).create_dict_iterator()\n",
    "data = ds_test.__next__()\n",
    "images = data[\"image\"].asnumpy()\n",
    "labels = data[\"label\"].asnumpy() # The subscript of data picture is the standard for us to judge whether it is correct or not\n",
    "\n",
    "output =model.predict(Tensor(data['image']))\n",
    "# The predict function returns the probability of 0-9 numbers corresponding to each picture\n",
    "prb = output.asnumpy()\n",
    "pred = np.argmax(output.asnumpy(), axis=1)\n",
    "err_num = []\n",
    "index = 1\n",
    "for i in range(len(labels)):\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    color = 'blue' if pred[i] == labels[i] else 'red'\n",
    "    plt.title(\"pre:{}\".format(pred[i]), color=color)\n",
    "    plt.imshow(np.squeeze(images[i]))\n",
    "    plt.axis(\"off\")\n",
    "    if color == 'red':\n",
    "        index = 0\n",
    "        # Print out the wrong data identified by the current group\n",
    "        print(\"Row {}, column {} is incorrectly identified as {}, the correct value should be {}\".format(int(i/8)+1, i%8+1, pred[i], labels[i]), '\\n')\n",
    "if index:\n",
    "    print(\"All the figures in this group are predicted correctly！\")\n",
    "print(pred, \"<--Predicted figures\") # Print the numbers recognized by each group of pictures\n",
    "print(labels, \"<--The right number\") # Print the subscript corresponding to each group of pictures\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d931e007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1 probability of corresponding numbers [0-9]:\n",
      " [-1.4181529  -8.584765    0.24522279 -1.714412    6.5558214  -4.1861544\n",
      " -8.743633    2.0351827  -2.7320871  17.729288  ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2 probability of corresponding numbers [0-9]:\n",
      " [-1.4111831e+01 -1.0683544e-02 -1.1710838e+00  2.9138346e+01\n",
      " -1.5666421e+01  8.7951517e+00 -1.0209985e+01  1.6514955e+00\n",
      "  4.8934469e+00  3.7869388e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the pie drawing function of probability analysis\n",
    "def plot_pie(prbs):\n",
    "    dict1 = {}\n",
    "    # Remove the negative number and build the dictionary dict1. The key is the number and the value is the probability value\n",
    "    for i in range(10):\n",
    "        if prbs[i] > 0:\n",
    "            dict1[str(i)] = prbs[i]\n",
    "\n",
    "    label_list = dict1.keys()    # Label of each part\n",
    "    size = dict1.values()    # Size of each part\n",
    "    colors = [\"red\", \"green\", \"pink\", \"blue\", \"purple\", \"orange\", \"gray\"] # Building a round cake pigment Library\n",
    "    color = colors[: len(size)]# Color of each part\n",
    "    plt.pie(size, colors=color, labels=label_list, labeldistance=1.1, autopct=\"%1.1f%%\", shadow=False, startangle=90, pctdistance=0.6)\n",
    "    plt.axis(\"equal\")    # Set the scale size of x-axis and y-axis to be equal\n",
    "    plt.legend()\n",
    "    plt.title(\"Image classification\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for i in range(2):\n",
    "    print(\"Figure {} probability of corresponding numbers [0-9]:\\n\".format(i+1), prb[i])\n",
    "    plot_pie(prb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac5b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
